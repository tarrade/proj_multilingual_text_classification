{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export BUCKET_STAGING_NAME=your_gcp_gs_bucket_staging_name` \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "from absl import logging\n",
    "from absl import flags\n",
    "from absl import app\n",
    "import logging as logger\n",
    "tf.get_logger().propagate = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import preprocessing.preprocessing as pp\n",
    "import utils.model_metrics as mm\n",
    "import utils.model_utils as mu\n",
    "import model.tf_custom_bert_classification.model as tf_custom_bert\n",
    "import model.tf_bert_classification.model as tf_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);\n",
    "importlib.reload(mm);\n",
    "importlib.reload(mu);\n",
    "importlib.reload(tf_bert);\n",
    "importlib.reload(tf_custom_bert);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc4-8-g2b96f3662b 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Read data from TFRecord files [local training of the model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Path of the directory with TFRecord files\n",
    "tfrecord_data_dir=data_dir+'/tfrecord/sst2/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# models\n",
    "MODELS = [(TFBertModel,         BertTokenizer,       'bert-base-multilingual-uncased'),\n",
    "          (TFXLMRobertaModel,   XLMRobertaTokenizer, 'jplu/tf-xlm-roberta-base')]\n",
    "model_index = 0 # BERT\n",
    "model_class        = MODELS[model_index][0] # i.e TFBertModel\n",
    "tokenizer_class    = MODELS[model_index][1] # i.e BertTokenizer\n",
    "pretrained_weights = MODELS[model_index][2] #'i.e bert-base-multilingual-uncased'\n",
    "number_label = 2                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model locally with AI Platform Training (for tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#savemodel_path = os.path.join(savemodel_dir, 'saved_model')\n",
    "pretrained_model_dir=savemodel_dir+'/pretrained_model/'+pretrained_weights\n",
    "model_name='tf_bert_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train locally\n",
    "os.environ['EPOCH'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '1' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = tfrecord_data_dir+'/valid'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = tfrecord_data_dir+'/train'\n",
    "os.environ['OUTPUT_DIR'] = savemodel_dir\n",
    "os.environ['PRETRAINED_MODEL_DIR']= pretrained_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model in local file system\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=$MAIN_TRAINER_MODULE \\\n",
    "   --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "   -- \\\n",
    "   --epochs=$EPOCH \\\n",
    "   --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "   --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "   --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "   --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "   --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "   --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "   --output_dir=$OUTPUT_DIR \\\n",
    "   --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "   --verbosity_level='INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Debug model's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f055b042dfb4606ac1729d8c6b00cb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=999358484.0, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# reset\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy.scope():\n",
    "    model=tf_bert.create_model(pretrained_weights, \n",
    "                               pretrained_model_dir=pretrained_model_dir,\n",
    "                               num_labels=number_label,\n",
    "                               learning_rate=3e-5,\n",
    "                               epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  167356416 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 167,357,954\n",
      "Trainable params: 167,357,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define default parameters\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "BATCH_SIZE_VALID = 64\n",
    "EPOCHS = 1\n",
    "STEP_EPOCH_TRAIN = 5\n",
    "STEP_EPOCH_VALID = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Using function\n",
    "train_files = tfrecord_data_dir+'/'+model.name+'/train'\n",
    "test_files = tfrecord_data_dir+'/'+model.name+'/test'\n",
    "valid_files = tfrecord_data_dir+'/'+model.name+'/valid'\n",
    "\n",
    "train_dataset = tf_bert.build_dataset(train_files, BATCH_SIZE_TRAIN)\n",
    "test_dataset = tf_bert.build_dataset(test_files, BATCH_SIZE_TEST)\n",
    "valid_dataset = tf_bert.build_dataset(valid_files, BATCH_SIZE_VALID)\n",
    "\n",
    "train_dataset=train_dataset.repeat(EPOCHS+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(64, 128), dtype=int32, numpy=\n",
      "array([[  101, 11526, 10855, ...,     0,     0,     0],\n",
      "       [  101, 10768,   112, ...,     0,     0,     0],\n",
      "       [  101, 10399, 10108, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10855, 11229, ...,     0,     0,     0],\n",
      "       [  101, 10923, 12207, ...,     0,     0,     0],\n",
      "       [  101,   151, 10407, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(64, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(64, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
      "array([1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
      "       1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0,\n",
      "       1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0])>)\n"
     ]
    }
   ],
   "source": [
    "for i in valid_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "    \n",
    "del_all_flags(flags.FLAGS)\n",
    "\n",
    "# to avoid crashes in Notebook\n",
    "flags.DEFINE_string('f', '', 'kernel') # just for jupyter notebook and avoir : \"UnrecognizedFlagError: Unknown command line flag 'f'\"\n",
    "# to avoid crashes with absl\n",
    "flags.DEFINE_enum('verbosity', 'INFO', ['VERBOSE', 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'FATAL'], 'verbosity in the logfile')\n",
    "\n",
    "# default parameters for training the model\n",
    "# compute and save accuracy and loss after N steps\n",
    "N_STEPS_HISTORY = 10\n",
    "\n",
    "# hyper parameters\n",
    "# adam parameters\n",
    "LEARNING_RATE = 3e-5\n",
    "EPSILON = 1e-08\n",
    "# learning rate decay parameters\n",
    "DECAY_LR = 0.95\n",
    "DECAY_TYPE = 'exponential'\n",
    "N_BATCH_DECAY = 2\n",
    "# number of classes\n",
    "NUM_CLASSES = 2\n",
    "# BERT Maximum length, be be careful BERT max length is 512!\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# get parameters for the training\n",
    "flags.DEFINE_float('learning_rate', LEARNING_RATE, 'learning rate')\n",
    "flags.DEFINE_float('decay_learning_rate', DECAY_LR, 'decay of the learning rate, e.g. 0.9')\n",
    "flags.DEFINE_float('epsilon', EPSILON, 'epsilon')\n",
    "flags.DEFINE_integer('epochs', EPOCHS, 'The number of epochs to train')\n",
    "flags.DEFINE_integer('steps_per_epoch_train', STEP_EPOCH_TRAIN, 'The number of steps per epoch to train')\n",
    "flags.DEFINE_integer('batch_size_train', BATCH_SIZE_TRAIN, 'Batch size for training')\n",
    "flags.DEFINE_integer('steps_per_epoch_eval', STEP_EPOCH_VALID, 'The number of steps per epoch to evaluate')\n",
    "flags.DEFINE_integer('batch_size_eval', BATCH_SIZE_VALID, 'Batch size for evaluation')\n",
    "flags.DEFINE_integer('num_classes', NUM_CLASSES, 'number of classes in our model')\n",
    "flags.DEFINE_integer('n_steps_history', N_STEPS_HISTORY, 'number of step for which we want custom history')\n",
    "flags.DEFINE_integer('n_batch_decay', N_BATCH_DECAY, 'number of batches after which the learning rate gets update')\n",
    "flags.DEFINE_string('decay_type', DECAY_TYPE, 'type of decay for the learning rate: exponential, stepwise, timebased, or constant')\n",
    "flags.DEFINE_string('input_train_tfrecords', None, 'input folder of tfrecords training data')\n",
    "flags.DEFINE_string('input_eval_tfrecords', None, 'input folder of tfrecords evaluation data')\n",
    "flags.DEFINE_string('output_dir', None, 'gs blob where are stored all the output of the model')\n",
    "flags.DEFINE_string('pretrained_model_dir', None, 'number of classes in our model')\n",
    "flags.DEFINE_enum('verbosity_level', 'INFO', ['VERBOSE', 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'FATAL'], 'verbosity in the logfile')\n",
    "flags.DEFINE_boolean('use_tpu', False, 'activate TPU for training')\n",
    "flags.DEFINE_boolean('use_decay_learning_rate', False, 'activate decay learning rate')\n",
    "flags.DEFINE_boolean('is_hyperparameter_tuning', False, 'automatic and inter flag')\n",
    "FLAGS(sys.argv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/ipykernel_launcher.py:\n",
      "  --batch_size_eval: Batch size for evaluation\n",
      "    (default: '64')\n",
      "    (an integer)\n",
      "  --batch_size_train: Batch size for training\n",
      "    (default: '32')\n",
      "    (an integer)\n",
      "  --decay_learning_rate: decay of the learning rate, e.g. 0.9\n",
      "    (default: '0.95')\n",
      "    (a number)\n",
      "  --decay_type: type of decay for the learning rate: exponential, stepwise,\n",
      "    timebased, or constant\n",
      "    (default: 'exponential')\n",
      "  --epochs: The number of epochs to train\n",
      "    (default: '1')\n",
      "    (an integer)\n",
      "  --epsilon: epsilon\n",
      "    (default: '1e-08')\n",
      "    (a number)\n",
      "  --f: kernel\n",
      "    (default: '')\n",
      "  --input_eval_tfrecords: input folder of tfrecords evaluation data\n",
      "  --input_train_tfrecords: input folder of tfrecords training data\n",
      "  --[no]is_hyperparameter_tuning: automatic and inter flag\n",
      "    (default: 'false')\n",
      "  --learning_rate: learning rate\n",
      "    (default: '3e-05')\n",
      "    (a number)\n",
      "  --n_batch_decay: number of batches after which the learning rate gets update\n",
      "    (default: '2')\n",
      "    (an integer)\n",
      "  --n_steps_history: number of step for which we want custom history\n",
      "    (default: '10')\n",
      "    (an integer)\n",
      "  --num_classes: number of classes in our model\n",
      "    (default: '2')\n",
      "    (an integer)\n",
      "  --output_dir: gs blob where are stored all the output of the model\n",
      "  --pretrained_model_dir: number of classes in our model\n",
      "  --steps_per_epoch_eval: The number of steps per epoch to evaluate\n",
      "    (default: '1')\n",
      "    (an integer)\n",
      "  --steps_per_epoch_train: The number of steps per epoch to train\n",
      "    (default: '5')\n",
      "    (an integer)\n",
      "  --[no]use_decay_learning_rate: activate decay learning rate\n",
      "    (default: 'false')\n",
      "  --[no]use_tpu: activate TPU for training\n",
      "    (default: 'false')\n",
      "  --verbosity: <VERBOSE|DEBUG|INFO|WARNING|ERROR|FATAL>: verbosity in the\n",
      "    logfile\n",
      "    (default: 'INFO')\n",
      "  --verbosity_level: <VERBOSE|DEBUG|INFO|WARNING|ERROR|FATAL>: verbosity in the\n",
      "    logfile\n",
      "    (default: 'INFO')\n",
      "\n",
      "absl.flags:\n",
      "  --flagfile: Insert flag definitions from the given file into the command line.\n",
      "    (default: '')\n",
      "  --undefok: comma-separated list of flag names that it is okay to specify on\n",
      "    the command line even if the program does not define a flag with that name.\n",
      "    IMPORTANT: flags in this list that have arguments MUST use the --flag=value\n",
      "    format.\n",
      "    (default: '')\n"
     ]
    }
   ],
   "source": [
    "print(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:training the model ...\n",
      "INFO:absl:model's callback:\n",
      " [<tensorflow.python.keras.callbacks.TensorBoard object at 0x7f8282e2e950>]\n",
      "INFO:absl:starting model.fit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 52s 26s/step - loss: 0.6787 - accuracy: 0.5781 - val_loss: 0.6753 - val_accuracy: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:\n",
      "execution time: 0:01:35\n",
      "INFO:absl:\n",
      "debugging .... : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   ({'input_ids': TensorShape([None, None]), 'attention_mask': TensorShape([None, None]), 'token_type_ids': TensorShape([None, None])}, TensorShape([None]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   ({'input_ids': <class 'tensorflow.python.framework.ops.Tensor'>, 'attention_mask': <class 'tensorflow.python.framework.ops.Tensor'>, 'token_type_ids': <class 'tensorflow.python.framework.ops.Tensor'>}, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (4210, 2)\n",
      "   ---> 4210 batches\n",
      "   ---> 2 dim\n",
      "        label\n",
      "           shape: (32,)\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [input_ids       / attention_mask  / token_type_ids ]\n",
      "           [(32, 128)       / (32, 128)       / (32, 128)      ]\n",
      "           [ndarray         / ndarray         / ndarray        ]\n",
      "\n",
      "\n",
      "# Examples of data:\n",
      "array([{'input_ids': array([[  101, 10497,   118, ...,     0,     0,     0],\n",
      "       [  101, 10197, 10367, ...,     0,     0,     0],\n",
      "       [  101, 13208, 23075, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10466, 50532, ...,     0,     0,     0],\n",
      "       [  101, 19552, 10536, ...,     0,     0,     0],\n",
      "       [  101, 12555, 45795, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0])], dtype=object)\n",
      "array([{'input_ids': array([[  101, 26210, 31519, ...,     0,     0,     0],\n",
      "       [  101, 14893,   156, ...,     0,     0,     0],\n",
      "       [  101, 15938, 10127, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 16398, 60665, ...,     0,     0,     0],\n",
      "       [  101, 10104, 10103, ...,     0,     0,     0],\n",
      "       [  101, 36848, 11917, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1])], dtype=object)\n",
      "array([{'input_ids': array([[  101, 10103, 13113, ...,     0,     0,     0],\n",
      "       [  101,   143, 85270, ...,     0,     0,     0],\n",
      "       [  101, 38225, 10323, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 96510, 10114, ...,     0,     0,     0],\n",
      "       [  101, 17735,   102, ...,     0,     0,     0],\n",
      "       [  101, 10372, 42135, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0])], dtype=object)\n",
      "array([{'input_ids': array([[  101, 10104, 23423, ...,     0,     0,     0],\n",
      "       [  101, 14957, 10158, ...,     0,     0,     0],\n",
      "       [  101, 28674, 12305, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 69381, 10871, ...,     0,     0,     0],\n",
      "       [  101, 10103, 63573, ...,     0,     0,     0],\n",
      "       [  101, 12140, 37566, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "       1, 1, 1, 0, 0, 1, 0, 0, 0, 1])], dtype=object)\n",
      "WARNING:tensorflow:From /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /Users/tarrade/tensorflow_model/saved_model/tf_bert_classification/assets\n"
     ]
    }
   ],
   "source": [
    "history_test=tf_bert.train_and_evaluate(model, \n",
    "                                        num_epochs=1, \n",
    "                                        steps_per_epoch=2, \n",
    "                                        train_data=train_dataset, \n",
    "                                        validation_steps=1, \n",
    "                                        eval_data=valid_dataset, \n",
    "                                        n_steps_history=1,\n",
    "                                        output_dir=savemodel_dir,\n",
    "                                        FLAGS=FLAGS,\n",
    "                                        decay_type='exponential',\n",
    "                                        learning_rate=3e-5,\n",
    "                                        s=0.95,\n",
    "                                        n_batch_decay=2,\n",
    "                                        metric_accuracy='NotDefined')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[  101, 21270, 94696, ...,     0,     0,     0],\n",
      "       [  101,   143, 45100, ...,     0,     0,     0],\n",
      "       [  101, 24220,   102, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 11008, 10346, ...,     0,     0,     0],\n",
      "       [  101, 43062, 15648, ...,     0,     0,     0],\n",
      "       [  101, 13178, 18418, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(32, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0,\n",
      "       1, 0, 1, 0, 1, 0, 1, 0, 0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  167356416 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 167,357,954\n",
      "Trainable params: 167,357,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#os.environ['MODEL_LOCAL']=savemodel_path+'/'+model.name\n",
    "os.environ['MODEL_LOCAL']=savemodel_dir+'/saved_model/'+model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#os.environ['MODEL_LOCAL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 18984\n",
      "drwxr-xr-x  5 tarrade  staff      160 Aug 30 14:36 \u001b[1m\u001b[36m.\u001b[m\u001b[m\n",
      "drwxr-xr-x  4 tarrade  staff      128 Aug 30 12:34 \u001b[1m\u001b[36m..\u001b[m\u001b[m\n",
      "drwxr-xr-x  2 tarrade  staff       64 Aug 30 12:34 \u001b[1m\u001b[36massets\u001b[m\u001b[m\n",
      "-rw-r--r--  1 tarrade  staff  9719354 Aug 30 14:36 saved_model.pb\n",
      "drwxr-xr-x  4 tarrade  staff      128 Aug 30 14:36 \u001b[1m\u001b[36mvariables\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls -la $MODEL_LOCAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['input_ids'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 5)\n",
      "      name: serving_default_input_ids:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['output_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 2)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_model_cli show --dir $MODEL_LOCAL --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57/57 [==============================] - 317s 6s/step - loss: 0.5533 - accuracy: 0.9995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5532639622688293, 0.9994508624076843]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': TensorShape([None, None]),\n",
       "  'attention_mask': TensorShape([None, None]),\n",
       "  'token_type_ids': TensorShape([None, None])},\n",
       " TensorShape([None]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.data.ops import dataset_ops\n",
    "dataset_ops.get_legacy_output_shapes(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   ({'input_ids': TensorShape([None, None]), 'attention_mask': TensorShape([None, None]), 'token_type_ids': TensorShape([None, None])}, TensorShape([None]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   ({'input_ids': <class 'tensorflow.python.framework.ops.Tensor'>, 'attention_mask': <class 'tensorflow.python.framework.ops.Tensor'>, 'token_type_ids': <class 'tensorflow.python.framework.ops.Tensor'>}, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (4210, 2)\n",
      "   ---> 4210 batches\n",
      "   ---> 2 dim\n",
      "        label\n",
      "           shape: (32,)\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [input_ids       / attention_mask  / token_type_ids ]\n",
      "           [(32, 128)       / (32, 128)       / (32, 128)      ]\n",
      "           [ndarray         / ndarray         / ndarray        ]\n",
      "\n",
      "\n",
      "# Examples of data:\n",
      "array([{'input_ids': array([[  101, 10372, 10947, ...,     0,     0,     0],\n",
      "       [  101, 53483,   117, ...,     0,     0,     0],\n",
      "       [  101, 13459,   102, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10127, 10103, ...,     0,     0,     0],\n",
      "       [  101, 10127, 19123, ...,     0,     0,     0],\n",
      "       [  101, 12296, 10574, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1,\n",
      "       1, 0, 0, 0, 0, 0, 1, 1, 1, 0])], dtype=object)\n",
      "array([{'input_ids': array([[  101, 32681, 10114, ...,     0,     0,     0],\n",
      "       [  101, 14471, 30738, ...,     0,     0,     0],\n",
      "       [  101,   112,   161, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,   143, 18418, ...,     0,     0,     0],\n",
      "       [  101, 34856, 10258, ...,     0,     0,     0],\n",
      "       [  101,   117, 41356, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "       0, 1, 0, 0, 1, 1, 1, 1, 0, 1])], dtype=object)\n",
      "array([{'input_ids': array([[  101, 10320, 10533, ...,     0,     0,     0],\n",
      "       [  101,   117, 10197, ...,     0,     0,     0],\n",
      "       [  101, 10151, 25839, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10113, 34438, ...,     0,     0,     0],\n",
      "       [  101,   117, 10372, ...,     0,     0,     0],\n",
      "       [  101, 23829, 35068, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
      "       1, 1, 1, 0, 1, 1, 0, 0, 0, 1])], dtype=object)\n",
      "array([{'input_ids': array([[  101, 10108, 10379, ...,     0,     0,     0],\n",
      "       [  101,   112,   161, ...,     0,     0,     0],\n",
      "       [  101, 11531,   143, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 78581, 29937, ...,     0,     0,     0],\n",
      "       [  101, 10114, 10399, ...,     0,     0,     0],\n",
      "       [  101, 10127, 10103, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 1, 0])], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(train_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
