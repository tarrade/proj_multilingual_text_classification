{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import preprocessing.preprocessing as pp\n",
    "import utils.model_metrics as mm\n",
    "import utils.model_utils as mu\n",
    "import model.tf_custom_bert_classification.model as tf_custom_bert\n",
    "import model.tf_bert_classification.model as tf_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);\n",
    "importlib.reload(mm);\n",
    "importlib.reload(mu);\n",
    "importlib.reload(tf_bert);\n",
    "importlib.reload(tf_custom_bert);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc4-8-g2b96f3662b 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Read data from TFRecord files [local training of the model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Path of the directory with TFRecord files\n",
    "tfrecord_data_dir=data_dir+'/tfrecord/sst2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# models\n",
    "MODELS = [(TFBertModel,         BertTokenizer,       'bert-base-multilingual-uncased'),\n",
    "          (TFXLMRobertaModel,   XLMRobertaTokenizer, 'jplu/tf-xlm-roberta-base')]\n",
    "model_index = 0 # BERT\n",
    "model_class        = MODELS[model_index][0] # i.e TFBertModel\n",
    "tokenizer_class    = MODELS[model_index][1] # i.e BertTokenizer\n",
    "pretrained_weights = MODELS[model_index][2] #'i.e bert-base-multilingual-uncased'\n",
    "number_label = 2                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model locally with AI Platform Training (for tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "savemodel_path = os.path.join(savemodel_dir, 'saved_model')\n",
    "pretrained_model_dir=savemodel_dir+'/pretrained_model/'+pretrained_weights\n",
    "model_name='tf_bert_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train locally\n",
    "os.environ['EPOCH'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '1' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = tfrecord_data_dir\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = tfrecord_data_dir\n",
    "os.environ['OUTPUT_DIR'] = savemodel_path\n",
    "os.environ['PRETRAINED_MODEL_DIR']= pretrained_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model in local file system\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=$MAIN_TRAINER_MODULE \\\n",
    "   --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "   -- \\\n",
    "   --epochs=$EPOCH \\\n",
    "   --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "   --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "   --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "   --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "   --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "   --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "   --output_dir=$OUTPUT_DIR \\\n",
    "   --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "   --verbosity_level='INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Debug model's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy.scope():\n",
    "    model=tf_bert.create_model(pretrained_weights, \n",
    "                               pretrained_model_dir=pretrained_model_dir,\n",
    "                               num_labels=number_label,\n",
    "                               learning_rate=3e-5,\n",
    "                               epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "BATCH_SIZE_TRAIN = 10 #32\n",
    "BATCH_SIZE_TEST = 10 #32\n",
    "BATCH_SIZE_VALID = 10 #64\n",
    "EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# TFRecords encode and store data\n",
    "#train_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord')\n",
    "#test_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/test/*.tfrecord')\n",
    "#valid_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/valid/*.tfrecord')\n",
    "#\n",
    "#train_files = tf.data.TFRecordDataset(tf.io.gfile.glob(tfrecord_data_dir+'/tf_bert_classification/train/*.tfrecord'))\n",
    "#test_files = tf.data.TFRecordDataset(tf.io.gfile.glob(tfrecord_data_dir+'/tf_bert_classification/test/*.tfrecord'))\n",
    "#valid_files = tf.data.TFRecordDataset(tf.io.gfile.glob(tfrecord_data_dir+'/tf_bert_classification/valid/*.tfrecord'))\n",
    "#\n",
    "#train_dataset = train_files.map(pp.parse_tfrecord_glue_files)\n",
    "#test_dataset = test_files.map(pp.parse_tfrecord_glue_files)\n",
    "#valid_dataset = valid_files.map(pp.parse_tfrecord_glue_files)\n",
    "#\n",
    "#train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE_TRAIN).repeat(EPOCH+1)\n",
    "#test_dataset = test_dataset.shuffle(100).batch(BATCH_SIZE_TEST).repeat(EPOCH+1)\n",
    "#valid_dataset = valid_dataset.batch(BATCH_SIZE_VALID) #.repeat(EPOCH+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Using function\n",
    "train_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord')\n",
    "test_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/test/*.tfrecord')\n",
    "valid_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/valid/*.tfrecord')\n",
    "\n",
    "train_dataset = tf_bert.build_dataset(train_files, BATCH_SIZE_TRAIN)\n",
    "test_dataset = tf_bert.build_dataset(test_files, BATCH_SIZE_TEST)\n",
    "valid_dataset = tf_bert.build_dataset(valid_files, BATCH_SIZE_VALID)\n",
    "\n",
    "train_dataset=train_dataset_2.repeat(EPOCH+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:training the model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training set -> batch:1 loss:0.6992158889770508 and acc: 0.5\n",
      "\n",
      " validation set -> batch:1 val loss:0.7030577063560486 and val acc: 0.5080459713935852\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6992 - accuracy: 0.5000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (167.323413). Check your callbacks.\n",
      "\n",
      " training set -> batch:2 loss:0.7036659717559814 and acc: 0.5068181753158569\n",
      "\n",
      " validation set -> batch:2 val loss:0.700046956539154 and val acc: 0.5080459713935852\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7037 - accuracy: 0.5068WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (164.321242). Check your callbacks.\n",
      "\n",
      "Epoch 00001: saving model to /Users/tarrade/tensorflow_model/saved_model/checkpoint_model/ckpt_01\n",
      "accuracy_train 0.5068181753158569 epoch 0 \n",
      "\n",
      "2/2 [==============================] - 195s 98s/step - loss: 0.7037 - accuracy: 0.5068 - val_loss: 0.6975 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:\n",
      "execution time: 0:06:10\n",
      "INFO:absl:timing per epoch:\n",
      "['0:06:09']\n",
      "INFO:absl:sum timing over all epochs:\n",
      "0:06:09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/tarrade/tensorflow_model/saved_model/saved_model/tf_bert_classification/assets\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().propagate = False\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "history_test=tf_bert.train_and_evaluate(model, \n",
    "                                        num_epochs=1, \n",
    "                                        steps_per_epoch=2, \n",
    "                                        train_data=train_dataset, \n",
    "                                        validation_steps=1, \n",
    "                                        eval_data=valid_dataset, \n",
    "                                        n_steps_history=1,\n",
    "                                        output_dir=savemodel_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
