{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.1.0-rc2-17-ge5bf8de410 2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train on GCP\n",
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['RUNTIME_VERSION'] = '2.1'\n",
    "os.environ['PYTHON_VERSION'] = '3.7'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "\n",
    "os.environ['EPOCHS'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '1' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: tf_bert_classification_2020_05_04_160650\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [tf_bert_classification_2020_05_04_160650] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_bert_classification_2020_05_04_160650\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_bert_classification_2020_05_04_160650\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --scale-tier basic \\\n",
    "        --python-version $PYTHON_VERSION \\\n",
    "        --runtime-version $RUNTIME_VERSION \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --region=$REGION \\\n",
    "        -- \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='DEBUG' \\\n",
    "#        --stream-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training using a config file (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['CONFIG']= os.environ['DIR_PROJ'].split('src')[0]+'deployment/training/tf_bert_classification/standard.yaml' #or custom.yaml or standard.yaml\n",
    "os.environ['EPOCHS'] = '2' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '5' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'Salut\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: tf_bert_classification_2020_05_02_151451\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [tf_bert_classification_2020_05_02_151451] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_bert_classification_2020_05_02_151451\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_bert_classification_2020_05_02_151451\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --config $CONFIG \\\n",
    "        -- \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='DEBUG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training using TPU (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train on GCP\n",
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['RUNTIME_VERSION'] = '2.1'\n",
    "os.environ['PYTHON_VERSION'] = '3.7'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['REGION_TPU'] = 'europe-west4'\n",
    "\n",
    "os.environ['USE_TPU'] = 'True' \n",
    "os.environ['EPOCHS'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '5' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --scale-tier basic_tpu\\\n",
    "        --python-version $PYTHON_VERSION \\\n",
    "        --runtime-version $RUNTIME_VERSION \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --region=$REGION_TPU \\\n",
    "        -- \\\n",
    "        --use_tpu=$USE_TPU \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='DEBUG' \\\n",
    "#        --stream-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Hyperparameter tunning of the model on AI Platform Training using a config file (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['CONFIG']= os.environ['DIR_PROJ'].split('src')[0]+'deployment/hp-tuning/tf_bert_classification/hyperparam.yaml'\n",
    "os.environ['EPOCHS'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '1' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "ah%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --config $CONFIG \\\n",
    "        -- \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TensorBoard for job running on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir {os.environ['OUTPUT_DIR']+'/tensorboard'} \\\n",
    "              #--port 6001 \\\n",
    "              #--debugger_port 6001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Description of the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# remove # to see execute the function\n",
    "#gcloud ai-platform jobs describe $JOB_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## See the logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# remove # to see execute the function\n",
    "#gcloud ai-platform jobs stream-logs $JOB_NAME "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[m\u001b[1mNAME\u001b[m\n",
      "    gcloud ai-platform jobs submit training - submit an AI Platform training\n",
      "        job\n",
      "\n",
      "\u001b[m\u001b[1mSYNOPSIS\u001b[m\n",
      "    \u001b[1mgcloud ai-platform jobs submit training\u001b[m \u001b[4mJOB\u001b[m [\u001b[1m--config\u001b[m=\u001b[4mCONFIG\u001b[m]\n",
      "        [\u001b[1m--job-dir\u001b[m=\u001b[4mJOB_DIR\u001b[m] [\u001b[1m--labels\u001b[m=[\u001b[4mKEY\u001b[m=\u001b[4mVALUE\u001b[m,...]]\n",
      "        [\u001b[1m--master-accelerator\u001b[m=[\u001b[4mcount\u001b[m=\u001b[4mCOUNT\u001b[m],[\u001b[4mtype\u001b[m=\u001b[4mTYPE\u001b[m]]\n",
      "        [\u001b[1m--master-image-uri\u001b[m=\u001b[4mMASTER_IMAGE_URI\u001b[m]\n",
      "        [\u001b[1m--master-machine-type\u001b[m=\u001b[4mMASTER_MACHINE_TYPE\u001b[m] [\u001b[1m--module-name\u001b[m=\u001b[4mMODULE_NAME\u001b[m]\n",
      "        [\u001b[1m--package-path\u001b[m=\u001b[4mPACKAGE_PATH\u001b[m] [\u001b[1m--packages\u001b[m=[\u001b[4mPACKAGE\u001b[m,...]]\n",
      "        [\u001b[1m--parameter-server-accelerator\u001b[m=[\u001b[4mcount\u001b[m=\u001b[4mCOUNT\u001b[m],[\u001b[4mtype\u001b[m=\u001b[4mTYPE\u001b[m]]\n",
      "        [\u001b[1m--parameter-server-image-uri\u001b[m=\u001b[4mPARAMETER_SERVER_IMAGE_URI\u001b[m]\n",
      "        [\u001b[1m--python-version\u001b[m=\u001b[4mPYTHON_VERSION\u001b[m] [\u001b[1m--region\u001b[m=\u001b[4mREGION\u001b[m]\n",
      "        [\u001b[1m--runtime-version\u001b[m=\u001b[4mRUNTIME_VERSION\u001b[m] [\u001b[1m--scale-tier\u001b[m=\u001b[4mSCALE_TIER\u001b[m]\n",
      "        [\u001b[1m--staging-bucket\u001b[m=\u001b[4mSTAGING_BUCKET\u001b[m]\n",
      "        [\u001b[1m--use-chief-in-tf-config\u001b[m=\u001b[4mUSE_CHIEF_IN_TF_CONFIG\u001b[m]\n",
      "        [\u001b[1m--worker-accelerator\u001b[m=[\u001b[4mcount\u001b[m=\u001b[4mCOUNT\u001b[m],[\u001b[4mtype\u001b[m=\u001b[4mTYPE\u001b[m]]\n",
      "        [\u001b[1m--worker-image-uri\u001b[m=\u001b[4mWORKER_IMAGE_URI\u001b[m] [\u001b[1m--async\u001b[m | \u001b[1m--stream-logs\u001b[m]\n",
      "        [\u001b[1m--parameter-server-count\u001b[m=\u001b[4mPARAMETER_SERVER_COUNT\u001b[m\n",
      "          \u001b[1m--parameter-server-machine-type\u001b[m=\u001b[4mPARAMETER_SERVER_MACHINE_TYPE\u001b[m]\n",
      "        [\u001b[1m--worker-count\u001b[m=\u001b[4mWORKER_COUNT\u001b[m \u001b[1m--worker-machine-type\u001b[m=\u001b[4mWORKER_MACHINE_TYPE\u001b[m]\n",
      "        [\u001b[4mGCLOUD_WIDE_FLAG ...\u001b[m] [-- \u001b[4mUSER_ARGS\u001b[m ...]\n",
      "\n",
      "\u001b[m\u001b[1mDESCRIPTION\u001b[m\n",
      "    Submit an AI Platform training job.\n",
      "\n",
      "    This creates temporary files and executes Python code staged by a user on\n",
      "    Google Cloud Storage. Model code can either be specified with a path, e.g.:\n",
      "\n",
      "        $ gcloud ai-platform jobs submit training my_job \\\n",
      "                --module-name trainer.task \\\n",
      "                --staging-bucket gs://my-bucket \\\n",
      "                --package-path /my/code/path/trainer \\\n",
      "                --packages additional-dep1.tar.gz,dep2.whl\n",
      "\n",
      "    Or by specifying an already built package:\n",
      "\n",
      "        $ gcloud ai-platform jobs submit training my_job \\\n",
      "                --module-name trainer.task \\\n",
      "                --staging-bucket gs://my-bucket \\\n",
      "                --packages trainer-0.0.1.tar.gz,additional-dep1.tar.gz,dep2.whl\n",
      "\n",
      "    If \u001b[1m--package-path=/my/code/path/trainer\u001b[m is specified and there is a\n",
      "    \u001b[1msetup.py\u001b[m file at \u001b[1m/my/code/path/setup.py\u001b[m, the setup file will be invoked\n",
      "    with \u001b[1msdist\u001b[m and the generated tar files will be uploaded to Cloud Storage.\n",
      "    Otherwise, a temporary \u001b[1msetup.py\u001b[m file will be generated for the build.\n",
      "\n",
      "    By default, this command runs asynchronously; it exits once the job is\n",
      "    successfully submitted.\n",
      "\n",
      "    To follow the progress of your job, pass the \u001b[1m--stream-logs\u001b[m flag (note that\n",
      "    even with the \u001b[1m--stream-logs\u001b[m flag, the job will continue to run after this\n",
      "    command exits and must be cancelled with \u001b[1mgcloud ai-platform jobs cancel\n",
      "    JOB_ID\u001b[m).\n",
      "\n",
      "    For more information, see:\n",
      "    https://cloud.google.com/ml/docs/concepts/training-overview\n",
      "\n",
      "\u001b[m\u001b[1mPOSITIONAL ARGUMENTS\u001b[m\n",
      "     \u001b[4mJOB\u001b[m\n",
      "        Name of the job.\n",
      "\n",
      "     [-- \u001b[4mUSER_ARGS\u001b[m ...]\n",
      "        Additional user arguments to be forwarded to user code\n",
      "\n",
      "        The '--' argument must be specified between gcloud specific args on the\n",
      "        left and USER_ARGS on the right.\n",
      "\n",
      "\u001b[m\u001b[1mFLAGS\u001b[m\n",
      "     \u001b[1m--config\u001b[m=\u001b[4mCONFIG\u001b[m\n",
      "        Path to the job configuration file. This file should be a YAML document\n",
      "        (JSON also accepted) containing a Job resource as defined in the API\n",
      "        (all fields are optional):\n",
      "        https://cloud.google.com/ml/reference/rest/v1/projects.jobs\n",
      "\n",
      "        EXAMPLES:\n",
      "\n",
      "        JSON:\n",
      "\n",
      "            {\n",
      "              \"jobId\": \"my_job\",\n",
      "              \"labels\": {\n",
      "                \"type\": \"prod\",\n",
      "                \"owner\": \"alice\"\n",
      "              },\n",
      "              \"trainingInput\": {\n",
      "                \"scaleTier\": \"BASIC\",\n",
      "                \"packageUris\": [\n",
      "                  \"gs://my/package/path\"\n",
      "                ],\n",
      "                \"region\": \"us-east1\"\n",
      "              }\n",
      "            }\n",
      "\n",
      "        YAML:\n",
      "\n",
      "            jobId: my_job\n",
      "            labels:\n",
      "              type: prod\n",
      "              owner: alice\n",
      "            trainingInput:\n",
      "              scaleTier: BASIC\n",
      "              packageUris:\n",
      "              - gs://my/package/path\n",
      "              region: us-east1\n",
      "\n",
      "    If an option is specified both in the configuration file **and** via\n",
      "    command line arguments, the command line arguments override the\n",
      "    configuration file.\n",
      "\n",
      "     \u001b[1m--job-dir\u001b[m=\u001b[4mJOB_DIR\u001b[m\n",
      "        Google Cloud Storage path in which to store training outputs and other\n",
      "        data needed for training.\n",
      "\n",
      "        This path will be passed to your TensorFlow program as the \u001b[1m--job_dir\u001b[m\n",
      "        command-line arg. The benefit of specifying this field is that AI\n",
      "        Platform will validate the path for use in training.\n",
      "\n",
      "        If packages must be uploaded and \u001b[1m--staging-bucket\u001b[m is not provided, this\n",
      "        path will be used instead.\n",
      "\n",
      "     \u001b[1m--labels\u001b[m=[\u001b[4mKEY\u001b[m=\u001b[4mVALUE\u001b[m,...]\n",
      "        List of label KEY=VALUE pairs to add.\n",
      "\n",
      "        Keys must start with a lowercase character and contain only hyphens\n",
      "        (\u001b[1m-\u001b[m), underscores (\u001b[1m_\u001b[m), lowercase characters, and numbers. Values must\n",
      "        contain only hyphens (\u001b[1m-\u001b[m), underscores (\u001b[1m_\u001b[m), lowercase characters, and\n",
      "        numbers.\n",
      "\n",
      "     \u001b[1m--master-accelerator\u001b[m=[\u001b[4mcount\u001b[m=\u001b[4mCOUNT\u001b[m],[\u001b[4mtype\u001b[m=\u001b[4mTYPE\u001b[m]\n",
      "        Hardware accelerator config for the master worker. Must specify both\n",
      "        the accelerator type (TYPE) for each server and the number of\n",
      "        accelerators to attach to each server (COUNT).\n",
      "\n",
      "         \u001b[1mtype\u001b[m\n",
      "            The type of the accelerator. Choices are\n",
      "            nvidia-tesla-k80,nvidia-tesla-p100,nvidia-tesla-p4,nvidia-tesla-t4,nvidia-tesla-v100,tpu-v2,tpu-v2-pod,tpu-v3,tpu-v3-pod\n",
      "\n",
      "         \u001b[1mcount\u001b[m\n",
      "            The number of accelerators to attach to each machine running the\n",
      "            job. Must be greater than 0.\n",
      "\n",
      "     \u001b[1m--master-image-uri\u001b[m=\u001b[4mMASTER_IMAGE_URI\u001b[m\n",
      "        Docker image to run on each master worker. This image must be in Google\n",
      "        Container Registry. Only one of \u001b[1m--master-image-uri\u001b[m and\n",
      "        \u001b[1m--runtime-version\u001b[m must be specified.\n",
      "\n",
      "     \u001b[1m--master-machine-type\u001b[m=\u001b[4mMASTER_MACHINE_TYPE\u001b[m\n",
      "        Specifies the type of virtual machine to use for training job's master\n",
      "        worker.\n",
      "\n",
      "        You must set this value when \u001b[1m--scale-tier\u001b[m is set to \u001b[1mCUSTOM\u001b[m.\n",
      "\n",
      "     \u001b[1m--module-name\u001b[m=\u001b[4mMODULE_NAME\u001b[m\n",
      "        Name of the module to run.\n",
      "\n",
      "     \u001b[1m--package-path\u001b[m=\u001b[4mPACKAGE_PATH\u001b[m\n",
      "        Path to a Python package to build. This should point to a directory\n",
      "        containing the Python source for the job. It will be built using\n",
      "        \u001b[1msetuptools\u001b[m (which must be installed) using its \u001b[1mparent\u001b[m directory as\n",
      "        context. If the parent directory contains a \u001b[1msetup.py\u001b[m file, the build\n",
      "        will use that; otherwise, it will use a simple built-in one.\n",
      "\n",
      "     \u001b[1m--packages\u001b[m=[\u001b[4mPACKAGE\u001b[m,...]\n",
      "        Path to Python archives used for training. These can be local paths\n",
      "        (absolute or relative), in which case they will be uploaded to the\n",
      "        Cloud Storage bucket given by \u001b[1m--staging-bucket\u001b[m, or Cloud Storage URLs\n",
      "        ('gs://bucket-name/path/to/package.tar.gz').\n",
      "\n",
      "     \u001b[1m--parameter-server-accelerator\u001b[m=[\u001b[4mcount\u001b[m=\u001b[4mCOUNT\u001b[m],[\u001b[4mtype\u001b[m=\u001b[4mTYPE\u001b[m]\n",
      "        Hardware accelerator config for the parameter servers. Must specify\n",
      "        both the accelerator type (TYPE) for each server and the number of\n",
      "        accelerators to attach to each server (COUNT).\n",
      "\n",
      "         \u001b[1mtype\u001b[m\n",
      "            The type of the accelerator. Choices are\n",
      "            nvidia-tesla-k80,nvidia-tesla-p100,nvidia-tesla-p4,nvidia-tesla-t4,nvidia-tesla-v100,tpu-v2,tpu-v2-pod,tpu-v3,tpu-v3-pod\n",
      "\n",
      "         \u001b[1mcount\u001b[m\n",
      "            The number of accelerators to attach to each machine running the\n",
      "            job. Must be greater than 0.\n",
      "\n",
      "     \u001b[1m--parameter-server-image-uri\u001b[m=\u001b[4mPARAMETER_SERVER_IMAGE_URI\u001b[m\n",
      "        Docker image to run on each parameter server. This image must be in\n",
      "        Google Container Registry. If not specified, the value of\n",
      "        \u001b[1m--master-image-uri\u001b[m is used.\n",
      "\n",
      "     \u001b[1m--python-version\u001b[m=\u001b[4mPYTHON_VERSION\u001b[m\n",
      "        Version of Python used during training. If not set, the default version\n",
      "        is 2.7. Python 3.5 is available when \u001b[1m--runtime-version\u001b[m is set to 1.4\n",
      "        and above. Python 2.7 works with all supported runtime versions.\n",
      "\n",
      "     \u001b[1m--region\u001b[m=\u001b[4mREGION\u001b[m\n",
      "        Region of the machine learning training job to submit. If not\n",
      "        specified, you may be prompted to select a region.\n",
      "\n",
      "        To avoid prompting when this flag is omitted, you can set the\n",
      "        \u001b[1m\u001b[1;4mcompute/region\u001b[1m\u001b[m property:\n",
      "\n",
      "            $ gcloud config set compute/region REGION\n",
      "\n",
      "        A list of regions can be fetched by running:\n",
      "\n",
      "            $ gcloud compute regions list\n",
      "\n",
      "        To unset the property, run:\n",
      "\n",
      "            $ gcloud config unset compute/region\n",
      "\n",
      "        Alternatively, the region can be stored in the environment variable\n",
      "        \u001b[1m\u001b[1;4mCLOUDSDK_COMPUTE_REGION\u001b[1m\u001b[m.\n",
      "\n",
      "     \u001b[1m--runtime-version\u001b[m=\u001b[4mRUNTIME_VERSION\u001b[m\n",
      "        AI Platform runtime version for this job. Must be specified unless\n",
      "        --master-image-uri is specified instead. It is defined in documentation\n",
      "        along with the list of supported versions:\n",
      "        https://cloud.google.com/ml-engine/docs/tensorflow/runtime-version-list\n",
      "\n",
      "     \u001b[1m--scale-tier\u001b[m=\u001b[4mSCALE_TIER\u001b[m\n",
      "        Specify the machine types, the number of replicas for workers, and\n",
      "        parameter servers. \u001b[4mSCALE_TIER\u001b[m must be one of:\n",
      "\n",
      "         \u001b[1mbasic\u001b[m\n",
      "            Single worker instance. This tier is suitable for learning how to\n",
      "            use AI Platform, and for experimenting with new models using small\n",
      "            datasets.\n",
      "         \u001b[1mbasic-gpu\u001b[m\n",
      "            Single worker instance with a GPU.\n",
      "         \u001b[1mbasic-tpu\u001b[m\n",
      "            Single worker instance with a Cloud TPU.\n",
      "         \u001b[1mcustom\u001b[m\n",
      "            CUSTOM tier is not a set tier, but rather enables you to use your\n",
      "            own cluster specification. When you use this tier, set values to\n",
      "            configure your processing cluster according to these guidelines\n",
      "            (using the \u001b[1m--config\u001b[m flag):\n",
      "\n",
      "            ▸ You \u001b[4mmust\u001b[m set \u001b[1mTrainingInput.masterType\u001b[m to specify the type of\n",
      "              machine to use for your master node. This is the only required\n",
      "              setting.\n",
      "            ▸ You \u001b[4mmay\u001b[m set \u001b[1mTrainingInput.workerCount\u001b[m to specify the number of\n",
      "              workers to use. If you specify one or more workers, you \u001b[4mmust\u001b[m also\n",
      "              set \u001b[1mTrainingInput.workerType\u001b[m to specify the type of machine to\n",
      "              use for your worker nodes.\n",
      "            ▸ You \u001b[4mmay\u001b[m set \u001b[1mTrainingInput.parameterServerCount\u001b[m to specify the\n",
      "              number of parameter servers to use. If you specify one or more\n",
      "              parameter servers, you \u001b[4mmust\u001b[m also set\n",
      "              \u001b[1mTrainingInput.parameterServerType\u001b[m to specify the type of machine\n",
      "              to use for your parameter servers. Note that all of your workers\n",
      "              must use the same machine type, which can be different from your\n",
      "              parameter server type and master type. Your parameter servers\n",
      "              must likewise use the same machine type, which can be different\n",
      "              from your worker type and master type.\n",
      "         \u001b[1mpremium-1\u001b[m\n",
      "            Large number of workers with many parameter servers.\n",
      "         \u001b[1mstandard-1\u001b[m\n",
      "            Many workers and a few parameter servers.\n",
      "\n",
      "     \u001b[1m--staging-bucket\u001b[m=\u001b[4mSTAGING_BUCKET\u001b[m\n",
      "        Bucket in which to stage training archives.\n",
      "\n",
      "        Required only if a file upload is necessary (that is, other flags\n",
      "        include local paths) and no other flags implicitly specify an upload\n",
      "        path.\n",
      "\n",
      "     \u001b[1m--use-chief-in-tf-config\u001b[m=\u001b[4mUSE_CHIEF_IN_TF_CONFIG\u001b[m\n",
      "        Use \"chief\" role in the cluster instead of \"master\". This is required\n",
      "        for TensorFlow 2.0 and newer versions. Unlike \"master\" node, \"chief\"\n",
      "        node does not run evaluation.\n",
      "\n",
      "     \u001b[1m--worker-accelerator\u001b[m=[\u001b[4mcount\u001b[m=\u001b[4mCOUNT\u001b[m],[\u001b[4mtype\u001b[m=\u001b[4mTYPE\u001b[m]\n",
      "        Hardware accelerator config for the worker nodes. Must specify both the\n",
      "        accelerator type (TYPE) for each server and the number of accelerators\n",
      "        to attach to each server (COUNT).\n",
      "\n",
      "         \u001b[1mtype\u001b[m\n",
      "            The type of the accelerator. Choices are\n",
      "            nvidia-tesla-k80,nvidia-tesla-p100,nvidia-tesla-p4,nvidia-tesla-t4,nvidia-tesla-v100,tpu-v2,tpu-v2-pod,tpu-v3,tpu-v3-pod\n",
      "\n",
      "         \u001b[1mcount\u001b[m\n",
      "            The number of accelerators to attach to each machine running the\n",
      "            job. Must be greater than 0.\n",
      "\n",
      "     \u001b[1m--worker-image-uri\u001b[m=\u001b[4mWORKER_IMAGE_URI\u001b[m\n",
      "        Docker image to run on each worker node. This image must be in Google\n",
      "        Container Registry. If not specified, the value of \u001b[1m--master-image-uri\u001b[m\n",
      "        is used.\n",
      "\n",
      "     At most one of these may be specified:\n",
      "\n",
      "       \u001b[1m--async\u001b[m\n",
      "          (DEPRECATED) Display information about the operation in progress\n",
      "          without waiting for the operation to complete. Enabled by default and\n",
      "          can be omitted; use \u001b[1m--stream-logs\u001b[m to run synchronously.\n",
      "\n",
      "       \u001b[1m--stream-logs\u001b[m\n",
      "          Block until job completion and stream the logs while the job runs.\n",
      "\n",
      "          Note that even if command execution is halted, the job will still run\n",
      "          until cancelled with\n",
      "\n",
      "              $ gcloud ai-platform jobs cancel JOB_ID\n",
      "\n",
      "     Configure parameter server machine type settings.\n",
      "\n",
      "       \u001b[1m--parameter-server-count\u001b[m=\u001b[4mPARAMETER_SERVER_COUNT\u001b[m\n",
      "          The number of parameter servers to use for the training job. This\n",
      "          flag must be specified if any of the other arguments in this group\n",
      "          are specified.\n",
      "\n",
      "       \u001b[1m--parameter-server-machine-type\u001b[m=\u001b[4mPARAMETER_SERVER_MACHINE_TYPE\u001b[m\n",
      "          Type of virtual machine to use for training job's parameter servers.\n",
      "          This flag must be specified if any of the other arguments in this\n",
      "          group are specified machine to use for training job's parameter\n",
      "          servers. This flag must be specified if any of the other arguments in\n",
      "          this group are specified.\n",
      "\n",
      "     Configure worker node machine type settings.\n",
      "\n",
      "       \u001b[1m--worker-count\u001b[m=\u001b[4mWORKER_COUNT\u001b[m\n",
      "          The number of worker nodes to use for the training job. This flag\n",
      "          must be specified if any of the other arguments in this group are\n",
      "          specified.\n",
      "\n",
      "       \u001b[1m--worker-machine-type\u001b[m=\u001b[4mWORKER_MACHINE_TYPE\u001b[m\n",
      "          Type of virtual machine to use for training job's worker nodes. This\n",
      "          flag must be specified if any of the other arguments in this group\n",
      "          are specified.\n",
      "\n",
      "\u001b[m\u001b[1mGCLOUD WIDE FLAGS\u001b[m\n",
      "    These flags are available to all commands: --account, --billing-project,\n",
      "    --configuration, --flags-file, --flatten, --format, --help,\n",
      "    --impersonate-service-account, --log-http, --project, --quiet,\n",
      "    --trace-token, --user-output-enabled, --verbosity.\n",
      "\n",
      "    Run \u001b[1m$ gcloud help\u001b[m for details.\n",
      "\n",
      "\u001b[m\u001b[1mNOTES\u001b[m\n",
      "    These variants are also available:\n",
      "\n",
      "        $ gcloud alpha ai-platform jobs submit training\n",
      "        $ gcloud beta ai-platform jobs submit training\n",
      "\n",
      "\u001b[m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform jobs submit training --help"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
