{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export BUCKET_STAGING_NAME=your_gcp_gs_bucket_staging_name` \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow_datasets\n",
    "from tensorboard import notebook\n",
    "import math\n",
    "#from google.cloud import storage\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import logging\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc4-8-g2b96f3662b 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import utils.model_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(mu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "project_name = os.environ['PROJECT_ID']\n",
    "project_id = 'projects/{}'.format(project_name)\n",
    "ai_platform_training = discovery.build('ml', 'v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " modifying Tensorflow env variable\n",
      "env_var['TF_CPP_MIN_LOG_LEVEL']= 0\n",
      "env_var['TF_CPP_MIN_VLOG_LEVEL']= 0\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "running sdist\n",
      "running egg_info\n",
      "writing bert_model.egg-info/PKG-INFO\n",
      "writing dependency_links to bert_model.egg-info/dependency_links.txt\n",
      "writing requirements to bert_model.egg-info/requires.txt\n",
      "writing top-level names to bert_model.egg-info/top_level.txt\n",
      "reading manifest file 'bert_model.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "writing manifest file 'bert_model.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating bert_model-0.1\n",
      "creating bert_model-0.1/analysis\n",
      "creating bert_model-0.1/bert_model.egg-info\n",
      "creating bert_model-0.1/model\n",
      "creating bert_model-0.1/model/sklearn_naive_bayes\n",
      "creating bert_model-0.1/model/test\n",
      "creating bert_model-0.1/model/tf_bert_classification\n",
      "creating bert_model-0.1/model/tf_custom_bert_classification\n",
      "creating bert_model-0.1/preprocessing\n",
      "creating bert_model-0.1/utils\n",
      "copying files to bert_model-0.1...\n",
      "copying MANIFEST.in -> bert_model-0.1\n",
      "copying setup.py -> bert_model-0.1\n",
      "copying analysis/__init__.py -> bert_model-0.1/analysis\n",
      "copying analysis/get_data.py -> bert_model-0.1/analysis\n",
      "copying bert_model.egg-info/PKG-INFO -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/SOURCES.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/dependency_links.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/requires.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/top_level.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying model/__init__.py -> bert_model-0.1/model\n",
      "copying model/sklearn_naive_bayes/__init__.py -> bert_model-0.1/model/sklearn_naive_bayes\n",
      "copying model/sklearn_naive_bayes/model.py -> bert_model-0.1/model/sklearn_naive_bayes\n",
      "copying model/sklearn_naive_bayes/task.py -> bert_model-0.1/model/sklearn_naive_bayes\n",
      "copying model/test/__init__.py -> bert_model-0.1/model/test\n",
      "copying model/test/task.py -> bert_model-0.1/model/test\n",
      "copying model/tf_bert_classification/__init__.py -> bert_model-0.1/model/tf_bert_classification\n",
      "copying model/tf_bert_classification/model.py -> bert_model-0.1/model/tf_bert_classification\n",
      "copying model/tf_bert_classification/task.py -> bert_model-0.1/model/tf_bert_classification\n",
      "copying model/tf_custom_bert_classification/__init__.py -> bert_model-0.1/model/tf_custom_bert_classification\n",
      "copying model/tf_custom_bert_classification/model.py -> bert_model-0.1/model/tf_custom_bert_classification\n",
      "copying model/tf_custom_bert_classification/task.py -> bert_model-0.1/model/tf_custom_bert_classification\n",
      "copying preprocessing/__init__.py -> bert_model-0.1/preprocessing\n",
      "copying preprocessing/preprocessing.py -> bert_model-0.1/preprocessing\n",
      "copying utils/__init__.py -> bert_model-0.1/utils\n",
      "copying utils/env_variables.json -> bert_model-0.1/utils\n",
      "copying utils/model_metrics.py -> bert_model-0.1/utils\n",
      "copying utils/model_tests.py -> bert_model-0.1/utils\n",
      "copying utils/model_utils.py -> bert_model-0.1/utils\n",
      "copying utils/ressources_utils.py -> bert_model-0.1/utils\n",
      "Writing bert_model-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'bert_model-0.1' (and everything under it)\n",
      "proj_multilingual_text_classification/bert_model-0.1.tar.gz\n",
      "Last modified: Thu Jun 11 10:32:49 2020\n",
      "Created: Thu Jun 11 10:32:49 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will compute accuracy on the test set every 4 step so 1 time\n"
     ]
    }
   ],
   "source": [
    "# choose the model\n",
    "model_name = 'tf_bert_classification'\n",
    "\n",
    "# variable used to build some variable's name\n",
    "type_production = 'test' #'test', 'production'\n",
    "hardware = 'cpu' #'cpu', 'gpu', 'tpu'\n",
    "owner = os.environ['OWNER']\n",
    "tier = 'basic' #'basic', 'custom'\n",
    "hp_tuning= True\n",
    "verbosity = 'INFO'\n",
    "profiling = False\n",
    "\n",
    "# overwrite parameter for testing logging\n",
    "test_logging = True\n",
    "\n",
    "print(' modifying Tensorflow env variable')\n",
    "# 0 = all messages are logged (default behavior)\n",
    "# 1 = INFO messages are not printed\n",
    "# 2 = INFO and WARNING messages are not printed\n",
    "# 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "with open(os.environ['DIR_PROJ']+'/utils/env_variables.json', 'r') as outfile:\n",
    "    env_var = json.load(outfile)\n",
    "if verbosity == 'DEBUG' or  verbosity == 'VERBOSE' or verbosity == 'INFO':\n",
    "    env_var['TF_CPP_MIN_LOG_LEVEL'] = 0\n",
    "    env_var['TF_CPP_MIN_VLOG_LEVEL'] = 0\n",
    "elif verbosity == 'WARNING':\n",
    "    env_var['TF_CPP_MIN_LOG_LEVEL'] = 1\n",
    "    env_var['TF_CPP_MIN_VLOG_LEVEL'] = 1\n",
    "elif verbosity == 'ERROR':\n",
    "    env_var['TF_CPP_MIN_LOG_LEVEL'] = 2\n",
    "    env_var['TF_CPP_MIN_VLOG_LEVEL'] = 2\n",
    "else:\n",
    "    env_var['TF_CPP_MIN_LOG_LEVEL'] = 3\n",
    "    env_var['TF_CPP_MIN_VLOG_LEVEL'] = 3\n",
    "print(\"env_var['TF_CPP_MIN_LOG_LEVEL']=\", env_var['TF_CPP_MIN_LOG_LEVEL'])\n",
    "print(\"env_var['TF_CPP_MIN_VLOG_LEVEL']=\", env_var['TF_CPP_MIN_VLOG_LEVEL'])\n",
    "data={}\n",
    "data['TF_CPP_MIN_LOG_LEVEL'] = env_var['TF_CPP_MIN_LOG_LEVEL']\n",
    "data['TF_CPP_MIN_VLOG_LEVEL'] = env_var['TF_CPP_MIN_VLOG_LEVEL']\n",
    "with open(os.environ['DIR_PROJ']+'/utils/env_variables.json', 'w') as outfile:\n",
    "    json.dump(data, outfile)\n",
    "\n",
    "# define parameters for ai platform training\n",
    "package_gcs = mu.create_module_tar_archive(model_name)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "if hp_tuning:\n",
    "    job_name = model_name+'_hp_tuning_'+hardware+'_'+timestamp\n",
    "else:\n",
    "    job_name = model_name+'_'+hardware+'_'+timestamp\n",
    "module_name = 'model.'+model_name+'.task'\n",
    "if tier=='basic' and hardware=='cpu':\n",
    "    # CPU\n",
    "    region = 'europe-west1'\n",
    "    \n",
    "elif tier=='basic' and hardware=='gpu':\n",
    "    # GPU\n",
    "    region = 'europe-west1'\n",
    "    \n",
    "elif tier=='custom' and hardware=='gpu':\n",
    "    # Custom GPU\n",
    "    region = 'europe-west4'\n",
    "    \n",
    "elif tier=='basic' and hardware=='tpu':\n",
    "    # TPU\n",
    "    #region = 'us-central1'\n",
    "    region = 'europe-west4' # No zone in region europe-west4 has accelerators of all requested types\n",
    "    #region = 'europe-west6' # The request for 8 TPU_V2 accelerators exceeds the allowed maximum of 0 K80, 0 P100, 0 P4, 0 T4, 0 TPU_V2, 0 TPU_V2_POD, 0 TPU_V3, 0 TPU_V3_POD, 0 V100\n",
    "    #region = 'europe-west2'  # No zone in region europe-west2 has accelerators of all requested types\n",
    "\n",
    "elif tier=='custom' and hardware=='tpu':\n",
    "    # TPU\n",
    "    #region = 'us-central1'\n",
    "    region = 'europe-west4'\n",
    "    #region = 'europe-west6'\n",
    "    #region = 'europe-west2'\n",
    "\n",
    "else:\n",
    "    # Default\n",
    "    region = 'europe-west1'\n",
    "\n",
    "# define parameters for training of the model\n",
    "if type_production=='production':\n",
    "    # reading metadata\n",
    "    _, info = tensorflow_datasets.load(name='glue/sst2',\n",
    "                                       data_dir=data_dir,\n",
    "                                       with_info=True)\n",
    "    # define parameters\n",
    "    epochs = 2 \n",
    "    batch_size_train = 32\n",
    "    #batch_size_test = 32\n",
    "    batch_size_eval = 64  \n",
    "    \n",
    "    # Maxium length, becarefull BERT max length is 512!\n",
    "    max_length = 128\n",
    "\n",
    "    # extract parameters\n",
    "    size_train_dataset=info.splits['train'].num_examples\n",
    "    #size_test_dataset=info.splits['test'].num_examples\n",
    "    size_valid_dataset=info.splits['validation'].num_examples\n",
    "\n",
    "    # computer parameter\n",
    "    steps_per_epoch_train = math.ceil(size_train_dataset/batch_size_train)\n",
    "    #steps_per_epoch_test = math.ceil(size_test_dataset/batch_size_test)\n",
    "    steps_per_epoch_eval = math.ceil(size_valid_dataset/batch_size_eval)\n",
    "\n",
    "    #print('Dataset size:          {:6}/{:6}/{:6}'.format(size_train_dataset, size_test_dataset, size_valid_dataset))\n",
    "    #print('Batch size:            {:6}/{:6}/{:6}'.format(batch_size_train, batch_size_test, batch_size_eval))\n",
    "    #print('Step per epoch:        {:6}/{:6}/{:6}'.format(steps_per_epoch_train, steps_per_epoch_test, steps_per_epoch_eval))\n",
    "    #print('Total number of batch: {:6}/{:6}/{:6}'.format(steps_per_epoch_train*(epochs+1), steps_per_epoch_test*(epochs+1), steps_per_epoch_eval*1))\n",
    "    print('Number of epoch:        {:6}'.format(epochs))\n",
    "    print('Batch size:            {:6}/{:6}'.format(batch_size_train, batch_size_eval))\n",
    "    print('Step per epoch:        {:6}/{:6}'.format(steps_per_epoch_train, steps_per_epoch_eval))\n",
    "\n",
    "else:\n",
    "    if hardware=='tpu':\n",
    "        epochs = 1 \n",
    "        steps_per_epoch_train = 5 \n",
    "        batch_size_train = 32 \n",
    "        steps_per_epoch_eval = 1 \n",
    "        batch_size_eval = 64\n",
    "    else:\n",
    "        epochs = 1 \n",
    "        steps_per_epoch_train = 5 \n",
    "        batch_size_train = 32 \n",
    "        steps_per_epoch_eval = 1 \n",
    "        batch_size_eval = 64\n",
    "        \n",
    "steps=epochs*steps_per_epoch_train\n",
    "if steps<=5:\n",
    "    n_steps_history=4\n",
    "elif steps>=5 and steps<1000:\n",
    "    n_steps_history=10\n",
    "    print('be carefull with profiling between step: 10-20')\n",
    "else:\n",
    "    n_steps_history=int(steps/100)\n",
    "    print('be carefull with profiling between step: 10-20')\n",
    "print('will compute accuracy on the test set every {} step so {} time'.format(n_steps_history, int(steps/n_steps_history)))\n",
    "\n",
    "if profiling:\n",
    "    print(' profilin ...')\n",
    "    steps_per_epoch_train = 100 \n",
    "    n_steps_history=25\n",
    "\n",
    "input_eval_tfrecords = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/valid' #'gs://public-test-data-gs/valid'\n",
    "input_train_tfrecords = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/train' #'gs://public-test-data-gs/train'\n",
    "if hp_tuning:\n",
    "    output_dir = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_hp_tuning_'+hardware+'_'+timestamp\n",
    "else:\n",
    "    output_dir = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+hardware+'_'+timestamp\n",
    "pretrained_model_dir = 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'\n",
    "epsilon = 1.7788921050163616e-06\n",
    "learning_rate= 0.0007763625134788308\n",
    "\n",
    "# bulding training_inputs\n",
    "parameters =  ['--epochs', str(epochs),\n",
    "               '--steps_per_epoch_train', str(steps_per_epoch_train),\n",
    "               '--batch_size_train', str(batch_size_train),\n",
    "               '--steps_per_epoch_eval', str(steps_per_epoch_eval),\n",
    "               '--n_steps_history', str(n_steps_history),\n",
    "               '--batch_size_eval', str(batch_size_eval),\n",
    "               '--input_eval_tfrecords', input_eval_tfrecords ,\n",
    "               '--input_train_tfrecords', input_train_tfrecords,\n",
    "               '--output_dir', output_dir,\n",
    "               '--pretrained_model_dir', pretrained_model_dir,\n",
    "               '--verbosity_level', verbosity,\n",
    "               '--epsilon', str(epsilon),\n",
    "               '--learning_rate', str(learning_rate)]\n",
    "if hardware=='tpu':\n",
    "    parameters.append('--use_tpu')\n",
    "    parameters.append('True')\n",
    "\n",
    "training_inputs = {\n",
    "    'packageUris': [package_gcs],\n",
    "    'pythonModule': module_name,\n",
    "    'args': parameters,\n",
    "    'region': region,\n",
    "    'runtimeVersion': '2.1',\n",
    "    'pythonVersion': '3.7',\n",
    "}\n",
    "\n",
    "if tier=='basic' and hardware=='cpu':\n",
    "    # CPU\n",
    "    training_inputs['scaleTier'] = 'BASIC'\n",
    "    \n",
    "elif tier=='basic' and hardware=='gpu':\n",
    "    # GPU\n",
    "    training_inputs['scaleTier'] = 'BASIC_GPU'\n",
    "    \n",
    "elif tier=='custom' and hardware=='gpu':\n",
    "    # Custom GPU\n",
    "    training_inputs['scaleTier'] = 'CUSTOM'\n",
    "    training_inputs['masterType'] = 'n1-standard-8'\n",
    "    accelerator_master = {'acceleratorConfig': {\n",
    "        'count': '1',\n",
    "        'type': 'NVIDIA_TESLA_V100'}\n",
    "    }\n",
    "    training_inputs['masterConfig'] = accelerator_master\n",
    "\n",
    "    \n",
    "elif tier=='basic' and hardware=='tpu':\n",
    "    # TPU\n",
    "    training_inputs['scaleTier'] = 'BASIC_TPU'\n",
    "    \n",
    "elif tier=='custom' and hardware=='tpu':\n",
    "    # Custom TPU\n",
    "    training_inputs['scaleTier'] = 'CUSTOM'\n",
    "    training_inputs['masterType'] = 'n1-highcpu-16'\n",
    "    training_inputs['workerType'] = 'cloud_tpu'\n",
    "    training_inputs['workerCount'] = '1'\n",
    "    accelerator_master = {'acceleratorConfig': {\n",
    "        'count': '8',\n",
    "        'type': 'TPU_V3'}\n",
    "    }\n",
    "    training_inputs['workerConfig'] = accelerator_master\n",
    "\n",
    "else:\n",
    "    # Default\n",
    "    training_inputs['scaleTier'] = 'BASIC'\n",
    "\n",
    "# add hyperparameter tuning to the job config.\n",
    "if hp_tuning:\n",
    "    hyperparams = {\n",
    "        'algorithm': 'ALGORITHM_UNSPECIFIED',\n",
    "        'goal': 'MAXIMIZE',\n",
    "        'hyperparameterMetricTag': 'metric1',\n",
    "        'maxTrials': 3,\n",
    "        'maxParallelTrials': 2,\n",
    "        'maxFailedTrials': 1,\n",
    "        'enableTrialEarlyStopping': True,\n",
    "        'hyperparameterMetricTag': 'metric_accuracy_train_epoch',\n",
    "        'params': []}\n",
    "\n",
    "    hyperparams['params'].append({\n",
    "        'parameterName':'learning_rate',\n",
    "        'type':'DOUBLE',\n",
    "        'minValue': 1.0e-8,\n",
    "        'maxValue': 1.0,\n",
    "        'scaleType': 'UNIT_LOG_SCALE'})\n",
    "    \n",
    "    hyperparams['params'].append({\n",
    "        'parameterName':'epsilon',\n",
    "        'type':'DOUBLE',\n",
    "        'minValue': 1.0e-9,\n",
    "        'maxValue': 1.0,\n",
    "        'scaleType': 'UNIT_LOG_SCALE'})\n",
    "\n",
    "    # Add hyperparameter specification to the training inputs dictionary.\n",
    "    training_inputs['hyperparameters'] = hyperparams\n",
    "    \n",
    "# building job_spec\n",
    "labels = {'accelerator': hardware,\n",
    "          'type': type_production,\n",
    "          'owner': owner}\n",
    "\n",
    "job_spec = {'jobId': job_name, \n",
    "            'labels': labels, \n",
    "            'trainingInput': training_inputs}\n",
    "\n",
    "if test_logging:\n",
    "    # test\n",
    "    # variable used to build some variable's name\n",
    "    owner = os.environ['OWNER']\n",
    "    tier = 'basic' \n",
    "    verbosity = 'INFO'\n",
    "\n",
    "    # define parameters for ai platform training\n",
    "    package_gcs = package_gcs\n",
    "\n",
    "    job_name = 'debug_test_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "\n",
    "    module_name = 'model.test.task'\n",
    "\n",
    "    region = 'europe-west1'\n",
    "\n",
    "    # bulding training_inputs\n",
    "    parameters =  ['--verbosity_level', verbosity]\n",
    "\n",
    "    training_inputs = {\n",
    "        'packageUris': [package_gcs],\n",
    "        'pythonModule': module_name,\n",
    "        'args': parameters,\n",
    "        'region': region,\n",
    "        'runtimeVersion': '2.1',\n",
    "        'pythonVersion': '3.7',\n",
    "    }\n",
    "\n",
    "    training_inputs['scaleTier'] = 'BASIC'\n",
    "\n",
    "    # building job_spec\n",
    "    labels = {'accelerator': 'cpu',\n",
    "              'type': 'debug',\n",
    "              'owner': owner}\n",
    "\n",
    "    job_spec = {'jobId': job_name, \n",
    "                'labels': labels, \n",
    "                'trainingInput': training_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'packageUris': ['gs://ai-platform-training-package-staging/tf_bert_classification_2020_06_11_103250/bert_model-0.1.tar.gz'],\n",
       " 'pythonModule': 'model.test.task',\n",
       " 'args': ['--verbosity_level', 'INFO'],\n",
       " 'region': 'europe-west1',\n",
       " 'runtimeVersion': '2.1',\n",
       " 'pythonVersion': '3.7',\n",
       " 'scaleTier': 'BASIC'}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status for debug_test_2020_06_11_103252:\n",
      "    state : QUEUED\n",
      "    createTime: 2020-06-11T08:32:53Z\n"
     ]
    }
   ],
   "source": [
    "# submit the training job\n",
    "request = ai_platform_training.projects().jobs().create(body=job_spec,\n",
    "                                                        parent=project_id)\n",
    "try:\n",
    "    response = request.execute()\n",
    "    print('Job status for {}:'.format(response['jobId']))\n",
    "    print('    state : {}'.format(response['state']))\n",
    "    print('    createTime: {}'.format(response['createTime']))\n",
    "\n",
    "except errors.HttpError as err:\n",
    "    # For this example, just send some text to the logs.\n",
    "    # You need to import logging for this to work.\n",
    "    logging.error('There was an error creating the training job.'\n",
    "                  ' Check the details:')\n",
    "    logging.error(err._get_reason())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status for tf_bert_classification_hp_tuning_cpu_2020_06_10_113245:\n",
      "    state : SUCCEEDED\n",
      "    trials : {'trialId': '1', 'hyperparameters': {'learning_rate': '0.0001000000000000001', 'epsilon': '3.1622776601683782e-05'}, 'startTime': '2020-06-10T09:33:06.656329913Z', 'endTime': '2020-06-10T09:50:00Z', 'state': 'FAILED'}\n",
      "    trials : {'trialId': '2', 'hyperparameters': {'learning_rate': '1.318427816398073e-05', 'epsilon': '3.2360572457522847e-06'}, 'startTime': '2020-06-10T09:33:06.656458859Z', 'endTime': '2020-06-10T09:48:29Z', 'state': 'FAILED'}\n",
      "    trials : {'trialId': '3', 'hyperparameters': {'learning_rate': '1.4459909039789456e-06', 'epsilon': '2.6917926744580865e-07'}, 'startTime': '2020-06-10T09:50:00.097393570Z', 'endTime': '2020-06-10T10:05:52Z', 'state': 'FAILED'}\n"
     ]
    }
   ],
   "source": [
    "# if you wnat to specify a specif job ID\n",
    "#job_name = 'tf_bert_classification_2020_05_16_193551'\n",
    "jobId = 'projects/{}/jobs/{}'.format(project_name, job_name)\n",
    "request = ai_platform_training.projects().jobs().get(name=jobId)\n",
    "response = None\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "    print('Job status for {}:'.format(response['jobId']))\n",
    "    print('    state : {}'.format(response['state']))\n",
    "    if 'trainingOutput' in response.keys():\n",
    "        if 'trials' in response['trainingOutput'].keys():\n",
    "            for sub_job in response['trainingOutput']['trials']:\n",
    "                print('    trials : {}'.format(sub_job))\n",
    "    if 'consumedMLUnits' in response.keys():\n",
    "        print('    consumedMLUnits : {}'.format(response['trainingOutput']['consumedMLUnits']))\n",
    "    if 'errorMessage' in response.keys():\n",
    "        print('    errorMessage : {}'.format(response['errorMessage']))\n",
    "    \n",
    "except errors.HttpError as err:\n",
    "    logging.error('There was an error getting the logs.'\n",
    "                  ' Check the details:')\n",
    "    logging.error(err._get_reason())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# how to stream logs\n",
    "# --stream-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TensorBoard for job running on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# View open TensorBoard instance\n",
    "#notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# View pid\n",
    "#!ps -ef|grep tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Killed Tensorboard process by using pid\n",
    "#!kill -9 pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-18716b7c2a588607\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-18716b7c2a588607\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6008;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir {output_dir+'/tensorboard'} \\\n",
    "              #--host 0.0.0.0 \\\n",
    "              #--port 6006 \\\n",
    "              #--debugger_port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-69b80dff4e1efbf\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-69b80dff4e1efbf\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6014;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir {output_dir+'/hparams_tuning'} \\\n",
    "              #--host 0.0.0.0 \\\n",
    "              #--port 6006 \\\n",
    "              #--debugger_port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
