{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow_datasets\n",
    "from tensorboard import notebook\n",
    "import math\n",
    "from google.cloud import storage\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "import logging\n",
    "import subprocess\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc4-8-g2b96f3662b 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model_name = 'tf_bert_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "running sdist\n",
      "running egg_info\n",
      "writing bert_model.egg-info/PKG-INFO\n",
      "writing dependency_links to bert_model.egg-info/dependency_links.txt\n",
      "writing requirements to bert_model.egg-info/requires.txt\n",
      "writing top-level names to bert_model.egg-info/top_level.txt\n",
      "reading manifest file 'bert_model.egg-info/SOURCES.txt'\n",
      "writing manifest file 'bert_model.egg-info/SOURCES.txt'\n",
      "running check\n",
      "creating bert_model-0.1\n",
      "creating bert_model-0.1/analysis\n",
      "creating bert_model-0.1/bert_model.egg-info\n",
      "creating bert_model-0.1/model\n",
      "creating bert_model-0.1/model/sklearn_naive_bayes\n",
      "creating bert_model-0.1/model/test\n",
      "creating bert_model-0.1/model/tf_bert_classification\n",
      "creating bert_model-0.1/model/tf_custom_bert_classification\n",
      "creating bert_model-0.1/preprocessing\n",
      "creating bert_model-0.1/utils\n",
      "copying files to bert_model-0.1...\n",
      "copying setup.py -> bert_model-0.1\n",
      "copying analysis/__init__.py -> bert_model-0.1/analysis\n",
      "copying analysis/get_data.py -> bert_model-0.1/analysis\n",
      "copying bert_model.egg-info/PKG-INFO -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/SOURCES.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/dependency_links.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/requires.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying bert_model.egg-info/top_level.txt -> bert_model-0.1/bert_model.egg-info\n",
      "copying model/__init__.py -> bert_model-0.1/model\n",
      "copying model/sklearn_naive_bayes/__init__.py -> bert_model-0.1/model/sklearn_naive_bayes\n",
      "copying model/sklearn_naive_bayes/model.py -> bert_model-0.1/model/sklearn_naive_bayes\n",
      "copying model/sklearn_naive_bayes/task.py -> bert_model-0.1/model/sklearn_naive_bayes\n",
      "copying model/test/__init__.py -> bert_model-0.1/model/test\n",
      "copying model/test/task.py -> bert_model-0.1/model/test\n",
      "copying model/tf_bert_classification/__init__.py -> bert_model-0.1/model/tf_bert_classification\n",
      "copying model/tf_bert_classification/model.py -> bert_model-0.1/model/tf_bert_classification\n",
      "copying model/tf_bert_classification/task.py -> bert_model-0.1/model/tf_bert_classification\n",
      "copying model/tf_custom_bert_classification/__init__.py -> bert_model-0.1/model/tf_custom_bert_classification\n",
      "copying model/tf_custom_bert_classification/model.py -> bert_model-0.1/model/tf_custom_bert_classification\n",
      "copying model/tf_custom_bert_classification/task.py -> bert_model-0.1/model/tf_custom_bert_classification\n",
      "copying preprocessing/__init__.py -> bert_model-0.1/preprocessing\n",
      "copying preprocessing/preprocessing.py -> bert_model-0.1/preprocessing\n",
      "copying utils/__init__.py -> bert_model-0.1/utils\n",
      "copying utils/model_metrics.py -> bert_model-0.1/utils\n",
      "copying utils/model_tests.py -> bert_model-0.1/utils\n",
      "copying utils/model_utils.py -> bert_model-0.1/utils\n",
      "copying utils/ressources_utils.py -> bert_model-0.1/utils\n",
      "Writing bert_model-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'bert_model-0.1' (and everything under it)\n"
     ]
    }
   ],
   "source": [
    "# create the package\n",
    "process=subprocess.Popen(['python','setup.py', 'sdist'], cwd=os.environ['DIR_PROJ'], shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "# wait for the process to terminate\n",
    "for line in process.stderr:\n",
    "    print(line.decode('utf8').replace('\\n',''))\n",
    "for line in process.stdout:\n",
    "    print(line.decode('utf8').replace('\\n',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proj_multilingual_text_classification/bert_model-0.1.tar.gz\n",
      "Last modified: Tue Jun  2 09:46:02 2020\n",
      "Created: Tue Jun  2 09:46:02 2020\n"
     ]
    }
   ],
   "source": [
    "path_package=''\n",
    "name_package=''\n",
    "for root, dirs, files in os.walk(os.environ['DIR_PROJ']+'/dist/'):\n",
    "    for filename in files:\n",
    "        print(root.split('/')[-4]+'/'+filename)\n",
    "        print('Last modified: {}'.format(time.ctime(os.path.getmtime(root+'/'+filename))))\n",
    "        print('Created: {}'.format(time.ctime(os.path.getctime(root+'/'+filename))))\n",
    "        path_package = root+'/'+filename\n",
    "        name_package = filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n",
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "bucket_name = os.environ['BUCKET_STAGING_NAME']\n",
    "output_folder = model_name +'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "blob = bucket.blob(output_folder+'/'+filename)\n",
    "blob.upload_from_filename(path_package)\n",
    "\n",
    "path_package_gcs='gs://'+os.environ['BUCKET_STAGING_NAME']+'/'+output_folder+'/'+filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "project_name = os.environ['PROJECT_ID']\n",
    "project_id = 'projects/{}'.format(project_name)\n",
    "ai_platform_training = discovery.build('ml', 'v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# variable used to build some variable's name\n",
    "type_production = 'test' #'test', 'production'\n",
    "hardware = 'cpu' #'cpu', 'gpu', 'tpu'\n",
    "owner = os.environ['OWNER']\n",
    "tier = 'basic' #'basic', 'custom'\n",
    "hp_tuning= False\n",
    "verbosity = 'DEBUG'\n",
    "\n",
    "# define parameters for ai platform training\n",
    "package_gcs = path_package_gcs\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "if hp_tuning:\n",
    "    job_name = model_name+'_hp_tuning_'+hardware+'_'+timestamp\n",
    "else:\n",
    "    job_name = model_name+'_'+hardware+'_'+timestamp\n",
    "module_name = 'model.'+model_name+'.task'\n",
    "if tier=='basic' and hardware=='cpu':\n",
    "    # CPU\n",
    "    region = 'europe-west1'\n",
    "    \n",
    "elif tier=='basic' and hardware=='gpu':\n",
    "    # GPU\n",
    "    region = 'europe-west1'\n",
    "    \n",
    "elif tier=='custom' and hardware=='gpu':\n",
    "    # Custom GPU\n",
    "    region = 'europe-west4'\n",
    "    \n",
    "elif tier=='basic' and hardware=='tpu':\n",
    "    # TPU\n",
    "    region = 'us-central1'\n",
    "    \n",
    "else:\n",
    "    # Default\n",
    "    region = 'europe-west1'\n",
    "\n",
    "# define parameters for training of the model\n",
    "if type_production=='production':\n",
    "    # reading metadata\n",
    "    _, info = tensorflow_datasets.load(name='glue/sst2',\n",
    "                                       data_dir=data_dir,\n",
    "                                       with_info=True)\n",
    "    # define parameters\n",
    "    epochs = 2 \n",
    "    batch_size_train = 32\n",
    "    #batch_size_test = 32\n",
    "    batch_size_eval = 64  \n",
    "    \n",
    "    # Maxium length, becarefull BERT max length is 512!\n",
    "    max_length = 128\n",
    "\n",
    "    # extract parameters\n",
    "    size_train_dataset=info.splits['train'].num_examples\n",
    "    #size_test_dataset=info.splits['test'].num_examples\n",
    "    size_valid_dataset=info.splits['validation'].num_examples\n",
    "\n",
    "    # computer parameter\n",
    "    steps_per_epoch_train = math.ceil(size_train_dataset/batch_size_train)\n",
    "    #steps_per_epoch_test = math.ceil(size_test_dataset/batch_size_test)\n",
    "    steps_per_epoch_eval = math.ceil(size_valid_dataset/batch_size_eval)\n",
    "\n",
    "    #print('Dataset size:          {:6}/{:6}/{:6}'.format(size_train_dataset, size_test_dataset, size_valid_dataset))\n",
    "    #print('Batch size:            {:6}/{:6}/{:6}'.format(batch_size_train, batch_size_test, batch_size_eval))\n",
    "    #print('Step per epoch:        {:6}/{:6}/{:6}'.format(steps_per_epoch_train, steps_per_epoch_test, steps_per_epoch_eval))\n",
    "    #print('Total number of batch: {:6}/{:6}/{:6}'.format(steps_per_epoch_train*(epochs+1), steps_per_epoch_test*(epochs+1), steps_per_epoch_eval*1))\n",
    "    print('Number of epoch:        {:6}'.format(epochs))\n",
    "    print('Batch size:            {:6}/{:6}'.format(batch_size_train, batch_size_eval))\n",
    "    print('Step per epoch:        {:6}/{:6}'.format(steps_per_epoch_train, steps_per_epoch_eval))\n",
    "\n",
    "else:\n",
    "    epochs = 1 \n",
    "    steps_per_epoch_train = 5 \n",
    "    batch_size_train = 32 \n",
    "    steps_per_epoch_eval = 1 \n",
    "    batch_size_eval = 64\n",
    "    \n",
    "input_eval_tfrecords = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/valid' #'gs://public-test-data-gs/valid'\n",
    "input_train_tfrecords = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/train' #'gs://public-test-data-gs/train'\n",
    "if hp_tuning:\n",
    "    output_dir = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_hp_tuning_'+hardware+'_'+timestamp\n",
    "else:\n",
    "    output_dir = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+hardware+'_'+timestamp\n",
    "pretrained_model_dir = 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'\n",
    "epsilon = 1.7788921050163616e-06\n",
    "learning_rate= 0.0007763625134788308\n",
    "\n",
    "# bulding training_inputs\n",
    "parameters =  ['--epochs', str(epochs),\n",
    "               '--steps_per_epoch_train', str(steps_per_epoch_train),\n",
    "               '--batch_size_train', str(batch_size_train),\n",
    "               '--steps_per_epoch_eval', str(steps_per_epoch_eval),\n",
    "               '--batch_size_eval', str(batch_size_eval),\n",
    "               '--input_eval_tfrecords', input_eval_tfrecords ,\n",
    "               '--input_train_tfrecords', input_train_tfrecords,\n",
    "               '--output_dir', output_dir,\n",
    "               '--pretrained_model_dir', pretrained_model_dir,\n",
    "               '--verbosity_level', verbosity,\n",
    "               '--epsilon', str(epsilon),\n",
    "               '--learning_rate', str(learning_rate)]\n",
    "if hardware=='tpu':\n",
    "    parameters.append('--use_tpu')\n",
    "    parameters.append('True')\n",
    "\n",
    "training_inputs = {\n",
    "    'packageUris': [package_gcs],\n",
    "    'pythonModule': module_name,\n",
    "    'args': parameters,\n",
    "    'region': region,\n",
    "    'runtimeVersion': '2.1',\n",
    "    'pythonVersion': '3.7',\n",
    "}\n",
    "\n",
    "if tier=='basic' and hardware=='cpu':\n",
    "    # CPU\n",
    "    training_inputs['scaleTier'] = 'BASIC'\n",
    "    \n",
    "elif tier=='basic' and hardware=='gpu':\n",
    "    # GPU\n",
    "    training_inputs['scaleTier'] = 'BASIC_GPU'\n",
    "    \n",
    "elif tier=='custom' and hardware=='gpu':\n",
    "    # Custom GPU\n",
    "    training_inputs['scaleTier'] = 'CUSTOM'\n",
    "    training_inputs['masterType'] = 'n1-standard-8'\n",
    "    accelerator_master = {'acceleratorConfig': {\n",
    "        'count': '1',\n",
    "        'type': 'NVIDIA_TESLA_V100'}\n",
    "    }\n",
    "    training_inputs['masterConfig'] = accelerator_master\n",
    "\n",
    "    \n",
    "elif tier=='basic' and hardware=='tpu':\n",
    "    # TPU\n",
    "    training_inputs['scaleTier'] = 'BASIC_TPU'\n",
    "\n",
    "else:\n",
    "    # Default\n",
    "    training_inputs['scaleTier'] = 'BASIC'\n",
    "\n",
    "# add hyperparameter tuning to the job config.\n",
    "if hp_tuning:\n",
    "    hyperparams = {\n",
    "        'algorithm': 'ALGORITHM_UNSPECIFIED',\n",
    "        'goal': 'MAXIMIZE',\n",
    "        'hyperparameterMetricTag': 'metric1',\n",
    "        'maxTrials': 3,\n",
    "        'maxParallelTrials': 2,\n",
    "        'maxFailedTrials': 1,\n",
    "        'enableTrialEarlyStopping': True,\n",
    "        'hyperparameterMetricTag': 'epoch_accuracy_train',\n",
    "        'params': []}\n",
    "\n",
    "    hyperparams['params'].append({\n",
    "        'parameterName':'learning_rate',\n",
    "        'type':'DOUBLE',\n",
    "        'minValue': 1.0e-8,\n",
    "        'maxValue': 1.0,\n",
    "        'scaleType': 'UNIT_LOG_SCALE'})\n",
    "    \n",
    "    hyperparams['params'].append({\n",
    "        'parameterName':'epsilon',\n",
    "        'type':'DOUBLE',\n",
    "        'minValue': 1.0e-9,\n",
    "        'maxValue': 1.0,\n",
    "        'scaleType': 'UNIT_LOG_SCALE'})\n",
    "\n",
    "    # Add hyperparameter specification to the training inputs dictionary.\n",
    "    training_inputs['hyperparameters'] = hyperparams\n",
    "    \n",
    "# building job_spec\n",
    "labels = {'accelerator': hardware,\n",
    "          'type': type_production,\n",
    "          'owner': owner}\n",
    "\n",
    "job_spec = {'jobId': job_name, \n",
    "            'labels': labels, \n",
    "            'trainingInput': training_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# test\n",
    "# variable used to build some variable's name\n",
    "owner = os.environ['OWNER']\n",
    "tier = 'basic' \n",
    "verbosity = 'INFO'\n",
    "\n",
    "# define parameters for ai platform training\n",
    "package_gcs = path_package_gcs\n",
    "\n",
    "job_name = 'debug_test_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "\n",
    "module_name = 'model.test.task'\n",
    "\n",
    "region = 'europe-west1'\n",
    "\n",
    "# bulding training_inputs\n",
    "parameters =  ['--verbosity_level', verbosity]\n",
    "\n",
    "training_inputs = {\n",
    "    'packageUris': [package_gcs],\n",
    "    'pythonModule': module_name,\n",
    "    'args': parameters,\n",
    "    'region': region,\n",
    "    'runtimeVersion': '2.1',\n",
    "    'pythonVersion': '3.7',\n",
    "}\n",
    "\n",
    "training_inputs['scaleTier'] = 'BASIC'\n",
    "    \n",
    "# building job_spec\n",
    "labels = {'accelerator': 'cpu',\n",
    "          'type': 'debug',\n",
    "          'owner': owner}\n",
    "\n",
    "job_spec = {'jobId': job_name, \n",
    "            'labels': labels, \n",
    "            'trainingInput': training_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'packageUris': ['gs://ai-platform-training-package-staging/tf_bert_classification_2020_06_02_094612/bert_model-0.1.tar.gz'],\n",
       " 'pythonModule': 'model.tf_bert_classification.task',\n",
       " 'args': ['--epochs',\n",
       "  '1',\n",
       "  '--steps_per_epoch_train',\n",
       "  '5',\n",
       "  '--batch_size_train',\n",
       "  '32',\n",
       "  '--steps_per_epoch_eval',\n",
       "  '1',\n",
       "  '--batch_size_eval',\n",
       "  '64',\n",
       "  '--input_eval_tfrecords',\n",
       "  'gs://multilingual_text_classification/tfrecord/sst2/bert-base-multilingual-uncased/valid',\n",
       "  '--input_train_tfrecords',\n",
       "  'gs://multilingual_text_classification/tfrecord/sst2/bert-base-multilingual-uncased/train',\n",
       "  '--output_dir',\n",
       "  'gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_cpu_2020_06_02_094642',\n",
       "  '--pretrained_model_dir',\n",
       "  'gs://multilingual_text_classification/pretrained_model/bert-base-multilingual-uncased',\n",
       "  '--verbosity_level',\n",
       "  'DEBUG',\n",
       "  '--epsilon',\n",
       "  '1.7788921050163616e-06',\n",
       "  '--learning_rate',\n",
       "  '0.0007763625134788308'],\n",
       " 'region': 'europe-west1',\n",
       " 'runtimeVersion': '2.1',\n",
       " 'pythonVersion': '3.7',\n",
       " 'scaleTier': 'BASIC'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#training_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job status for tf_bert_classification_cpu_2020_06_02_094642:\n",
      "    state : QUEUED\n",
      "    createTime: 2020-06-02T07:46:55Z\n"
     ]
    }
   ],
   "source": [
    "# submit the training job\n",
    "request = ai_platform_training.projects().jobs().create(body=job_spec,\n",
    "                                                        parent=project_id)\n",
    "try:\n",
    "    response = request.execute()\n",
    "    print('Job status for {}:'.format(response['jobId']))\n",
    "    print('    state : {}'.format(response['state']))\n",
    "    print('    createTime: {}'.format(response['createTime']))\n",
    "\n",
    "except errors.HttpError as err:\n",
    "    # For this example, just send some text to the logs.\n",
    "    # You need to import logging for this to work.\n",
    "    logging.error('There was an error creating the training job.'\n",
    "                  ' Check the details:')\n",
    "    logging.error(err._get_reason())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:There was an error getting the logs. Check the details:\n",
      "ERROR:root:Field: name Error: The specified job was not found.\n"
     ]
    }
   ],
   "source": [
    "# if you wnat to specify a specif job ID\n",
    "#job_name = 'tf_bert_classification_2020_05_16_193551'\n",
    "jobId = 'projects/{}/jobs/{}'.format(project_name, job_name)\n",
    "request = ai_platform_training.projects().jobs().get(name=jobId)\n",
    "response = None\n",
    "\n",
    "try:\n",
    "    response = request.execute()\n",
    "    print('Job status for {}:'.format(response['jobId']))\n",
    "    print('    state : {}'.format(response['state']))\n",
    "    if 'trainingOutput' in response.keys():\n",
    "        if 'trials' in response['trainingOutput'].keys():\n",
    "            for sub_job in response['trainingOutput']['trials']:\n",
    "                print('    trials : {}'.format(sub_job))\n",
    "    if 'consumedMLUnits' in response.keys():\n",
    "        print('    consumedMLUnits : {}'.format(response['trainingOutput']['consumedMLUnits']))\n",
    "    if 'errorMessage' in response.keys():\n",
    "        print('    errorMessage : {}'.format(response['errorMessage']))\n",
    "    \n",
    "except errors.HttpError as err:\n",
    "    logging.error('There was an error getting the logs.'\n",
    "                  ' Check the details:')\n",
    "    logging.error(err._get_reason())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# how to stream logs\n",
    "# --stream-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TensorBoard for job running on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# View open TensorBoard instance\n",
    "#notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# View pid\n",
    "#!ps -ef|grep tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Killed Tensorboard process by using pid\n",
    "#!kill -9 pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-653e8d63f75ea9f\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-653e8d63f75ea9f\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6012;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir {output_dir+'/tensorboard'} \\\n",
    "              #--host 0.0.0.0 \\\n",
    "              #--port 6006 \\\n",
    "              #--debugger_port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6f8995c0570aa964\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6f8995c0570aa964\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6010;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir {output_dir+'/hparams_tuning'} \\\n",
    "              #--host 0.0.0.0 \\\n",
    "              #--port 6006 \\\n",
    "              #--debugger_port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
