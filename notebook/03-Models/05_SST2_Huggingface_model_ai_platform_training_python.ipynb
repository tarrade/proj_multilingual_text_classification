{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import os\n",
    "from datetime import datetime\n",
    "import tensorflow_datasets\n",
    "from tensorboard import notebook\n",
    "import math\n",
    "\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc4-8-g2b96f3662b 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "project_name = os.environ['PROJECT_ID']\n",
    "project_id = 'projects/{}'.format(project_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/google/auth/_default.py:69: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK. We recommend that most server applications use service accounts instead. If your application continues to use end user credentials from Cloud SDK, you might receive a \"quota exceeded\" or \"API not enabled\" error. For more information about service accounts, see https://cloud.google.com/docs/authentication/\n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "ai_platform_training = discovery.build('ml', 'v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "training_inputs = {\n",
    "    'scaleTier': 'CUSTOM',\n",
    "    'masterType': 'complex_model_m',\n",
    "    'workerType': 'complex_model_m',\n",
    "    'parameterServerType': 'large_model',\n",
    "    'workerCount': 9,\n",
    "    'parameterServerCount': 3,\n",
    "    'packageUris': ['gs://my/trainer/path/package-0.0.0.tar.gz'],\n",
    "    'pythonModule': 'trainer.task',\n",
    "    'args': ['--arg1', 'value1', '--arg2', 'value2'],\n",
    "    'region': 'us-central1',\n",
    "    'jobDir': 'gs://my/training/job/directory',\n",
    "    'runtimeVersion': '2.1',\n",
    "    'pythonVersion': '3.7',\n",
    "}\n",
    "\n",
    "job_spec = {'jobId': 'my_job_name', 'trainingInput': training_inputs}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train on GCP\n",
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['RUNTIME_VERSION'] = '2.1'\n",
    "os.environ['PYTHON_VERSION'] = '3.7'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['REGION'] = 'europe-west1'\n",
    "\n",
    "os.environ['EPOCHS'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '5' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/valid'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/train'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: tf_bert_classification_2020_05_12_100204\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [tf_bert_classification_2020_05_12_100204] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_bert_classification_2020_05_12_100204\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_bert_classification_2020_05_12_100204\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --scale-tier basic \\\n",
    "        --python-version $PYTHON_VERSION \\\n",
    "        --runtime-version $RUNTIME_VERSION \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --region=$REGION \\\n",
    "        -- \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='DEBUG' \\\n",
    "#        --stream-logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training with GPU (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/Users/tarrade/tensorflow_datasets/glue/sst2/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/tarrade/tensorflow_datasets/glue/sst2/1.0.0\n"
     ]
    }
   ],
   "source": [
    "# reading metadata\n",
    "_, info = tensorflow_datasets.load(name='glue/sst2',\n",
    "                                   data_dir=data_dir,\n",
    "                                   with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:           67349/  1821/   872\n",
      "Batch size:                32/    32/    64\n",
      "Step per epoch:          2105/    57/    14\n",
      "Total number of batch:   6315/   171/    14\n"
     ]
    }
   ],
   "source": [
    "# Maxium length, becarefull BERT max length is 512!\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "# define parameters\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "BATCH_SIZE_VALID = 64\n",
    "EPOCH = 2\n",
    "\n",
    "# extract parameters\n",
    "size_train_dataset=info.splits['train'].num_examples\n",
    "size_test_dataset=info.splits['test'].num_examples\n",
    "size_valid_dataset=info.splits['validation'].num_examples\n",
    "\n",
    "# computer parameter\n",
    "STEP_EPOCH_TRAIN = math.ceil(size_train_dataset/BATCH_SIZE_TRAIN)\n",
    "STEP_EPOCH_TEST = math.ceil(size_test_dataset/BATCH_SIZE_TEST)\n",
    "STEP_EPOCH_VALID = math.ceil(size_valid_dataset/BATCH_SIZE_VALID)\n",
    "\n",
    "\n",
    "print('Dataset size:          {:6}/{:6}/{:6}'.format(size_train_dataset, size_test_dataset, size_valid_dataset))\n",
    "print('Batch size:            {:6}/{:6}/{:6}'.format(BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, BATCH_SIZE_VALID))\n",
    "print('Step per epoch:        {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN, STEP_EPOCH_TEST, STEP_EPOCH_VALID))\n",
    "print('Total number of batch: {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN*(EPOCH+1), STEP_EPOCH_TEST*(EPOCH+1), STEP_EPOCH_VALID*1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train on GCP\n",
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['RUNTIME_VERSION'] = '2.1'\n",
    "os.environ['PYTHON_VERSION'] = '3.7'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['REGION'] = 'europe-west1'\n",
    "os.environ['CONFIG']= os.environ['DIR_PROJ'].split('src')[0]+'deployment/training/tf_bert_classification/custom.yaml'\n",
    "\n",
    "os.environ['EPOCHS'] = str(EPOCH)\n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = str(STEP_EPOCH_TRAIN)\n",
    "os.environ['BATCH_SIZE_TRAIN'] = str(BATCH_SIZE_TRAIN)\n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = str(STEP_EPOCH_VALID) \n",
    "os.environ['BATCH_SIZE_EVAL'] = str(BATCH_SIZE_VALID)\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/valid'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/train'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of epoch:        2     \n",
      "Batch size:            32    /64    \n",
      "Step per epoch:        2105  /14    \n"
     ]
    }
   ],
   "source": [
    "print('Number of epoch:        {:6}'.format(os.environ['EPOCHS']))\n",
    "print('Batch size:            {:6}/{:6}'.format(os.environ['BATCH_SIZE_TRAIN'], os.environ['BATCH_SIZE_EVAL']))\n",
    "print('Step per epoch:        {:6}/{:6}'.format(os.environ['STEPS_PER_EPOCH_TRAIN'], os.environ['STEPS_PER_EPOCH_EVAL']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: tf_bert_classification_2020_05_06_115937\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [tf_bert_classification_2020_05_06_115937] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_bert_classification_2020_05_06_115937\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_bert_classification_2020_05_06_115937\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --config $CONFIG \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --region=$REGION \\\n",
    "        -- \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training using a config file (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['CONFIG']= os.environ['DIR_PROJ'].split('src')[0]+'deployment/training/tf_bert_classification/standard.yaml' #or custom.yaml or standard.yaml\n",
    "os.environ['EPOCHS'] = '2' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '5' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/valid'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/train'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: tf_bert_classification_2020_05_02_151451\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [tf_bert_classification_2020_05_02_151451] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_bert_classification_2020_05_02_151451\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_bert_classification_2020_05_02_151451\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --config $CONFIG \\\n",
    "        -- \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='DEBUG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model on AI Platform Training using TPU (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train on GCP\n",
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['RUNTIME_VERSION'] = '2.1'\n",
    "os.environ['PYTHON_VERSION'] = '3.7'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['REGION_TPU'] = 'europe-west4' #'us-central1' #'europe-west4'\n",
    "\n",
    "os.environ['USE_TPU'] = 'True' \n",
    "os.environ['EPOCHS'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '1' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/valid'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/train'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# europe-west6\n",
    "ERROR: (gcloud.ai-platform.jobs.submit.training) RESOURCE_EXHAUSTED: Quota failure for project axarevvicnonprod. The request for 8 TPU_V2 accelerators exceeds the allowed maximum of 0 K80, 0 P100, 0 P4, 0 T4, 0 TPU_V2, 0 TPU_V2_POD, 0 TPU_V3, 0 TPU_V3_POD, 0 V100. To read more about Cloud ML Engine quota, see https://cloud.google.com/ml-engine/quotas.\n",
    "- '@type': type.googleapis.com/google.rpc.QuotaFailure\n",
    "  violations:\n",
    "  - description: The request for 8 TPU_V2 accelerators exceeds the allowed maximum\n",
    "      of 0 K80, 0 P100, 0 P4, 0 T4, 0 TPU_V2, 0 TPU_V2_POD, 0 TPU_V3, 0 TPU_V3_POD,\n",
    "      0 V100.\n",
    "    subject: axarevvicnonprod\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: (gcloud.ai-platform.jobs.submit.training) RESOURCE_EXHAUSTED: Field: region Error: No zone in region europe-west4 has accelerators of all requested types.\n",
      "- '@type': type.googleapis.com/google.rpc.BadRequest\n",
      "  fieldViolations:\n",
      "  - description: No zone in region europe-west4 has accelerators of all requested\n",
      "      types.\n",
      "    field: region\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"# Use Cloud Machine Learning Engine to train the model on GCP\\ngcloud ai-platform jobs submit training $JOB_NAME \\\\\\n        --scale-tier basic_tpu\\\\\\n        --python-version $PYTHON_VERSION \\\\\\n        --runtime-version $RUNTIME_VERSION \\\\\\n        --module-name=$MAIN_TRAINER_MODULE \\\\\\n        --package-path=$TRAINER_PACKAGE_PATH \\\\\\n        --staging-bucket=$PACKAGE_STAGING_PATH \\\\\\n        --region=$REGION_TPU \\\\\\n        -- \\\\\\n        --use_tpu=$USE_TPU \\\\\\n        --epochs=$EPOCHS \\\\\\n        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\\\\n        --batch_size_train=$BATCH_SIZE_TRAIN \\\\\\n        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\\\\n        --batch_size_eval=$BATCH_SIZE_EVAL \\\\\\n        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\\\\n        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\\\\n        --output_dir=$OUTPUT_DIR \\\\\\n        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\\\\n        --verbosity_level='DEBUG' \\\\\\n#        --stream-logsgithub\\n\"' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-04099d7922f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bash'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"# Use Cloud Machine Learning Engine to train the model on GCP\\ngcloud ai-platform jobs submit training $JOB_NAME \\\\\\n        --scale-tier basic_tpu\\\\\\n        --python-version $PYTHON_VERSION \\\\\\n        --runtime-version $RUNTIME_VERSION \\\\\\n        --module-name=$MAIN_TRAINER_MODULE \\\\\\n        --package-path=$TRAINER_PACKAGE_PATH \\\\\\n        --staging-bucket=$PACKAGE_STAGING_PATH \\\\\\n        --region=$REGION_TPU \\\\\\n        -- \\\\\\n        --use_tpu=$USE_TPU \\\\\\n        --epochs=$EPOCHS \\\\\\n        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\\\\n        --batch_size_train=$BATCH_SIZE_TRAIN \\\\\\n        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\\\\n        --batch_size_eval=$BATCH_SIZE_EVAL \\\\\\n        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\\\\n        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\\\\n        --output_dir=$OUTPUT_DIR \\\\\\n        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\\\\n        --verbosity_level='DEBUG' \\\\\\n#        --stream-logsgithub\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mnamed_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshebang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# write a basic docstring:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-110>\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m~/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/IPython/core/magics/script.py\u001b[0m in \u001b[0;36mshebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_error\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mCalledProcessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_script\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_close\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"# Use Cloud Machine Learning Engine to train the model on GCP\\ngcloud ai-platform jobs submit training $JOB_NAME \\\\\\n        --scale-tier basic_tpu\\\\\\n        --python-version $PYTHON_VERSION \\\\\\n        --runtime-version $RUNTIME_VERSION \\\\\\n        --module-name=$MAIN_TRAINER_MODULE \\\\\\n        --package-path=$TRAINER_PACKAGE_PATH \\\\\\n        --staging-bucket=$PACKAGE_STAGING_PATH \\\\\\n        --region=$REGION_TPU \\\\\\n        -- \\\\\\n        --use_tpu=$USE_TPU \\\\\\n        --epochs=$EPOCHS \\\\\\n        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\\\\n        --batch_size_train=$BATCH_SIZE_TRAIN \\\\\\n        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\\\\n        --batch_size_eval=$BATCH_SIZE_EVAL \\\\\\n        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\\\\n        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\\\\n        --output_dir=$OUTPUT_DIR \\\\\\n        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\\\\n        --verbosity_level='DEBUG' \\\\\\n#        --stream-logsgithub\\n\"' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --scale-tier basic_tpu\\\n",
    "        --python-version $PYTHON_VERSION \\\n",
    "        --runtime-version $RUNTIME_VERSION \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --region=$REGION_TPU \\\n",
    "        -- \\\n",
    "        --use_tpu=$USE_TPU \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='DEBUG' \\\n",
    "#        --stream-logsgithub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Hyperparameter tunning of the model on AI Platform Training using a config file (for production)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model_name='tf_bert_classification'\n",
    "os.environ['JOB_NAME'] = model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']+'/model'\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['CONFIG']= os.environ['DIR_PROJ'].split('src')[0]+'deployment/hp-tuning/tf_bert_classification/hyperparam.yaml'\n",
    "os.environ['EPOCHS'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '5' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['PACKAGE_STAGING_PATH'] = 'gs://'+os.environ['BUCKET_STAGING_NAME']\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/valid'\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = 'gs://'+os.environ['BUCKET_NAME']+'/tfrecord/sst2/bert-base-multilingual-uncased/train'\n",
    "os.environ['OUTPUT_DIR'] = 'gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/'+model_name+'_'+datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "os.environ['PRETRAINED_MODEL_DIR']= 'gs://'+os.environ['BUCKET_NAME']+'/pretrained_model/bert-base-multilingual-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: tf_bert_classification_2020_05_12_200340\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [tf_bert_classification_2020_05_12_200340] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe tf_bert_classification_2020_05_12_200340\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs tf_bert_classification_2020_05_12_200340\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model on GCP\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "        --module-name=$MAIN_TRAINER_MODULE \\\n",
    "        --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "        --staging-bucket=$PACKAGE_STAGING_PATH \\\n",
    "        --config $CONFIG \\\n",
    "        -- \\\n",
    "        --epochs=$EPOCHS \\\n",
    "        --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "        --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "        --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "        --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "        --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "        --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "        --output_dir=$OUTPUT_DIR \\\n",
    "        --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "        --verbosity_level='INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# TensorBoard for job running on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# View open TensorBoard instance\n",
    "#notebook.list() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  501 31125     1   0 Sun03PM ??        13:49.70 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_10_150723/tensorboard\n",
      "  501 62245 43999   0  8:27PM ??         3:36.47 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_vera_2020_05_08_063801/tensorboard/ --reload_multifile True\n",
      "  501 62318 43999   0  8:29PM ??         3:55.12 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_vera_2020_05_08_063801/tensorboard/ --reload_multifile true\n",
      "  501 67515 43999   0  8:55AM ??         0:37.58 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_12_085500/tensorboard\n",
      "  501 67793 43999   0  9:03AM ??         0:39.30 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_12_090133/tensorboard\n",
      "  501 70132 43999   0 10:06AM ??         3:58.91 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_12_100204/tensorboard\n",
      "  501 70533 43999   0 10:14AM ??         7:57.34 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_12_101446/tensorboard\n",
      "  501 81562 43999   0  4:17PM ??         3:39.76 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_12_161158/tensorboard\n",
      "  501 82859 43999   0  4:54PM ??         3:10.33 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_12_165410/tensorboard\n",
      "  501 88009 43999   0  8:03PM ??         0:03.59 /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/python /Users/tarrade/anaconda-release/conda-env/env_multilingual_class/bin/tensorboard --logdir gs://multilingual_text_classification/training_model_gcp/tf_bert_classification_2020_05_12_200340/tensorboard\n",
      "  501 88114 43999   0  8:06PM ttys007    0:00.03 /bin/sh -c ps -ef|grep tensorboard\n",
      "  501 88116 88114   0  8:06PM ttys007    0:00.00 grep tensorboard\n"
     ]
    }
   ],
   "source": [
    "# View pid\n",
    "!ps -ef|grep tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Killed Tensorboard process by using pid\n",
    "#!kill -9 pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-13d5c45bb75ab214\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-13d5c45bb75ab214\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6015;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir {os.environ['OUTPUT_DIR']+'/tensorboard'} \\\n",
    "              #--host 0.0.0.0 \\\n",
    "              #--port 6006 \\\n",
    "              #--debugger_port 6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-f607dbc3deca61fc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-f607dbc3deca61fc\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6010;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir {os.environ['OUTPUT_DIR']+'/hparams_tuning'} \\\n",
    "              #--host 0.0.0.0 \\\n",
    "              #--port 6006 \\\n",
    "              #--debugger_port 6006"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
