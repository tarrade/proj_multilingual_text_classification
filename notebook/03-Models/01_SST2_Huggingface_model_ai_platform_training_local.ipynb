{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    ")\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tarrade/anaconda-release/conda-env/env_multilingual_class/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import preprocessing.preprocessing as pp\n",
    "import utils.model_metrics as mm\n",
    "import utils.model_utils as mu\n",
    "import model.tf_custom_bert_classification.model as tf_custom_bert\n",
    "import model.tf_bert_classification.model as tf_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);\n",
    "importlib.reload(mm);\n",
    "importlib.reload(mu);\n",
    "importlib.reload(tf_bert);\n",
    "importlib.reload(tf_custom_bert);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc4-8-g2b96f3662b 2.2.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Read data from TFRecord files [local training of the model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Path of the directory with TFRecord files\n",
    "tfrecord_data_dir=data_dir+'/tfrecord/sst2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# models\n",
    "MODELS = [(TFBertModel,         BertTokenizer,       'bert-base-multilingual-uncased'),\n",
    "          (TFXLMRobertaModel,   XLMRobertaTokenizer, 'jplu/tf-xlm-roberta-base')]\n",
    "model_index = 0 # BERT\n",
    "model_class        = MODELS[model_index][0] # i.e TFBertModel\n",
    "tokenizer_class    = MODELS[model_index][1] # i.e BertTokenizer\n",
    "pretrained_weights = MODELS[model_index][2] #'i.e bert-base-multilingual-uncased'\n",
    "number_label = 2                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Train the model locally with AI Platform Training (for tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "savemodel_path = os.path.join(savemodel_dir, 'saved_model')\n",
    "pretrained_model_dir=savemodel_dir+'/pretrained_model/'+pretrained_weights\n",
    "model_name='tf_bert_classification'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# train locally\n",
    "os.environ['EPOCH'] = '1' \n",
    "os.environ['STEPS_PER_EPOCH_TRAIN'] = '1' \n",
    "os.environ['BATCH_SIZE_TRAIN'] = '32' \n",
    "os.environ['STEPS_PER_EPOCH_EVAL'] = '1' \n",
    "os.environ['BATCH_SIZE_EVAL'] = '64'\n",
    "os.environ['TRAINER_PACKAGE_PATH'] = os.environ['PYTHONPATH']\n",
    "os.environ['MAIN_TRAINER_MODULE'] = 'model.'+model_name+'.task'\n",
    "os.environ['INPUT_EVAL_TFRECORDS'] = tfrecord_data_dir\n",
    "os.environ['INPUT_TRAIN_TFRECORDS'] = tfrecord_data_dir\n",
    "os.environ['OUTPUT_DIR'] = savemodel_path\n",
    "os.environ['PRETRAINED_MODEL_DIR']= pretrained_model_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is interrupted.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Use Cloud Machine Learning Engine to train the model in local file system\n",
    "gcloud ai-platform local train \\\n",
    "   --module-name=$MAIN_TRAINER_MODULE \\\n",
    "   --package-path=$TRAINER_PACKAGE_PATH \\\n",
    "   -- \\\n",
    "   --epochs=$EPOCH \\\n",
    "   --steps_per_epoch_train=$STEPS_PER_EPOCH_TRAIN \\\n",
    "   --batch_size_train=$BATCH_SIZE_TRAIN \\\n",
    "   --steps_per_epoch_eval=$STEPS_PER_EPOCH_EVAL \\\n",
    "   --batch_size_eval=$BATCH_SIZE_EVAL \\\n",
    "   --input_eval_tfrecords=$INPUT_EVAL_TFRECORDS \\\n",
    "   --input_train_tfrecords=$INPUT_TRAIN_TFRECORDS \\\n",
    "   --output_dir=$OUTPUT_DIR \\\n",
    "   --pretrained_model_dir=$PRETRAINED_MODEL_DIR \\\n",
    "   --verbosity_level='INFO'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Debug model's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n",
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy.scope():\n",
    "    model=tf_bert.create_model(pretrained_weights, \n",
    "                               pretrained_model_dir=pretrained_model_dir,\n",
    "                               num_labels=number_label,\n",
    "                               learning_rate=3e-5,\n",
    "                               epsilon=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define parameters\n",
    "BATCH_SIZE_TRAIN = 10 #32\n",
    "BATCH_SIZE_TEST = 10 #32\n",
    "BATCH_SIZE_VALID = 10 #64\n",
    "EPOCH = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# TFRecords encode and store data\n",
    "train_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord')\n",
    "test_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/test/*.tfrecord')\n",
    "valid_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/valid/*.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# TFRecords encode and store data\n",
    "train_files = tf.data.TFRecordDataset(tf.io.gfile.glob(tfrecord_data_dir+'/tf_bert_classification/train/*.tfrecord'))\n",
    "test_files = tf.data.TFRecordDataset(tf.io.gfile.glob(tfrecord_data_dir+'/tf_bert_classification/test/*.tfrecord'))\n",
    "valid_files = tf.data.TFRecordDataset(tf.io.gfile.glob(tfrecord_data_dir+'/tf_bert_classification/valid/*.tfrecord'))\n",
    "\n",
    "train_dataset = train_files.map(pp.parse_tfrecord_glue_files)\n",
    "test_dataset = test_files.map(pp.parse_tfrecord_glue_files)\n",
    "valid_dataset = valid_files.map(pp.parse_tfrecord_glue_files)\n",
    "\n",
    "train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE_TRAIN).repeat(EPOCH+1)\n",
    "test_dataset = test_dataset.shuffle(100).batch(BATCH_SIZE_TEST).repeat(EPOCH+1)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE_VALID) #.repeat(EPOCH+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Using function\n",
    "train_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord')\n",
    "test_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/test/*.tfrecord')\n",
    "valid_files = tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/valid/*.tfrecord')\n",
    "\n",
    "train_dataset_2 = tf_bert.build_dataset(train_files, BATCH_SIZE_TRAIN)\n",
    "test_dataset_2 = tf_bert.build_dataset(test_files, BATCH_SIZE_TEST)\n",
    "valid_dataset_2 = tf_bert.build_dataset(valid_files, BATCH_SIZE_VALID)\n",
    "\n",
    "train_dataset_2=train_dataset_2.repeat(EPOCH+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'/Users/tarrade/tensorflow_datasets/tfrecord/sst2/tf_bert_classification/train/train_dataset.tfrecord'>]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tf.data.Dataset.list_files(tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord'))) #.interleave(tf.data.TFRecordDataset, cycle_length=tf.data.experimental.AUTOTUNE, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=string, numpy=b'/Users/tarrade/tensorflow_datasets/tfrecord/sst2/tf_bert_classification/train/train_dataset.tfrecord'>]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tf.data.Dataset.list_files(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/tarrade/tensorflow_datasets/tfrecord/sst2/tf_bert_classification/train/train_dataset.tfrecord']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.io.gfile.glob(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.data.Dataset.list_files(tfrecord_data_dir+'/'+model.name+'/train/*.tfrecord')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:training the model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training set -> batch:1 loss:0.6349474787712097 and acc: 0.699999988079071\n",
      "\n",
      " validation set -> batch:1 val loss:0.7102639675140381 and val acc: 0.5091742873191833\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.6349 - accuracy: 0.7000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (137.805910). Check your callbacks.\n",
      "\n",
      " training set -> batch:2 loss:0.7102600932121277 and acc: 0.5090702772140503\n",
      "\n",
      " validation set -> batch:2 val loss:0.7107909321784973 and val acc: 0.5091742873191833\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7103 - accuracy: 0.5091WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (137.301724). Check your callbacks.\n",
      "\n",
      "Epoch 00001: saving model to /Users/tarrade/tensorflow_model/saved_model/checkpoint_model/ckpt_01\n",
      "accuracy_train 0.5090702772140503 epoch 0 \n",
      "\n",
      "2/2 [==============================] - 163s 82s/step - loss: 0.7103 - accuracy: 0.5091 - val_loss: 0.6687 - val_accuracy: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:\n",
      "execution time: 0:05:07\n",
      "INFO:absl:timing per epoch:\n",
      "['0:05:07']\n",
      "INFO:absl:sum timing over all epochs:\n",
      "0:05:07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/tarrade/tensorflow_model/saved_model/saved_model/tf_bert_classification/assets\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().propagate = False\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "history_test=tf_bert.train_and_evaluate(model, \n",
    "                                        num_epochs=1, \n",
    "                                        steps_per_epoch=2, \n",
    "                                        train_data=train_dataset, \n",
    "                                        validation_steps=1, \n",
    "                                        eval_data=valid_dataset, \n",
    "                                        n_steps_history=1,\n",
    "                                        output_dir=savemodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10144, 19141, ...,     0,     0,     0],\n",
      "       [  101, 11811,   143, ...,     0,     0,     0],\n",
      "       [  101, 33631, 26976, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10160, 13645, ...,     0,     0,     0],\n",
      "       [  101, 25164, 48740, ...,     0,     0,     0],\n",
      "       [  101, 19567, 14478, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 1, 0, 1, 1, 1, 1, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 74737, 17623, ...,     0,     0,     0],\n",
      "       [  101, 10160, 11471, ...,     0,     0,     0],\n",
      "       [  101, 10144, 10119, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10763, 11157, ...,     0,     0,     0],\n",
      "       [  101, 30738, 10158, ...,     0,     0,     0],\n",
      "       [  101, 10235, 60724, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 0, 0, 0, 1, 0, 1, 0, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10359, 52428, ...,     0,     0,     0],\n",
      "       [  101, 15278, 10246, ...,     0,     0,     0],\n",
      "       [  101, 61014, 16155, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 16612, 10285, ...,     0,     0,     0],\n",
      "       [  101, 10103, 15891, ...,     0,     0,     0],\n",
      "       [  101, 36396, 10127, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 1, 1, 1, 1, 0, 1, 1, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10483, 10283, ...,     0,     0,     0],\n",
      "       [  101, 55399, 31838, ...,     0,     0,     0],\n",
      "       [  101,   117, 85287, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10181, 22917, ...,     0,     0,     0],\n",
      "       [  101, 10103, 11397, ...,     0,     0,     0],\n",
      "       [  101, 31696, 21699, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 0, 0, 1, 0, 0, 0, 0, 0, 0])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10948,   143, ...,     0,     0,     0],\n",
      "       [  101, 10283, 10165, ...,     0,     0,     0],\n",
      "       [  101, 14618, 95502, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 11327, 17797, ...,     0,     0,     0],\n",
      "       [  101, 15282, 10563, ...,     0,     0,     0],\n",
      "       [  101, 11975, 10961, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 1, 0, 0, 1, 0, 1, 0, 0])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101,   143, 15931, ...,     0,     0,     0],\n",
      "       [  101,   117, 28737, ...,     0,     0,     0],\n",
      "       [  101, 10139, 10103, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 12112, 11866, ...,     0,     0,     0],\n",
      "       [  101, 47818, 18682, ...,     0,     0,     0],\n",
      "       [  101, 29859,   102, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 0, 0, 0, 0, 0, 0, 1, 0, 1])>)\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in train_dataset:\n",
    "    print(i)\n",
    "    k+=1\n",
    "    if  k>5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:training the model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training set -> batch:1 loss:0.7118471264839172 and acc: 0.5\n",
      "\n",
      " validation set -> batch:1 val loss:0.7104257345199585 and val acc: 0.5091953873634338\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.7118 - accuracy: 0.5000WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (135.707702). Check your callbacks.\n",
      "\n",
      " training set -> batch:2 loss:0.7106784582138062 and acc: 0.5090909004211426\n",
      "\n",
      " validation set -> batch:2 val loss:0.7091863751411438 and val acc: 0.5091953873634338\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.7107 - accuracy: 0.5091WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (136.603427). Check your callbacks.\n",
      "\n",
      "Epoch 00001: saving model to /Users/tarrade/tensorflow_model/saved_model/checkpoint_model/ckpt_01\n",
      "accuracy_train 0.5090909004211426 epoch 0 \n",
      "\n",
      "2/2 [==============================] - 163s 82s/step - loss: 0.7107 - accuracy: 0.5091 - val_loss: 0.7120 - val_accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:\n",
      "execution time: 0:05:05\n",
      "INFO:absl:timing per epoch:\n",
      "['0:05:04']\n",
      "INFO:absl:sum timing over all epochs:\n",
      "0:05:04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/tarrade/tensorflow_model/saved_model/saved_model/tf_bert_classification/assets\n"
     ]
    }
   ],
   "source": [
    "tf.get_logger().propagate = False\n",
    "from absl import logging\n",
    "logging.set_verbosity(logging.INFO)\n",
    "history_test=tf_bert.train_and_evaluate(model, \n",
    "                                        num_epochs=1, \n",
    "                                        steps_per_epoch=2, \n",
    "                                        train_data=train_dataset_2, \n",
    "                                        validation_steps=1, \n",
    "                                        eval_data=valid_dataset_2, \n",
    "                                        n_steps_history=1,\n",
    "                                        output_dir=savemodel_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 79296, 59289, ...,     0,     0,     0],\n",
      "       [  101, 56352, 10486, ...,     0,     0,     0],\n",
      "       [  101, 27635, 29233, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 20524, 10563, ...,     0,     0,     0],\n",
      "       [  101, 11811,   143, ...,     0,     0,     0],\n",
      "       [  101, 89441, 18771, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 0, 0, 1, 0, 1, 1, 1, 0, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10103, 11828, ...,     0,     0,     0],\n",
      "       [  101,   158, 19755, ...,     0,     0,     0],\n",
      "       [  101, 61494, 10151, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,   114, 23259, ...,     0,     0,     0],\n",
      "       [  101, 10431, 10346, ...,     0,     0,     0],\n",
      "       [  101, 10139, 13869, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 1, 0, 0, 1, 0, 0, 0, 0, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10104, 12787, ...,     0,     0,     0],\n",
      "       [  101, 82983,   117, ...,     0,     0,     0],\n",
      "       [  101, 14893,   156, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 26009,   117, ...,     0,     0,     0],\n",
      "       [  101, 11526, 10855, ...,     0,     0,     0],\n",
      "       [  101, 13144, 10491, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 0, 0, 1, 1, 0, 0, 1, 1, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10203, 11050, ...,     0,     0,     0],\n",
      "       [  101,   112,   161, ...,     0,     0,     0],\n",
      "       [  101, 13415, 87430, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 11157,   143, ...,     0,     0,     0],\n",
      "       [  101, 10235, 33631, ...,     0,     0,     0],\n",
      "       [  101, 10197, 11327, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 0, 1, 0, 1, 0, 1, 1, 0, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 10768,   112, ...,     0,     0,     0],\n",
      "       [  101, 12963, 81285, ...,     0,     0,     0],\n",
      "       [  101, 22492, 11406, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10103, 11397, ...,     0,     0,     0],\n",
      "       [  101,   117, 34648, ...,     0,     0,     0],\n",
      "       [  101, 10127, 52150, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([1, 1, 0, 0, 0, 1, 1, 0, 0, 1])>)\n",
      "({'input_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[  101, 23051, 10390, ...,     0,     0,     0],\n",
      "       [  101,   143, 52586, ...,     0,     0,     0],\n",
      "       [  101, 11008, 35240, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10768, 10114, ...,     0,     0,     0],\n",
      "       [  101, 38225, 10323, ...,     0,     0,     0],\n",
      "       [  101, 88172, 10107, ...,     0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(10, 128), dtype=int32, numpy=\n",
      "array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}, <tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 1, 0, 1, 0, 0, 0, 1, 0, 0])>)\n"
     ]
    }
   ],
   "source": [
    "k=0\n",
    "for i in train_dataset_2:\n",
    "    print(i)\n",
    "    k+=1\n",
    "    if  k>5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
