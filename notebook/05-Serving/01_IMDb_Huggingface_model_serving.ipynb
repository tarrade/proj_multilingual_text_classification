{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`\n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model` \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`\n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`\n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    XLMRobertaTokenizer,\n",
    "    TFBertModel,\n",
    "    TFXLMRobertaModel,\n",
    "    TFBertForSequenceClassification,\n",
    "    glue_convert_examples_to_features,\n",
    "    glue_processors\n",
    ")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re\n",
    "import codecs\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unknown 2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    savemodel_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/.conda-env/env_multilingual_class/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import preprocessing.preprocessing as pp\n",
    "import utils.model_metrics as mm\n",
    "import utils.model_utils as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);\n",
    "importlib.reload(mm);\n",
    "importlib.reload(mu);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check the local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "savemodel_path = os.path.join(savemodel_dir, 'saved_model_512')\n",
    "os.makedirs(savemodel_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model(os.path.join(savemodel_path, 'tf_bert_classification'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: tf_bert_classification\n",
      "  assets\n",
      "  variables\n",
      "  saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# check the saved model\n",
    "print('Model: {}'.format(model.name))\n",
    "for i in os.listdir(os.path.join(savemodel_path,model.name)):\n",
    "        print(' ',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  167356416 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 167,357,954\n",
      "Trainable params: 167,357,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "source_bucket_name = savemodel_path+'/'+model.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".../saved_model.pb\n",
      ".../variables.index\n",
      ".../variables.data-00000-of-00001\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(source_bucket_name):\n",
    "    for name in files:\n",
    "        if not 'history' in name:\n",
    "            print(os.path.join('.../', name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The **variables** directory contains a standard training checkpoint (see the guide to training checkpoints).  \n",
    "The **assets** directory contains files used by the TensorFlow graph, for example text files used to initialize vocabulary tables.  \n",
    "The **saved_model.pb** file stores the actual TensorFlow program, or model, and a set of named signatures, each identifying a function that accepts tensor inputs and produces tensor outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ['MODEL_LOCAL']=savemodel_path+'/'+model.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "SavedModels may contain multiple variants of the model (multiple v1.MetaGraphDefs, identified with the **--tag_set** flag to saved_model_cli), but this is rare. APIs which create multiple variants of a model include "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
      "SignatureDef key: \"__saved_model_init_op\"\n",
      "SignatureDef key: \"serving_default\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_model_cli show --dir $MODEL_LOCAL --tag_set serve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['attention_mask'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 512)\n",
      "      name: serving_default_attention_mask:0\n",
      "  inputs['input_ids'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 512)\n",
      "      name: serving_default_input_ids:0\n",
      "  inputs['token_type_ids'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 512)\n",
      "      name: serving_default_token_type_ids:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['output_1'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 2)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "saved_model_cli show --dir $MODEL_LOCAL --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Copy the local model on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "def copy_local_directory_to_gcs(local_path, bucket, gcs_path):\n",
    "    \"\"\"Recursively copy a directory of files to GCS.\n",
    "\n",
    "    local_path should be a directory and not have a trailing slash.\n",
    "    \"\"\"\n",
    "    assert os.path.isdir(local_path)\n",
    "    for root, dirs, files in os.walk(local_path):\n",
    "        for name in files:\n",
    "            local_file = os.path.join(root, name)\n",
    "            remote_path = os.path.join(gcs_path, local_file[1 + len(local_path) :])\n",
    "            print(remote_path)\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)\n",
    "            print('copy of the file on gs:// done !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "destination_bucket_name = 'saved_model/tf_bert_classification'\n",
    "copy_model_gcp=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# will take some time since the size of the model is 2 GB!\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(os.environ['BUCKET_NAME'])\n",
    "if copy_model_gcp:\n",
    "    mu.copy_local_directory_to_gcs(source_bucket_name, bucket, destination_bucket_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model serving setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Defined a name, a region for our model on GCP and create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# defining the name of the model for online prediction\n",
    "os.environ['MODEL_NAME']=model.name+'_IMDb_512'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Normal VM has a model size of 500 MB for more you need to use a specific n1-standard-2 VM (2 GB) for online prediction. It is only available in us-central1.\n",
    "os.environ['REGION_PRED']='us-central1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Check models already deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                         DEFAULT_VERSION_NAME\n",
      "tf_bert_classification_test  v1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud ai-platform models create $MODEL_NAME \\\n",
    "       --regions=$REGION_PRED \\\n",
    "       --enable-logging > /dev/null 2>&1 \\\n",
    "|| printf \"\\n\\nthe model already exists !\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "the model already exists !\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform models create $MODEL_NAME \\\n",
    "       --regions=$REGION_PRED \\\n",
    "       --enable-logging > /dev/null 2>&1 \\\n",
    "|| printf \"\\n\\nthe model already exists !\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Defined all parameters and upload our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# define python and run time version\n",
    "os.environ['RUNTIME_VERSION'] = '2.1'\n",
    "os.environ['PYTHON_VERSION'] = '3.7'\n",
    "os.environ['MODEL_BINARIES'] = 'gs://'+os.environ['BUCKET_NAME']+'/saved_model_512/'+model.name\n",
    "os.environ['MODEL_VERSION'] = 'v1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://multilingual_text_classification/saved_model_512/tf_bert_classification/saved_model.pb\n",
      "gs://multilingual_text_classification/saved_model_512/tf_bert_classification/variables/\n"
     ]
    }
   ],
   "source": [
    "#!gsutil ls gs://multilingual_text_classification/saved_model_512/tf_bert_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud beta ai-platform versions create $MODEL_VERSION \\\n",
    "       --model $MODEL_NAME \\\n",
    "       --origin $MODEL_BINARIES \\\n",
    "       --runtime-version $RUNTIME_VERSION \\\n",
    "       --python-version $PYTHON_VERSION \\\n",
    "       --machine-type n1-standard-2 \\\n",
    "       --description \"This is sentiment classifier using BERT and fine tune on SST-2 for test\" > /dev/null 2>&1 \\\n",
    "|| printf \"\\n\\nsame version of the model already exists !\\n\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Check that the new modelal was deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             DEFAULT_VERSION_NAME\n",
      "tf_bert_classification_IMDb_512  v1\n",
      "tf_bert_classification_test      v1\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform models list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model serving inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Prepare data for online prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "example of format:\n",
    "\n",
    "```--json-request  \n",
    "  {  \n",
    "    \"instances\": [  \n",
    "      {\"x\": [1, 2], \"y\": [3, 4]},  \n",
    "      {\"x\": [-1, -2], \"y\": [-3, -4]}  \n",
    "    ]  \n",
    "  }  ```\n",
    "  \n",
    "```--json-instances  \n",
    "  {\"images\": [0.0, …, 0.1], \"key\": 3}  \n",
    "  {\"images\": [0.0, …, 0.1], \"key\": 2}  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "#tfrecord_data_dir=data_dir+'/tfrecord/sst2'\n",
    "#os.makedirs(tfrecord_data_dir, exist_ok=True)\n",
    "#valid_files = tf.data.TFRecordDataset(tfrecord_data_dir+'/valid_dataset.tfrecord')\n",
    "#valid_dataset = valid_files.map(pp.parse_tfrecord_glue_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "TOKENIZER = 'bert-base-multilingual-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def transform(text):\n",
    "    #print('example of input:\\n\\n{}\\n \\nlength:{}\\n'.format(text[0], len(text)))\n",
    "    #print(‘text:{} length:{}\\n’.format(text, len(text)))\n",
    "    # get probablility for each classes\n",
    "    tokens=tokenizer.batch_encode_plus(text, return_tensors=\"tf\", pad_to_max_length=True, max_length=512)\n",
    "   \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text = [\"Ich habe nicht wirklich viele Filme mit Holly Hunter gesehen, aber es war eine angenehme Überraschung, sie in den Broadcast News zu sehen. Sie ist eine hartgesottene Journalistin, Jane Craig, die ihre ganze Zeit der TV-Nachrichtensendung gewidmet hat. Ihr Kollege Aaron Altman hat ihre Fackel lange getragen, ohne etwas zu sagen. Das Liebesdreieck wird von Tom Grunnick vervollständigt. Er ist der etwas distanzierte Ex-Sportcaster, der der neue Reporter ist. Für Jane symbolisiert er alles, was sie an Nachrichten nicht mag, und verwandelt sie in Edutainment, nicht in ernsthafte Geschäfte. Zu ihrer großen Überraschung fühlt sich Jane von Tom angezogen. Holly Hunter macht eine großartige Leistung als freche Journalistin. Aber ich verstehe nicht ganz, was sie an ihrem neuen Kollegen Tom so reizvoll findet. Es ist etwas mit ihnen, das uns daran hindert, ihm ganz nah zu kommen. Fast ebenso beeindruckend ist Albert Brooks, der alles in der Rolle eines Profis gibt, der mehr als 100 Prozent für seinen Job gibt, aber nicht ganz so viel dafür bekommt. Eigentlich dachte ich eine Weile, er sei Steve Guttenberg von der Police Academy (1984). Er hat ein paar lustige Zeilen und wenn dies ein Bild von Meg Ryan wäre, würden sie es eine romantische Komödie nennen. Bei einer Laufzeit von mehr als zwei Stunden könnten einige Szenen bearbeitet oder komplett weggelassen worden sein, z. Janes und Aarons Reise nach Mittelamerika. Außerdem bin ich ein Trottel für Happy Ends und hatte sieben Jahre später ein anderes Ende als nur ein Wiedersehen zwischen den dreien vorgezogen.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[  101, 12373, 21072, 10801, 24257, 90393, 16994, 14373, 10234,\n",
       "         37948, 19282, 55223,   117, 11712, 10153, 10313, 10361, 56085,\n",
       "         57290, 10688, 10859, 13026, 31267,   117, 10271, 10104, 10129,\n",
       "         19826, 11636, 10331, 24622,   119, 10271, 10339, 10361, 19799,\n",
       "         13217, 82486, 10111, 19477, 10262,   117, 13758, 21859,   117,\n",
       "         10121, 12566, 37930, 12201, 10118, 10827,   118, 60583, 62307,\n",
       "         10728, 25463, 95001, 11193,   119, 13329, 39407, 21432, 24028,\n",
       "         13631, 10629, 11193, 12566, 72010, 15135, 13405, 79571,   117,\n",
       "         15025, 23345, 10331, 58528,   119, 10216, 24941, 69246, 31261,\n",
       "         22067, 10888, 10168, 11956, 27730, 27342, 15405, 44655, 73166,\n",
       "         10123,   119, 10162, 10339, 10118, 23345, 91023, 16149, 11460,\n",
       "           118, 13148, 28265, 10177,   117, 10118, 10118, 13931, 24136,\n",
       "         10339,   119, 10325, 13758, 24395, 51180, 10162, 21785,   117,\n",
       "         10140, 10271, 10144, 60583, 10801, 18115,   117, 10138, 15405,\n",
       "         30253, 16380, 10271, 10104, 22291, 41722, 10503,   117, 10801,\n",
       "         10104, 14749, 72357, 62523, 19207, 10111,   119, 10331, 13788,\n",
       "         15589, 10859, 13026, 31267, 10961, 50332, 10409, 13758, 10168,\n",
       "         11956, 56085, 38350,   119, 37948, 19282, 20403, 10361, 25858,\n",
       "         46378, 10592, 44588, 10233, 12053, 45222, 19477, 10262,   119,\n",
       "         11712, 12373, 12541, 10218, 11448, 10801, 21272,   117, 10140,\n",
       "         10271, 10144, 17827, 15272, 86251, 11956, 10297, 13380, 10311,\n",
       "         44655, 22213,   119, 10153, 10339, 23345, 10234, 23379,   117,\n",
       "         10216, 15905, 45984, 67602, 11781,   117, 13285, 21272, 45866,\n",
       "         10331, 23496,   119, 14024, 24399, 18560, 31844, 49056, 18931,\n",
       "         10339, 12669, 27244,   117, 10118, 21785, 10104, 10118, 16870,\n",
       "         11858, 79496, 10107, 14289,   117, 10118, 12600, 10233, 10445,\n",
       "         19783, 10325, 12879, 19594, 14289,   117, 11712, 10801, 21272,\n",
       "         10297, 23232, 23986, 87975, 10785, 16499,   119, 46413, 56563,\n",
       "         10218, 12373, 10361, 19571, 10111,   117, 10162, 13741, 13629,\n",
       "         18986, 65379, 10168, 10118, 13202, 12640,   113, 10680,   114,\n",
       "           119, 10162, 11193, 10299, 26914, 71890, 11939, 10514, 24146,\n",
       "         10138, 14805, 13015, 10299, 27576, 10168, 11697, 16827, 25810,\n",
       "           117, 11108, 10271, 10153, 10361, 11458, 20400, 82661, 85545,\n",
       "           119, 10478, 10673, 55333, 16445, 10168, 12600, 10233, 11658,\n",
       "         35266, 19423, 15419, 76180, 21364, 66718, 10843, 52096, 15809,\n",
       "         79832, 11639, 11386,   117,   168,   119, 13758, 10107, 10138,\n",
       "         24028, 10107, 31565, 10439, 44830, 88264,   119, 16688, 15521,\n",
       "         12373, 10299, 13210, 68125, 10325, 19308, 25963, 10138, 11688,\n",
       "         22326, 11709, 12654, 10299, 12137, 10107, 12865, 10233, 11450,\n",
       "         10299, 12665, 58155, 11788, 10129, 12344, 10142, 11239, 45309,\n",
       "           119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       dtype=int32)>,\n",
       " 'token_type_ids': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]], dtype=int32)>,\n",
       " 'attention_mask': <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]], dtype=int32)>}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = transform(text)\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101, 12373, 21072, 10801, 24257, 90393, 16994, 14373, 10234,\n",
       "       37948, 19282, 55223,   117, 11712, 10153, 10313, 10361, 56085,\n",
       "       57290, 10688, 10859, 13026, 31267,   117, 10271, 10104, 10129,\n",
       "       19826, 11636, 10331, 24622,   119, 10271, 10339, 10361, 19799,\n",
       "       13217, 82486, 10111, 19477, 10262,   117, 13758, 21859,   117,\n",
       "       10121, 12566, 37930, 12201, 10118, 10827,   118, 60583, 62307,\n",
       "       10728, 25463, 95001, 11193,   119, 13329, 39407, 21432, 24028,\n",
       "       13631, 10629, 11193, 12566, 72010, 15135, 13405, 79571,   117,\n",
       "       15025, 23345, 10331, 58528,   119, 10216, 24941, 69246, 31261,\n",
       "       22067, 10888, 10168, 11956, 27730, 27342, 15405, 44655, 73166,\n",
       "       10123,   119, 10162, 10339, 10118, 23345, 91023, 16149, 11460,\n",
       "         118, 13148, 28265, 10177,   117, 10118, 10118, 13931, 24136,\n",
       "       10339,   119, 10325, 13758, 24395, 51180, 10162, 21785,   117,\n",
       "       10140, 10271, 10144, 60583, 10801, 18115,   117, 10138, 15405,\n",
       "       30253, 16380, 10271, 10104, 22291, 41722, 10503,   117, 10801,\n",
       "       10104, 14749, 72357, 62523, 19207, 10111,   119, 10331, 13788,\n",
       "       15589, 10859, 13026, 31267, 10961, 50332, 10409, 13758, 10168,\n",
       "       11956, 56085, 38350,   119, 37948, 19282, 20403, 10361, 25858,\n",
       "       46378, 10592, 44588, 10233, 12053, 45222, 19477, 10262,   119,\n",
       "       11712, 12373, 12541, 10218, 11448, 10801, 21272,   117, 10140,\n",
       "       10271, 10144, 17827, 15272, 86251, 11956, 10297, 13380, 10311,\n",
       "       44655, 22213,   119, 10153, 10339, 23345, 10234, 23379,   117,\n",
       "       10216, 15905, 45984, 67602, 11781,   117, 13285, 21272, 45866,\n",
       "       10331, 23496,   119, 14024, 24399, 18560, 31844, 49056, 18931,\n",
       "       10339, 12669, 27244,   117, 10118, 21785, 10104, 10118, 16870,\n",
       "       11858, 79496, 10107, 14289,   117, 10118, 12600, 10233, 10445,\n",
       "       19783, 10325, 12879, 19594, 14289,   117, 11712, 10801, 21272,\n",
       "       10297, 23232, 23986, 87975, 10785, 16499,   119, 46413, 56563,\n",
       "       10218, 12373, 10361, 19571, 10111,   117, 10162, 13741, 13629,\n",
       "       18986, 65379, 10168, 10118, 13202, 12640,   113, 10680,   114,\n",
       "         119, 10162, 11193, 10299, 26914, 71890, 11939, 10514, 24146,\n",
       "       10138, 14805, 13015, 10299, 27576, 10168, 11697, 16827, 25810,\n",
       "         117, 11108, 10271, 10153, 10361, 11458, 20400, 82661, 85545,\n",
       "         119, 10478, 10673, 55333, 16445, 10168, 12600, 10233, 11658,\n",
       "       35266, 19423, 15419, 76180, 21364, 66718, 10843, 52096, 15809,\n",
       "       79832, 11639, 11386,   117,   168,   119, 13758, 10107, 10138,\n",
       "       24028, 10107, 31565, 10439, 44830, 88264,   119, 16688, 15521,\n",
       "       12373, 10299, 13210, 68125, 10325, 19308, 25963, 10138, 11688,\n",
       "       22326, 11709, 12654, 10299, 12137, 10107, 12865, 10233, 11450,\n",
       "       10299, 12665, 58155, 11788, 10129, 12344, 10142, 11239, 45309,\n",
       "         119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np_array = np.array(list(example.as_numpy_iterator()))\n",
    "example['input_ids'].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# create a json file with the right format for online prediction\n",
    "serving_data_dir=data_dir+'/serving/sst2'\n",
    "os.makedirs(serving_data_dir, exist_ok=True)\n",
    "json_file = serving_data_dir+'/input_predict_gcloud.json' \n",
    "\n",
    "with codecs.open(json_file, 'w', encoding='utf-8') as f:\n",
    "    #for el in np_array[0:10]:\n",
    "        instance={'input_ids': example['input_ids'].numpy()[0].tolist(), 'attention_mask': example['attention_mask'].numpy()[0].tolist(), 'token_type_ids': example['token_type_ids'].numpy()[0].tolist()}\n",
    "        json.dump(instance, f , sort_keys=True)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['attention_mask', 'input_ids', 'token_type_ids'])\n",
      "{'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'input_ids': [101, 12373, 21072, 10801, 24257, 90393, 16994, 14373, 10234, 37948, 19282, 55223, 117, 11712, 10153, 10313, 10361, 56085, 57290, 10688, 10859, 13026, 31267, 117, 10271, 10104, 10129, 19826, 11636, 10331, 24622, 119, 10271, 10339, 10361, 19799, 13217, 82486, 10111, 19477, 10262, 117, 13758, 21859, 117, 10121, 12566, 37930, 12201, 10118, 10827, 118, 60583, 62307, 10728, 25463, 95001, 11193, 119, 13329, 39407, 21432, 24028, 13631, 10629, 11193, 12566, 72010, 15135, 13405, 79571, 117, 15025, 23345, 10331, 58528, 119, 10216, 24941, 69246, 31261, 22067, 10888, 10168, 11956, 27730, 27342, 15405, 44655, 73166, 10123, 119, 10162, 10339, 10118, 23345, 91023, 16149, 11460, 118, 13148, 28265, 10177, 117, 10118, 10118, 13931, 24136, 10339, 119, 10325, 13758, 24395, 51180, 10162, 21785, 117, 10140, 10271, 10144, 60583, 10801, 18115, 117, 10138, 15405, 30253, 16380, 10271, 10104, 22291, 41722, 10503, 117, 10801, 10104, 14749, 72357, 62523, 19207, 10111, 119, 10331, 13788, 15589, 10859, 13026, 31267, 10961, 50332, 10409, 13758, 10168, 11956, 56085, 38350, 119, 37948, 19282, 20403, 10361, 25858, 46378, 10592, 44588, 10233, 12053, 45222, 19477, 10262, 119, 11712, 12373, 12541, 10218, 11448, 10801, 21272, 117, 10140, 10271, 10144, 17827, 15272, 86251, 11956, 10297, 13380, 10311, 44655, 22213, 119, 10153, 10339, 23345, 10234, 23379, 117, 10216, 15905, 45984, 67602, 11781, 117, 13285, 21272, 45866, 10331, 23496, 119, 14024, 24399, 18560, 31844, 49056, 18931, 10339, 12669, 27244, 117, 10118, 21785, 10104, 10118, 16870, 11858, 79496, 10107, 14289, 117, 10118, 12600, 10233, 10445, 19783, 10325, 12879, 19594, 14289, 117, 11712, 10801, 21272, 10297, 23232, 23986, 87975, 10785, 16499, 119, 46413, 56563, 10218, 12373, 10361, 19571, 10111, 117, 10162, 13741, 13629, 18986, 65379, 10168, 10118, 13202, 12640, 113, 10680, 114, 119, 10162, 11193, 10299, 26914, 71890, 11939, 10514, 24146, 10138, 14805, 13015, 10299, 27576, 10168, 11697, 16827, 25810, 117, 11108, 10271, 10153, 10361, 11458, 20400, 82661, 85545, 119, 10478, 10673, 55333, 16445, 10168, 12600, 10233, 11658, 35266, 19423, 15419, 76180, 21364, 66718, 10843, 52096, 15809, 79832, 11639, 11386, 117, 168, 119, 13758, 10107, 10138, 24028, 10107, 31565, 10439, 44830, 88264, 119, 16688, 15521, 12373, 10299, 13210, 68125, 10325, 19308, 25963, 10138, 11688, 22326, 11709, 12654, 10299, 12137, 10107, 12865, 10233, 11450, 10299, 12665, 58155, 11788, 10129, 12344, 10142, 11239, 45309, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "# inspecting the new created data\n",
    "with open(json_file) as f:\n",
    "    for i in f:\n",
    "        # convert string to dictionary\n",
    "        json_data = eval(i)\n",
    "        print(json_data.keys())\n",
    "        print(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Make prediction using local model to test the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ['DATA_ONLINE_PRED']=json_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT_1\n",
      "[-1.6366816759109497, 1.5684915781021118]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: WARNING:tensorflow:From /home/.conda-env/env_multilingual_class/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "2020-04-14 15:49:45.212670: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2020-04-14 15:49:45.225213: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2800235000 Hz\n",
      "2020-04-14 15:49:45.228970: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55fdca7496a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2020-04-14 15:49:45.229019: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2020-04-14 15:49:45.229294: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:233: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From /usr/lib/google-cloud-sdk/lib/third_party/ml_sdk/cloud/ml/prediction/frameworks/tf_prediction_lib.py:233: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
      "WARNING:root:Error updating signature __saved_model_init_op: The name 'NoOp' refers to an Operation, not a Tensor. Tensor names must be of the form \"<op_name>:<output_index>\".\n",
      "\n",
      "INFO: Display format: \"default \n",
      "          table(\n",
      "              predictions:format=\"table(\n",
      "                  output_1\n",
      "              )\"\n",
      "          )\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform local predict \\\n",
    "       --signature-name serving_default \\\n",
    "       --model-dir $MODEL_LOCAL \\\n",
    "       --json-instances $DATA_ONLINE_PRED \\\n",
    "       --verbosity info \\\n",
    "       #--log-http "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Make online prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.63668013, 1.56849098]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Display format: \"default table[no-heading](predictions)\"\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gcloud ai-platform predict \\\n",
    "       --model $MODEL_NAME \\\n",
    "       --version $MODEL_VERSION \\\n",
    "       --json-instances $DATA_ONLINE_PRED\\\n",
    "       --verbosity info \\\n",
    "       #--log-http"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
