{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# First Steps with Huggingface\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export REGION=your_region`  \n",
       "`export MODEL_DIR_ESTIMATOR_PATH=your_path_to_save_model` \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# only running this cell leads to problems when kernel has not been restarted\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.data.ops import dataset_ops\n",
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "from absl import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "from transformers import (BertTokenizer,\n",
    "                          glue_convert_examples_to_features,\n",
    "                          TFBertForSequenceClassification,\n",
    "                          TFBertModel,\n",
    "                          TFBertForPreTraining)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except:\n",
    "    print('missing PATH_DATASETS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import preprocessing.preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading a Dataset from Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset imdb_reviews (/Users/tarrade/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split ('train[:60%]', 'train[60%:]', 'test'), from /Users/tarrade/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n",
      "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset imdb_reviews (/Users/tarrade/tensorflow_datasets/imdb_reviews/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/tarrade/tensorflow_datasets/imdb_reviews/plain_text/1.0.0\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = tfds.load(name=\"imdb_reviews\",\n",
    "                                                   data_dir=data_dir,\n",
    "                                                   split=('train[:60%]', 'train[60%:]', 'test'),\n",
    "                                                   as_supervised=True)\n",
    "# trying to extract the info\n",
    "data_ex, data_ex_info = tfds.load(name=\"imdb_reviews\",\n",
    "                                                   data_dir=data_dir,\n",
    "                                                   as_supervised=True,\n",
    "                                                   with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'train': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'unsupervised': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ex_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=string, numpy=b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\">, <tf.Tensor: shape=(), dtype=int64, numpy=0>)\n"
     ]
    }
   ],
   "source": [
    "for i in data_ex['train']:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
       " TensorSpec(shape=(), dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ex['train'].element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np_array=np.array(list(data_ex['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
       "       b'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.',\n",
       "       b'Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.',\n",
       "       ...,\n",
       "       b'Okay. So I just got back. Before I start my review, let me tell you one thing: I wanted to like this movie. I know I\\'ve been negative in the past, but I was hoping to be surprised and actually come out liking the film. I didn\\'t.<br /><br />It\\'s not just the fact that every horror clich\\xc3\\xa9 imaginable is in this. And it\\'s not just the fact that they make every little thing into a jump scare (walking into a baseball bat left on the floor? Are you kidding me?). It just wasn\\'t scary. One thing I was surprised about: there was more blood than I thought there was going to be.. which isn\\'t saying much.<br /><br />The film starts off with Donna being dropped off by Lisa\\'s mom at her house. She comes in.. goes upstairs. Camera pans to her father dead on the couch. Spooky. She goes upstairs, where the aforementioned baseball bat scene happens. Finds her brother on his bed, apparently dead (how could she tell? He didn\\'t have a spot of blood on him). Killer comes in, Donna hides under bed, mom dies. She runs outside screaming for help. Killer behind her: \"I did it for us.\" Cut to therapy session. This confused a lot of people- everyone was asking whether or not her family actually died or if she imagined it- and she mentions how the nightmares have started coming back. Filler dialogue ensues.<br /><br />THey cut to the chase pretty quick. Few scenes at the salon, they go to the hotel. Of course the killer is already there (for some reason, he escaped 3 days ago but the police/family weren\\'t informed until he\\'s already there). More filler ensues.<br /><br />I\\'m not going to go on about what happens in the film, because I don\\'t want to spoil it too much. If you want to know who dies, Horror_Fan made a post about it already. But on the subjects of deaths: they weren\\'t that exciting. People in the theatre actually laughed out loud (an experience I\\'ve never had before in a horror movie, not even in When A Stranger Calls) during several of them. One in particular: the bus boy guy who gives the most hilarious \\'scared\\' face I\\'ve ever seen. The only death involving any blood was Lisa\\'s, and that was pretty scarce. Her throat is slashed, blood (if you can even call it that- it was practically black) splatters on the curtain-thing. The only other blood was on Claire when we see her body. Apparently, Fenton decided to stab her a few times after he choked her to death. Um, okay? The movie was one of the most clich\\xc3\\xa9d I\\'ve ever seen. Let\\'s see here.. obligatory close-mirror-curtain-BOOM! scene. Check. Twice, actually (you could tell they were struggling). Mandatory backing-up-into-killer. Check. There\\'s also the backing-up-into-lamp scene, but you\\'ve all seen that. Oh, you say you want a birds-flying-away scare? Well, you got it! (Yes, they managed to incorporate one of those in here). And, of course, the we-have-security-on-all-exits-but-he-still-escaped scene. Shall I go on? I could.<br /><br />For anyone saying the characters weren\\'t stupid, are you kidding me? \"Oh, even though the massive alarm is ringing, literally saying PLEASE VACATE THE BUILDING, and 3 of my friends are missing, I\\'m going to go upstairs to get my wrap.\" These characters were some of the most flawed and stupid characters ever. The only likable character - Lisa - made one of the most stupid moves in the movie. \"Oh, I just realized the psycho-teacher is here! I must leave my strong boyfriend behind to run off by myself to warn her! Oh, shoot, the elevator is being to slow? Guess I\\'ll take the stairs and run off into the construction site!\" Ugh. By the end of the film, they all deserved to die. The only death anyone felt any remorse for was Donna\\'s boyfriend (I can\\'t even remember his name- is that bad?), and by that time, the audience was completely drained out of this scareless, clich\\xc3\\xa9d film.<br /><br />There were SOME positives- the acting was decent for the most part, and it was well-shot. But that\\'s about it.<br /><br />I\\'d give it a 1/5, and that\\'s being generous. Just for the laughs (and believe me, the audience had a few), and Brittany Snow.<br /><br />Oh, and the reaction was bad. Very bad. People were boo-ing after the movie ended and buzz afterwards was very negative. Expect bad legs for this one.',\n",
       "       b'When I saw this trailer on TV I was surprised. In May of 2008 I was at Six Flags in New Jersey and this was showing at a 4-D attraction (you know, the attraction that the seats move). I take it that the version I saw was a shortened version (15 min.) and also re-created to add the motion effects. It was a cute movie... but that was it. It was educational and told about the first mission but the ending of a CGI spacewalk seemed a bit...well...trite. I was not a big fan of the movie but i would recommend this movie for any parent wanting to inform their children in a fun way about the first moonwalk. I will say, the character actors were well selected and the characters themselves were cute. So all-in-all, I would say, if you want to bring the younger kids... go for it. But if you are wanting to take your older kids, take them to another movie... they will thank you.',\n",
       "       b'First of all, Riget is wonderful. Good comedy and mystery thriller at the same time. Nice combination of strange \\'dogma\\' style of telling the story together with good music and great actors. But unfortunately there\\'s no \\'the end\\'. As for me it\\'s unacceptable. I was thinking... how it will be possible to continue the story without Helmer and Drusse? ...and I have some idea. I think Lars should make RIGET III a little bit different. I\\'m sure that 3rd part without Helmer wouldn\\'t be the same. So here\\'s my suggestion. Mayble little bit stupid, maybe not. I know that Lars likes to experiment. So why not to make small experiment with Riget3? I think the only solution here is to create puppet-driven animation (like for example \"team America\" by Trey Parker) or even computer 3d animation. I know it\\'s not the same as real actors, but in principle I believe it could work... only this way it\\'s possible to make actors alive again. For Riget fans this shouldn\\'t be so big difference - if the animation will be done in good way average \\'watcher\\' will consider it normal just after first few shots of the movie. The most important thing now is the story. It\\'s completely understandable that it\\'s not possible to create Riget 3 with the actors nowadays. So why not to play with animation? And... look for the possibilities that it gives to you! Even marketing one! Great director finishes his trilogy after 10 years using puppet animation. Just dreams?<br /><br />I hope to see Riget 3 someday... or even to see just the script. I\\'m curious how the story ends... and as I expect- everybody here do.<br /><br />greets, slaj<br /><br />ps: I\\'m not talking about the \"kingdom hospital\" by Stephen King ;-)'],\n",
       "      dtype='|S13704')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np_array[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(np_array[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "to_string = lambda t: t.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sentence=np_array[:,0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sentence=list(map(to_string, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\",\n",
       " 'I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "to_int = lambda t: int(t.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "label=np_array[:,1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "label=list(map(to_int, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 1, 1, 1, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "idx=[j for j in range(0,len(label))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# {'idx': tf.int32, 'label': tf.int64, 'sentence': tf.string}\n",
    "data_set = {'idx': idx, 'label': label, 'sentence': sentence}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#json_dump = json.dumps(data_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['idx', 'label', 'sentence'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, idx, label,sentence):\n",
    "        self.idx = idx\n",
    "        self.sentence = sentence\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "features=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for i,x  in enumerate(sentence):\n",
    "    ##print(sentence[i],label[i],idx[i])\n",
    "    #print(type(sentence[i]),type(label[i]),type(idx[i]))\n",
    "    #break\n",
    "          \n",
    "    features.append(InputFeatures(np.int32(idx[i]), np.int64(label[i]), sentence[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def gen():\n",
    "    for ex in features:\n",
    "        yield ({\"idx\": ex.idx,\n",
    "                \"label\": ex.label,\n",
    "                \"sentence\": ex.sentence,\n",
    "               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.gen()>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "my_data=tf.data.Dataset.from_generator(\n",
    "    gen,\n",
    "    ({\"idx\": tf.int32, \"label\": tf.int64, \"sentence\": tf.string}),\n",
    "    ({\"idx\": tf.TensorShape([]),\n",
    "      \"label\": tf.TensorShape([]),\n",
    "      \"sentence\": tf.TensorShape([]),\n",
    "     })\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'label': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'sentence': TensorSpec(shape=(), dtype=tf.string, name=None)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "my_transform_data=glue_convert_examples_to_features(my_data, tokenizer, max_length=128, task='sst-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "my_data2=pp.create_tf_example(idx, label, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': TensorSpec(shape=(), dtype=tf.int32, name=None),\n",
       " 'label': TensorSpec(shape=(), dtype=tf.int64, name=None),\n",
       " 'sentence': TensorSpec(shape=(), dtype=tf.string, name=None)}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_data2.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "my_transform_data2=glue_convert_examples_to_features(my_data2, tokenizer, max_length=128, task='sst-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_transform_data2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "len_element = []\n",
    "for index, element in enumerate(train_data.as_numpy_iterator()): \n",
    "    len_element.append(len(element[0]))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sequence:  13704\n",
      "Shortest sequence:  64\n",
      "Average:  1323.1707333333334\n",
      "Standard deviation:  997.3094992023667\n"
     ]
    }
   ],
   "source": [
    "import statistics as st\n",
    "print(\"Longest sequence: \", max(len_element))\n",
    "print(\"Shortest sequence: \", min(len_element))\n",
    "print(\"Average: \", st.mean(len_element))\n",
    "print(\"Standard deviation: \", st.stdev(len_element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of the length of the sequences\n",
    "import matplotlib.pyplot as plt\n",
    "_ = plt.hist(len_element, bins='auto')  \n",
    "plt.title(\"Histogram of the sequence length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Given the relatively large mean of the sequence length, choosing a max_length of 512 may not be appropriate and should be increased to 1024. This will increase the computation time, though.\n",
    "\n",
    "*Is it an option to choose a relatively small max_length and still get good results?*\n",
    "\n",
    "*Kick out outliers?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# what do those really long sequences look like?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Experimenting with Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Defining the Tokenizer Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pretrained_weights = 'bert-base-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#try out also different weights\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "L: glue_convert_examples_to_features only works when a valid task is defined. This is only the case for specific data sets (found in GLUE).\n",
    "What glue_convert_examples_to_features does is convert the data to a tf.dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'I', 'am', 'looking', 'for', 'an', 'em', '##bed', '##ding', '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenized_sequence = tokenizer.tokenize(\"Hello, I am looking for an embedding.\")\n",
    "bert_tokenized_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Setting up the Data for the Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# code from https://github.com/strongio/keras-bert/blob/master/keras-bert.py\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "    Args:\n",
    "      guid: Unique id for the example.\n",
    "      text_a: string. The untokenized text of the first sequence. For single\n",
    "        sequence tasks, only this sequence must be specified.\n",
    "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "        Only must be specified for sequence pair tasks.\n",
    "      label: (Optional) string. The label of the example. This should be\n",
    "        specified for train and dev examples, but not for test examples.\n",
    "    \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "        \n",
    "\n",
    "def convert_single_example(tokenizer, example, max_seq_length=256):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        input_ids = [0] * max_seq_length\n",
    "        input_mask = [0] * max_seq_length\n",
    "        segment_ids = [0] * max_seq_length\n",
    "        label = 0\n",
    "        return input_ids, input_mask, segment_ids, label\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    if len(tokens_a) > max_seq_length - 2:\n",
    "        tokens_a = tokens_a[0 : (max_seq_length - 2)]\n",
    "\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    return input_ids, input_mask, segment_ids, example.label\n",
    "\n",
    "\n",
    "# changed this to fit the input data better\n",
    "def convert_text_to_examples(data):\n",
    "    \"\"\"Create InputExamples\"\"\"\n",
    "    InputExamples = []\n",
    "    for data in data:\n",
    "        InputExamples.append(\n",
    "            # need to access only the text here\n",
    "            #InputExample(guid=None, text_a=\" \".join(text), text_b=None, label=label)\n",
    "        )\n",
    "    return InputExamples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n",
    "    \"\"\"Convert a set of `InputExample`s to a list of `InputFeatures`.\"\"\"\n",
    "\n",
    "    input_ids, input_masks, segment_ids, labels = [], [], [], []\n",
    "    for example in tqdm(examples, desc=\"Converting examples to features\"):\n",
    "        input_id, input_mask, segment_id, label = convert_single_example(\n",
    "            tokenizer, example, max_seq_length\n",
    "        )\n",
    "        input_ids.append(input_id)\n",
    "        input_masks.append(input_mask)\n",
    "        segment_ids.append(segment_id)\n",
    "        labels.append(label)\n",
    "    return (\n",
    "        np.array(input_ids),\n",
    "        np.array(input_masks),\n",
    "        np.array(segment_ids),\n",
    "        np.array(labels).reshape(-1, 1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_original' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-f5a6bf385f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglue_convert_examples_to_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_original\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mrpc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_original' is not defined"
     ]
    }
   ],
   "source": [
    "train_data_original = glue_convert_examples_to_features(data_original['train'], tokenizer, max_length=128, task='mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "print(type(train_data_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#train_data_examples = convert_text_to_examples(train_data)\n",
    "\n",
    "#train_data_tokenized = convert_examples_to_features(train_data, tokenizer, max_seq_length=128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#x = tf.cast(train_data, str)\n",
    "#print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#train_data_encoded = tokenizer.encode_plus(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = TFBertForSequenceClassification.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
