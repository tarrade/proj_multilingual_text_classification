{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/root/tensorflow_datasets/glue/mrpc/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/glue/mrpc/1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "data = tensorflow_datasets.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>,\n",
       " 'train': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>,\n",
       " 'validation': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': TensorShape([]),\n",
       " 'label': TensorShape([]),\n",
       " 'sentence1': TensorShape([]),\n",
       " 'sentence2': TensorShape([])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.data.ops import dataset_ops\n",
    "dataset_ops.get_legacy_output_shapes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tf.int32,\n",
       " 'label': tf.int64,\n",
       " 'sentence1': tf.string,\n",
       " 'sentence2': tf.string}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_types(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tensorflow.python.framework.ops.Tensor,\n",
       " 'label': tensorflow.python.framework.ops.Tensor,\n",
       " 'sentence1': tensorflow.python.framework.ops.Tensor,\n",
       " 'sentence2': tensorflow.python.framework.ops.Tensor}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_classes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx', 'label', 'sentence1', 'sentence2'])\n",
      "{'idx': <tf.Tensor: shape=(), dtype=int32, numpy=1680>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'The identical rovers will act as robotic geologists , searching for evidence of past water .'>, 'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'The rovers act as robotic geologists , moving on six wheels .'>}\n",
      "tf.Tensor(1680, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(b'The identical rovers will act as robotic geologists , searching for evidence of past water .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for l in data['train']:\n",
    "    print(l.keys())\n",
    "    print(l)\n",
    "    print(l['idx'])\n",
    "    print(l['label'])\n",
    "    print(l['sentence1'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 1680, 'label': 0, 'sentence1': b'The identical rovers will act as robotic geologists , searching for evidence of past water .', 'sentence2': b'The rovers act as robotic geologists , moving on six wheels .'}\n"
     ]
    }
   ],
   "source": [
    "# get numpy array\n",
    "for element in data['train'].as_numpy_iterator(): \n",
    "    print(element) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(list(data['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 1680,\n",
       "  'label': 0,\n",
       "  'sentence1': b'The identical rovers will act as robotic geologists , searching for evidence of past water .',\n",
       "  'sentence2': b'The rovers act as robotic geologists , moving on six wheels .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['train'].take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset = valid_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample size/batch size and repeat 2 times\n",
    "math.ceil((3668/32)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1456,  1237,  5809,  1845,  3216,  1107,  2447,  2008,\n",
       "        6157,  6356,  1112, 18155,  1265,  3471,  1106,  3345,  1105,\n",
       "        2670, 24091,  1815,  1154,  1103, 24787,   119,   102,  1456,\n",
       "        1237,  5809,  3182,  1346, 12535,  6356,  2106,   117,  1112,\n",
       "       18155,  1265,  3471,  1106,  3345,  1105,  2670, 24091,  1321,\n",
       "        1103, 24787,   119,   102,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1109    ---->    T h e\n",
      "   3085    ---->    b a n k\n",
      "   1145    ---->    a l s o\n",
      "   1163    ---->    s a i d\n",
      "   1157    ---->    i t s\n",
      "   2906    ---->    o f f e r\n",
      "   1108    ---->    w a s\n",
      "   2548    ---->    s u b j e c t\n",
      "   1106    ---->    t o\n",
      "   1103    ---->    t h e\n",
      "   3311    ---->    a g r e e m e n t\n",
      "   1104    ---->    o f\n",
      "   1987    ---->    D r\n",
      "   7897    ---->    # # a x\n",
      "    112    ---->    '\n",
      "    188    ---->    s\n",
      "   2682    ---->    s e n i o r\n",
      "   5482    ---->    b a n k s\n",
      "    117    ---->    ,\n",
      "   2682    ---->    s e n i o r\n",
      "   7069    ---->    b o n d\n",
      "  14322    ---->    h o l d e r s\n",
      "   1105    ---->    a n d\n",
      "   1119    ---->    h e\n",
      "  13556    ---->    # # d g i n g\n",
      "   5482    ---->    b a n k s\n",
      "   1118    ---->    b y\n",
      "   1476    ---->    3 0\n",
      "   1347    ---->    S e p t e m b e r\n",
      "   1581    ---->    2 0 0 3\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n",
      "   1109    ---->    T h e\n",
      "   2906    ---->    o f f e r\n",
      "   1110    ---->    i s\n",
      "   1145    ---->    a l s o\n",
      "   2548    ---->    s u b j e c t\n",
      "   1106    ---->    t o\n",
      "  19085    ---->    G o l d m a n\n",
      "   6086    ---->    s i g n i n g\n",
      "   1126    ---->    a n\n",
      "   3311    ---->    a g r e e m e n t\n",
      "   1114    ---->    w i t h\n",
      "   1987    ---->    D r\n",
      "   7897    ---->    # # a x\n",
      "    112    ---->    '\n",
      "    188    ---->    s\n",
      "   2682    ---->    s e n i o r\n",
      "   5482    ---->    b a n k s\n",
      "    117    ---->    ,\n",
      "   2682    ---->    s e n i o r\n",
      "   7069    ---->    b o n d\n",
      "  14322    ---->    h o l d e r s\n",
      "   1105    ---->    a n d\n",
      "   1119    ---->    h e\n",
      "  13556    ---->    # # d g i n g\n",
      "   5482    ---->    b a n k s\n",
      "   1118    ---->    b y\n",
      "  20456    ---->    S e p t\n",
      "    119    ---->    .\n",
      "   1476    ---->    3 0\n",
      "    117    ---->    ,\n",
      "   1122    ---->    i t\n",
      "   1163    ---->    s a i d\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n"
     ]
    }
   ],
   "source": [
    "for i in list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'][0]:\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,   107,  1409, ...,     0,     0,     0],\n",
       "       [  101,  1124,  1108, ...,     0,     0,     0],\n",
       "       [  101,  1130,  1382, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  1109, 11451, ...,     0,     0,     0],\n",
       "       [  101, 13719,  2105, ...,     0,     0,     0],\n",
       "       [  101,  1188,  1214, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[ 101, 4673, 1108, ...,    0,    0,    0],\n",
       "        [ 101,  155, 4538, ...,    0,    0,    0],\n",
       "        [ 101, 1124, 1163, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1456, 1237, ...,    0,    0,    0],\n",
       "        [ 101,  146, 1458, ...,    0,    0,    0],\n",
       "        [ 101, 1332,  170, ...,    0,    0,    0]], dtype=int32),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]], dtype=int32),\n",
       " 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: [101, 1188, 1110, 170, 3014, 7758, 1106, 1129, 22559, 2200, 102]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text=list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The identical rovers will act as robotic geologists , searching for evidence of past water .'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The           ---->    [101, 1109, 102]\n",
      "identical     ---->    [101, 6742, 102]\n",
      "rovers        ---->    [101, 187, 24985, 102]\n",
      "will          ---->    [101, 1209, 102]\n",
      "act           ---->    [101, 2496, 102]\n",
      "as            ---->    [101, 1112, 102]\n",
      "robotic       ---->    [101, 24628, 102]\n",
      "geologists    ---->    [101, 25166, 1116, 102]\n",
      ",             ---->    [101, 117, 102]\n",
      "searching     ---->    [101, 6205, 102]\n",
      "for           ---->    [101, 1111, 102]\n",
      "evidence      ---->    [101, 2554, 102]\n",
      "of            ---->    [101, 1104, 102]\n",
      "past          ---->    [101, 1763, 102]\n",
      "water         ---->    [101, 1447, 102]\n",
      ".             ---->    [101, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "for word in text.split(' ') :\n",
    "    print('{:10}    ---->    {}'.format(word, tokenizer.encode(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1109    ---->    T h e\n",
      "   6742    ---->    i d e n t i c a l\n",
      "    187    ---->    r\n",
      "  24985    ---->    # # o v e r s\n",
      "   1209    ---->    w i l l\n",
      "   2496    ---->    a c t\n",
      "   1112    ---->    a s\n",
      "  24628    ---->    r o b o t i c\n",
      "  25166    ---->    g e o l o g i s t\n",
      "   1116    ---->    # # s\n",
      "    117    ---->    ,\n",
      "   6205    ---->    s e a r c h i n g\n",
      "   1111    ---->    f o r\n",
      "   2554    ---->    e v i d e n c e\n",
      "   1104    ---->    o f\n",
      "   1763    ---->    p a s t\n",
      "   1447    ---->    w a t e r\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\")):\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1109    ---->    T h e\n",
      "    187    ---->    r\n",
      "  24985    ---->    # # o v e r s\n",
      "   2496    ---->    a c t\n",
      "   1112    ---->    a s\n",
      "  24628    ---->    r o b o t i c\n",
      "  25166    ---->    g e o l o g i s t\n",
      "   1116    ---->    # # s\n",
      "    117    ---->    ,\n",
      "   2232    ---->    m o v i n g\n",
      "   1113    ---->    o n\n",
      "   1565    ---->    s i x\n",
      "   8089    ---->    w h e e l s\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence2'].decode(\"utf-8\")):\n",
    "        print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence2'].decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-e27c1f060d6c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-e27c1f060d6c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    this is an error\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# stop here\n",
    "this is an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "                    validation_data=valid_dataset, validation_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the TensorFlow model in PyTorch for inspection\n",
    "model.save_pretrained('./save/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
    "sentence_0 = \"This research was consistent with his findings.\"\n",
    "sentence_1 = \"His findings were compatible with this research.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
    "pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n",
    "\n",
    "print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
