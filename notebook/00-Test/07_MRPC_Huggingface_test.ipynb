{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import *\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/Users/tarrade/tensorflow_datasets/glue/mrpc/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/tarrade/tensorflow_datasets/glue/mrpc/1.0.0\n"
     ]
    }
   ],
   "source": [
    "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "data = tensorflow_datasets.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>,\n",
       " 'train': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>,\n",
       " 'validation': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': TensorShape([]),\n",
       " 'label': TensorShape([]),\n",
       " 'sentence1': TensorShape([]),\n",
       " 'sentence2': TensorShape([])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.data.ops import dataset_ops\n",
    "dataset_ops.get_legacy_output_shapes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tf.int32,\n",
       " 'label': tf.int64,\n",
       " 'sentence1': tf.string,\n",
       " 'sentence2': tf.string}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_types(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tensorflow.python.framework.ops.Tensor,\n",
       " 'label': tensorflow.python.framework.ops.Tensor,\n",
       " 'sentence1': tensorflow.python.framework.ops.Tensor,\n",
       " 'sentence2': tensorflow.python.framework.ops.Tensor}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_classes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx', 'label', 'sentence1', 'sentence2'])\n",
      "{'idx': <tf.Tensor: shape=(), dtype=int32, numpy=1680>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'The identical rovers will act as robotic geologists , searching for evidence of past water .'>, 'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'The rovers act as robotic geologists , moving on six wheels .'>}\n",
      "tf.Tensor(1680, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(b'The identical rovers will act as robotic geologists , searching for evidence of past water .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for l in data['train']:\n",
    "    print(l.keys())\n",
    "    print(l)\n",
    "    print(l['idx'])\n",
    "    print(l['label'])\n",
    "    print(l['sentence1'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 1680, 'label': 0, 'sentence1': b'The identical rovers will act as robotic geologists , searching for evidence of past water .', 'sentence2': b'The rovers act as robotic geologists , moving on six wheels .'}\n"
     ]
    }
   ],
   "source": [
    "# get numpy array\n",
    "for element in data['train'].as_numpy_iterator(): \n",
    "    print(element) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(list(data['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 1680,\n",
       "  'label': 0,\n",
       "  'sentence1': b'The identical rovers will act as robotic geologists , searching for evidence of past water .',\n",
       "  'sentence2': b'The rovers act as robotic geologists , moving on six wheels .'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['train'].take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset = valid_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(train_dataset.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample size/batch size and repeat 2 times\n",
    "math.ceil((3668/32)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  3473,  1103,  2732,  1425,  1111,  2742,  1107,  5726,\n",
       "        1110,  1407,   117,  1103,  1583,  2412, 21073,  1116,  1103,\n",
       "       22153,  3904,  1104,  4768,  2027, 24536,   119,   102,  3473,\n",
       "        1103,  2732,  1425,  1111,  2742,  1107,  5726,  1110,  1407,\n",
       "         117,  1103,  1583,  2412, 21073,  1116,  1103,  3904,  1104,\n",
       "        4768,  2027, 24536,  1621, 13120,   117,  1112,   144,  1183,\n",
       "        3491,  1905,  1132,  1145,  1227,   119,   102,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "  14748    ---->    G M\n",
      "    112    ---->    '\n",
      "    188    ---->    s\n",
      "   4733    ---->    o f f e r i n g\n",
      "   1110    ---->    i s\n",
      "   1145    ---->    a l s o\n",
      "   2637    ---->    e x p e c t e d\n",
      "   1106    ---->    t o\n",
      "   1511    ---->    i n c l u d e\n",
      "   1164    ---->    a b o u t\n",
      "    109    ---->    $\n",
      "    124    ---->    3\n",
      "    119    ---->    .\n",
      "    126    ---->    5\n",
      "   3775    ---->    b i l l i o n\n",
      "   1107    ---->    i n\n",
      "  28086    ---->    c o n v e r t i b l e\n",
      "  19313    ---->    s e c u r i t i e s\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n",
      "  14748    ---->    G M\n",
      "   1110    ---->    i s\n",
      "   1145    ---->    a l s o\n",
      "   2637    ---->    e x p e c t e d\n",
      "   1106    ---->    t o\n",
      "   2486    ---->    i s s u e\n",
      "    109    ---->    $\n",
      "    124    ---->    3\n",
      "    119    ---->    .\n",
      "    126    ---->    5\n",
      "   3775    ---->    b i l l i o n\n",
      "   2258    ---->    v i a\n",
      "    170    ---->    a\n",
      "  28086    ---->    c o n v e r t i b l e\n",
      "   7069    ---->    b o n d\n",
      "   4733    ---->    o f f e r i n g\n",
      "    117    ---->    ,\n",
      "   2319    ---->    m a r k e t\n",
      "   3509    ---->    s o u r c e s\n",
      "   1163    ---->    s a i d\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n"
     ]
    }
   ],
   "source": [
    "for i in list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'][0]:\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101, 10859, 19065, ...,     0,     0,     0],\n",
       "       [  101,  1284,  1132, ...,     0,     0,     0],\n",
       "       [  101,  3414,  1964, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,   148, 12888, ...,     0,     0,     0],\n",
       "       [  101,  1109,  5626, ...,     0,     0,     0],\n",
       "       [  101,  1247,  1132, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: [101, 1188, 1110, 170, 3014, 7758, 1106, 1129, 22559, 2200, 102]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The identical rovers will act as robotic geologists , searching for evidence of past water .'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1109    ---->    T h e\n",
      "   6742    ---->    i d e n t i c a l\n",
      "    187    ---->    r\n",
      "  24985    ---->    # # o v e r s\n",
      "   1209    ---->    w i l l\n",
      "   2496    ---->    a c t\n",
      "   1112    ---->    a s\n",
      "  24628    ---->    r o b o t i c\n",
      "  25166    ---->    g e o l o g i s t\n",
      "   1116    ---->    # # s\n",
      "    117    ---->    ,\n",
      "   6205    ---->    s e a r c h i n g\n",
      "   1111    ---->    f o r\n",
      "   2554    ---->    e v i d e n c e\n",
      "   1104    ---->    o f\n",
      "   1763    ---->    p a s t\n",
      "   1447    ---->    w a t e r\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\")):\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1109    ---->    T h e\n",
      "    187    ---->    r\n",
      "  24985    ---->    # # o v e r s\n",
      "   2496    ---->    a c t\n",
      "   1112    ---->    a s\n",
      "  24628    ---->    r o b o t i c\n",
      "  25166    ---->    g e o l o g i s t\n",
      "   1116    ---->    # # s\n",
      "    117    ---->    ,\n",
      "   2232    ---->    m o v i n g\n",
      "   1113    ---->    o n\n",
      "   1565    ---->    s i x\n",
      "   8089    ---->    w h e e l s\n",
      "    119    ---->    .\n",
      "    102    ---->    [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence2'].decode(\"utf-8\")):\n",
    "        print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence2'].decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-e27c1f060d6c>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-e27c1f060d6c>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    this is an error\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# stop here\n",
    "this is an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "                    validation_data=valid_dataset, validation_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the TensorFlow model in PyTorch for inspection\n",
    "model.save_pretrained('./save/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
    "sentence_0 = \"This research was consistent with his findings.\"\n",
    "sentence_1 = \"His findings were compatible with this research.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
    "pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n",
    "\n",
    "print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
