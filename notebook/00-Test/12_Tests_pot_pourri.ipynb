{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data = tensorflow_datasets.load('glue/mrpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>,\n",
       " 'train': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>,\n",
       " 'validation': <DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {idx: (), label: (), sentence1: (), sentence2: ()}, types: {idx: tf.int32, label: tf.int64, sentence1: tf.string, sentence2: tf.string}>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': TensorShape([]),\n",
       " 'label': TensorShape([]),\n",
       " 'sentence1': TensorShape([]),\n",
       " 'sentence2': TensorShape([])}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.data.ops import dataset_ops\n",
    "dataset_ops.get_legacy_output_shapes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tf.int32,\n",
       " 'label': tf.int64,\n",
       " 'sentence1': tf.string,\n",
       " 'sentence2': tf.string}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_types(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tensorflow.python.framework.ops.Tensor,\n",
       " 'label': tensorflow.python.framework.ops.Tensor,\n",
       " 'sentence1': tensorflow.python.framework.ops.Tensor,\n",
       " 'sentence2': tensorflow.python.framework.ops.Tensor}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_classes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx', 'label', 'sentence1', 'sentence2'])\n",
      "{'idx': <tf.Tensor: shape=(), dtype=int32, numpy=1680>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'sentence1': <tf.Tensor: shape=(), dtype=string, numpy=b'The identical rovers will act as robotic geologists , searching for evidence of past water .'>, 'sentence2': <tf.Tensor: shape=(), dtype=string, numpy=b'The rovers act as robotic geologists , moving on six wheels .'>}\n",
      "tf.Tensor(1680, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(b'The identical rovers will act as robotic geologists , searching for evidence of past water .', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for l in data['train']:\n",
    "    print(l.keys())\n",
    "    print(l)\n",
    "    print(l['idx'])\n",
    "    print(l['label'])\n",
    "    print(l['sentence1'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 1680, 'label': 0, 'sentence1': b'The identical rovers will act as robotic geologists , searching for evidence of past water .', 'sentence2': b'The rovers act as robotic geologists , moving on six wheels .'}\n"
     ]
    }
   ],
   "source": [
    "# get numpy array\n",
    "for element in data['train'].as_numpy_iterator(): \n",
    "    print(element) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3668,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(list(data['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3668"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(data['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'idx': 1680,\n",
       "  'label': 0,\n",
       "  'sentence1': b'The identical rovers will act as robotic geologists , searching for evidence of past water .',\n",
       "  'sentence2': b'The rovers act as robotic geologists , moving on six wheels .'}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['train'].take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='mrpc')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='mrpc')\n",
    "train_dataset = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset = valid_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids'])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  1258, 12421, ...,     0,     0,     0],\n",
       "       [  101,  1124,  1163, ...,     0,     0,     0],\n",
       "       [  101,  1573,  5567, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101,  4673,  1108, ...,     0,     0,     0],\n",
       "       [  101, 10789,  2142, ...,     0,     0,     0],\n",
       "       [  101, 23306,  1163, ...,     0,     0,     0]], dtype=int32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())[0][0]['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded string: [101, 1188, 1110, 170, 3014, 7758, 1106, 1129, 22559, 2200, 102]\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode(\"This is a simple input to be tokenized\")\n",
    "\n",
    "print(\"Encoded string: {}\".format(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The identical rovers will act as robotic geologists , searching for evidence of past water .'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1109,\n",
       " 6742,\n",
       " 187,\n",
       " 24985,\n",
       " 1209,\n",
       " 2496,\n",
       " 1112,\n",
       " 24628,\n",
       " 25166,\n",
       " 1116,\n",
       " 117,\n",
       " 6205,\n",
       " 1111,\n",
       " 2554,\n",
       " 1104,\n",
       " 1763,\n",
       " 1447,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence1'].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1109,\n",
       " 187,\n",
       " 24985,\n",
       " 2496,\n",
       " 1112,\n",
       " 24628,\n",
       " 25166,\n",
       " 1116,\n",
       " 117,\n",
       " 2232,\n",
       " 1113,\n",
       " 1565,\n",
       " 8089,\n",
       " 119,\n",
       " 102]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(list(data['train'].take(1).as_numpy_iterator())[0]['sentence2'].decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "                    validation_data=valid_dataset, validation_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the TensorFlow model in PyTorch for inspection\n",
    "model.save_pretrained('./save/')\n",
    "pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
    "sentence_0 = \"This research was consistent with his findings.\"\n",
    "sentence_1 = \"His findings were compatible with this research.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
    "pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n",
    "\n",
    "print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TF_Config env variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG']='{\"cluster\": {\"chief\": [\"127.0.0.1:2222\"]}, \"task\": {\"type\": \"chief\", \"index\": 0, \"trial\": \"2\", \"cloud\": \"cxxx-ml\"}, \"job\": {\"package_uris\": [\"gs://xxx/tf_bert_classification_2020_05_12_141041/b0d287bd012da97a62f662b2e9df291e899c2e9705ca751a5a0f7dcaf2849765/bert_model-0.1.tar.gz\"], \"python_module\": \"model.tf_bert_classification.task\", \"args\": [\"--epochs=1\", \"--steps_per_epoch_train=5\", \"--batch_size_train=32\", \"--steps_per_epoch_eval=1\", \"--batch_size_eval=64\", \"--input_eval_tfrecords=gs://multilingual_text_classification/tfrecord/sst2/bert-base-multilingual-uncased/valid\", \"--input_train_tfrecords=gs://xxx/tfrecord/sst2/bert-base-multilingual-uncased/train\", \"--output_dir=gs://xxx/training_model_gcp/tf_bert_classification_2020_05_12_141041\", \"--pretrained_model_dir=gs://xxx/pretrained_model/bert-base-multilingual-uncased\", \"--verbosity_level=INFO\", \"--epsilon\", \"1.7788921050163616e-06\", \"--learning_rate\", \"0.0007763625134788308\"], \"hyperparameters\": {\"goal\": \"MAXIMIZE\", \"params\": [{\"parameter_name\": \"learning_rate\", \"min_value\": 1e-08, \"max_value\": 1.0, \"type\": \"DOUBLE\", \"scale_type\": \"UNIT_LOG_SCALE\"}, {\"parameter_name\": \"epsilon\", \"min_value\": 1e-09, \"max_value\": 1.0, \"type\": \"DOUBLE\", \"scale_type\": \"UNIT_LOG_SCALE\"}], \"max_trials\": 3, \"max_parallel_trials\": 2, \"hyperparameter_metric_tag\": \"accuracy_train\", \"enable_trial_early_stopping\": true, \"max_failed_trials\": 1}, \"region\": \"europe-west4\", \"runtime_version\": \"2.1\", \"run_on_raw_vm\": true, \"python_version\": \"3.7\"}, \"environment\": \"cloud\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"cluster\": {\"chief\": [\"127.0.0.1:2222\"]}, \"task\": {\"type\": \"chief\", \"index\": 0, \"trial\": \"2\", \"cloud\": \"cxxx-ml\"}, \"job\": {\"package_uris\": [\"gs://xxx/tf_bert_classification_2020_05_12_141041/b0d287bd012da97a62f662b2e9df291e899c2e9705ca751a5a0f7dcaf2849765/bert_model-0.1.tar.gz\"], \"python_module\": \"model.tf_bert_classification.task\", \"args\": [\"--epochs=1\", \"--steps_per_epoch_train=5\", \"--batch_size_train=32\", \"--steps_per_epoch_eval=1\", \"--batch_size_eval=64\", \"--input_eval_tfrecords=gs://multilingual_text_classification/tfrecord/sst2/bert-base-multilingual-uncased/valid\", \"--input_train_tfrecords=gs://xxx/tfrecord/sst2/bert-base-multilingual-uncased/train\", \"--output_dir=gs://xxx/training_model_gcp/tf_bert_classification_2020_05_12_141041\", \"--pretrained_model_dir=gs://xxx/pretrained_model/bert-base-multilingual-uncased\", \"--verbosity_level=INFO\", \"--epsilon\", \"1.7788921050163616e-06\", \"--learning_rate\", \"0.0007763625134788308\"], \"hyperparameters\": {\"goal\": \"MAXIMIZE\", \"params\": [{\"parameter_name\": \"learning_rate\", \"min_value\": 1e-08, \"max_value\": 1.0, \"type\": \"DOUBLE\", \"scale_type\": \"UNIT_LOG_SCALE\"}, {\"parameter_name\": \"epsilon\", \"min_value\": 1e-09, \"max_value\": 1.0, \"type\": \"DOUBLE\", \"scale_type\": \"UNIT_LOG_SCALE\"}], \"max_trials\": 3, \"max_parallel_trials\": 2, \"hyperparameter_metric_tag\": \"accuracy_train\", \"enable_trial_early_stopping\": true, \"max_failed_trials\": 1}, \"region\": \"europe-west4\", \"runtime_version\": \"2.1\", \"run_on_raw_vm\": true, \"python_version\": \"3.7\"}, \"environment\": \"cloud\"}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['TF_CONFIG']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'package_uris': ['gs://xxx/tf_bert_classification_2020_05_12_141041/b0d287bd012da97a62f662b2e9df291e899c2e9705ca751a5a0f7dcaf2849765/bert_model-0.1.tar.gz'],\n",
       " 'python_module': 'model.tf_bert_classification.task',\n",
       " 'args': ['--epochs=1',\n",
       "  '--steps_per_epoch_train=5',\n",
       "  '--batch_size_train=32',\n",
       "  '--steps_per_epoch_eval=1',\n",
       "  '--batch_size_eval=64',\n",
       "  '--input_eval_tfrecords=gs://multilingual_text_classification/tfrecord/sst2/bert-base-multilingual-uncased/valid',\n",
       "  '--input_train_tfrecords=gs://xxx/tfrecord/sst2/bert-base-multilingual-uncased/train',\n",
       "  '--output_dir=gs://xxx/training_model_gcp/tf_bert_classification_2020_05_12_141041',\n",
       "  '--pretrained_model_dir=gs://xxx/pretrained_model/bert-base-multilingual-uncased',\n",
       "  '--verbosity_level=INFO',\n",
       "  '--epsilon',\n",
       "  '1.7788921050163616e-06',\n",
       "  '--learning_rate',\n",
       "  '0.0007763625134788308'],\n",
       " 'hyperparameters': {'goal': 'MAXIMIZE',\n",
       "  'params': [{'parameter_name': 'learning_rate',\n",
       "    'min_value': 1e-08,\n",
       "    'max_value': 1.0,\n",
       "    'type': 'DOUBLE',\n",
       "    'scale_type': 'UNIT_LOG_SCALE'},\n",
       "   {'parameter_name': 'epsilon',\n",
       "    'min_value': 1e-09,\n",
       "    'max_value': 1.0,\n",
       "    'type': 'DOUBLE',\n",
       "    'scale_type': 'UNIT_LOG_SCALE'}],\n",
       "  'max_trials': 3,\n",
       "  'max_parallel_trials': 2,\n",
       "  'hyperparameter_metric_tag': 'accuracy_train',\n",
       "  'enable_trial_early_stopping': True,\n",
       "  'max_failed_trials': 1},\n",
       " 'region': 'europe-west4',\n",
       " 'runtime_version': '2.1',\n",
       " 'run_on_raw_vm': True,\n",
       " 'python_version': '3.7'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"job\",{}) #.get(\"args\",{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "params=json.loads(os.environ.get(\"TF_CONFIG\", \"{}\")).get(\"job\",{}).get(\"hyperparameters\",{}).get(\"params\",{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HParam(name='learning_rate', domain=RealInterval(1e-08, 1.0), display_name=None, description=None), HParam(name='epsilon', domain=RealInterval(1e-09, 1.0), display_name=None, description=None)]\n"
     ]
    }
   ],
   "source": [
    "list_hp=[]\n",
    "for el in params:\n",
    "    hp_dict=dict(el)\n",
    "    if hp_dict.get('type')=='DOUBLE':\n",
    "        list_hp.append(hp.HParam(hp_dict.get('parameter_name'), hp.RealInterval(hp_dict.get('min_value'), hp_dict.get('max_value'))))\n",
    "\n",
    "print(list_hp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from absl import flags\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def del_all_flags(FLAGS):\n",
    "    flags_dict = FLAGS._flags()    \n",
    "    keys_list = [keys for keys in flags_dict]    \n",
    "    for keys in keys_list:\n",
    "        FLAGS.__delattr__(keys)\n",
    "    \n",
    "del_all_flags(flags.FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "flags.DEFINE_string('f', '', 'kernel') # just for jupyter notebook and avoir : \"UnrecognizedFlagError: Unknown command line flag 'f'\"\n",
    "flags.DEFINE_float('float', 3.14, 'using floats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "FLAGS(sys.argv);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key doesn t exist\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    FLAGS['float2'].value\n",
    "except KeyError:\n",
    "    print('key doesn t exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## TF.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.1.0\n",
      "  Using cached tensorflow-2.1.0-cp37-cp37m-macosx_10_11_x86_64.whl (120.8 MB)\n",
      "Collecting tensorflow-estimator<2.2.0,>=2.1.0rc0\n",
      "  Using cached tensorflow_estimator-2.1.0-py2.py3-none-any.whl (448 kB)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (3.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.24.3)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.9.0)\n",
      "Processing /Users/tarrade/Library/Caches/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd/gast-0.2.2-cp37-none-any.whl\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (3.2.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.34.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Collecting astor>=0.6.0\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting keras-applications>=1.0.8\n",
      "  Using cached Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.12.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.14.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.18.1)\n",
      "Collecting tensorboard<2.2.0,>=2.1.0\n",
      "  Using cached tensorboard-2.1.1-py3-none-any.whl (3.8 MB)\n",
      "Requirement already satisfied: setuptools in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from protobuf>=3.8.0->tensorflow==2.1.0) (46.1.3.post20200330)\n",
      "Requirement already satisfied: h5py in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.23.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.0.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.11.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2020.4.5.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/tarrade/anaconda-release/conda-env/env_test/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "\u001b[31mERROR: bert-model 0.1 requires cloud-tpu-client==0.8, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: bert-model 0.1 requires google-cloud-logging==1.15.0, which is not installed.\u001b[0m\n",
      "\u001b[31mERROR: bert-model 0.1 has requirement google-cloud-bigquery==1.24.0, but you'll have google-cloud-bigquery 1.17.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: bert-model 0.1 has requirement pip==20.1, but you'll have pip 20.0.2 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: bert-model 0.1 has requirement tensorboard==2.2.1, but you'll have tensorboard 2.1.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: bert-model 0.1 has requirement tensorflow==2.2.0, but you'll have tensorflow 2.1.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: bert-model 0.1 has requirement transformers==2.9.0, but you'll have transformers 2.8.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement absl-py<0.9,>=0.1.6, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement apache-beam[gcp]<2.18,>=2.17, but you'll have apache-beam 2.18.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx 0.21.2 has requirement grpcio!=1.27.2,<2,>=1.25, but you'll have grpcio 1.24.3 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tfx-bsl 0.21.4 has requirement absl-py<0.9,>=0.7, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-transform 0.21.2 has requirement absl-py<0.9,>=0.7, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-model-analysis 0.21.6 has requirement absl-py<0.9,>=0.7, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement absl-py<0.9,>=0.7, but you'll have absl-py 0.9.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow-data-validation 0.21.5 has requirement pandas<1,>=0.24, but you'll have pandas 1.0.1 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorflow-estimator, gast, astor, keras-applications, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.2.0\n",
      "    Uninstalling tensorflow-estimator-2.2.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.2.0\n",
      "    Uninstalling tensorboard-2.2.0:\n",
      "      Successfully uninstalled tensorboard-2.2.0\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.2.0rc4\n",
      "    Uninstalling tensorflow-2.2.0rc4:\n",
      "      Successfully uninstalled tensorflow-2.2.0rc4\n",
      "Successfully installed astor-0.8.1 gast-0.2.2 keras-applications-1.0.8 tensorboard-2.1.1 tensorflow-2.1.0 tensorflow-estimator-2.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    }
   ],
   "source": [
    "strategy = tf.distribute.MirroredStrategy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "a = tf.constant([2, 2], name=\"vector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "b = tf.constant(2.0, name=\"scalar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 2], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    writer = tf.summary.create_file_writer(\"mylogs\")\n",
    "    with writer.as_default():\n",
    "        tf.summary.scalar('metric_accuracy', data=1.0, step=1)\n",
    "        tf.summary.scalar('metric_accuracy_2', data=3.0, step=1)\n",
    "        tf.summary.scalar('test', data=b, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorboard.backend.event_processing.event_accumulator.EventAccumulator at 0x63ce43850>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_acc = event_accumulator.EventAccumulator(\"mylogs\")\n",
    "event_acc.Reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'images': [],\n",
       " 'audio': [],\n",
       " 'histograms': [],\n",
       " 'scalars': [],\n",
       " 'distributions': [],\n",
       " 'tensors': ['metric_accuracy', 'metric_accuracy_2', 'test'],\n",
       " 'graph': False,\n",
       " 'meta_graph': False,\n",
       " 'run_metadata': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event_acc.Tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: metric_accuracy value: 1.0 step: 1\n",
      "name: metric_accuracy_2 value: 3.0 step: 1\n",
      "name: test value: 2.0 step: 1\n"
     ]
    }
   ],
   "source": [
    "for tag in sorted(event_acc.Tags()['tensors']):\n",
    "    for scalar_event in event_acc.Tensors(tag):\n",
    "        print('name: {} value: {} step: {}'.format(tag, tf.make_ndarray(scalar_event.tensor_proto), scalar_event.step))\n",
    "        #print(scalar_event.step)\n",
    "        #print(tf.make_ndarray(scalar_event.tensor_proto))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': ['epoch_loss', 'epoch_accuracy'], 'distributions': [], 'tensors': ['testtesttest_1'], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "event_acc = event_accumulator.EventAccumulator('gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/tf_bert_classification_hp_tuning_cpu_2020_06_10_095311/tensorboard/trial_id_1/validation')\n",
    "event_acc.Reload()\n",
    "print(event_acc.Tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': ['epoch_loss', 'epoch_accuracy'], 'distributions': [], 'tensors': ['testtesttest_1'], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "event_acc = event_accumulator.EventAccumulator('gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/tf_bert_classification_hp_tuning_cpu_2020_06_10_095311/tensorboard/trial_id_1/validation')\n",
    "event_acc.Reload()\n",
    "print(event_acc.Tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': ['epoch_accuracy', 'epoch_loss'], 'distributions': [], 'tensors': ['metric_accuracy_train_epoch', 'testtesttest_1'], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "event_acc = event_accumulator.EventAccumulator('gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/tf_bert_classification_hp_tuning_cpu_2020_06_10_134028/tensorboard/trial_id_1/validation')\n",
    "event_acc.Reload()\n",
    "print(event_acc.Tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': [], 'distributions': [], 'tensors': [], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "event_acc = event_accumulator.EventAccumulator('gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/tf_bert_classification_hp_tuning_cpu_2020_06_10_134028/tensorboard/trial_id_1/train')\n",
    "print(event_acc.Tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "2020_06_10_113245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'images': [], 'audio': [], 'histograms': [], 'scalars': ['epoch_accuracy', 'epoch_loss'], 'distributions': [], 'tensors': ['metric_accuracy_train_epoch', 'testtesttest_1'], 'graph': False, 'meta_graph': False, 'run_metadata': []}\n"
     ]
    }
   ],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "event_acc = event_accumulator.EventAccumulator('gs://'+os.environ['BUCKET_NAME']+'/training_model_gcp/tf_bert_classification_hp_tuning_cpu_2020_06_10_134028/tensorboard/trial_id_1/validation')\n",
    "event_acc.Reload()\n",
    "print(event_acc.Tags())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: metric_accuracy_train_epoch value: 0.5106981992721558 step: 0\n",
      "name: testtesttest_1 value: 0.5106981992721558 step: 0\n"
     ]
    }
   ],
   "source": [
    "for tag in sorted(event_acc.Tags()['tensors']):\n",
    "    for scalar_event in event_acc.Tensors(tag):\n",
    "        print('name: {} value: {} step: {}'.format(tag, tf.make_ndarray(scalar_event.tensor_proto), scalar_event.step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_accuracy\n",
      "0 0.46875\n",
      "epoch_loss\n",
      "0 0.71041339635849\n"
     ]
    }
   ],
   "source": [
    "for tag in sorted(event_acc.Tags()['scalars']):\n",
    "    print(tag)\n",
    "    for scalar_event in event_acc.Scalars(tag):\n",
    "        print(scalar_event.step, scalar_event.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
