{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os, pathlib, sys, re, string, spacy, bs4, apache_beam as beam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Working Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "workingdir=os.getcwd()\n",
    "d=[d for d in os.listdir(workingdir)]\n",
    "n=0\n",
    "while not set(['notebook']).issubset(set(d)):\n",
    "    workingdir=str(pathlib.Path(workingdir).parents[0])\n",
    "\n",
    "    d=[d for d in os.listdir(str(workingdir))]\n",
    "    n+=1\n",
    "    if n>5:\n",
    "        break\n",
    "sys.path.insert(0, workingdir)\n",
    "os.chdir(workingdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Configuring spaCy for NLP Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Apache Beam and GCP Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pipeline_options = beam.options.pipeline_options.PipelineOptions()\n",
    "gcp_options = beam.options.pipeline_options.GoogleCloudOptions\n",
    "standard_options = beam.options.pipeline_options.StandardOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "google_cloud_options = pipeline_options.view_as(gcp_options)\n",
    "google_cloud_options.project = 'nlp-text-classification'\n",
    "google_cloud_options.job_name = 'stackoverflow-preprocessing-2'\n",
    "google_cloud_options.region = 'us-central1'\n",
    "google_cloud_options.staging_location = 'gs://nlp-text-classification/beam/stage'\n",
    "google_cloud_options.temp_location = 'gs://nlp-text-classification/beam/temp'\n",
    "pipeline_options.view_as(standard_options).runner = 'DataflowRunner'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Creating a DoFn Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class CleanText(beam.DoFn):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def __decode_html(self, input_str: str) -> str:\n",
    "        self.soup = bs4.BeautifulSoup(input_str, 'html.parser')\n",
    "        self.output = self.soup.text\n",
    "        return self.output\n",
    "\n",
    "    def __nlp(self, input_str: str) -> list:\n",
    "        self.doc = self.spacy(input_str)\n",
    "        self.stopwords = list(string.punctuation + string.digits) + ['-pron-']\n",
    "        self.output = [token.lemma_.lower() for token in self.doc if not token.is_stop \n",
    "                  and token.lemma_.lower() not in self.stopwords]\n",
    "        return ' '.join(self.output)\n",
    "\n",
    "    def process(self, element):\n",
    "        \n",
    "        return element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Local Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "local_file = 'data/beam_test.csv'\n",
    "if os.path.exists('data/beam_output.txt'):\n",
    "    os.remove('data/beam_output.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with beam.Pipeline(argv=sys.argv) as p:\n",
    "    file = p                  | \"ReadLocalFile\" >> beam.io.ReadFromText(local_file)\n",
    "    table = file              | \"CreateDictionary\"  >> beam.ParDo(Split())\n",
    "    clean_text = table        | \"ProcessFields\" >> beam.ParDo(CleanText())\n",
    "    clean_text                | \"WriteLocalFile\" >> beam.io.WriteToText('data/beam_output.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## GCP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "query = '''SELECT\n",
    "  id,\n",
    "  title,\n",
    "  body,\n",
    "  tags\n",
    "FROM\n",
    "  `nlp-text-classification.stackoverflow.posts_p1_subset`'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "table_schema = {'fields': [\n",
    "    {'name': 'id', 'type': 'INTEGER', 'mode': 'NULLABLE'},\n",
    "    {'name': 'title', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "    {'name': 'body', 'type': 'STRING', 'mode': 'NULLABLE'},\n",
    "    {'name': 'tags', 'type': 'STRING', 'mode': 'REPEATED'},\n",
    "]}\n",
    "new_table = beam.io.gcp.internal.clients.bigquery.TableReference(\n",
    "    projectId='nlp-text-classification',\n",
    "    datasetId='stackoverflow',\n",
    "    tableId='posts_p2_subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-c1e88fc6-ed60-4317-97db-97d53e88cf61.json']\n",
      "WARNING:root:Discarding unparseable args: ['-f', '/root/.local/share/jupyter/runtime/kernel-c1e88fc6-ed60-4317-97db-97d53e88cf61.json']\n"
     ]
    },
    {
     "ename": "DataflowRuntimeException",
     "evalue": "Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/apache_beam/internal/pickler.py\", line 261, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 577, in _load_type\n    return _reverse_typemap[name]\nKeyError: 'ClassType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/dataflow_worker/batchworker.py\", line 649, in do_work\n    work_executor.execute()\n  File \"/usr/local/lib/python3.6/site-packages/dataflow_worker/executor.py\", line 176, in execute\n    op.start()\n  File \"apache_beam/runners/worker/operations.py\", line 587, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 588, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 589, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 220, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 224, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 535, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"apache_beam/runners/worker/operations.py\", line 540, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"/usr/local/lib/python3.6/site-packages/apache_beam/internal/pickler.py\", line 265, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 577, in _load_type\n    return _reverse_typemap[name]\nKeyError: 'ClassType'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bff066a77796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m                                                     \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                     \u001b[0mwrite_disposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBigQueryDisposition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWRITE_TRUNCATE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                                                     create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)\n\u001b[0m",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    425\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mvisit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisitor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/.conda-env/env_nlp_text_class/lib/python3.6/site-packages/apache_beam/runners/dataflow/dataflow_runner.py\u001b[0m in \u001b[0;36mwait_until_finish\u001b[0;34m(self, duration)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         raise DataflowRuntimeException(\n\u001b[1;32m   1346\u001b[0m             \u001b[0;34m'Dataflow pipeline failed. State: %s, Error:\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1347\u001b[0;31m             (self.state, getattr(self._runner, 'last_error_msg', None)), self)\n\u001b[0m\u001b[1;32m   1348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDataflowRuntimeException\u001b[0m: Dataflow pipeline failed. State: FAILED, Error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/apache_beam/internal/pickler.py\", line 261, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 577, in _load_type\n    return _reverse_typemap[name]\nKeyError: 'ClassType'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.6/site-packages/dataflow_worker/batchworker.py\", line 649, in do_work\n    work_executor.execute()\n  File \"/usr/local/lib/python3.6/site-packages/dataflow_worker/executor.py\", line 176, in execute\n    op.start()\n  File \"apache_beam/runners/worker/operations.py\", line 587, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 588, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 589, in apache_beam.runners.worker.operations.DoOperation.start\n  File \"apache_beam/runners/worker/operations.py\", line 220, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 224, in apache_beam.runners.worker.operations.Operation.start\n  File \"apache_beam/runners/worker/operations.py\", line 535, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"apache_beam/runners/worker/operations.py\", line 540, in apache_beam.runners.worker.operations.DoOperation.setup\n  File \"/usr/local/lib/python3.6/site-packages/apache_beam/internal/pickler.py\", line 265, in loads\n    return dill.loads(s)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 317, in loads\n    return load(file, ignore)\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 305, in load\n    obj = pik.load()\n  File \"/usr/local/lib/python3.6/site-packages/dill/_dill.py\", line 577, in _load_type\n    return _reverse_typemap[name]\nKeyError: 'ClassType'\n"
     ]
    }
   ],
   "source": [
    "with beam.Pipeline(options=pipeline_options) as p:\n",
    "    table = p                 | \"QueryTable\" >> beam.io.Read(beam.io.BigQuerySource(\n",
    "                                                    query=query,\n",
    "                                                    use_standard_sql=True))\n",
    "    clean_text = table        | \"Preprocessing\" >> beam.ParDo(CleanText())\n",
    "    clean_text                | \"WriteTable\" >> beam.io.WriteToBigQuery(\n",
    "                                                    new_table,\n",
    "                                                    schema=table_schema,\n",
    "                                                    write_disposition=beam.io.BigQueryDisposition.WRITE_TRUNCATE,\n",
    "                                                    create_disposition=beam.io.BigQueryDisposition.CREATE_IF_NEEDED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_nlp_text_class]",
   "language": "python",
   "name": "conda-env-env_nlp_text_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
