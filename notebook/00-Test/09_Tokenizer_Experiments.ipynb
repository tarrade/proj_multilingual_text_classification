{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Tokenizer Experiments\n",
    "\n",
    "This notebook experiments with how the tokenizers of NLP models like BERT and GPT-2 handle certain characters. This encompasses non-linguistic characters like '\\' and html inputs like \\<br>, but also \"Umlaute\" and accents. Since we aim to look at multiple languages, special characters that are used in French and German are especially examined. \n",
    "\n",
    "This notebook also aims to explore in what way we need to preprocess language data in the future, especially when compared to classical machine learning.\n",
    "\n",
    "As a reference please consult https://huggingface.co/transformers/main_classes/tokenizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export REGION=your_region`  \n",
       "`export MODEL_DIR_ESTIMATOR_PATH=your_path_to_save_model` \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import (BertTokenizer,\n",
    "                          GPT2Tokenizer,\n",
    "                          glue_convert_examples_to_features,\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing PATH_DATASETS\n"
     ]
    }
   ],
   "source": [
    "#try:\n",
    "#    data_dir=os.environ['PATH_DATASETS']\n",
    "#except:\n",
    "#    print('missing PATH_DATASETS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#print(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tokenizer Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "General differences between the tokenizers of BERT and GPT-2:\n",
    "\n",
    "- the style of decoding\n",
    "- the type of special tokens that are added to the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7cfb7578fb4027a01e01d5a3c6ca1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f574b406790640a48ecc28c13c60ba75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9ed41beb73141d0b8955d46256ed4bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=254365.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa668fc99430494eb950d94bc10b712a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5365857cfe6945d791f360b954dfdc9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f92fd00ed2410e937772a46728c8d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b28d1c71ea845008d2d23b46287427b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "tokenizer_bert_cased = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer_bert_uncased = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_bert_german = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "# did not find a french-specific tokenizer yet\n",
    "#tokenizer_bert_french = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer_bert_multi_cased = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer_bert_multi_uncased = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# GPT-2\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Language-Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sequence_en = \"This is testing \\what happens to cases<br></br> with words containing hyphens like mother-in-law. Also, what happens to the second sentence?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      " ['This', 'is', 'testing', '\\\\', 'what', 'happens', 'to', 'cases', '<', 'br', '>', '<', '/', 'br', '>', 'with', 'words', 'containing', 'h', '##y', '##phe', '##ns', 'like', 'mother', '-', 'in', '-', 'law', '.', 'Also', ',', 'what', 'happens', 'to', 'the', 'second', 'sentence', '?'] \n",
      "Length: 38\n",
      "\n",
      "Uncased, English sequence:\n",
      " ['this', 'is', 'testing', '\\\\', 'what', 'happens', 'to', 'cases', '<', 'br', '>', '<', '/', 'br', '>', 'with', 'words', 'containing', 'h', '##yp', '##hen', '##s', 'like', 'mother', '-', 'in', '-', 'law', '.', 'also', ',', 'what', 'happens', 'to', 'the', 'second', 'sentence', '?'] \n",
      "Length: 38\n",
      "\n",
      "Cased, multilingual sequence:\n",
      " ['This', 'is', 'testing', '\\\\', 'what', 'happens', 'to', 'cases', '<', 'br', '>', '<', '/', 'br', '>', 'with', 'words', 'containing', 'hy', '##phen', '##s', 'like', 'mother', '-', 'in', '-', 'law', '.', 'Also', ',', 'what', 'happens', 'to', 'the', 'second', 'sentence', '?'] \n",
      "Length: 37\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      " ['this', 'is', 'testing', '\\\\', 'what', 'happens', 'to', 'cases', '<', 'br', '>', '<', '/', 'br', '>', 'with', 'words', 'containing', 'hy', '##phe', '##ns', 'like', 'mother', '-', 'in', '-', 'law', '.', 'also', ',', 'what', 'happens', 'to', 'the', 'second', 'sentence', '?'] \n",
      "Length: 37\n"
     ]
    }
   ],
   "source": [
    "bert_cased_tokenized_sequence = tokenizer_bert_cased.tokenize(sequence_en)\n",
    "bert_uncased_tokenized_sequence = tokenizer_bert_uncased.tokenize(sequence_en)\n",
    "bert_multi_cased_tokenized_sequence = tokenizer_bert_multi_cased.tokenize(sequence_en)\n",
    "bert_multi_uncased_tokenized_sequence = tokenizer_bert_multi_uncased.tokenize(sequence_en)\n",
    "print(\"Cased, English sequence:\\n\",bert_cased_tokenized_sequence,\"\\nLength:\",len(bert_cased_tokenized_sequence))\n",
    "print(\"\\nUncased, English sequence:\\n\",bert_uncased_tokenized_sequence,\"\\nLength:\",len(bert_uncased_tokenized_sequence))\n",
    "print(\"\\nCased, multilingual sequence:\\n\",bert_multi_cased_tokenized_sequence,\"\\nLength:\",len(bert_multi_cased_tokenized_sequence))\n",
    "print(\"\\nUncased, multilingual sequence:\\n\",bert_multi_uncased_tokenized_sequence,\"\\nLength:\",len(bert_multi_uncased_tokenized_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- When special characters are not removed from the text, they get tokenized separately.\n",
    "- The difference between the cased and uncased sequences (English) are that uncased sequences are lower case only (which was to be expected). Also, the word \"hyphen\" was split up differently.\n",
    "- The multilingual tokenizers work slightly differently than the English ones. The only difference in this specific sequence concerns the tokenization of the word \"hyphen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Token IDs: [1188, 1110, 5193, 165, 1184, 5940, 1106, 2740, 133, 9304, 135, 133, 120, 9304, 135, 1114, 1734, 4051, 177, 1183, 27801, 2316, 1176, 1534, 118, 1107, 118, 1644, 119, 2907, 117, 1184, 5940, 1106, 1103, 1248, 5650, 136] \n",
      "Length: 38\n",
      "\n",
      "Uncased, English sequence:\n",
      "Token IDs: [2023, 2003, 5604, 1032, 2054, 6433, 2000, 3572, 1026, 7987, 1028, 1026, 1013, 7987, 1028, 2007, 2616, 4820, 1044, 22571, 10222, 2015, 2066, 2388, 1011, 1999, 1011, 2375, 1012, 2036, 1010, 2054, 6433, 2000, 1996, 2117, 6251, 1029] \n",
      "Length: 38\n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Token IDs: [10747, 10124, 38306, 165, 12976, 105315, 10114, 16480, 133, 33989, 135, 133, 120, 33989, 135, 10169, 21296, 27248, 15165, 60383, 10107, 11850, 15293, 118, 10106, 118, 13255, 119, 20593, 117, 12976, 105315, 10114, 10105, 11132, 49219, 136] \n",
      "Length: 37\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Token IDs: [10372, 10127, 32311, 139, 11523, 76959, 10114, 16379, 133, 18710, 135, 133, 120, 18710, 135, 10171, 18281, 26648, 13051, 47607, 10933, 11531, 13907, 118, 10104, 118, 11785, 119, 10398, 117, 11523, 76959, 10114, 10103, 10981, 45261, 136] \n",
      "Length: 37\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_cased = tokenizer_bert_cased.convert_tokens_to_ids(bert_cased_tokenized_sequence)\n",
    "tokens_ids_uncased = tokenizer_bert_uncased.convert_tokens_to_ids(bert_uncased_tokenized_sequence)\n",
    "tokens_ids_multi_cased = tokenizer_bert_multi_cased.convert_tokens_to_ids(bert_multi_cased_tokenized_sequence)\n",
    "tokens_ids_multi_uncased = tokenizer_bert_multi_uncased.convert_tokens_to_ids(bert_multi_uncased_tokenized_sequence)\n",
    "print(\"Cased, English sequence:\\nToken IDs:\",tokens_ids_cased,\"\\nLength:\",len(tokens_ids_cased))\n",
    "print(\"\\nUncased, English sequence:\\nToken IDs:\",tokens_ids_uncased,\"\\nLength:\",len(tokens_ids_uncased))\n",
    "print(\"\\nCased, multilingual sequence:\\nToken IDs:\",tokens_ids_multi_cased,\"\\nLength:\",len(tokens_ids_multi_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\nToken IDs:\",tokens_ids_multi_uncased,\"\\nLength:\",len(tokens_ids_multi_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1188    ---->    T h i s\n",
      "   1110    ---->    i s\n",
      "   5193    ---->    t e s t i n g\n",
      "    165    ---->    \\\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   2740    ---->    c a s e s\n",
      "    133    ---->    <\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "    133    ---->    <\n",
      "    120    ---->    /\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "   1114    ---->    w i t h\n",
      "   1734    ---->    w o r d s\n",
      "   4051    ---->    c o n t a i n i n g\n",
      "    177    ---->    h\n",
      "   1183    ---->    # # y\n",
      "  27801    ---->    # # p h e\n",
      "   2316    ---->    # # n s\n",
      "   1176    ---->    l i k e\n",
      "   1534    ---->    m o t h e r\n",
      "    118    ---->    -\n",
      "   1107    ---->    i n\n",
      "    118    ---->    -\n",
      "   1644    ---->    l a w\n",
      "    119    ---->    .\n",
      "   2907    ---->    A l s o\n",
      "    117    ---->    ,\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   1103    ---->    t h e\n",
      "   1248    ---->    s e c o n d\n",
      "   5650    ---->    s e n t e n c e\n",
      "    136    ---->    ?\n"
     ]
    }
   ],
   "source": [
    "for i in tokens_ids_cased:\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer_bert_cased.decode(int(i))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "All tokens have different ids because the two tokenizers have a different number of total tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      " [101, 1188, 1110, 5193, 165, 1184, 5940, 1106, 2740, 133, 9304, 135, 133, 120, 9304, 135, 1114, 1734, 4051, 177, 1183, 27801, 2316, 1176, 1534, 118, 1107, 118, 1644, 119, 2907, 117, 1184, 5940, 1106, 1103, 1248, 5650, 136, 102] \n",
      "Length: 40\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      " [101, 10372, 10127, 32311, 139, 11523, 76959, 10114, 16379, 133, 18710, 135, 133, 120, 18710, 135, 10171, 18281, 26648, 13051, 47607, 10933, 11531, 13907, 118, 10104, 118, 11785, 119, 10398, 117, 11523, 76959, 10114, 10103, 10981, 45261, 136, 102] \n",
      "Length: 39\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_with_special_cased = tokenizer_bert_cased.build_inputs_with_special_tokens(tokens_ids_cased)\n",
    "tokens_ids_with_special_multi_uncased = tokenizer_bert_multi_uncased.build_inputs_with_special_tokens(tokens_ids_multi_uncased)\n",
    "print(\"Cased, English sequence:\\n\",tokens_ids_with_special_cased,\"\\nLength:\", len(tokens_ids_with_special_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\n\",tokens_ids_with_special_multi_uncased,\"\\nLength:\", len(tokens_ids_with_special_multi_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1188    ---->    T h i s\n",
      "   1110    ---->    i s\n",
      "   5193    ---->    t e s t i n g\n",
      "    165    ---->    \\\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   2740    ---->    c a s e s\n",
      "    133    ---->    <\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "    133    ---->    <\n",
      "    120    ---->    /\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "   1114    ---->    w i t h\n",
      "   1734    ---->    w o r d s\n",
      "   4051    ---->    c o n t a i n i n g\n",
      "    177    ---->    h\n",
      "   1183    ---->    # # y\n",
      "  27801    ---->    # # p h e\n",
      "   2316    ---->    # # n s\n",
      "   1176    ---->    l i k e\n",
      "   1534    ---->    m o t h e r\n",
      "    118    ---->    -\n",
      "   1107    ---->    i n\n",
      "    118    ---->    -\n",
      "   1644    ---->    l a w\n",
      "    119    ---->    .\n",
      "   2907    ---->    A l s o\n",
      "    117    ---->    ,\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   1103    ---->    t h e\n",
      "   1248    ---->    s e c o n d\n",
      "   5650    ---->    s e n t e n c e\n",
      "    136    ---->    ?\n",
      "    102    ---->    [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for i in tokens_ids_with_special_cased:\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer_bert_cased.decode(int(i))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Special tokens are simply [CLS] and [SEP] which wrap around the whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      " {'input_ids': [101, 1188, 1110, 5193, 165, 1184, 5940, 1106, 2740, 133, 9304, 135, 133, 120, 9304, 135, 1114, 1734, 4051, 177, 1183, 27801, 2316, 1176, 1534, 118, 1107, 118, 1644, 119, 2907, 117, 1184, 5940, 1106, 1103, 1248, 5650, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Uncased, mulitlingual sequence:\n",
      " {'input_ids': [101, 10372, 10127, 32311, 139, 11523, 76959, 10114, 16379, 133, 18710, 135, 133, 120, 18710, 135, 10171, 18281, 26648, 13051, 47607, 10933, 11531, 13907, 118, 10104, 118, 11785, 119, 10398, 117, 11523, 76959, 10114, 10103, 10981, 45261, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens_cased = tokenizer_bert_cased.encode_plus(sequence_en, max_length=50, pad_to_max_length=True)\n",
    "encoded_tokens_multi_uncased = tokenizer_bert_multi_uncased.encode_plus(sequence_en, max_length=50, pad_to_max_length=True)\n",
    "print(\"Cased, English sequence:\\n\",encoded_tokens_cased)\n",
    "print(\"\\nUncased, mulitlingual sequence:\\n\",encoded_tokens_multi_uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- When encoding the tokens the token_type_ids specify to which sequence the token belongs to. This is used when two or more sequences are fed at the same time to be compared to each other. In this case, all tokens belong to the same sequence, 0.\n",
    "- The attention_mask specifies which of the tokens the model should consider and which tokens are merely padding which are added for sequences that are not as long as the specified max_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sequence_de = \"Äußerst interessant ist die Frage, wie österreichische Übersetzungsdienstleistungsunternehmen mit Tokenisierung umgehen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Tokens: ['Ä', '##u', '##ße', '##rst', 'inter', '##ess', '##ant', 'is', '##t', 'die', 'Fr', '##age', ',', 'w', '##ie', 'ö', '##ster', '##re', '##ichi', '##sche', 'Ü', '##bers', '##et', '##zu', '##ng', '##s', '##die', '##nst', '##le', '##ist', '##ung', '##sun', '##tern', '##eh', '##men', 'mit', 'To', '##ken', '##isi', '##er', '##ung', 'um', '##ge', '##hen', '.'] \n",
      "Length: 45 \n",
      "\n",
      "Uncased, English sequence:\n",
      "Tokens: ['au', '##ße', '##rst', 'inter', '##ess', '##ant', 'ist', 'die', 'fra', '##ge', ',', 'wi', '##e', 'os', '##ter', '##re', '##ichi', '##sche', 'uber', '##set', '##zu', '##ng', '##sd', '##iens', '##tle', '##ist', '##ung', '##sun', '##tern', '##eh', '##men', 'mit', 'token', '##isi', '##er', '##ung', 'um', '##ge', '##hen', '.'] \n",
      "Length: 40 \n",
      "\n",
      "Cased, German sequence:\n",
      "Tokens: ['Äußer', '##st', 'interessant', 'ist', 'die', 'Frage', ',', 'wie', 'österreichische', 'Übersetzung', '##s', '##dienst', '##leistungs', '##unternehmen', 'mit', 'Tok', '##en', '##isierung', 'umgehen', '.'] \n",
      "Length: 20 \n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Tokens: ['Ä', '##u', '##ßer', '##st', 'interessant', 'ist', 'die', 'Frage', ',', 'wie', 'österreichische', 'Übersetzung', '##sdienst', '##leistung', '##sunt', '##erne', '##hmen', 'mit', 'To', '##ken', '##isierung', 'um', '##gehen', '.'] \n",
      "Length: 24 \n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Tokens: ['außer', '##st', 'interessant', 'ist', 'die', 'frage', ',', 'wie', 'osterreichische', 'ubersetzung', '##sdienst', '##leistung', '##sun', '##tern', '##ehmen', 'mit', 'tok', '##enis', '##ierung', 'um', '##gehen', '.'] \n",
      "Length: 22\n"
     ]
    }
   ],
   "source": [
    "bert_de_cased_tokenized_sequence = tokenizer_bert_cased.tokenize(sequence_de)\n",
    "bert_de_uncased_tokenized_sequence = tokenizer_bert_uncased.tokenize(sequence_de)\n",
    "bert_de_german_tokenized_sequence = tokenizer_bert_german.tokenize(sequence_de)\n",
    "bert_de_multi_cased_tokenized_sequence = tokenizer_bert_multi_cased.tokenize(sequence_de)\n",
    "bert_de_multi_uncased_tokenized_sequence = tokenizer_bert_multi_uncased.tokenize(sequence_de)\n",
    "\n",
    "print(\"Cased, English sequence:\\nTokens:\",bert_de_cased_tokenized_sequence,\"\\nLength:\",len(bert_de_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, English sequence:\\nTokens:\",bert_de_uncased_tokenized_sequence,\"\\nLength:\",len(bert_de_uncased_tokenized_sequence),\"\\n\")\n",
    "print(\"Cased, German sequence:\\nTokens:\",bert_de_german_tokenized_sequence,\"\\nLength:\",len(bert_de_german_tokenized_sequence),\"\\n\")\n",
    "print(\"Cased, multilingual sequence:\\nTokens:\",bert_de_multi_cased_tokenized_sequence,\"\\nLength:\",len(bert_de_multi_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, multilingual sequence:\\nTokens:\",bert_de_multi_uncased_tokenized_sequence,\"\\nLength:\",len(bert_de_multi_uncased_tokenized_sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- The tokenization varies extremely between the different tokenizers. Both of the English tokenizers do not make much sense since they do not recognize most of the words. This is why these tokenizers are ignored in this case.\n",
    "- The two multilingual tokenizers work slightly differently, especially when it comes to Umlaute since those are eliminated in the uncased setting.\n",
    "- Compared to the German only tokenizer, the uncased multilingual version is more similar than the cased multilingual one. This is mainly based on the first word which gets extremely split up in the cased multilingual setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sequence_fr = \"À Noël, les garçons Pierre et Philippe visitent un café près d'un château où un roi a vécu.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Tokens: ['À', 'No', '##ël', ',', 'les', 'g', '##ar', '##ç', '##ons', 'Pierre', 'et', 'Philippe', 'visit', '##ent', 'un', 'café', 'p', '##r', '##ès', 'd', \"'\", 'un', 'ch', '##â', '##teau', 'o', '##ù', 'un', 'r', '##oi', 'a', 'v', '##é', '##cu', '.'] \n",
      "Length: 35 \n",
      "\n",
      "Uncased, English sequence:\n",
      "Tokens: ['a', 'noel', ',', 'les', 'ga', '##rco', '##ns', 'pierre', 'et', 'philippe', 'visit', '##ent', 'un', 'cafe', 'pre', '##s', 'd', \"'\", 'un', 'chateau', 'ou', 'un', 'roi', 'a', 've', '##cu', '.'] \n",
      "Length: 27 \n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Tokens: ['À', 'Noël', ',', 'les', 'garçon', '##s', 'Pierre', 'et', 'Philippe', 'visite', '##nt', 'un', 'café', 'près', 'd', \"'\", 'un', 'château', 'où', 'un', 'roi', 'a', 'vécu', '.'] \n",
      "Length: 24 \n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Tokens: ['a', 'noel', ',', 'les', 'garcon', '##s', 'pierre', 'et', 'philippe', 'visite', '##nt', 'un', 'cafe', 'pres', 'd', \"'\", 'un', 'chateau', 'ou', 'un', 'roi', 'a', 'vecu', '.'] \n",
      "Length: 24\n"
     ]
    }
   ],
   "source": [
    "bert_fr_cased_tokenized_sequence = tokenizer_bert_cased.tokenize(sequence_fr)\n",
    "bert_fr_uncased_tokenized_sequence = tokenizer_bert_uncased.tokenize(sequence_fr)\n",
    "bert_fr_multi_cased_tokenized_sequence = tokenizer_bert_multi_cased.tokenize(sequence_fr)\n",
    "bert_fr_multi_uncased_tokenized_sequence = tokenizer_bert_multi_uncased.tokenize(sequence_fr)\n",
    "\n",
    "print(\"Cased, English sequence:\\nTokens:\",bert_fr_cased_tokenized_sequence,\"\\nLength:\",len(bert_fr_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, English sequence:\\nTokens:\",bert_fr_uncased_tokenized_sequence,\"\\nLength:\",len(bert_fr_uncased_tokenized_sequence),\"\\n\")\n",
    "print(\"Cased, multilingual sequence:\\nTokens:\",bert_fr_multi_cased_tokenized_sequence,\"\\nLength:\",len(bert_fr_multi_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, multilingual sequence:\\nTokens:\",bert_fr_multi_uncased_tokenized_sequence,\"\\nLength:\",len(bert_fr_multi_uncased_tokenized_sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- The cased english tokenizer does the worst job at representing the sequence of all the options.\n",
    "- The cased and uncased multilingual representations are remarkably similar which was not the case for all German words. (Note: this could just be biased by the choice of words in the test sequence.) The question remains "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Experiments with Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sequence = \"Hello, I am looking for an embedding for a really long and windy sentence that is definitely not going to fit max length. I am even going to start another sentence to make this sequence especially long.\"\n",
    "sequence_long = sequence + \"This was still not long enough which is why I am still adding words to this sentence to make sure that it is longer than necessary.\"\n",
    "\n",
    "bert_tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "bert_tokenized_sequence_long = tokenizer.tokenize(sequence_long)\n",
    "\n",
    "bert_tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tokens_ids = tokenizer.convert_tokens_to_ids(bert_tokenized_sequence)\n",
    "tokens_ids_long = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence_long))\n",
    "\n",
    "print(\"Tokens ids (short): {}\".format(tokens_ids))\n",
    "print(\"\")\n",
    "print(\"Tokens ids (long): {}\".format(tokens_ids_long))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Amount of tokens: \", len(tokens_ids), \"(short sequence), \", len(tokens_ids_long), \"(long sequence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tokens_ids_with_special = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "print(tokens_ids_with_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "encoded_tokens = tokenizer.encode_plus(\"Hello, I am looking for an embedding.\", \n",
    "                                       max_length=20, \n",
    "                                       pad_to_max_length=True)\n",
    "print(encoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# comparing sentence lengths\n",
    "# truncation_strategy is set to longest_first by default, this is why the longer sequence gets automatically cut off\n",
    "# special tokens are also automatically added by default (i.e. start and end token in this case)\n",
    "encoded_tokens_short = tokenizer.encode_plus(sequence, \n",
    "                                       max_length=64, \n",
    "                                       pad_to_max_length=True)\n",
    "\n",
    "encoded_tokens_long = tokenizer.encode_plus(sequence_long, \n",
    "                                       max_length=64, \n",
    "                                       pad_to_max_length=True)\n",
    "\n",
    "print(\"Output for the shorter sequence with\", len(tokens_ids), \"tokens: \\n\", encoded_tokens_short)\n",
    "\n",
    "print(\"\\nOutput for the longer sequence with\", len(tokens_ids_long), \"tokens: \\n\", encoded_tokens_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# showing the sentence / what remained\n",
    "#for i in list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'][0]:\n",
    "#    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))\n",
    "\n",
    "tokenizer.decode(encoded_tokens_long['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
