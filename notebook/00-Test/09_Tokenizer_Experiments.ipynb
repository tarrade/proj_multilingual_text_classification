{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Tokenizer Experiments\n",
    "\n",
    "This notebook experiments with how the tokenizers of NLP models like BERT and GPT-2 handle certain characters. This encompasses non-linguistic characters like '\\\\' and html inputs like \\<br>, but also \"Umlaute\" and accents. Since we aim to look at multiple languages, special characters that are used in French and German are especially examined. \n",
    "\n",
    "This notebook also aims to explore in what way we need to preprocess language data in the future, especially when compared to classical machine learning.\n",
    "\n",
    "As a reference please consult https://huggingface.co/transformers/main_classes/tokenizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export BUCKET_TRANSLATION_NAME=your_gcp_gs_bucket_translation_name`  \n",
       "`export BUCKET_STAGING_NAME=your_gcp_gs_bucket_staging_name` \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model`  \n",
       "`export CLOUDSDK_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "`export CLOUDSDK_GSUTIL_PYTHON=your_path/conda-env/env_gcp_sdk/bin/python`  \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import (BertTokenizer,\n",
    "                          GPT2Tokenizer,\n",
    "                          glue_convert_examples_to_features,\n",
    "                         )\n",
    "import itertools\n",
    "\n",
    "# local packages\n",
    "import preprocessing.preprocessing as pp\n",
    "import importlib\n",
    "importlib.reload(pp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define Paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#try:\n",
    "#    data_dir=os.environ['PATH_DATASETS']\n",
    "#except:\n",
    "#    print('missing PATH_DATASETS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tokenizer Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "General differences between the tokenizers of BERT and GPT-2:\n",
    "\n",
    "- the style of decoding\n",
    "- the type of special tokens that are added to the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4e8eecabbe4f458ff2b1987a9b6be5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=254728.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d024e4ea30f14b21ade765f06cd6e36d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18404cd188c541de91003962ae9cca72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca2a98ce295443c981c68a4f507b84b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "tokenizer_bert_cased = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer_bert_uncased = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer_bert_german = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "# did not find a french-specific tokenizer yet\n",
    "#tokenizer_bert_french = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "tokenizer_bert_multi_cased = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer_bert_multi_uncased = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "\n",
    "# GPT-2\n",
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained('gpt2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## BERT - Special Characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Language-Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#sequence_en = \"This is testing \\what happens to cases<br></br> with words containing hyphens like mother-in-law. Also, what happens to the second sentence?\"\n",
    "sequence_en = \"Sensor Artikel 51317350597 Frontscheibe grün Regensensor STK 1 768.74\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of token if split by space:\n",
      " ['Sensor', 'Artikel', '51317350597', 'Frontscheibe', 'grün', 'Regensensor', 'STK', '1', '768.74'] \n",
      "Length: 9\n",
      "Cased, English sequence:\n",
      " ['Sen', '##sor', 'Art', '##ike', '##l', '51', '##31', '##7', '##35', '##0', '##5', '##9', '##7', 'Front', '##sche', '##ibe', 'g', '##r', '##ü', '##n', 'Reg', '##ense', '##nso', '##r', 'ST', '##K', '1', '76', '##8', '.', '74'] \n",
      "Length: 31\n",
      "\n",
      "Uncased, English sequence:\n",
      " ['sensor', 'art', '##ike', '##l', '51', '##31', '##7', '##35', '##0', '##59', '##7', 'fronts', '##che', '##ibe', 'gr', '##un', 'reg', '##ense', '##nsor', 'st', '##k', '1', '76', '##8', '.', '74'] \n",
      "Length: 26\n",
      "\n",
      "Cased, multilingual sequence:\n",
      " ['Sens', '##or', 'Artikel', '513', '##17', '##35', '##0', '##59', '##7', 'Front', '##sche', '##ibe', 'gr', '##ün', 'Reg', '##ense', '##ns', '##or', 'ST', '##K', '1', '768', '.', '74'] \n",
      "Length: 24\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      " ['sensor', 'artikel', '513', '##17', '##35', '##0', '##59', '##7', 'front', '##sche', '##ibe', 'grun', 'regen', '##sens', '##or', 'st', '##k', '1', '768', '.', '74'] \n",
      "Length: 21\n"
     ]
    }
   ],
   "source": [
    "bert_cased_tokenized_sequence = tokenizer_bert_cased.tokenize(sequence_en)\n",
    "bert_uncased_tokenized_sequence = tokenizer_bert_uncased.tokenize(sequence_en)\n",
    "bert_multi_cased_tokenized_sequence = tokenizer_bert_multi_cased.tokenize(sequence_en)\n",
    "bert_multi_uncased_tokenized_sequence = tokenizer_bert_multi_uncased.tokenize(sequence_en)\n",
    "print(\"Number of token if split by space:\\n\",sequence_en.split(' '),\"\\nLength:\", len(sequence_en.split(' ')))\n",
    "print(\"Cased, English sequence:\\n\",bert_cased_tokenized_sequence,\"\\nLength:\",len(bert_cased_tokenized_sequence))\n",
    "print(\"\\nUncased, English sequence:\\n\",bert_uncased_tokenized_sequence,\"\\nLength:\",len(bert_uncased_tokenized_sequence))\n",
    "print(\"\\nCased, multilingual sequence:\\n\",bert_multi_cased_tokenized_sequence,\"\\nLength:\",len(bert_multi_cased_tokenized_sequence))\n",
    "print(\"\\nUncased, multilingual sequence:\\n\",bert_multi_uncased_tokenized_sequence,\"\\nLength:\",len(bert_multi_uncased_tokenized_sequence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- When special characters are not removed from the text, they get tokenized separately.\n",
    "- The difference between the cased and uncased sequences (English) are that uncased sequences are lower case only (which was to be expected). Also, the word \"hyphen\" was split up differently.\n",
    "- The multilingual tokenizers work slightly differently than the English ones. The only difference in this specific sequence concerns the tokenization of the word \"hyphen\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Token IDs: [1188, 1110, 5193, 165, 1184, 5940, 1106, 2740, 133, 9304, 135, 133, 120, 9304, 135, 1114, 1734, 4051, 177, 1183, 27801, 2316, 1176, 1534, 118, 1107, 118, 1644, 119, 2907, 117, 1184, 5940, 1106, 1103, 1248, 5650, 136] \n",
      "Length: 38\n",
      "\n",
      "Uncased, English sequence:\n",
      "Token IDs: [2023, 2003, 5604, 1032, 2054, 6433, 2000, 3572, 1026, 7987, 1028, 1026, 1013, 7987, 1028, 2007, 2616, 4820, 1044, 22571, 10222, 2015, 2066, 2388, 1011, 1999, 1011, 2375, 1012, 2036, 1010, 2054, 6433, 2000, 1996, 2117, 6251, 1029] \n",
      "Length: 38\n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Token IDs: [10747, 10124, 38306, 165, 12976, 105315, 10114, 16480, 133, 33989, 135, 133, 120, 33989, 135, 10169, 21296, 27248, 15165, 60383, 10107, 11850, 15293, 118, 10106, 118, 13255, 119, 20593, 117, 12976, 105315, 10114, 10105, 11132, 49219, 136] \n",
      "Length: 37\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Token IDs: [10372, 10127, 32311, 139, 11523, 76959, 10114, 16379, 133, 18710, 135, 133, 120, 18710, 135, 10171, 18281, 26648, 13051, 47607, 10933, 11531, 13907, 118, 10104, 118, 11785, 119, 10398, 117, 11523, 76959, 10114, 10103, 10981, 45261, 136] \n",
      "Length: 37\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_cased = tokenizer_bert_cased.convert_tokens_to_ids(bert_cased_tokenized_sequence)\n",
    "tokens_ids_uncased = tokenizer_bert_uncased.convert_tokens_to_ids(bert_uncased_tokenized_sequence)\n",
    "tokens_ids_multi_cased = tokenizer_bert_multi_cased.convert_tokens_to_ids(bert_multi_cased_tokenized_sequence)\n",
    "tokens_ids_multi_uncased = tokenizer_bert_multi_uncased.convert_tokens_to_ids(bert_multi_uncased_tokenized_sequence)\n",
    "print(\"Cased, English sequence:\\nToken IDs:\",tokens_ids_cased,\"\\nLength:\",len(tokens_ids_cased))\n",
    "print(\"\\nUncased, English sequence:\\nToken IDs:\",tokens_ids_uncased,\"\\nLength:\",len(tokens_ids_uncased))\n",
    "print(\"\\nCased, multilingual sequence:\\nToken IDs:\",tokens_ids_multi_cased,\"\\nLength:\",len(tokens_ids_multi_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\nToken IDs:\",tokens_ids_multi_uncased,\"\\nLength:\",len(tokens_ids_multi_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1188    ---->    T h i s\n",
      "   1110    ---->    i s\n",
      "   5193    ---->    t e s t i n g\n",
      "    165    ---->    \\\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   2740    ---->    c a s e s\n",
      "    133    ---->    <\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "    133    ---->    <\n",
      "    120    ---->    /\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "   1114    ---->    w i t h\n",
      "   1734    ---->    w o r d s\n",
      "   4051    ---->    c o n t a i n i n g\n",
      "    177    ---->    h\n",
      "   1183    ---->    # # y\n",
      "  27801    ---->    # # p h e\n",
      "   2316    ---->    # # n s\n",
      "   1176    ---->    l i k e\n",
      "   1534    ---->    m o t h e r\n",
      "    118    ---->    -\n",
      "   1107    ---->    i n\n",
      "    118    ---->    -\n",
      "   1644    ---->    l a w\n",
      "    119    ---->    .\n",
      "   2907    ---->    A l s o\n",
      "    117    ---->    ,\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   1103    ---->    t h e\n",
      "   1248    ---->    s e c o n d\n",
      "   5650    ---->    s e n t e n c e\n",
      "    136    ---->    ?\n"
     ]
    }
   ],
   "source": [
    "for i in tokens_ids_cased:\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer_bert_cased.decode(int(i))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "All tokens have different ids because the two tokenizers have a different number of total tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      " [101, 1188, 1110, 5193, 165, 1184, 5940, 1106, 2740, 133, 9304, 135, 133, 120, 9304, 135, 1114, 1734, 4051, 177, 1183, 27801, 2316, 1176, 1534, 118, 1107, 118, 1644, 119, 2907, 117, 1184, 5940, 1106, 1103, 1248, 5650, 136, 102] \n",
      "Length: 40\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      " [101, 10372, 10127, 32311, 139, 11523, 76959, 10114, 16379, 133, 18710, 135, 133, 120, 18710, 135, 10171, 18281, 26648, 13051, 47607, 10933, 11531, 13907, 118, 10104, 118, 11785, 119, 10398, 117, 11523, 76959, 10114, 10103, 10981, 45261, 136, 102] \n",
      "Length: 39\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_with_special_cased = tokenizer_bert_cased.build_inputs_with_special_tokens(tokens_ids_cased)\n",
    "tokens_ids_with_special_multi_uncased = tokenizer_bert_multi_uncased.build_inputs_with_special_tokens(tokens_ids_multi_uncased)\n",
    "print(\"Cased, English sequence:\\n\",tokens_ids_with_special_cased,\"\\nLength:\", len(tokens_ids_with_special_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\n\",tokens_ids_with_special_multi_uncased,\"\\nLength:\", len(tokens_ids_with_special_multi_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1188    ---->    T h i s\n",
      "   1110    ---->    i s\n",
      "   5193    ---->    t e s t i n g\n",
      "    165    ---->    \\\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   2740    ---->    c a s e s\n",
      "    133    ---->    <\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "    133    ---->    <\n",
      "    120    ---->    /\n",
      "   9304    ---->    b r\n",
      "    135    ---->    >\n",
      "   1114    ---->    w i t h\n",
      "   1734    ---->    w o r d s\n",
      "   4051    ---->    c o n t a i n i n g\n",
      "    177    ---->    h\n",
      "   1183    ---->    # # y\n",
      "  27801    ---->    # # p h e\n",
      "   2316    ---->    # # n s\n",
      "   1176    ---->    l i k e\n",
      "   1534    ---->    m o t h e r\n",
      "    118    ---->    -\n",
      "   1107    ---->    i n\n",
      "    118    ---->    -\n",
      "   1644    ---->    l a w\n",
      "    119    ---->    .\n",
      "   2907    ---->    A l s o\n",
      "    117    ---->    ,\n",
      "   1184    ---->    w h a t\n",
      "   5940    ---->    h a p p e n s\n",
      "   1106    ---->    t o\n",
      "   1103    ---->    t h e\n",
      "   1248    ---->    s e c o n d\n",
      "   5650    ---->    s e n t e n c e\n",
      "    136    ---->    ?\n",
      "    102    ---->    [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for i in tokens_ids_with_special_cased:\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer_bert_cased.decode(int(i))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Special tokens are simply [CLS] and [SEP] which wrap around the whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      " {'input_ids': [101, 1188, 1110, 5193, 165, 1184, 5940, 1106, 2740, 133, 9304, 135, 133, 120, 9304, 135, 1114, 1734, 4051, 177, 1183, 27801, 2316, 1176, 1534, 118, 1107, 118, 1644, 119, 2907, 117, 1184, 5940, 1106, 1103, 1248, 5650, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Uncased, mulitlingual sequence:\n",
      " {'input_ids': [101, 10372, 10127, 32311, 139, 11523, 76959, 10114, 16379, 133, 18710, 135, 133, 120, 18710, 135, 10171, 18281, 26648, 13051, 47607, 10933, 11531, 13907, 118, 10104, 118, 11785, 119, 10398, 117, 11523, 76959, 10114, 10103, 10981, 45261, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens_cased = tokenizer_bert_cased.encode_plus(sequence_en, max_length=50, pad_to_max_length=True)\n",
    "encoded_tokens_multi_uncased = tokenizer_bert_multi_uncased.encode_plus(sequence_en, max_length=50, pad_to_max_length=True)\n",
    "print(\"Cased, English sequence:\\n\",encoded_tokens_cased)\n",
    "print(\"\\nUncased, mulitlingual sequence:\\n\",encoded_tokens_multi_uncased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- When encoding the tokens the token_type_ids specify to which sequence the token belongs to. This is used when two or more sequences are fed at the same time to be compared to each other. In this case, all tokens belong to the same sequence, 0.\n",
    "- The attention_mask specifies which of the tokens the model should consider and which tokens are merely padding which are added for sequences that are not as long as the specified max_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sequence_de = \"Äußerst interessant ist die Frage, wie österreichische Übersetzungsdienstleistungsunternehmen mit Tokenisierung und \\\n",
    "              Schreibweisen wie Ueberfuehrung oder ueberfuehren umgehen.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Tokens: ['Ä', '##u', '##ße', '##rst', 'inter', '##ess', '##ant', 'is', '##t', 'die', 'Fr', '##age', ',', 'w', '##ie', 'ö', '##ster', '##re', '##ichi', '##sche', 'Ü', '##bers', '##et', '##zu', '##ng', '##s', '##die', '##nst', '##le', '##ist', '##ung', '##sun', '##tern', '##eh', '##men', 'mit', 'To', '##ken', '##isi', '##er', '##ung', 'und', 'Sc', '##hr', '##ei', '##b', '##wei', '##sen', 'w', '##ie', 'U', '##eb', '##er', '##fu', '##eh', '##rung', 'o', '##der', 'u', '##eb', '##er', '##fu', '##eh', '##ren', 'um', '##ge', '##hen', '.'] \n",
      "Length: 68 \n",
      "\n",
      "Uncased, English sequence:\n",
      "Tokens: ['au', '##ße', '##rst', 'inter', '##ess', '##ant', 'ist', 'die', 'fra', '##ge', ',', 'wi', '##e', 'os', '##ter', '##re', '##ichi', '##sche', 'uber', '##set', '##zu', '##ng', '##sd', '##iens', '##tle', '##ist', '##ung', '##sun', '##tern', '##eh', '##men', 'mit', 'token', '##isi', '##er', '##ung', 'und', 'sc', '##hre', '##ib', '##wei', '##sen', 'wi', '##e', 'u', '##eber', '##fu', '##eh', '##run', '##g', 'oder', 'u', '##eber', '##fu', '##eh', '##ren', 'um', '##ge', '##hen', '.'] \n",
      "Length: 60 \n",
      "\n",
      "Cased, German sequence:\n",
      "Tokens: ['Äußer', '##st', 'interessant', 'ist', 'die', 'Frage', ',', 'wie', 'österreichische', 'Übersetzung', '##s', '##dienst', '##leistungs', '##unternehmen', 'mit', 'Tok', '##en', '##isierung', 'und', 'Schreibweise', '##n', 'wie', 'Ue', '##ber', '##f', '##ue', '##hr', '##ung', 'oder', 'u', '##eb', '##er', '##f', '##ue', '##hren', 'umgehen', '.'] \n",
      "Length: 37 \n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Tokens: ['Ä', '##u', '##ßer', '##st', 'interessant', 'ist', 'die', 'Frage', ',', 'wie', 'österreichische', 'Übersetzung', '##sdienst', '##leistung', '##sunt', '##erne', '##hmen', 'mit', 'To', '##ken', '##isierung', 'und', 'Sc', '##hre', '##ib', '##weisen', 'wie', 'U', '##eber', '##fu', '##ehr', '##ung', 'oder', 'u', '##eber', '##fu', '##ehr', '##en', 'um', '##gehen', '.'] \n",
      "Length: 41 \n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Tokens: ['außer', '##st', 'interessant', 'ist', 'die', 'frage', ',', 'wie', 'osterreichische', 'ubersetzung', '##sdienst', '##leistung', '##sun', '##tern', '##ehmen', 'mit', 'tok', '##enis', '##ierung', 'und', 'sc', '##hre', '##ib', '##weisen', 'wie', 'ue', '##ber', '##fu', '##eh', '##rung', 'oder', 'ue', '##ber', '##fu', '##eh', '##ren', 'um', '##gehen', '.'] \n",
      "Length: 39\n"
     ]
    }
   ],
   "source": [
    "bert_de_cased_tokenized_sequence = tokenizer_bert_cased.tokenize(sequence_de)\n",
    "bert_de_uncased_tokenized_sequence = tokenizer_bert_uncased.tokenize(sequence_de)\n",
    "bert_de_german_tokenized_sequence = tokenizer_bert_german.tokenize(sequence_de)\n",
    "bert_de_multi_cased_tokenized_sequence = tokenizer_bert_multi_cased.tokenize(sequence_de)\n",
    "bert_de_multi_uncased_tokenized_sequence = tokenizer_bert_multi_uncased.tokenize(sequence_de)\n",
    "\n",
    "print(\"Cased, English sequence:\\nTokens:\",bert_de_cased_tokenized_sequence,\"\\nLength:\",len(bert_de_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, English sequence:\\nTokens:\",bert_de_uncased_tokenized_sequence,\"\\nLength:\",len(bert_de_uncased_tokenized_sequence),\"\\n\")\n",
    "print(\"Cased, German sequence:\\nTokens:\",bert_de_german_tokenized_sequence,\"\\nLength:\",len(bert_de_german_tokenized_sequence),\"\\n\")\n",
    "print(\"Cased, multilingual sequence:\\nTokens:\",bert_de_multi_cased_tokenized_sequence,\"\\nLength:\",len(bert_de_multi_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, multilingual sequence:\\nTokens:\",bert_de_multi_uncased_tokenized_sequence,\"\\nLength:\",len(bert_de_multi_uncased_tokenized_sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- The tokenization varies between the different tokenizers. Both of the English tokenizers do not make much sense since they do not recognize most of the words or word pieces. This is why these tokenizers are ignored in this case.\n",
    "- The two multilingual tokenizers work slightly differently, especially when it comes to Umlaute since those are eliminated in the uncased setting.\n",
    "- Compared to the German only tokenizer, the uncased multilingual version is more similar than the cased multilingual one. This is mainly based on the first word which gets extremely split up in the cased multilingual setting.\n",
    "- The cased, German tokenizer seems to represent the words the best, but the uncased, multilingual tokenizer should also work.\n",
    "- Alternative spellings of Umlaute like \"ue\" for \"ü\" are not captured well. Hereby, capital words behave similarly to the others. **We may need to account for these alternative spellings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Token IDs: [229, 1358, 13750, 9731, 9455, 5800, 2861, 1110, 1204, 2939, 13359, 2553, 117, 192, 1663, 268, 4648, 1874, 11985, 15460, 243, 11697, 2105, 10337, 2118, 1116, 7782, 22399, 1513, 1776, 4380, 23294, 16748, 10486, 2354, 26410, 1706, 6378, 26868, 1200, 4380, 5576, 20452, 8167, 6851, 1830, 24078, 3792, 192, 1663, 158, 15581, 1200, 14703, 10486, 20901, 184, 2692, 190, 15581, 1200, 14703, 10486, 5123, 15276, 2176, 10436, 119] \n",
      "Length: 68\n",
      "\n",
      "Cased, German sequence:\n",
      "Token IDs: [7166, 13, 19168, 127, 30, 1685, 2036, 246, 11675, 10919, 26902, 2068, 9992, 4039, 114, 9626, 7, 3873, 42, 21779, 26898, 246, 23808, 73, 26913, 1790, 48, 27, 309, 2118, 731, 6, 26913, 1790, 2306, 18978, 4813] \n",
      "Length: 37\n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Token IDs: [229, 10138, 88376, 10562, 103691, 10298, 10128, 39278, 117, 10953, 69515, 47038, 65069, 73003, 100489, 17894, 58348, 10221, 11469, 11062, 34600, 10130, 55260, 32206, 17609, 35130, 10953, 158, 68977, 20758, 89707, 10716, 10760, 189, 68977, 20758, 89707, 10136, 10293, 40578, 119] \n",
      "Length: 41\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Token IDs: [41623, 10607, 65039, 10339, 10121, 38563, 117, 11005, 44098, 46114, 62411, 75281, 26184, 33557, 70416, 10234, 16925, 86574, 21727, 10138, 16427, 29855, 17887, 36233, 11005, 37298, 12827, 17848, 25984, 18064, 10843, 37298, 12827, 17848, 25984, 10922, 10316, 45362, 119] \n",
      "Length: 39\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_de_cased = tokenizer_bert_cased.convert_tokens_to_ids(bert_de_cased_tokenized_sequence)\n",
    "tokens_ids_de_german = tokenizer_bert_german.convert_tokens_to_ids(bert_de_german_tokenized_sequence)\n",
    "tokens_ids_de_multi_cased = tokenizer_bert_multi_cased.convert_tokens_to_ids(bert_de_multi_cased_tokenized_sequence)\n",
    "tokens_ids_de_multi_uncased = tokenizer_bert_multi_uncased.convert_tokens_to_ids(bert_de_multi_uncased_tokenized_sequence)\n",
    "print(\"Cased, English sequence:\\nToken IDs:\",tokens_ids_de_cased,\"\\nLength:\",len(tokens_ids_de_cased))\n",
    "print(\"\\nCased, German sequence:\\nToken IDs:\",tokens_ids_de_german,\"\\nLength:\",len(tokens_ids_de_german))\n",
    "print(\"\\nCased, multilingual sequence:\\nToken IDs:\",tokens_ids_de_multi_cased,\"\\nLength:\",len(tokens_ids_de_multi_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\nToken IDs:\",tokens_ids_de_multi_uncased,\"\\nLength:\",len(tokens_ids_de_multi_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Token IDs [101, 229, 1358, 13750, 9731, 9455, 5800, 2861, 1110, 1204, 2939, 13359, 2553, 117, 192, 1663, 268, 4648, 1874, 11985, 15460, 243, 11697, 2105, 10337, 2118, 1116, 7782, 22399, 1513, 1776, 4380, 23294, 16748, 10486, 2354, 26410, 1706, 6378, 26868, 1200, 4380, 5576, 20452, 8167, 6851, 1830, 24078, 3792, 192, 1663, 158, 15581, 1200, 14703, 10486, 20901, 184, 2692, 190, 15581, 1200, 14703, 10486, 5123, 15276, 2176, 10436, 119, 102] \n",
      "Length: 70\n",
      "\n",
      "Cased, German sequence:\n",
      "Token IDs: [3, 7166, 13, 19168, 127, 30, 1685, 2036, 246, 11675, 10919, 26902, 2068, 9992, 4039, 114, 9626, 7, 3873, 42, 21779, 26898, 246, 23808, 73, 26913, 1790, 48, 27, 309, 2118, 731, 6, 26913, 1790, 2306, 18978, 4813, 4] \n",
      "Length: 39\n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Token IDs: [101, 229, 10138, 88376, 10562, 103691, 10298, 10128, 39278, 117, 10953, 69515, 47038, 65069, 73003, 100489, 17894, 58348, 10221, 11469, 11062, 34600, 10130, 55260, 32206, 17609, 35130, 10953, 158, 68977, 20758, 89707, 10716, 10760, 189, 68977, 20758, 89707, 10136, 10293, 40578, 119, 102] \n",
      "Length: 43\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Token IDs: [101, 41623, 10607, 65039, 10339, 10121, 38563, 117, 11005, 44098, 46114, 62411, 75281, 26184, 33557, 70416, 10234, 16925, 86574, 21727, 10138, 16427, 29855, 17887, 36233, 11005, 37298, 12827, 17848, 25984, 18064, 10843, 37298, 12827, 17848, 25984, 10922, 10316, 45362, 119, 102] \n",
      "Length: 41\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_with_special_de_cased = tokenizer_bert_cased.build_inputs_with_special_tokens(tokens_ids_de_cased)\n",
    "tokens_ids_with_special_de_german = tokenizer_bert_german.build_inputs_with_special_tokens(tokens_ids_de_german)\n",
    "tokens_ids_with_special_de_multi_cased = tokenizer_bert_multi_cased.build_inputs_with_special_tokens(tokens_ids_de_multi_cased)\n",
    "tokens_ids_with_special_de_multi_uncased = tokenizer_bert_multi_uncased.build_inputs_with_special_tokens(tokens_ids_de_multi_uncased)\n",
    "print(\"Cased, English sequence:\\nToken IDs\",tokens_ids_with_special_de_cased,\"\\nLength:\", len(tokens_ids_with_special_de_cased))\n",
    "print(\"\\nCased, German sequence:\\nToken IDs:\",tokens_ids_with_special_de_german,\"\\nLength:\",len(tokens_ids_with_special_de_german))\n",
    "print(\"\\nCased, multilingual sequence:\\nToken IDs:\",tokens_ids_with_special_de_multi_cased,\"\\nLength:\",len(tokens_ids_with_special_de_multi_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\nToken IDs:\",tokens_ids_with_special_de_multi_uncased,\"\\nLength:\",len(tokens_ids_with_special_de_multi_uncased))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Special tokens have different id's for the German tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      " {'input_ids': [101, 229, 1358, 13750, 9731, 9455, 5800, 2861, 1110, 1204, 2939, 13359, 2553, 117, 192, 1663, 268, 4648, 1874, 11985, 15460, 243, 11697, 2105, 10337, 2118, 1116, 7782, 22399, 1513, 1776, 4380, 23294, 16748, 10486, 2354, 26410, 1706, 6378, 26868, 1200, 4380, 5576, 20452, 8167, 6851, 1830, 24078, 3792, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "\n",
      "Cased, German sequence:\n",
      " {'input_ids': [3, 7166, 13, 19168, 127, 30, 1685, 2036, 246, 11675, 10919, 26902, 2068, 9992, 4039, 114, 9626, 7, 3873, 42, 21779, 26898, 246, 23808, 73, 26913, 1790, 48, 27, 309, 2118, 731, 6, 26913, 1790, 2306, 18978, 4813, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Cased, mulitlingual sequence:\n",
      " {'input_ids': [101, 229, 10138, 88376, 10562, 103691, 10298, 10128, 39278, 117, 10953, 69515, 47038, 65069, 73003, 100489, 17894, 58348, 10221, 11469, 11062, 34600, 10130, 55260, 32206, 17609, 35130, 10953, 158, 68977, 20758, 89707, 10716, 10760, 189, 68977, 20758, 89707, 10136, 10293, 40578, 119, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Uncased, mulitlingual sequence:\n",
      " {'input_ids': [101, 41623, 10607, 65039, 10339, 10121, 38563, 117, 11005, 44098, 46114, 62411, 75281, 26184, 33557, 70416, 10234, 16925, 86574, 21727, 10138, 16427, 29855, 17887, 36233, 11005, 37298, 12827, 17848, 25984, 18064, 10843, 37298, 12827, 17848, 25984, 10922, 10316, 45362, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens_de_cased = tokenizer_bert_cased.encode_plus(sequence_de, max_length=50, pad_to_max_length=True)\n",
    "encoded_tokens_de_german = tokenizer_bert_german.encode_plus(sequence_de, max_length=50, pad_to_max_length=True)\n",
    "encoded_tokens_de_multi_cased = tokenizer_bert_multi_cased.encode_plus(sequence_de, max_length=50, pad_to_max_length=True)\n",
    "encoded_tokens_de_multi_uncased = tokenizer_bert_multi_uncased.encode_plus(sequence_de, max_length=50, pad_to_max_length=True)\n",
    "print(\"Cased, English sequence:\\n\",encoded_tokens_de_cased)\n",
    "print(\"\\nCased, German sequence:\\n\",encoded_tokens_de_german)\n",
    "print(\"\\nCased, mulitlingual sequence:\\n\",encoded_tokens_de_multi_cased)\n",
    "print(\"\\nUncased, mulitlingual sequence:\\n\",encoded_tokens_de_multi_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->                        [ C L S ]                                    [ C L S ]                         <----        101\n",
      "    229    ---->                            Ä                                        a u ß e r                         <----      41623\n",
      "  10138    ---->                          # # u                                       # # s t                          <----      10607\n",
      "  88376    ---->                        # # ß e r                              i n t e r e s s a n t                   <----      65039\n",
      "  10562    ---->                         # # s t                                       i s t                           <----      10339\n",
      " 103691    ---->                  i n t e r e s s a n t                                d i e                           <----      10121\n",
      "  10298    ---->                          i s t                                      f r a g e                         <----      38563\n",
      "  10128    ---->                          d i e                                          ,                             <----        117\n",
      "  39278    ---->                        F r a g e                                      w i e                           <----      11005\n",
      "    117    ---->                            ,                              o s t e r r e i c h i s c h e               <----      44098\n",
      "  10953    ---->                          w i e                                u b e r s e t z u n g                   <----      46114\n",
      "  69515    ---->              ö s t e r r e i c h i s c h e                      # # s d i e n s t                     <----      62411\n",
      "  47038    ---->                  Ü b e r s e t z u n g                         # # l e i s t u n g                    <----      75281\n",
      "  65069    ---->                    # # s d i e n s t                                # # s u n                         <----      26184\n",
      "  73003    ---->                   # # l e i s t u n g                              # # t e r n                        <----      33557\n",
      " 100489    ---->                       # # s u n t                                 # # e h m e n                       <----      70416\n",
      "  17894    ---->                       # # e r n e                                     m i t                           <----      10234\n",
      "  58348    ---->                       # # h m e n                                     t o k                           <----      16925\n",
      "  10221    ---->                          m i t                                     # # e n i s                        <----      86574\n",
      "  11469    ---->                           T o                                    # # i e r u n g                      <----      21727\n",
      "  11062    ---->                        # # k e n                                      u n d                           <----      10138\n",
      "  34600    ---->                   # # i s i e r u n g                                  s c                            <----      16427\n",
      "  10130    ---->                          u n d                                      # # h r e                         <----      29855\n",
      "  55260    ---->                           S c                                        # # i b                          <----      17887\n",
      "  32206    ---->                        # # h r e                                 # # w e i s e n                      <----      36233\n",
      "  17609    ---->                         # # i b                                       w i e                           <----      11005\n",
      "  35130    ---->                     # # w e i s e n                                    u e                            <----      37298\n",
      "  10953    ---->                          w i e                                      # # b e r                         <----      12827\n",
      "    158    ---->                            U                                         # # f u                          <----      17848\n",
      "  68977    ---->                       # # e b e r                                    # # e h                          <----      25984\n",
      "  20758    ---->                         # # f u                                    # # r u n g                        <----      18064\n",
      "  89707    ---->                        # # e h r                                     o d e r                          <----      10843\n",
      "  10716    ---->                        # # u n g                                       u e                            <----      37298\n",
      "  10760    ---->                         o d e r                                     # # b e r                         <----      12827\n",
      "    189    ---->                            u                                         # # f u                          <----      17848\n",
      "  68977    ---->                       # # e b e r                                    # # e h                          <----      25984\n",
      "  20758    ---->                         # # f u                                     # # r e n                         <----      10922\n",
      "  89707    ---->                        # # e h r                                       u m                            <----      10316\n",
      "  10136    ---->                         # # e n                                   # # g e h e n                       <----      45362\n",
      "  10293    ---->                           u m                                           .                             <----        119\n",
      "  40578    ---->                      # # g e h e n                                  [ S E P ]                         <----        102\n",
      "    119    ---->                            .                                        [ P A D ]                         <----          0\n",
      "    102    ---->                        [ S E P ]                                    [ P A D ]                         <----          0\n"
     ]
    }
   ],
   "source": [
    "for i,j in itertools.zip_longest(tokens_ids_with_special_de_multi_cased, tokens_ids_with_special_de_multi_uncased):\n",
    "    if i==None:\n",
    "        i = 0\n",
    "    elif j==None:\n",
    "        j = 0\n",
    "    print('{:7d}    ---->         {:^40}     {:^40}         <----    {:7d}'.format(i, tokenizer_bert_multi_cased.decode(int(i)),tokenizer_bert_multi_uncased.decode(int(j)),j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sequence_fr = \"À Noël, les garçons Pierre et Philippe ramassent une pierre et puis visitent un café près d'un château où un roi a vécu.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Tokens: ['À', 'No', '##ël', ',', 'les', 'g', '##ar', '##ç', '##ons', 'Pierre', 'et', 'Philippe', 'ram', '##ass', '##ent', 'une', 'pier', '##re', 'et', 'pu', '##is', 'visit', '##ent', 'un', 'café', 'p', '##r', '##ès', 'd', \"'\", 'un', 'ch', '##â', '##teau', 'o', '##ù', 'un', 'r', '##oi', 'a', 'v', '##é', '##cu', '.'] \n",
      "Length: 44 \n",
      "\n",
      "Uncased, English sequence:\n",
      "Tokens: ['a', 'noel', ',', 'les', 'ga', '##rco', '##ns', 'pierre', 'et', 'philippe', 'rama', '##ssen', '##t', 'une', 'pierre', 'et', 'pu', '##is', 'visit', '##ent', 'un', 'cafe', 'pre', '##s', 'd', \"'\", 'un', 'chateau', 'ou', 'un', 'roi', 'a', 've', '##cu', '.'] \n",
      "Length: 35 \n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Tokens: ['À', 'Noël', ',', 'les', 'garçon', '##s', 'Pierre', 'et', 'Philippe', 'ramas', '##sent', 'une', 'pierre', 'et', 'puis', 'visite', '##nt', 'un', 'café', 'près', 'd', \"'\", 'un', 'château', 'où', 'un', 'roi', 'a', 'vécu', '.'] \n",
      "Length: 30 \n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Tokens: ['a', 'noel', ',', 'les', 'garcon', '##s', 'pierre', 'et', 'philippe', 'ramas', '##sent', 'une', 'pierre', 'et', 'puis', 'visite', '##nt', 'un', 'cafe', 'pres', 'd', \"'\", 'un', 'chateau', 'ou', 'un', 'roi', 'a', 'vecu', '.'] \n",
      "Length: 30\n"
     ]
    }
   ],
   "source": [
    "bert_fr_cased_tokenized_sequence = tokenizer_bert_cased.tokenize(sequence_fr)\n",
    "bert_fr_uncased_tokenized_sequence = tokenizer_bert_uncased.tokenize(sequence_fr)\n",
    "bert_fr_multi_cased_tokenized_sequence = tokenizer_bert_multi_cased.tokenize(sequence_fr)\n",
    "bert_fr_multi_uncased_tokenized_sequence = tokenizer_bert_multi_uncased.tokenize(sequence_fr)\n",
    "\n",
    "print(\"Cased, English sequence:\\nTokens:\",bert_fr_cased_tokenized_sequence,\"\\nLength:\",len(bert_fr_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, English sequence:\\nTokens:\",bert_fr_uncased_tokenized_sequence,\"\\nLength:\",len(bert_fr_uncased_tokenized_sequence),\"\\n\")\n",
    "print(\"Cased, multilingual sequence:\\nTokens:\",bert_fr_multi_cased_tokenized_sequence,\"\\nLength:\",len(bert_fr_multi_cased_tokenized_sequence),\"\\n\")\n",
    "print(\"Uncased, multilingual sequence:\\nTokens:\",bert_fr_multi_uncased_tokenized_sequence,\"\\nLength:\",len(bert_fr_multi_uncased_tokenized_sequence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- The cased english tokenizer does the worst job at representing the sequence of all the options.\n",
    "- The cased and uncased multilingual representations are remarkably similar which was not the case for all German words. (Note: this could just be biased by the choice of words in the test sequence.) This means that we can also use the uncased multilingual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Token IDs: [226, 1302, 17259, 117, 8241, 176, 1813, 28201, 4199, 4855, 3084, 11162, 26084, 11192, 3452, 25731, 16331, 1874, 3084, 23609, 1548, 3143, 3452, 8362, 20583, 185, 1197, 10695, 173, 112, 8362, 22572, 28198, 17871, 184, 22702, 8362, 187, 8136, 170, 191, 2744, 10182, 119] \n",
      "Length: 44\n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Token IDs: [225, 38835, 117, 10152, 90381, 10107, 11609, 10131, 15408, 95304, 30832, 10231, 35346, 10131, 12451, 46770, 10368, 10119, 34551, 16092, 172, 112, 10119, 17890, 11814, 10119, 15681, 169, 92416, 119] \n",
      "Length: 30\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Token IDs: [143, 23171, 117, 10152, 74185, 10107, 11676, 10137, 15753, 45289, 37164, 10249, 11676, 10137, 12505, 40560, 10368, 10119, 18427, 13913, 146, 112, 10119, 14785, 10391, 10119, 13277, 143, 60579, 119] \n",
      "Length: 30\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_fr_cased = tokenizer_bert_cased.convert_tokens_to_ids(bert_fr_cased_tokenized_sequence)\n",
    "tokens_ids_fr_multi_cased = tokenizer_bert_multi_cased.convert_tokens_to_ids(bert_fr_multi_cased_tokenized_sequence)\n",
    "tokens_ids_fr_multi_uncased = tokenizer_bert_multi_uncased.convert_tokens_to_ids(bert_fr_multi_uncased_tokenized_sequence)\n",
    "print(\"Cased, English sequence:\\nToken IDs:\",tokens_ids_fr_cased,\"\\nLength:\",len(tokens_ids_fr_cased))\n",
    "print(\"\\nCased, multilingual sequence:\\nToken IDs:\",tokens_ids_fr_multi_cased,\"\\nLength:\",len(tokens_ids_fr_multi_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\nToken IDs:\",tokens_ids_fr_multi_uncased,\"\\nLength:\",len(tokens_ids_fr_multi_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      "Token IDs [101, 226, 1302, 17259, 117, 8241, 176, 1813, 28201, 4199, 4855, 3084, 11162, 26084, 11192, 3452, 25731, 16331, 1874, 3084, 23609, 1548, 3143, 3452, 8362, 20583, 185, 1197, 10695, 173, 112, 8362, 22572, 28198, 17871, 184, 22702, 8362, 187, 8136, 170, 191, 2744, 10182, 119, 102] \n",
      "Length: 46\n",
      "\n",
      "Cased, multilingual sequence:\n",
      "Token IDs: [101, 225, 38835, 117, 10152, 90381, 10107, 11609, 10131, 15408, 95304, 30832, 10231, 35346, 10131, 12451, 46770, 10368, 10119, 34551, 16092, 172, 112, 10119, 17890, 11814, 10119, 15681, 169, 92416, 119, 102] \n",
      "Length: 32\n",
      "\n",
      "Uncased, multilingual sequence:\n",
      "Token IDs: [101, 143, 23171, 117, 10152, 74185, 10107, 11676, 10137, 15753, 45289, 37164, 10249, 11676, 10137, 12505, 40560, 10368, 10119, 18427, 13913, 146, 112, 10119, 14785, 10391, 10119, 13277, 143, 60579, 119, 102] \n",
      "Length: 32\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_with_special_fr_cased = tokenizer_bert_cased.build_inputs_with_special_tokens(tokens_ids_fr_cased)\n",
    "tokens_ids_with_special_fr_multi_cased = tokenizer_bert_multi_cased.build_inputs_with_special_tokens(tokens_ids_fr_multi_cased)\n",
    "tokens_ids_with_special_fr_multi_uncased = tokenizer_bert_multi_uncased.build_inputs_with_special_tokens(tokens_ids_fr_multi_uncased)\n",
    "print(\"Cased, English sequence:\\nToken IDs\",tokens_ids_with_special_fr_cased,\"\\nLength:\", len(tokens_ids_with_special_fr_cased))\n",
    "print(\"\\nCased, multilingual sequence:\\nToken IDs:\",tokens_ids_with_special_fr_multi_cased,\"\\nLength:\",len(tokens_ids_with_special_fr_multi_cased))\n",
    "print(\"\\nUncased, multilingual sequence:\\nToken IDs:\",tokens_ids_with_special_fr_multi_uncased,\"\\nLength:\",len(tokens_ids_with_special_fr_multi_uncased))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cased, English sequence:\n",
      " {'input_ids': [101, 226, 1302, 17259, 117, 8241, 176, 1813, 28201, 4199, 4855, 3084, 11162, 26084, 11192, 3452, 25731, 16331, 1874, 3084, 23609, 1548, 3143, 3452, 8362, 20583, 185, 1197, 10695, 173, 112, 8362, 22572, 28198, 17871, 184, 22702, 8362, 187, 8136, 170, 191, 2744, 10182, 119, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}\n",
      "\n",
      "Cased, mulitlingual sequence:\n",
      " {'input_ids': [101, 225, 38835, 117, 10152, 90381, 10107, 11609, 10131, 15408, 95304, 30832, 10231, 35346, 10131, 12451, 46770, 10368, 10119, 34551, 16092, 172, 112, 10119, 17890, 11814, 10119, 15681, 169, 92416, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Uncased, mulitlingual sequence:\n",
      " {'input_ids': [101, 143, 23171, 117, 10152, 74185, 10107, 11676, 10137, 15753, 45289, 37164, 10249, 11676, 10137, 12505, 40560, 10368, 10119, 18427, 13913, 146, 112, 10119, 14785, 10391, 10119, 13277, 143, 60579, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens_fr_cased = tokenizer_bert_cased.encode_plus(sequence_fr, max_length=50, pad_to_max_length=True)\n",
    "encoded_tokens_fr_multi_cased = tokenizer_bert_multi_cased.encode_plus(sequence_fr, max_length=50, pad_to_max_length=True)\n",
    "encoded_tokens_fr_multi_uncased = tokenizer_bert_multi_uncased.encode_plus(sequence_fr, max_length=50, pad_to_max_length=True)\n",
    "print(\"Cased, English sequence:\\n\",encoded_tokens_fr_cased)\n",
    "print(\"\\nCased, mulitlingual sequence:\\n\",encoded_tokens_fr_multi_cased)\n",
    "print(\"\\nUncased, mulitlingual sequence:\\n\",encoded_tokens_fr_multi_uncased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->            [ C L S ]                     [ C L S ]            <----        101\n",
      "    225    ---->                À                             a                <----        143\n",
      "  38835    ---->             N o ë l                       n o e l             <----      23171\n",
      "    117    ---->                ,                             ,                <----        117\n",
      "  10152    ---->              l e s                         l e s              <----      10152\n",
      "  90381    ---->           g a r ç o n                   g a r c o n           <----      74185\n",
      "  10107    ---->              # # s                         # # s              <----      10107\n",
      "  11609    ---->           P i e r r e                   p i e r r e           <----      11676\n",
      "  10131    ---->               e t                           e t               <----      10137\n",
      "  15408    ---->         P h i l i p p e               p h i l i p p e         <----      15753\n",
      "  95304    ---->            r a m a s                     r a m a s            <----      45289\n",
      "  30832    ---->           # # s e n t                   # # s e n t           <----      37164\n",
      "  10231    ---->              u n e                         u n e              <----      10249\n",
      "  35346    ---->           p i e r r e                   p i e r r e           <----      11676\n",
      "  10131    ---->               e t                           e t               <----      10137\n",
      "  12451    ---->             p u i s                       p u i s             <----      12505\n",
      "  46770    ---->           v i s i t e                   v i s i t e           <----      40560\n",
      "  10368    ---->             # # n t                       # # n t             <----      10368\n",
      "  10119    ---->               u n                           u n               <----      10119\n",
      "  34551    ---->             c a f é                       c a f e             <----      18427\n",
      "  16092    ---->             p r è s                       p r e s             <----      13913\n",
      "    172    ---->                d                             d                <----        146\n",
      "    112    ---->                '                             '                <----        112\n",
      "  10119    ---->               u n                           u n               <----      10119\n",
      "  17890    ---->          c h â t e a u                 c h a t e a u          <----      14785\n",
      "  11814    ---->               o ù                           o u               <----      10391\n",
      "  10119    ---->               u n                           u n               <----      10119\n",
      "  15681    ---->              r o i                         r o i              <----      13277\n",
      "    169    ---->                a                             a                <----        143\n",
      "  92416    ---->             v é c u                       v e c u             <----      60579\n",
      "    119    ---->                .                             .                <----        119\n",
      "    102    ---->            [ S E P ]                     [ S E P ]            <----        102\n"
     ]
    }
   ],
   "source": [
    "for i,j in zip(tokens_ids_with_special_fr_multi_cased, tokens_ids_with_special_fr_multi_uncased):\n",
    "    print('{:7d}    ---->    {:^25}     {:^25}    <----    {:7d}'.format(i, tokenizer_bert_multi_cased.decode(int(i)),tokenizer_bert_multi_uncased.decode(int(j)),j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Findings:\n",
    "- As seen above, when we choose the uncased multilingual tokenizer, Pierre and pierre are assigned the same token even though they mean different things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Experiments with Truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " ',',\n",
       " 'I',\n",
       " 'am',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'an',\n",
       " 'em',\n",
       " '##bed',\n",
       " '##ding',\n",
       " 'for',\n",
       " 'a',\n",
       " 'really',\n",
       " 'long',\n",
       " 'and',\n",
       " 'wind',\n",
       " '##y',\n",
       " 'sentence',\n",
       " 'that',\n",
       " 'is',\n",
       " 'definitely',\n",
       " 'not',\n",
       " 'going',\n",
       " 'to',\n",
       " 'fit',\n",
       " 'ma',\n",
       " '##x',\n",
       " 'length',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'even',\n",
       " 'going',\n",
       " 'to',\n",
       " 'start',\n",
       " 'another',\n",
       " 'sentence',\n",
       " 'to',\n",
       " 'make',\n",
       " 'this',\n",
       " 'sequence',\n",
       " 'especially',\n",
       " 'long',\n",
       " '.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"Hello, I am looking for an embedding for a really long and windy sentence that is definitely not going to fit max length. I am even going to start another sentence to make this sequence especially long.\"\n",
    "sequence_long = sequence + \"This was still not long enough which is why I am still adding words to this sentence to make sure that it is longer than necessary.\"\n",
    "\n",
    "bert_tokenized_sequence = tokenizer.tokenize(sequence)\n",
    "bert_tokenized_sequence_long = tokenizer.tokenize(sequence_long)\n",
    "\n",
    "bert_tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens ids (short): [8667, 117, 146, 1821, 1702, 1111, 1126, 9712, 4774, 3408, 1111, 170, 1541, 1263, 1105, 3223, 1183, 5650, 1115, 1110, 5397, 1136, 1280, 1106, 4218, 12477, 1775, 2251, 119, 146, 1821, 1256, 1280, 1106, 1838, 1330, 5650, 1106, 1294, 1142, 4954, 2108, 1263, 119]\n",
      "\n",
      "Tokens ids (long): [8667, 117, 146, 1821, 1702, 1111, 1126, 9712, 4774, 3408, 1111, 170, 1541, 1263, 1105, 3223, 1183, 5650, 1115, 1110, 5397, 1136, 1280, 1106, 4218, 12477, 1775, 2251, 119, 146, 1821, 1256, 1280, 1106, 1838, 1330, 5650, 1106, 1294, 1142, 4954, 2108, 1263, 119, 1188, 1108, 1253, 1136, 1263, 1536, 1134, 1110, 1725, 146, 1821, 1253, 5321, 1734, 1106, 1142, 5650, 1106, 1294, 1612, 1115, 1122, 1110, 2039, 1190, 3238, 119]\n",
      "\n",
      "Amount of tokens:  44 (short sequence),  71 (long sequence)\n"
     ]
    }
   ],
   "source": [
    "tokens_ids = tokenizer.convert_tokens_to_ids(bert_tokenized_sequence)\n",
    "tokens_ids_long = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sequence_long))\n",
    "\n",
    "print(\"Tokens ids (short): {}\".format(tokens_ids))\n",
    "print(\"\")\n",
    "print(\"Tokens ids (long): {}\".format(tokens_ids_long))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Amount of tokens: \", len(tokens_ids), \"(short sequence), \", len(tokens_ids_long), \"(long sequence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 8667, 117, 146, 1821, 1702, 1111, 1126, 9712, 4774, 3408, 1111, 170, 1541, 1263, 1105, 3223, 1183, 5650, 1115, 1110, 5397, 1136, 1280, 1106, 4218, 12477, 1775, 2251, 119, 146, 1821, 1256, 1280, 1106, 1838, 1330, 5650, 1106, 1294, 1142, 4954, 2108, 1263, 119, 102]\n"
     ]
    }
   ],
   "source": [
    "tokens_ids_with_special = tokenizer.build_inputs_with_special_tokens(tokens_ids)\n",
    "print(tokens_ids_with_special)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 8667, 117, 146, 1821, 1702, 1111, 1126, 9712, 4774, 3408, 119, 102, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "encoded_tokens = tokenizer.encode_plus(\"Hello, I am looking for an embedding.\", \n",
    "                                       max_length=20, \n",
    "                                       pad_to_max_length=True)\n",
    "print(encoded_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output for the shorter sequence with 44 tokens: \n",
      " {'input_ids': [101, 8667, 117, 146, 1821, 1702, 1111, 1126, 9712, 4774, 3408, 1111, 170, 1541, 1263, 1105, 3223, 1183, 5650, 1115, 1110, 5397, 1136, 1280, 1106, 4218, 12477, 1775, 2251, 119, 146, 1821, 1256, 1280, 1106, 1838, 1330, 5650, 1106, 1294, 1142, 4954, 2108, 1263, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "\n",
      "Output for the longer sequence with 71 tokens: \n",
      " {'input_ids': [101, 8667, 117, 146, 1821, 1702, 1111, 1126, 9712, 4774, 3408, 1111, 170, 1541, 1263, 1105, 3223, 1183, 5650, 1115, 1110, 5397, 1136, 1280, 1106, 4218, 12477, 1775, 2251, 119, 146, 1821, 1256, 1280, 1106, 1838, 1330, 5650, 1106, 1294, 1142, 4954, 2108, 1263, 119, 1188, 1108, 1253, 1136, 1263, 1536, 1134, 1110, 1725, 146, 1821, 1253, 5321, 1734, 1106, 1142, 5650, 1106, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# comparing sentence lengths\n",
    "# truncation_strategy is set to longest_first by default, this is why the longer sequence gets automatically cut off\n",
    "# special tokens are also automatically added by default (i.e. start and end token in this case)\n",
    "encoded_tokens_short = tokenizer.encode_plus(sequence, \n",
    "                                       max_length=64, \n",
    "                                       pad_to_max_length=True)\n",
    "\n",
    "encoded_tokens_long = tokenizer.encode_plus(sequence_long, \n",
    "                                       max_length=64, \n",
    "                                       pad_to_max_length=True)\n",
    "\n",
    "print(\"Output for the shorter sequence with\", len(tokens_ids), \"tokens: \\n\", encoded_tokens_short)\n",
    "\n",
    "print(\"\\nOutput for the longer sequence with\", len(tokens_ids_long), \"tokens: \\n\", encoded_tokens_long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] Hello, I am looking for an embedding for a really long and windy sentence that is definitely not going to fit max length. I am even going to start another sentence to make this sequence especially long. This was still not long enough which is why I am still adding words to this sentence to [SEP]'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing the sentence / what remained\n",
    "#for i in list(train_dataset.take(1).as_numpy_iterator())[0][0]['input_ids'][0]:\n",
    "#    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))\n",
    "\n",
    "tokenizer.decode(encoded_tokens_long['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "*How does truncation work? Where does it come in?*\n",
    "\n",
    "Sequences that are longer than the specified length limit are simply cut off. This can theoretically be specified in 'encode_plus', but there is not much wiggle room to truncate differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
