{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    "    TFBertForSequenceClassification,\n",
    "    glue_convert_examples_to_features,\n",
    "    glue_processors,\n",
    ")\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# local packages\n",
    "import utils.model_metrics  as model_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(model_metrics);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load dataset, tokenizer, model from pretrained model/vocabulary\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/root/tensorflow_datasets/glue/sst2/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /root/tensorflow_datasets/glue/sst2/1.0.0\n"
     ]
    }
   ],
   "source": [
    "data, info = tensorflow_datasets.load(name='glue/sst2',with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    version=1.0.0,\n",
       "    description='GLUE, the General Language Understanding Evaluation benchmark\n",
       "(https://gluebenchmark.com/) is a collection of resources for training,\n",
       "evaluating, and analyzing natural language understanding systems.\n",
       "\n",
       "            The Stanford Sentiment Treebank consists of sentences from movie reviews and\n",
       "            human annotations of their sentiment. The task is to predict the sentiment of a\n",
       "            given sentence. We use the two-way (positive/negative) class split, and use only\n",
       "            sentence-level labels.',\n",
       "    homepage='https://nlp.stanford.edu/sentiment/index.html',\n",
       "    features=FeaturesDict({\n",
       "        'idx': tf.int32,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'sentence': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=70042,\n",
       "    splits={\n",
       "        'test': 1821,\n",
       "        'train': 67349,\n",
       "        'validation': 872,\n",
       "    },\n",
       "    supervised_keys=None,\n",
       "    citation=\"\"\"@inproceedings{socher2013recursive,\n",
       "                  title={Recursive deep models for semantic compositionality over a sentiment treebank},\n",
       "                  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},\n",
       "                  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},\n",
       "                  pages={1631--1642},\n",
       "                  year={2013}\n",
       "                }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative', 'positive']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info.features[\"label\"].num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'train': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'validation': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': TensorShape([]), 'label': TensorShape([]), 'sentence': TensorShape([])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.data.ops import dataset_ops\n",
    "dataset_ops.get_legacy_output_shapes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tf.int32, 'label': tf.int64, 'sentence': tf.string}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_types(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': tensorflow.python.framework.ops.Tensor,\n",
       " 'label': tensorflow.python.framework.ops.Tensor,\n",
       " 'sentence': tensorflow.python.framework.ops.Tensor}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_ops.get_legacy_output_classes(data['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['idx', 'label', 'sentence'])\n",
      "{'idx': <tf.Tensor: shape=(), dtype=int32, numpy=16399>, 'label': <tf.Tensor: shape=(), dtype=int64, numpy=0>, 'sentence': <tf.Tensor: shape=(), dtype=string, numpy=b'for the uninitiated plays better on video with the sound '>}\n",
      "tf.Tensor(16399, shape=(), dtype=int32)\n",
      "tf.Tensor(0, shape=(), dtype=int64)\n",
      "tf.Tensor(b'for the uninitiated plays better on video with the sound ', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for l in data['train']:\n",
    "    print(l.keys())\n",
    "    print(l)\n",
    "    print(l['idx'])\n",
    "    print(l['label'])\n",
    "    print(l['sentence'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np_array=np.array(list(data['train'].as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': 16399, 'label': 0, 'sentence': b'for the uninitiated plays better on video with the sound '}\n",
      "{'idx': 1680, 'label': 0, 'sentence': b'like a giant commercial for universal studios , where much of the action takes place '}\n",
      "{'idx': 47917, 'label': 1, 'sentence': b'company once again dazzle and delight us '}\n",
      "{'idx': 17307, 'label': 1, 'sentence': b\"'s no surprise that as a director washington demands and receives excellent performances , from himself and from newcomer derek luke \"}\n",
      "{'idx': 27051, 'label': 0, 'sentence': b', this cross-cultural soap opera is painfully formulaic and stilted . '}\n",
      "{'idx': 67082, 'label': 1, 'sentence': b\", the film is n't nearly as downbeat as it sounds , but strikes a tone that 's alternately melancholic , hopeful and strangely funny . \"}\n",
      "{'idx': 54784, 'label': 0, 'sentence': b'only masochistic moviegoers need apply . '}\n",
      "{'idx': 50543, 'label': 1, 'sentence': b'convince almost everyone that it was put on the screen , just for them '}\n",
      "{'idx': 48736, 'label': 1, 'sentence': b\"like the english patient and the unbearable lightness of being , the hours is one of those reputedly `` unfilmable '' novels that has bucked the odds to emerge as an exquisite motion picture in its own right . \"}\n",
      "{'idx': 26263, 'label': 1, 'sentence': b'his supple understanding of the role '}\n",
      "{'idx': 1456, 'label': 0, 'sentence': b'revelatory nor truly edgy -- merely crassly flamboyant and comedically labored . '}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for element in np_array: \n",
    "    print(element) \n",
    "    i += 1\n",
    "    if i > 10:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'idx': 16399, 'label': 0, 'sentence': b'for the uninitiated plays better on video with the sound '},\n",
       "       {'idx': 1680, 'label': 0, 'sentence': b'like a giant commercial for universal studios , where much of the action takes place '},\n",
       "       {'idx': 47917, 'label': 1, 'sentence': b'company once again dazzle and delight us '},\n",
       "       {'idx': 17307, 'label': 1, 'sentence': b\"'s no surprise that as a director washington demands and receives excellent performances , from himself and from newcomer derek luke \"},\n",
       "       {'idx': 27051, 'label': 0, 'sentence': b', this cross-cultural soap opera is painfully formulaic and stilted . '}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67349,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': 16399,\n",
       " 'label': 0,\n",
       " 'sentence': b'for the uninitiated plays better on video with the sound '}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for GLUE as a tf.data.Dataset instance\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], tokenizer, max_length=128, task='sst-2')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], tokenizer, max_length=128, task='sst-2')\n",
    "train_dataset_batch = train_dataset.shuffle(100).batch(32).repeat(2)\n",
    "valid_dataset_batch = valid_dataset.batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'input_ids': array([  101,  1111,  1103,  8362,  4729, 10691,  1906,  2399,  1618,\n",
       "           1113,  1888,  1114,  1103,  1839,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)},\n",
       "  0)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_dataset.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'input_ids': array([  101,  1111,  1103,  8362,  4729, 10691,  1906,  2399,  1618,\n",
       "           1113,  1888,  1114,  1103,  1839,   102,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)},\n",
       "  0),\n",
       " ({'input_ids': array([ 101, 1176,  170, 4994, 2595, 1111, 8462, 8522,  117, 1187, 1277,\n",
       "          1104, 1103, 2168, 2274, 1282,  102,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)},\n",
       "  0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tensorflow_datasets.as_numpy(train_dataset))[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'input_ids': array([  101,   170,  2860,  2008,  5102,  7782,   185,  5024,  1389,\n",
       "           1106,  5250,  3163,  1223, 16674,  1118,  1103,   183,  2822,\n",
       "            119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0], dtype=int32),\n",
       "   'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       "   'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)},\n",
       "  0)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(valid_dataset.take(1).as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np_train_array=np.array(list(train_dataset_batch.as_numpy_iterator()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np_train_dataset=list(train_dataset.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67349"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4210, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_array[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4210"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample size/batch size and repeat 2 times\n",
    "math.ceil((67349/32)*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 0, 1])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# label\n",
    "np_train_array[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'token_type_ids'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_dataset[0][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([  101,  1111,  1103,  8362,  4729, 10691,  1906,  2399,  1618,\n",
       "         1113,  1888,  1114,  1103,  1839,   102,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0], dtype=int32),\n",
       " 'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_dataset[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 101, 1103, 2322,  118, 5126, 3181,  102,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_array[0][0]['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1103    ---->    t h e\n",
      "   2322    ---->    c a m p a i g n\n",
      "    118    ---->    -\n",
      "   5126    ---->    t r a i l\n",
      "   3181    ---->    p r e s s\n",
      "    102    ---->    [ S E P ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n",
      "      0    ---->    [ P A D ]\n"
     ]
    }
   ],
   "source": [
    "for i in np_train_array[0][0]['input_ids'][0]:\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1111,  1103,  8362,  4729, 10691,  1906,  2399,  1618,\n",
       "        1113,  1888,  1114,  1103,  1839,   102,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_dataset[0][0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_dataset[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_dataset[0][0]['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_array[0][0]['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_array[0][0]['token_type_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 128)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_train_array[0][0]['token_type_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "text=np_array[0]['sentence'].decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for the uninitiated plays better on video with the sound '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for           ---->    [101, 1111, 102]\n",
      "the           ---->    [101, 1103, 102]\n",
      "uninitiated    ---->    [101, 8362, 4729, 10691, 1906, 102]\n",
      "plays         ---->    [101, 2399, 102]\n",
      "better        ---->    [101, 1618, 102]\n",
      "on            ---->    [101, 1113, 102]\n",
      "video         ---->    [101, 1888, 102]\n",
      "with          ---->    [101, 1114, 102]\n",
      "the           ---->    [101, 1103, 102]\n",
      "sound         ---->    [101, 1839, 102]\n",
      "              ---->    [101, 102]\n"
     ]
    }
   ],
   "source": [
    "for word in text.split(' ') :\n",
    "    print('{:10}    ---->    {}'.format(word, tokenizer.encode(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    101    ---->    [ C L S ]\n",
      "   1111    ---->    f o r\n",
      "   1103    ---->    t h e\n",
      "   8362    ---->    u n\n",
      "   4729    ---->    # # i n i\n",
      "  10691    ---->    # # t i a\n",
      "   1906    ---->    # # t e d\n",
      "   2399    ---->    p l a y s\n",
      "   1618    ---->    b e t t e r\n",
      "   1113    ---->    o n\n",
      "   1888    ---->    v i d e o\n",
      "   1114    ---->    w i t h\n",
      "   1103    ---->    t h e\n",
      "   1839    ---->    s o u n d\n",
      "    102    ---->    [ S E P ]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenizer.encode(text):\n",
    "    print('{:7d}    ---->    {}'.format(i, tokenizer.decode(int(i))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model_2 = TFBertForSequenceClassification.from_pretrained('bert-base-cased')\n",
    "model_4 = TFBertForSequenceClassification.from_pretrained('bert-base-cased',num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model_2.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "model_4.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.modeling_tf_bert.TFBertMainLayer at 0x7f3604293910>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7f35ca385f50>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7f35ca394890>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert [] []\n",
      "dropout_37 [] []\n",
      "classifier [] []\n"
     ]
    }
   ],
   "source": [
    "for layer in model_2.layers:\n",
    "    print(layer.name, layer.inbound_nodes, layer.outbound_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i in model_2.layers:\n",
    "    print(i.inbound_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.layers[0].inbound_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_TF_MODULE_IGNORED_PROPERTIES',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_activity_regularizer',\n",
       " '_add_inbound_node',\n",
       " '_add_trackable',\n",
       " '_add_variable_with_custom_getter',\n",
       " '_attribute_sentinel',\n",
       " '_autocast',\n",
       " '_call_accepts_kwargs',\n",
       " '_call_arg_was_passed',\n",
       " '_call_fn_args',\n",
       " '_call_full_argspec',\n",
       " '_callable_losses',\n",
       " '_checkpoint_dependencies',\n",
       " '_clear_losses',\n",
       " '_collect_input_masks',\n",
       " '_compute_dtype',\n",
       " '_dedup_weights',\n",
       " '_deferred_dependencies',\n",
       " '_dtype',\n",
       " '_dtype_defaulted_to_floatx',\n",
       " '_dtype_policy',\n",
       " '_dynamic',\n",
       " '_eager_add_metric',\n",
       " '_eager_losses',\n",
       " '_expects_mask_arg',\n",
       " '_expects_training_arg',\n",
       " '_flatten',\n",
       " '_gather_children_attribute',\n",
       " '_gather_saveables_for_checkpoint',\n",
       " '_get_call_arg_value',\n",
       " '_get_existing_metric',\n",
       " '_get_node_attribute_at_index',\n",
       " '_get_trainable_state',\n",
       " '_handle_activity_regularization',\n",
       " '_handle_deferred_dependencies',\n",
       " '_handle_weight_regularization',\n",
       " '_inbound_nodes',\n",
       " '_init_call_fn_args',\n",
       " '_init_set_name',\n",
       " '_initial_weights',\n",
       " '_input_spec',\n",
       " '_is_layer',\n",
       " '_keras_api_names',\n",
       " '_keras_api_names_v1',\n",
       " '_layers',\n",
       " '_list_extra_dependencies_for_serialization',\n",
       " '_list_functions_for_serialization',\n",
       " '_lookup_dependency',\n",
       " '_losses',\n",
       " '_maybe_build',\n",
       " '_maybe_cast_inputs',\n",
       " '_maybe_create_attribute',\n",
       " '_maybe_initialize_trackable',\n",
       " '_metrics',\n",
       " '_name',\n",
       " '_name_based_attribute_restore',\n",
       " '_name_based_restores',\n",
       " '_name_scope',\n",
       " '_no_dependency',\n",
       " '_non_trainable_weights',\n",
       " '_obj_reference_counts',\n",
       " '_obj_reference_counts_dict',\n",
       " '_object_identifier',\n",
       " '_outbound_nodes',\n",
       " '_preload_simple_restoration',\n",
       " '_restore_from_checkpoint_position',\n",
       " '_self_name_based_restores',\n",
       " '_self_setattr_tracking',\n",
       " '_self_unconditional_checkpoint_dependencies',\n",
       " '_self_unconditional_deferred_dependencies',\n",
       " '_self_unconditional_dependency_names',\n",
       " '_self_update_uid',\n",
       " '_set_connectivity_metadata_',\n",
       " '_set_dtype_policy',\n",
       " '_set_mask_metadata',\n",
       " '_set_trainable_state',\n",
       " '_setattr_tracking',\n",
       " '_should_compute_mask',\n",
       " '_single_restoration_from_checkpoint_position',\n",
       " '_stateful',\n",
       " '_supports_ragged_inputs',\n",
       " '_symbolic_add_metric',\n",
       " '_symbolic_call',\n",
       " '_tf_api_names',\n",
       " '_tf_api_names_v1',\n",
       " '_thread_local',\n",
       " '_track_trackable',\n",
       " '_trackable_saved_model_saver',\n",
       " '_tracking_metadata',\n",
       " '_trainable',\n",
       " '_trainable_weights',\n",
       " '_unconditional_checkpoint_dependencies',\n",
       " '_unconditional_dependency_names',\n",
       " '_update_uid',\n",
       " '_updates',\n",
       " '_warn_about_input_casting',\n",
       " 'activation',\n",
       " 'activity_regularizer',\n",
       " 'add_loss',\n",
       " 'add_metric',\n",
       " 'add_update',\n",
       " 'add_variable',\n",
       " 'add_weight',\n",
       " 'apply',\n",
       " 'bias',\n",
       " 'bias_constraint',\n",
       " 'bias_initializer',\n",
       " 'bias_regularizer',\n",
       " 'build',\n",
       " 'built',\n",
       " 'call',\n",
       " 'compute_mask',\n",
       " 'compute_output_shape',\n",
       " 'compute_output_signature',\n",
       " 'count_params',\n",
       " 'dtype',\n",
       " 'dynamic',\n",
       " 'from_config',\n",
       " 'get_config',\n",
       " 'get_input_at',\n",
       " 'get_input_mask_at',\n",
       " 'get_input_shape_at',\n",
       " 'get_losses_for',\n",
       " 'get_output_at',\n",
       " 'get_output_mask_at',\n",
       " 'get_output_shape_at',\n",
       " 'get_updates_for',\n",
       " 'get_weights',\n",
       " 'inbound_nodes',\n",
       " 'input',\n",
       " 'input_mask',\n",
       " 'input_shape',\n",
       " 'input_spec',\n",
       " 'kernel',\n",
       " 'kernel_constraint',\n",
       " 'kernel_initializer',\n",
       " 'kernel_regularizer',\n",
       " 'losses',\n",
       " 'metrics',\n",
       " 'name',\n",
       " 'name_scope',\n",
       " 'non_trainable_variables',\n",
       " 'non_trainable_weights',\n",
       " 'outbound_nodes',\n",
       " 'output',\n",
       " 'output_mask',\n",
       " 'output_shape',\n",
       " 'set_weights',\n",
       " 'stateful',\n",
       " 'submodules',\n",
       " 'supports_masking',\n",
       " 'trainable',\n",
       " 'trainable_variables',\n",
       " 'trainable_weights',\n",
       " 'units',\n",
       " 'updates',\n",
       " 'use_bias',\n",
       " 'variables',\n",
       " 'weights',\n",
       " 'with_name_scope']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model_2.layers[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10 steps\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 336s 34s/step - loss: 0.6603 - accuracy: 0.6062\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 298s 30s/step - loss: 0.5682 - accuracy: 0.7156\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "history_2 = model_2.fit(train_dataset_batch, epochs=2, steps_per_epoch=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 10 steps\n",
      "Epoch 1/2\n",
      "10/10 [==============================] - 333s 33s/step - loss: 1.3865 - accuracy: 0.3000\n",
      "Epoch 2/2\n",
      "10/10 [==============================] - 298s 30s/step - loss: 0.8561 - accuracy: 0.5906\n"
     ]
    }
   ],
   "source": [
    "history_4 = model_4.fit(train_dataset_batch, epochs=2, steps_per_epoch=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_2.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6602770328521729, 0.5682326197624207],\n",
       " 'accuracy': [0.60625, 0.715625]}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_2.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(list((valid_dataset_batch).as_numpy_iterator())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_valid_dataset = list((valid_dataset_batch).as_numpy_iterator())[0][1]\n",
    "len(np_valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "y_hat_2 = model_2.predict(valid_dataset_batch)\n",
    "#y_hat = model.predict_on_batch(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_hat_4 = model_4.predict(valid_dataset_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.7738919 ,  0.30267423],\n",
       "       [-0.6332481 ,  0.06294108],\n",
       "       [-0.24078533, -0.7245116 ],\n",
       "       ...,\n",
       "       [-0.9456006 ,  0.8402885 ],\n",
       "       [-0.4257586 , -0.33841658],\n",
       "       [-0.8196051 ,  0.38295242]], dtype=float32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_2\n",
    "\n",
    "#y_hat.shape\n",
    "# using the whole dataset\n",
    "# for 4 categories: (872,4)\n",
    "# for 2 categories: (872,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.1233698 ,  1.0862385 , -1.1531808 , -0.875806  ],\n",
       "       [ 1.0463811 ,  1.2987903 , -1.1533064 , -0.94067997],\n",
       "       [ 1.1344936 ,  0.5214642 , -0.9104535 , -0.62809724],\n",
       "       ...,\n",
       "       [ 0.7448484 ,  1.5976157 , -1.2212645 , -1.0037866 ],\n",
       "       [ 0.9065665 ,  1.4701813 , -1.3181516 , -1.0046307 ],\n",
       "       [ 1.0439866 ,  1.2180257 , -1.2309015 , -0.94497764]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# confusion matrix\n",
    "predict_index_2 = np.argmax(y_hat_2, axis=1)\n",
    "#predict_index = np.argmax(y_hat, axis=2)\n",
    "# for 4 categories: categories 2 and 3 never get picked (which was to be hoped)\n",
    "# for 2 categories: always 1, lots of negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_index_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "predict_index_4 = np.argmax(y_hat_4, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_index_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np_valid_array = np.array(list(valid_dataset_batch.as_numpy_iterator()))\n",
    "true_index = []\n",
    "for batch in range(0,np_valid_array.shape[0]):\n",
    "    true_index.append(np_valid_array[batch][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 14)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(0,np_valid_array.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "true_index = np.concatenate(true_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1,\n",
       "       0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#true_index = np_valid_dataset[0][1]\n",
    "true_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872\n",
      "872\n",
      "872\n"
     ]
    }
   ],
   "source": [
    "print(len(predict_index_2))\n",
    "print(len(predict_index_4))\n",
    "print(len(true_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f357c45c690>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(true_index, predict_index_2), \n",
    "            annot=True, cbar=False, fmt='d')#, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f357c45cc10>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(confusion_matrix(true_index, predict_index_4), \n",
    "            annot=True, cbar=False, fmt='d')#, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape = (128,), dtype='int64')  \n",
    "bert_ini = TFBertModel.from_pretrained('bert-base-cased')(input_layer)\n",
    "bert = bert_ini[1]    \n",
    "dropout = tf.keras.layers.Dropout(0.1)(bert)\n",
    "flat = tf.keras.layers.Flatten()(dropout)\n",
    "classifier = tf.keras.layers.Dense(units=5)(flat)                  \n",
    "model2 = tf.keras.Model(inputs=input_layer, outputs=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'tf_bert_model/Identity:0' shape=(None, 128, 768) dtype=float32>,\n",
       " <tf.Tensor 'tf_bert_model/Identity_1:0' shape=(None, 768) dtype=float32>)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model2.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  ((None, 128, 768), (None, 108310272 \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 3845      \n",
      "=================================================================\n",
      "Total params: 108,314,117\n",
      "Trainable params: 108,314,117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "#history = model2.fit(train_dataset, epochs=2, steps_per_epoch=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "#history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "#                    validation_data=valid_dataset, validation_steps=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load the TensorFlow model in PyTorch for inspection\n",
    "#model.save_pretrained('./save/')\n",
    "#pytorch_model = BertForSequenceClassification.from_pretrained('./save/', from_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers.tokenization_utils:Unable to convert output to tensors format pt, PyTorch or TensorFlow is not available.\n",
      "WARNING:transformers.tokenization_utils:Unable to convert output to tensors format pt, PyTorch or TensorFlow is not available.\n"
     ]
    }
   ],
   "source": [
    "# Quickly test a few predictions - MRPC is a paraphrasing task, let's see if our model learned the task\n",
    "sentence_0 = \"This research was consistent with his findings.\"\n",
    "sentence_1 = \"His findings were compatible with this research.\"\n",
    "sentence_2 = \"His findings were not compatible with this research.\"\n",
    "inputs_1 = tokenizer.encode_plus(sentence_0, sentence_1, add_special_tokens=True, return_tensors='pt')\n",
    "inputs_2 = tokenizer.encode_plus(sentence_0, sentence_2, add_special_tokens=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#pred_1 = pytorch_model(inputs_1['input_ids'], token_type_ids=inputs_1['token_type_ids'])[0].argmax().item()\n",
    "#pred_2 = pytorch_model(inputs_2['input_ids'], token_type_ids=inputs_2['token_type_ids'])[0].argmax().item()\n",
    "\n",
    "#print(\"sentence_1 is\", \"a paraphrase\" if pred_1 else \"not a paraphrase\", \"of sentence_0\")\n",
    "#print(\"sentence_2 is\", \"a paraphrase\" if pred_2 else \"not a paraphrase\", \"of sentence_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
