{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The IMDb Dataset\n",
    "The IMDb dataset consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model` \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    "    TFBertForSequenceClassification,\n",
    "    glue_convert_examples_to_features,\n",
    "    glue_processors\n",
    ")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# new\n",
    "import re\n",
    "from keras.models import Sequential, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.1.0-rc2-17-ge5bf8de 2.1.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.4-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available !!!!\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# note: these need to be specified in the config.sh file\n",
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    checkpoint_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import preprocessing.preprocessing as pp\n",
    "import utils.model_metrics as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);\n",
    "importlib.reload(mm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "TOKENIZER = 'bert-base-multilingual-uncased'\n",
    "SEQUENCE_LENGTH = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading a data from Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:No config specified, defaulting to first: imdb_reviews/plain_text\n",
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset imdb_reviews (/home/vera_luechinger/data/imdb_reviews/plain_text/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/vera_luechinger/data/imdb_reviews/plain_text/1.0.0\n"
     ]
    }
   ],
   "source": [
    "#data, info = tensorflow_datasets.load(name='glue/sst2',\n",
    "#                                      data_dir=data_dir,\n",
    "#                                      with_info=True)\n",
    "\n",
    "data, info = tensorflow_datasets.load(name=\"imdb_reviews\",\n",
    "                            data_dir=data_dir,\n",
    "                            as_supervised=True,\n",
    "                            with_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# IMDb specific:\n",
    "data_valid = data['test'].take(1000)\n",
    "\n",
    "# trying to create a true validation data set for after the computation\n",
    "#data_valid_ext = data['test'].take(2000)\n",
    "#data_valid = data_valid_ext.take(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking basic info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='imdb_reviews',\n",
       "    version=1.0.0,\n",
       "    description='Large Movie Review Dataset.\n",
       "This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets. We provide a set of 25,000 highly polar movie reviews for training, and 25,000 for testing. There is additional unlabeled data for use as well.',\n",
       "    homepage='http://ai.stanford.edu/~amaas/data/sentiment/',\n",
       "    features=FeaturesDict({\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'text': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=100000,\n",
       "    splits={\n",
       "        'test': 25000,\n",
       "        'train': 25000,\n",
       "        'unsupervised': 50000,\n",
       "    },\n",
       "    supervised_keys=('text', 'label'),\n",
       "    citation=\"\"\"@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
       "      author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
       "      title     = {Learning Word Vectors for Sentiment Analysis},\n",
       "      booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
       "      month     = {June},\n",
       "      year      = {2011},\n",
       "      address   = {Portland, Oregon, USA},\n",
       "      publisher = {Association for Computational Linguistics},\n",
       "      pages     = {142--150},\n",
       "      url       = {http://www.aclweb.org/anthology/P11-1015}\n",
       "    }\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "      ['neg', 'pos']\n",
      "\n",
      "Number of label:\n",
      "      2\n",
      "\n",
      "Structure of the data:\n",
      "      dict_keys(['text', 'label'])\n",
      "\n",
      "Number of entries:\n",
      "   Train dataset: 25000\n",
      "   Test dataset:  25000\n",
      "--> validation dataset not defined\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_dataset(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking basic info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'train': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>,\n",
       " 'unsupervised': <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'unsupervised'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   (TensorShape([]), TensorShape([]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   (tf.string, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   (<class 'tensorflow.python.framework.ops.Tensor'>, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (25000, 2)\n",
      "data format incompatible\n"
     ]
    }
   ],
   "source": [
    "# only works for glue-compatible datasets\n",
    "try:\n",
    "    pp.print_info_data(data['train'])\n",
    "except AttributeError:\n",
    "    print('data format incompatible')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:           25000/  1000\n",
      "Batch size:                32/    64\n",
      "Step per epoch:           782/    16\n",
      "Total number of batch:   2346/    48\n"
     ]
    }
   ],
   "source": [
    "# changes: had to eliminate all lines concerning a test data set because we only have train and valid\n",
    "\n",
    "\n",
    "# define parameters\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "BATCH_SIZE_VALID = 64\n",
    "EPOCH = 2\n",
    "\n",
    "# extract parameters\n",
    "size_train_dataset = info.splits['train'].num_examples\n",
    "#size_test_dataset = info.splits['test'].num_examples\n",
    "#size_valid_dataset = info.splits['validation'].num_examples\n",
    "\n",
    "# the size for the validation data set has been manually computed according to the function \n",
    "# pp.print_info_data because the test set has been manually split above\n",
    "size_valid_dataset = np.shape(np.array(list(data_valid.as_numpy_iterator())))[0]\n",
    "number_label = info.features[\"label\"].num_classes\n",
    "\n",
    "# computer parameter\n",
    "STEP_EPOCH_TRAIN = math.ceil(size_train_dataset/BATCH_SIZE_TRAIN)\n",
    "#STEP_EPOCH_TEST = math.ceil(size_test_dataset/BATCH_SIZE_TEST)\n",
    "STEP_EPOCH_VALID = math.ceil(size_valid_dataset/BATCH_SIZE_VALID)\n",
    "\n",
    "\n",
    "#print('Dataset size:          {:6}/{:6}/{:6}'.format(size_train_dataset, size_test_dataset, size_valid_dataset))\n",
    "#print('Batch size:            {:6}/{:6}/{:6}'.format(BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, BATCH_SIZE_VALID))\n",
    "#print('Step per epoch:        {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN, STEP_EPOCH_TEST, STEP_EPOCH_VALID))\n",
    "#print('Total number of batch: {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN*(EPOCH+1), STEP_EPOCH_TEST*(EPOCH+1), STEP_EPOCH_VALID*(EPOCH+1)))\n",
    "print('Dataset size:          {:6}/{:6}'.format(size_train_dataset, size_valid_dataset))\n",
    "print('Batch size:            {:6}/{:6}'.format(BATCH_SIZE_TRAIN, BATCH_SIZE_VALID))\n",
    "print('Step per epoch:        {:6}/{:6}'.format(STEP_EPOCH_TRAIN, STEP_EPOCH_VALID))\n",
    "print('Total number of batch: {:6}/{:6}'.format(STEP_EPOCH_TRAIN*(EPOCH+1), STEP_EPOCH_VALID*(EPOCH+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tokenizer and prepare data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdf833cc0ab4268a1db3d7dacb38f75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=871891.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# recap of input dataset\n",
    "print(data['train'])\n",
    "print(tf.data.experimental.cardinality(data['train']))\n",
    "#print(tf.data.experimental.cardinality(data['test']))\n",
    "#print(tf.data.experimental.cardinality(data['validation']))\n",
    "print(tf.data.experimental.cardinality(data_valid))\n",
    "# super slow since looping over all data\n",
    "#print(len(list(data['train'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Additional steps for the IMDb dataset specifically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews):\n",
    "    #REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "    REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "    \n",
    "    #ae, oe, ue => only for GERMAN data\n",
    "    #REPLACE_UMLAUT_AE = re.compile(\"(ae)\")\n",
    "    #REPLACE_UMLAUT_OE = re.compile(\"(oe)\")\n",
    "    #REPLACE_UMLAUT_UE = re.compile(\"(ue)\")\n",
    "    \n",
    "    #reviews = [REPLACE_NO_SPACE.sub(\"\", line[0].decode(\"utf-8\").lower()) for line in np.array(list(reviews.as_numpy_iterator()))]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(\" \", line[0].decode(\"utf-8\")) for line in np.array(list(reviews.as_numpy_iterator()))]# for line in reviews]\n",
    "    #reviews = [REPLACE_UMLAUT_AE.sub(\"ä\", line[0]) for line in reviews]\n",
    "    #reviews = [REPLACE_UMLAUT_OE.sub(\"ö\", line[0]) for line in reviews]\n",
    "    #reviews = [REPLACE_UMLAUT_UE.sub(\"ü\", line[0]) for line in reviews]\n",
    "    \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "reviews_train_clean = preprocess_reviews(data['train'])\n",
    "reviews_valid_clean = preprocess_reviews(data_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Converting Data to GLUE Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "labels_train = [int(line[1].decode(\"utf-8\")) for line in np.array(list(data['train'].as_numpy_iterator()))]\n",
    "labels_valid = [int(line[1].decode(\"utf-8\")) for line in np.array(list(data_valid.as_numpy_iterator()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_data_np = pp.convert_np_array_to_glue_format(reviews_train_clean, labels_train)\n",
    "valid_data_np = pp.convert_np_array_to_glue_format(reviews_valid_clean, labels_valid, shift=len(list(train_data_np)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare data for BERT\n",
    "#train_dataset = glue_convert_examples_to_features(data['train'], \n",
    "#                                                  tokenizer, \n",
    "#                                                  max_length=128, \n",
    "#                                                  task='sst-2')\n",
    "#test_dataset = glue_convert_examples_to_features(data['test'], \n",
    "#                                                  tokenizer, \n",
    "#                                                  max_length=128, \n",
    "#                                                  task='sst-2')\n",
    "#valid_dataset = glue_convert_examples_to_features(data['validation'], \n",
    "#                                                  tokenizer, \n",
    "#                                                  max_length=128, \n",
    "#                                                  task='sst-2')\n",
    "\n",
    "train_dataset = glue_convert_examples_to_features(train_data_np, \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=SEQUENCE_LENGTH, \n",
    "                                                  task='sst-2')\n",
    "valid_dataset = glue_convert_examples_to_features(valid_data_np, \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=SEQUENCE_LENGTH, \n",
    "                                                  task='sst-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# recap of pre processing dataset\n",
    "print(train_dataset)\n",
    "print(tf.data.experimental.cardinality(train_dataset))\n",
    "#print(tf.data.experimental.cardinality(test_dataset))\n",
    "print(tf.data.experimental.cardinality(valid_dataset))\n",
    "# super slow since looping over all data\n",
    "print(len(list(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# set shuffle and batch size\n",
    "train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE_TRAIN).repeat(EPOCH+1)\n",
    "#test_dataset = test_dataset.shuffle(100).batch(BATCH_SIZE_TEST).repeat(EPOCH+1)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE_VALID) #.repeat(EPOCH+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <BatchDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   ({'input_ids': TensorShape([None, None]), 'attention_mask': TensorShape([None, None]), 'token_type_ids': TensorShape([None, None])}, TensorShape([None]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   ({'input_ids': <class 'tensorflow.python.framework.ops.Tensor'>, 'attention_mask': <class 'tensorflow.python.framework.ops.Tensor'>, 'token_type_ids': <class 'tensorflow.python.framework.ops.Tensor'>}, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (16, 2)\n",
      "   ---> 16 batches\n",
      "   ---> 2 dim\n",
      "        label\n",
      "           shape: (64,)\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [input_ids       / attention_mask  / token_type_ids ]\n",
      "           [(64, 512)       / (64, 512)       / (64, 512)      ]\n",
      "           [ndarray         / ndarray         / ndarray        ]\n",
      "\n",
      "\n",
      "# Examples of data:\n",
      "array([{'input_ids': array([[  101, 10768, 10320, ...,     0,     0,     0],\n",
      "       [  101,   143, 11418, ...,     0,     0,     0],\n",
      "       [  101, 92672, 10158, ..., 17313,   117,   102],\n",
      "       ...,\n",
      "       [  101, 42700, 11999, ...,     0,     0,     0],\n",
      "       [  101, 11530,   112, ...,     0,     0,     0],\n",
      "       [  101, 20220, 10935, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1,\n",
      "       0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "       0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0])],\n",
      "      dtype=object)\n",
      "array([{'input_ids': array([[  101, 11403, 88456, ..., 13202, 53092,   102],\n",
      "       [  101,   151, 10140, ...,     0,     0,     0],\n",
      "       [  101, 10372, 13113, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 10497, 11260, ...,     0,     0,     0],\n",
      "       [  101, 10372, 13113, ...,     0,     0,     0],\n",
      "       [  101,   151, 16289, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 1, 1, 1],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0,\n",
      "       0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
      "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1])],\n",
      "      dtype=object)\n",
      "array([{'input_ids': array([[  101,   151, 84447, ...,     0,     0,     0],\n",
      "       [  101,   151, 10574, ...,     0,     0,     0],\n",
      "       [  101,   151, 16289, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101,   151, 84447, ...,     0,     0,     0],\n",
      "       [  101,   151, 14650, ...,     0,     0,     0],\n",
      "       [  101, 24395, 79035, ..., 10141, 41863,   102]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 1, 1, 1]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
      "       0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0,\n",
      "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1])],\n",
      "      dtype=object)\n",
      "array([{'input_ids': array([[  101, 10139, 10103, ...,     0,     0,     0],\n",
      "       [  101, 17511, 13626, ...,     0,     0,     0],\n",
      "       [  101,   151, 10902, ...,     0,     0,     0],\n",
      "       ...,\n",
      "       [  101, 11523,   143, ...,     0,     0,     0],\n",
      "       [  101, 67715, 14433, ...,     0,     0,     0],\n",
      "       [  101, 10372, 13113, ...,     0,     0,     0]], dtype=int32), 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32), 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32)},\n",
      "       array([0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0,\n",
      "       1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1,\n",
      "       1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0])],\n",
      "      dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   ({'input_ids': TensorShape([None, None]), 'attention_mask': TensorShape([None, None]), 'token_type_ids': TensorShape([None, None])}, TensorShape([None]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   ({'input_ids': <class 'tensorflow.python.framework.ops.Tensor'>, 'attention_mask': <class 'tensorflow.python.framework.ops.Tensor'>, 'token_type_ids': <class 'tensorflow.python.framework.ops.Tensor'>}, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (2346, 2)\n",
      "   ---> 2346 batches\n",
      "   ---> 2 dim\n",
      "        label\n",
      "           shape: (32,)\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [input_ids       / attention_mask  / token_type_ids ]\n",
      "           [(32, 512)       / (32, 512)       / (32, 512)      ]\n",
      "           [ndarray         / ndarray         / ndarray        ]\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(train_dataset,print_example=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_ids     ---->    attention_mask    token_type_ids    modified text                 \n",
      "\n",
      "       101     ---->           1                 1          [ C L S ]                     \n",
      "       107     ---->           1                 1          \"                             \n",
      "     10103     ---->           1                 1          t h e                         \n",
      "     42592     ---->           1                 1          p l a i n s                   \n",
      "     10629     ---->           1                 1          # # m a n                     \n",
      "       107     ---->           1                 1          \"                             \n",
      "     36076     ---->           1                 1          r e p r e s e n t s           \n",
      "     10103     ---->           1                 1          t h e                         \n",
      "     11866     ---->           1                 1          d i r e c t o r               \n",
      "     17333     ---->           1                 1          # # i a l                     \n",
      "     11061     ---->           1                 1          p r o                         \n",
      "     39035     ---->           1                 1          # # w e s                     \n",
      "     10107     ---->           1                 1          # # s                         \n",
      "     10108     ---->           1                 1          o f                           \n",
      "     36464     ---->           1                 1          c e c i l                     \n",
      "       144     ---->           1                 1          b                             \n",
      "       119     ---->           1                 1          .                             \n",
      "     17793     ---->           1                 1          d e m i                       \n",
      "     11455     ---->           1                 1          # # l l e                     \n",
      "     10160     ---->           1                 1          a t                           \n",
      "     10491     ---->           1                 1          i t s                         \n",
      "     10889     ---->           1                 1          m o s t                       \n",
      "     20150     ---->           1                 1          i n a                         \n",
      "     28276     ---->           1                 1          # # c c                       \n",
      "     48111     ---->           1                 1          # # u r a t                   \n",
      "     10111     ---->           1                 1          # # e                         \n",
      "     10110     ---->           1                 1          a n d                         \n",
      "     10119     ---->           1                 1          u n                           \n",
      "     17772     ---->           1                 1          f a c t                       \n",
      "     19785     ---->           1                 1          # # u a l                     \n",
      "       119     ---->           1                 1          .                             \n",
      "     10197     ---->           1                 1          i t                           \n",
      "     22655     ---->           1                 1          s e t s                       \n",
      "     10700     ---->           1                 1          u p                           \n",
      "     22340     ---->           1                 1          p a r a l l e l               \n",
      "     28674     ---->           1                 1          p l o t                       \n",
      "     10107     ---->           1                 1          # # s                         \n",
      "     10139     ---->           1                 1          f o r                         \n",
      "     10181     ---->           1                 1          n o                           \n",
      "     15095     ---->           1                 1          l e s s                       \n",
      "     90120     ---->           1                 1          s t e l l a r                 \n",
      "     10144     ---->           1                 1          a n                           \n"
     ]
    }
   ],
   "source": [
    "pp.print_detail_tokeniser(train_dataset, tokenizer, max_entries=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define the callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                         save_weights_only=True),\n",
    "                                                         #save_freq=2000),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Function for decaying the learning rate.\n",
    "def decay(epoch):\n",
    "    if epoch < 3:\n",
    "        return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "decay_callback = tf.keras.callbacks.LearningRateScheduler(decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Print learning rate at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200406-170034\n"
     ]
    }
   ],
   "source": [
    "# checking existing folders\n",
    "for i in os.listdir(tensorboard_dir):\n",
    "    if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200406-170034\n"
     ]
    }
   ],
   "source": [
    "# clean old TensorBoard directory \n",
    "for i in os.listdir(tensorboard_dir):\n",
    "        if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "            print(i)\n",
    "            shutil.rmtree(tensorboard_dir+'/'+i, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "log_dir=tensorboard_dir+'/'+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1, \n",
    "                                                      embeddings_freq=1,\n",
    "                                                      write_graph=True,\n",
    "                                                      update_freq='batch',\n",
    "                                                      profile_batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Loss and efficiency per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class History_per_step(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, validation_data, N):\n",
    "        self.validation_data = validation_data\n",
    "        self.N = N\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_train_begin(self, validation_data, logs={}):\n",
    "        self.steps = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.val_steps = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('accuracy'))\n",
    "        self.steps.append(self.batch)\n",
    "        print('\\n training set -> batch:{} loss:{} and acc: {}'.format(self.batch,logs.get('loss'),logs.get('accuracy')))\n",
    "        \n",
    "        if self.batch % self.N == 0:\n",
    "            loss_val, acc_val = self.model.evaluate(self.validation_data, verbose=0)\n",
    "            self.val_losses.append(loss_val)\n",
    "            self.val_accuracies.append(acc_val)\n",
    "            self.val_steps.append(self.batch)\n",
    "            print('\\n validation set -> batch:{} val loss:{} and val acc: {}'.format(self.batch,loss_val, acc_val))\n",
    "\n",
    "        self.batch += 1\n",
    "    \n",
    "    def on_test_batch_end(self, batch, logs={}):    \n",
    "        #print('{}\\n'.format(logs))\n",
    "        return\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}): \n",
    "        #print('{}\\n'.format(logs))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checks callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelCheckpoint need to unpack this tuple by adding *\n"
     ]
    }
   ],
   "source": [
    "list_callback = [tensorboard_callback, checkpoint_callback, decay_callback]\n",
    "for cb in list_callback:\n",
    "    if type(cb).__name__=='tuple':\n",
    "        print(cb[0].__class__.__name__, 'need to unpack this tuple by adding *')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Use TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c61d60b2d6334f03891d373fe3f6f20b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=569.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9543e62a174536a853e5e4d2ddf2e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=999358484.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "# switched to default values\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "# Gradient clipping in the optimizer (by setting clipnorm or clipvalue) is currently unsupported when using a distribution strategy\n",
    "# clipnorm=1.0\n",
    "\n",
    "# loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Uses the tf.distribute.MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine\n",
    "strategy_model_1 = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy_model_1.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy_model_1.scope():\n",
    "    # metric\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    \n",
    "    # model\n",
    "    model_1 = TFBertForSequenceClassification.from_pretrained(TOKENIZER,num_labels=number_label)\n",
    "    #model.layers[-1].activation = tf.keras.activations.softmax\n",
    "    model_1._name='tf_bert_classification'\n",
    "    model_1.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  167356416 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 167,357,954\n",
      "Trainable params: 167,357,954\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Building a custom classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def custom_keras_model(number_classes, bert_model):\n",
    "\n",
    "    # create model\n",
    "    input_layer = tf.keras.Input(shape = (128,), dtype='int64')    \n",
    "    bert_ini = TFBertModel.from_pretrained('bert-base-cased') (input_layer)\n",
    "    # This is because in a bert pretraining progress, there are two tasks: \n",
    "    # masked token prediction and next sentence predition . \n",
    "    # The first needs hidden state of each tokens ( shape: [batch_size, sequence_length, hidden_size]) \n",
    "    # the second needs the embedding of the whole sequence (shape : [batch_size, hidden_size] ) .\n",
    "    bert = bert_ini[1]    \n",
    "    dropout = tf.keras.layers.Dropout(0.1)(bert)\n",
    "    flat = tf.keras.layers.Flatten()(dropout)\n",
    "    classifier = tf.keras.layers.Dense(units=number_classes )(flat) # activation='softmax'               \n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=classifier, name='custom_tf_bert_classification')\n",
    "\n",
    "    return model, bert_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da730747c52f4f14820b6c9f32bfe75c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=361.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19927d11c7ec4fc48b4178fb336baa2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=526681800.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "# Gradient clipping in the optimizer (by setting clipnorm or clipvalue) is currently unsupported when using a distribution strategy\n",
    "# clipnorm=1.0\n",
    "\n",
    "# loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Uses the tf.distribute.MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine\n",
    "strategy_model_2 = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy_model_1.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy_model_2.scope():\n",
    "    # metric\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    \n",
    "    # model\n",
    "    model_2, bert_ini = custom_keras_model(number_label, 'bert-base-cased')\n",
    "    model_2.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'tf_bert_model/Identity:0' shape=(None, 128, 768) dtype=float32>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ini[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'tf_bert_model/Identity_1:0' shape=(None, 768) dtype=float32>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ini[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  ((None, 128, 768), (None, 108310272 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Choose the model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's name: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "#model=model_2\n",
    "model=model_1\n",
    "print('model\\'s name: {}'.format(model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-640aded294089a1a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-640aded294089a1a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir   {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Final feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def data_feature_extraction(data, name):\n",
    "    if name=='custom_tf_bert_classification':\n",
    "        print('custom model: {}'.format(name))\n",
    "        return data.map(pp.feature_selection)\n",
    "    elif name=='tf_bert_classification':\n",
    "        print('standard model: {}'.format(name))\n",
    "        return data\n",
    "    else:\n",
    "        print('!!! non defined model !!!!')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(list(valid_dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard model: tf_bert_classification\n",
      "standard model: tf_bert_classification\n",
      "Train for 782 steps, validate for 16 steps\n",
      "\n",
      " training set -> batch:1 loss:0.7071684002876282 and acc: 0.4375\n",
      "  1/782 [..............................] - ETA: 10:27:11 - loss: 0.7072 - accuracy: 0.4375\n",
      " training set -> batch:2 loss:0.6903931498527527 and acc: 0.515625\n",
      "  2/782 [..............................] - ETA: 7:52:09 - loss: 0.6988 - accuracy: 0.5156 \n",
      " training set -> batch:3 loss:0.6876280903816223 and acc: 0.5\n",
      "  3/782 [..............................] - ETA: 6:58:57 - loss: 0.6951 - accuracy: 0.5000\n",
      " training set -> batch:4 loss:0.6836985349655151 and acc: 0.5234375\n",
      "  4/782 [..............................] - ETA: 6:32:04 - loss: 0.6922 - accuracy: 0.5234\n",
      " training set -> batch:5 loss:0.6875916123390198 and acc: 0.543749988079071\n",
      "  5/782 [..............................] - ETA: 6:15:12 - loss: 0.6913 - accuracy: 0.5437\n",
      " training set -> batch:6 loss:0.6818273067474365 and acc: 0.5572916865348816\n",
      "  6/782 [..............................] - ETA: 6:03:36 - loss: 0.6897 - accuracy: 0.5573\n",
      " training set -> batch:7 loss:0.6930062770843506 and acc: 0.5491071343421936\n",
      "  7/782 [..............................] - ETA: 5:56:07 - loss: 0.6902 - accuracy: 0.5491\n",
      " training set -> batch:8 loss:0.6939849853515625 and acc: 0.54296875\n",
      "  8/782 [..............................] - ETA: 5:48:59 - loss: 0.6907 - accuracy: 0.5430\n",
      " training set -> batch:9 loss:0.6770714521408081 and acc: 0.5590277910232544\n",
      "  9/782 [..............................] - ETA: 5:43:14 - loss: 0.6892 - accuracy: 0.5590\n",
      " training set -> batch:10 loss:0.6852560043334961 and acc: 0.559374988079071\n",
      " 10/782 [..............................] - ETA: 5:38:41 - loss: 0.6888 - accuracy: 0.5594\n",
      " training set -> batch:11 loss:0.6892625093460083 and acc: 0.5511363744735718\n",
      " 11/782 [..............................] - ETA: 5:35:00 - loss: 0.6888 - accuracy: 0.5511\n",
      " training set -> batch:12 loss:0.6729637980461121 and acc: 0.5520833134651184\n",
      " 12/782 [..............................] - ETA: 5:31:50 - loss: 0.6875 - accuracy: 0.5521\n",
      " training set -> batch:13 loss:0.7246671915054321 and acc: 0.5480769276618958\n",
      " 13/782 [..............................] - ETA: 5:28:46 - loss: 0.6903 - accuracy: 0.5481\n",
      " training set -> batch:14 loss:0.6808217167854309 and acc: 0.546875\n",
      " 14/782 [..............................] - ETA: 5:26:35 - loss: 0.6897 - accuracy: 0.5469\n",
      " training set -> batch:15 loss:0.6946110725402832 and acc: 0.5458333492279053\n",
      " 15/782 [..............................] - ETA: 5:24:41 - loss: 0.6900 - accuracy: 0.5458\n",
      " training set -> batch:16 loss:0.6680445075035095 and acc: 0.55078125\n",
      " 16/782 [..............................] - ETA: 5:22:29 - loss: 0.6886 - accuracy: 0.5508\n",
      " training set -> batch:17 loss:0.6617188453674316 and acc: 0.5569853186607361\n",
      " 17/782 [..............................] - ETA: 5:20:42 - loss: 0.6870 - accuracy: 0.5570\n",
      " training set -> batch:18 loss:0.6540430188179016 and acc: 0.5590277910232544\n",
      " 18/782 [..............................] - ETA: 5:19:00 - loss: 0.6852 - accuracy: 0.5590\n",
      " training set -> batch:19 loss:0.7145689725875854 and acc: 0.5542762875556946\n",
      " 19/782 [..............................] - ETA: 5:17:35 - loss: 0.6868 - accuracy: 0.5543\n",
      " training set -> batch:20 loss:0.6587620973587036 and acc: 0.557812511920929\n",
      " 20/782 [..............................] - ETA: 5:16:27 - loss: 0.6854 - accuracy: 0.5578\n",
      " training set -> batch:21 loss:0.7261396646499634 and acc: 0.5565476417541504\n",
      " 21/782 [..............................] - ETA: 5:15:24 - loss: 0.6873 - accuracy: 0.5565\n",
      " training set -> batch:22 loss:0.7804040908813477 and acc: 0.5497159361839294\n",
      " 22/782 [..............................] - ETA: 5:14:07 - loss: 0.6915 - accuracy: 0.5497\n",
      " training set -> batch:23 loss:0.6603189706802368 and acc: 0.551630437374115\n",
      " 23/782 [..............................] - ETA: 5:13:02 - loss: 0.6902 - accuracy: 0.5516\n",
      " training set -> batch:24 loss:0.7052205801010132 and acc: 0.5494791865348816\n",
      " 24/782 [..............................] - ETA: 5:12:05 - loss: 0.6908 - accuracy: 0.5495\n",
      " training set -> batch:25 loss:0.6540943384170532 and acc: 0.5575000047683716\n",
      " 25/782 [..............................] - ETA: 5:11:28 - loss: 0.6893 - accuracy: 0.5575\n",
      " training set -> batch:26 loss:0.6932995319366455 and acc: 0.5552884340286255\n",
      " 26/782 [..............................] - ETA: 5:12:10 - loss: 0.6895 - accuracy: 0.5553\n",
      " training set -> batch:27 loss:0.6645588874816895 and acc: 0.5601851940155029\n",
      " 27/782 [>.............................] - ETA: 5:11:04 - loss: 0.6886 - accuracy: 0.5602\n",
      " training set -> batch:28 loss:0.6390194296836853 and acc: 0.5680803656578064\n",
      " 28/782 [>.............................] - ETA: 5:10:24 - loss: 0.6868 - accuracy: 0.5681\n",
      " training set -> batch:29 loss:0.6978905200958252 and acc: 0.5635775923728943\n",
      " 29/782 [>.............................] - ETA: 5:09:28 - loss: 0.6872 - accuracy: 0.5636\n",
      " training set -> batch:30 loss:0.6881760358810425 and acc: 0.5625\n",
      " 30/782 [>.............................] - ETA: 5:08:37 - loss: 0.6872 - accuracy: 0.5625\n",
      " training set -> batch:31 loss:0.669501781463623 and acc: 0.5614919066429138\n",
      " 31/782 [>.............................] - ETA: 5:07:36 - loss: 0.6866 - accuracy: 0.5615\n",
      " training set -> batch:32 loss:0.679126501083374 and acc: 0.560546875\n",
      " 32/782 [>.............................] - ETA: 5:06:42 - loss: 0.6864 - accuracy: 0.5605\n",
      " training set -> batch:33 loss:0.6609534025192261 and acc: 0.560606062412262\n",
      " 33/782 [>.............................] - ETA: 5:06:02 - loss: 0.6856 - accuracy: 0.5606\n",
      " training set -> batch:34 loss:0.6789321899414062 and acc: 0.5615808963775635\n",
      " 34/782 [>.............................] - ETA: 5:05:17 - loss: 0.6854 - accuracy: 0.5616\n",
      " training set -> batch:35 loss:0.6810266375541687 and acc: 0.5625\n",
      " 35/782 [>.............................] - ETA: 5:04:29 - loss: 0.6853 - accuracy: 0.5625\n",
      " training set -> batch:36 loss:0.6566706299781799 and acc: 0.5616319179534912\n",
      " 36/782 [>.............................] - ETA: 5:03:41 - loss: 0.6845 - accuracy: 0.5616\n",
      " training set -> batch:37 loss:0.6736472249031067 and acc: 0.5608108043670654\n",
      " 37/782 [>.............................] - ETA: 5:02:53 - loss: 0.6842 - accuracy: 0.5608\n",
      " training set -> batch:38 loss:0.706331729888916 and acc: 0.5600329041481018\n",
      " 38/782 [>.............................] - ETA: 5:02:06 - loss: 0.6848 - accuracy: 0.5600\n",
      " training set -> batch:39 loss:0.6519606709480286 and acc: 0.5608974099159241\n",
      " 39/782 [>.............................] - ETA: 5:01:58 - loss: 0.6840 - accuracy: 0.5609\n",
      " training set -> batch:40 loss:0.6524474024772644 and acc: 0.5640624761581421\n",
      " 40/782 [>.............................] - ETA: 5:01:20 - loss: 0.6832 - accuracy: 0.5641\n",
      " training set -> batch:41 loss:0.655021071434021 and acc: 0.5663110017776489\n",
      " 41/782 [>.............................] - ETA: 5:00:50 - loss: 0.6825 - accuracy: 0.5663\n",
      " training set -> batch:42 loss:0.6260535717010498 and acc: 0.569940447807312\n",
      " 42/782 [>.............................] - ETA: 5:00:09 - loss: 0.6811 - accuracy: 0.5699\n",
      " training set -> batch:43 loss:0.6088235378265381 and acc: 0.573401153087616\n",
      " 43/782 [>.............................] - ETA: 4:59:33 - loss: 0.6795 - accuracy: 0.5734\n",
      " training set -> batch:44 loss:0.5797110199928284 and acc: 0.5774147510528564\n",
      " 44/782 [>.............................] - ETA: 4:58:53 - loss: 0.6772 - accuracy: 0.5774\n",
      " training set -> batch:45 loss:0.6484052538871765 and acc: 0.5791666507720947\n",
      " 45/782 [>.............................] - ETA: 4:58:18 - loss: 0.6766 - accuracy: 0.5792\n",
      " training set -> batch:46 loss:0.593262255191803 and acc: 0.5822010636329651\n",
      " 46/782 [>.............................] - ETA: 4:57:46 - loss: 0.6747 - accuracy: 0.5822\n",
      " training set -> batch:47 loss:0.5924996137619019 and acc: 0.5857712626457214\n",
      " 47/782 [>.............................] - ETA: 4:57:16 - loss: 0.6730 - accuracy: 0.5858\n",
      " training set -> batch:48 loss:0.5734259486198425 and acc: 0.587890625\n",
      " 48/782 [>.............................] - ETA: 4:56:40 - loss: 0.6709 - accuracy: 0.5879\n",
      " training set -> batch:49 loss:0.495990514755249 and acc: 0.5918367505073547\n",
      " 49/782 [>.............................] - ETA: 4:56:06 - loss: 0.6673 - accuracy: 0.5918\n",
      " training set -> batch:50 loss:0.6122815012931824 and acc: 0.59375\n",
      "\n",
      " validation set -> batch:50 val loss:0.44662781059741974 and val acc: 0.8389999866485596\n",
      " 50/782 [>.............................] - ETA: 6:01:46 - loss: 0.6662 - accuracy: 0.5938\n",
      " training set -> batch:51 loss:0.37547510862350464 and acc: 0.8401162624359131\n",
      " 51/782 [>.............................] - ETA: 5:59:48 - loss: 0.6605 - accuracy: 0.8401\n",
      " training set -> batch:52 loss:0.4271814525127411 and acc: 0.8383458852767944\n",
      " 52/782 [>.............................] - ETA: 5:57:52 - loss: 0.6561 - accuracy: 0.8383\n",
      " training set -> batch:53 loss:0.36190348863601685 and acc: 0.8385036587715149\n",
      " 53/782 [=>............................] - ETA: 5:56:07 - loss: 0.6505 - accuracy: 0.8385\n",
      " training set -> batch:54 loss:0.43796515464782715 and acc: 0.8377659320831299\n",
      " 54/782 [=>............................] - ETA: 5:54:21 - loss: 0.6466 - accuracy: 0.8378\n",
      " training set -> batch:55 loss:0.2718249261379242 and acc: 0.8405172228813171\n",
      " 55/782 [=>............................] - ETA: 5:52:38 - loss: 0.6398 - accuracy: 0.8405\n",
      " training set -> batch:56 loss:0.499179482460022 and acc: 0.8406040072441101\n",
      " 56/782 [=>............................] - ETA: 5:50:54 - loss: 0.6372 - accuracy: 0.8406\n",
      " training set -> batch:57 loss:0.3022420108318329 and acc: 0.8423202633857727\n",
      " 57/782 [=>............................] - ETA: 5:49:15 - loss: 0.6314 - accuracy: 0.8423\n",
      " training set -> batch:58 loss:0.651329755783081 and acc: 0.8399681448936462\n",
      " 58/782 [=>............................] - ETA: 5:47:39 - loss: 0.6317 - accuracy: 0.8400\n",
      " training set -> batch:59 loss:0.41636842489242554 and acc: 0.8392857313156128\n",
      " 59/782 [=>............................] - ETA: 5:46:04 - loss: 0.6281 - accuracy: 0.8393\n",
      " training set -> batch:60 loss:0.3496810495853424 and acc: 0.8401514887809753\n",
      " 60/782 [=>............................] - ETA: 5:44:27 - loss: 0.6234 - accuracy: 0.8402\n",
      " training set -> batch:61 loss:0.34754231572151184 and acc: 0.8402366638183594\n",
      " 61/782 [=>............................] - ETA: 5:42:58 - loss: 0.6189 - accuracy: 0.8402\n",
      " training set -> batch:62 loss:0.16670311987400055 and acc: 0.8424855470657349\n",
      " 62/782 [=>............................] - ETA: 5:41:34 - loss: 0.6116 - accuracy: 0.8425\n",
      " training set -> batch:63 loss:0.3880094587802887 and acc: 0.8425140976905823\n",
      " 63/782 [=>............................] - ETA: 5:40:06 - loss: 0.6081 - accuracy: 0.8425\n",
      " training set -> batch:64 loss:0.32502076029777527 and acc: 0.8425414562225342\n",
      " 64/782 [=>............................] - ETA: 5:38:41 - loss: 0.6036 - accuracy: 0.8425\n",
      " training set -> batch:65 loss:0.24256077408790588 and acc: 0.8452702760696411\n",
      " 65/782 [=>............................] - ETA: 5:37:17 - loss: 0.5981 - accuracy: 0.8453\n",
      " training set -> batch:66 loss:0.7624834179878235 and acc: 0.8425925970077515\n",
      " 66/782 [=>............................] - ETA: 5:35:55 - loss: 0.6006 - accuracy: 0.8426\n",
      " training set -> batch:67 loss:0.34818750619888306 and acc: 0.8432642221450806\n",
      " 67/782 [=>............................] - ETA: 5:34:36 - loss: 0.5968 - accuracy: 0.8433\n",
      " training set -> batch:68 loss:0.4186483919620514 and acc: 0.8426395654678345\n",
      " 68/782 [=>............................] - ETA: 5:33:20 - loss: 0.5942 - accuracy: 0.8426\n",
      " training set -> batch:69 loss:0.27334126830101013 and acc: 0.8432835936546326\n",
      " 69/782 [=>............................] - ETA: 5:32:04 - loss: 0.5895 - accuracy: 0.8433\n",
      " training set -> batch:70 loss:0.28889524936676025 and acc: 0.8439024686813354\n",
      " 70/782 [=>............................] - ETA: 5:30:50 - loss: 0.5852 - accuracy: 0.8439\n",
      " training set -> batch:71 loss:0.35566446185112 and acc: 0.8438995480537415\n",
      " 71/782 [=>............................] - ETA: 5:29:34 - loss: 0.5820 - accuracy: 0.8439\n",
      " training set -> batch:72 loss:0.20109277963638306 and acc: 0.8456572890281677\n",
      " 72/782 [=>............................] - ETA: 5:28:23 - loss: 0.5767 - accuracy: 0.8457\n",
      " training set -> batch:73 loss:0.4956778883934021 and acc: 0.8450461030006409\n",
      " 73/782 [=>............................] - ETA: 5:27:11 - loss: 0.5756 - accuracy: 0.8450\n",
      " training set -> batch:74 loss:0.38134336471557617 and acc: 0.8450226187705994\n",
      " 74/782 [=>............................] - ETA: 5:26:03 - loss: 0.5730 - accuracy: 0.8450\n",
      " training set -> batch:75 loss:0.3957473039627075 and acc: 0.8438888788223267\n",
      " 75/782 [=>............................] - ETA: 5:25:04 - loss: 0.5706 - accuracy: 0.8439\n",
      " training set -> batch:76 loss:0.35925212502479553 and acc: 0.8433406352996826\n",
      " 76/782 [=>............................] - ETA: 5:23:57 - loss: 0.5678 - accuracy: 0.8433\n",
      " training set -> batch:77 loss:0.45924001932144165 and acc: 0.8422746658325195\n",
      " 77/782 [=>............................] - ETA: 5:22:49 - loss: 0.5664 - accuracy: 0.8423\n",
      " training set -> batch:78 loss:0.4145154356956482 and acc: 0.8412446975708008\n",
      " 78/782 [=>............................] - ETA: 5:21:45 - loss: 0.5645 - accuracy: 0.8412\n",
      " training set -> batch:79 loss:0.25193458795547485 and acc: 0.8428423404693604\n",
      " 79/782 [==>...........................] - ETA: 5:20:44 - loss: 0.5605 - accuracy: 0.8428\n",
      " training set -> batch:80 loss:0.33446043729782104 and acc: 0.8428571224212646\n",
      " 80/782 [==>...........................] - ETA: 5:19:43 - loss: 0.5577 - accuracy: 0.8429\n",
      " training set -> batch:81 loss:0.25183889269828796 and acc: 0.843875527381897\n",
      " 81/782 [==>...........................] - ETA: 5:18:44 - loss: 0.5539 - accuracy: 0.8439\n",
      " training set -> batch:82 loss:0.38505661487579346 and acc: 0.8438735008239746\n",
      " 82/782 [==>...........................] - ETA: 5:17:44 - loss: 0.5519 - accuracy: 0.8439\n",
      " training set -> batch:83 loss:0.3995361924171448 and acc: 0.8438715934753418\n",
      " 83/782 [==>...........................] - ETA: 5:16:47 - loss: 0.5500 - accuracy: 0.8439\n",
      " training set -> batch:84 loss:0.5643594264984131 and acc: 0.8419540524482727\n",
      " 84/782 [==>...........................] - ETA: 5:15:48 - loss: 0.5502 - accuracy: 0.8420\n",
      " training set -> batch:85 loss:0.3132462501525879 and acc: 0.8424528241157532\n",
      " 85/782 [==>...........................] - ETA: 5:14:50 - loss: 0.5474 - accuracy: 0.8425\n",
      " training set -> batch:86 loss:0.5677610039710999 and acc: 0.8410780429840088\n",
      " 86/782 [==>...........................] - ETA: 5:13:53 - loss: 0.5476 - accuracy: 0.8411\n",
      " training set -> batch:87 loss:0.5387153029441833 and acc: 0.8392857313156128\n",
      " 87/782 [==>...........................] - ETA: 5:12:56 - loss: 0.5475 - accuracy: 0.8393\n",
      " training set -> batch:88 loss:0.3150697350502014 and acc: 0.8398014307022095\n",
      " 88/782 [==>...........................] - ETA: 5:12:06 - loss: 0.5449 - accuracy: 0.8398\n",
      " training set -> batch:89 loss:0.3527156710624695 and acc: 0.8403024673461914\n",
      " 89/782 [==>...........................] - ETA: 5:11:13 - loss: 0.5427 - accuracy: 0.8403\n",
      " training set -> batch:90 loss:0.343351274728775 and acc: 0.840350866317749\n",
      " 90/782 [==>...........................] - ETA: 5:10:18 - loss: 0.5405 - accuracy: 0.8404\n",
      " training set -> batch:91 loss:0.49173712730407715 and acc: 0.8391003608703613\n",
      " 91/782 [==>...........................] - ETA: 5:09:24 - loss: 0.5400 - accuracy: 0.8391\n",
      " training set -> batch:92 loss:0.2628408670425415 and acc: 0.8400170803070068\n",
      " 92/782 [==>...........................] - ETA: 5:08:33 - loss: 0.5370 - accuracy: 0.8400\n",
      " training set -> batch:93 loss:0.24286970496177673 and acc: 0.8409090638160706\n",
      " 93/782 [==>...........................] - ETA: 5:07:41 - loss: 0.5338 - accuracy: 0.8409\n",
      " training set -> batch:94 loss:0.3430153727531433 and acc: 0.840531587600708\n",
      " 94/782 [==>...........................] - ETA: 5:06:51 - loss: 0.5318 - accuracy: 0.8405\n",
      " training set -> batch:95 loss:0.16266193985939026 and acc: 0.8422130942344666\n",
      " 95/782 [==>...........................] - ETA: 5:06:00 - loss: 0.5279 - accuracy: 0.8422\n",
      " training set -> batch:96 loss:0.30664655566215515 and acc: 0.8422330021858215\n",
      " 96/782 [==>...........................] - ETA: 5:05:09 - loss: 0.5256 - accuracy: 0.8422\n",
      " training set -> batch:97 loss:0.2016620934009552 and acc: 0.8430511355400085\n",
      " 97/782 [==>...........................] - ETA: 5:04:18 - loss: 0.5223 - accuracy: 0.8431\n",
      " training set -> batch:98 loss:0.29302024841308594 and acc: 0.8438485860824585\n",
      " 98/782 [==>...........................] - ETA: 5:03:29 - loss: 0.5199 - accuracy: 0.8438\n",
      " training set -> batch:99 loss:0.30139195919036865 and acc: 0.8446261882781982\n",
      " 99/782 [==>...........................] - ETA: 5:02:41 - loss: 0.5177 - accuracy: 0.8446\n",
      " training set -> batch:100 loss:0.4004225432872772 and acc: 0.8442307710647583\n",
      "\n",
      " validation set -> batch:100 val loss:0.3687014440074563 and val acc: 0.8560000061988831\n",
      "100/782 [==>...........................] - ETA: 5:33:13 - loss: 0.5165 - accuracy: 0.8442\n",
      " training set -> batch:101 loss:0.696576714515686 and acc: 0.8527131676673889\n",
      "101/782 [==>...........................] - ETA: 5:32:08 - loss: 0.5183 - accuracy: 0.8527\n",
      " training set -> batch:102 loss:0.28213876485824585 and acc: 0.8524436354637146\n",
      "102/782 [==>...........................] - ETA: 5:31:02 - loss: 0.5160 - accuracy: 0.8524\n",
      " training set -> batch:103 loss:0.30193978548049927 and acc: 0.8521897792816162\n",
      "103/782 [==>...........................] - ETA: 5:30:00 - loss: 0.5139 - accuracy: 0.8522\n",
      " training set -> batch:104 loss:0.3503645658493042 and acc: 0.8519503474235535\n",
      "104/782 [==>...........................] - ETA: 5:28:52 - loss: 0.5124 - accuracy: 0.8520\n",
      " training set -> batch:105 loss:0.2109331339597702 and acc: 0.8543103337287903\n",
      "105/782 [===>..........................] - ETA: 5:27:47 - loss: 0.5095 - accuracy: 0.8543\n",
      " training set -> batch:106 loss:0.4684222638607025 and acc: 0.8540268540382385\n",
      "106/782 [===>..........................] - ETA: 5:26:40 - loss: 0.5091 - accuracy: 0.8540\n",
      " training set -> batch:107 loss:0.23432566225528717 and acc: 0.8562091588973999\n",
      "107/782 [===>..........................] - ETA: 5:25:33 - loss: 0.5065 - accuracy: 0.8562\n",
      " training set -> batch:108 loss:0.41241222620010376 and acc: 0.8542993664741516\n",
      "108/782 [===>..........................] - ETA: 5:24:31 - loss: 0.5057 - accuracy: 0.8543\n",
      " training set -> batch:109 loss:0.43586525321006775 and acc: 0.8517080545425415\n",
      "109/782 [===>..........................] - ETA: 5:23:28 - loss: 0.5050 - accuracy: 0.8517\n",
      " training set -> batch:110 loss:0.2500850558280945 and acc: 0.8522727489471436\n",
      "110/782 [===>..........................] - ETA: 5:22:25 - loss: 0.5027 - accuracy: 0.8523\n",
      " training set -> batch:111 loss:0.44079098105430603 and acc: 0.8528106212615967\n",
      "111/782 [===>..........................] - ETA: 5:21:22 - loss: 0.5021 - accuracy: 0.8528\n",
      " training set -> batch:112 loss:0.5490754842758179 and acc: 0.849711000919342\n",
      "112/782 [===>..........................] - ETA: 5:20:19 - loss: 0.5026 - accuracy: 0.8497\n",
      " training set -> batch:113 loss:0.3499899208545685 and acc: 0.848870038986206\n",
      "113/782 [===>..........................] - ETA: 5:19:18 - loss: 0.5012 - accuracy: 0.8489\n",
      " training set -> batch:114 loss:0.22379948198795319 and acc: 0.84944748878479\n",
      "114/782 [===>..........................] - ETA: 5:18:17 - loss: 0.4988 - accuracy: 0.8494\n",
      " training set -> batch:115 loss:0.3976173996925354 and acc: 0.8486486673355103\n",
      "115/782 [===>..........................] - ETA: 5:17:18 - loss: 0.4979 - accuracy: 0.8486\n",
      " training set -> batch:116 loss:0.15691567957401276 and acc: 0.851190447807312\n",
      "116/782 [===>..........................] - ETA: 5:16:21 - loss: 0.4950 - accuracy: 0.8512\n",
      " training set -> batch:117 loss:0.6605620384216309 and acc: 0.8490932583808899\n",
      "117/782 [===>..........................] - ETA: 5:15:22 - loss: 0.4964 - accuracy: 0.8491\n",
      " training set -> batch:118 loss:0.36402586102485657 and acc: 0.8502538204193115\n",
      "118/782 [===>..........................] - ETA: 5:14:25 - loss: 0.4953 - accuracy: 0.8503\n",
      " training set -> batch:119 loss:0.38726234436035156 and acc: 0.8501243591308594\n",
      "119/782 [===>..........................] - ETA: 5:13:36 - loss: 0.4943 - accuracy: 0.8501\n",
      " training set -> batch:120 loss:0.30094507336616516 and acc: 0.8512195348739624\n",
      "120/782 [===>..........................] - ETA: 5:12:39 - loss: 0.4927 - accuracy: 0.8512\n",
      " training set -> batch:121 loss:0.4824399948120117 and acc: 0.8498803973197937\n",
      "121/782 [===>..........................] - ETA: 5:11:44 - loss: 0.4926 - accuracy: 0.8499\n",
      " training set -> batch:122 loss:0.26640915870666504 and acc: 0.8509389758110046\n",
      "122/782 [===>..........................] - ETA: 5:10:51 - loss: 0.4908 - accuracy: 0.8509\n",
      " training set -> batch:123 loss:0.4253295063972473 and acc: 0.8502303957939148\n",
      "123/782 [===>..........................] - ETA: 5:10:12 - loss: 0.4903 - accuracy: 0.8502\n",
      " training set -> batch:124 loss:0.3877272605895996 and acc: 0.8495475053787231\n",
      "124/782 [===>..........................] - ETA: 5:09:23 - loss: 0.4894 - accuracy: 0.8495\n",
      " training set -> batch:125 loss:0.5296967029571533 and acc: 0.8472222089767456\n",
      "125/782 [===>..........................] - ETA: 5:08:31 - loss: 0.4898 - accuracy: 0.8472\n",
      " training set -> batch:126 loss:0.30654650926589966 and acc: 0.8477074503898621\n",
      "126/782 [===>..........................] - ETA: 5:07:39 - loss: 0.4883 - accuracy: 0.8477\n",
      " training set -> batch:127 loss:0.48766157031059265 and acc: 0.8465664982795715\n",
      "127/782 [===>..........................] - ETA: 5:06:46 - loss: 0.4883 - accuracy: 0.8466\n",
      " training set -> batch:128 loss:0.33247560262680054 and acc: 0.847046434879303\n",
      "128/782 [===>..........................] - ETA: 5:05:55 - loss: 0.4871 - accuracy: 0.8470\n",
      " training set -> batch:129 loss:0.3389800190925598 and acc: 0.8480290174484253\n",
      "129/782 [===>..........................] - ETA: 5:05:06 - loss: 0.4859 - accuracy: 0.8480\n",
      " training set -> batch:130 loss:0.5536867380142212 and acc: 0.8469387888908386\n",
      "130/782 [===>..........................] - ETA: 5:04:13 - loss: 0.4865 - accuracy: 0.8469\n",
      " training set -> batch:131 loss:0.44335195422172546 and acc: 0.8458835482597351\n",
      "131/782 [====>.........................] - ETA: 5:03:22 - loss: 0.4861 - accuracy: 0.8459\n",
      " training set -> batch:132 loss:0.39763975143432617 and acc: 0.845355749130249\n",
      "132/782 [====>.........................] - ETA: 5:02:32 - loss: 0.4855 - accuracy: 0.8454\n",
      " training set -> batch:133 loss:0.2325247824192047 and acc: 0.8463035225868225\n",
      "133/782 [====>.........................] - ETA: 5:01:43 - loss: 0.4836 - accuracy: 0.8463\n",
      " training set -> batch:134 loss:0.16008248925209045 and acc: 0.8477011322975159\n",
      "134/782 [====>.........................] - ETA: 5:00:56 - loss: 0.4811 - accuracy: 0.8477\n",
      " training set -> batch:135 loss:0.254999577999115 and acc: 0.849056601524353\n",
      "135/782 [====>.........................] - ETA: 5:00:05 - loss: 0.4795 - accuracy: 0.8491\n",
      " training set -> batch:136 loss:0.33100253343582153 and acc: 0.8489776849746704\n",
      "136/782 [====>.........................] - ETA: 4:59:19 - loss: 0.4784 - accuracy: 0.8490\n",
      " training set -> batch:137 loss:0.4765934646129608 and acc: 0.848901093006134\n",
      "137/782 [====>.........................] - ETA: 4:58:30 - loss: 0.4784 - accuracy: 0.8489\n",
      " training set -> batch:138 loss:0.4001756012439728 and acc: 0.8488267064094543\n",
      "138/782 [====>.........................] - ETA: 4:57:41 - loss: 0.4778 - accuracy: 0.8488\n",
      " training set -> batch:139 loss:0.32781362533569336 and acc: 0.8483096361160278\n",
      "139/782 [====>.........................] - ETA: 4:56:54 - loss: 0.4767 - accuracy: 0.8483\n",
      " training set -> batch:140 loss:0.28235989809036255 and acc: 0.8486841917037964\n",
      "140/782 [====>.........................] - ETA: 4:56:05 - loss: 0.4753 - accuracy: 0.8487\n",
      " training set -> batch:141 loss:0.5301233530044556 and acc: 0.8477508425712585\n",
      "141/782 [====>.........................] - ETA: 4:55:17 - loss: 0.4757 - accuracy: 0.8478\n",
      " training set -> batch:142 loss:0.42507147789001465 and acc: 0.8472696542739868\n",
      "142/782 [====>.........................] - ETA: 4:54:31 - loss: 0.4754 - accuracy: 0.8473\n",
      " training set -> batch:143 loss:0.5313572883605957 and acc: 0.8463804721832275\n",
      "143/782 [====>.........................] - ETA: 4:53:45 - loss: 0.4757 - accuracy: 0.8464\n",
      " training set -> batch:144 loss:0.36963987350463867 and acc: 0.8463455438613892\n",
      "144/782 [====>.........................] - ETA: 4:52:58 - loss: 0.4750 - accuracy: 0.8463\n",
      " training set -> batch:145 loss:0.3856918513774872 and acc: 0.8463114500045776\n",
      "145/782 [====>.........................] - ETA: 4:52:13 - loss: 0.4744 - accuracy: 0.8463\n",
      " training set -> batch:146 loss:0.3244835436344147 and acc: 0.8462783098220825\n",
      "146/782 [====>.........................] - ETA: 4:51:29 - loss: 0.4734 - accuracy: 0.8463\n",
      " training set -> batch:147 loss:0.38836783170700073 and acc: 0.8466453552246094\n",
      "147/782 [====>.........................] - ETA: 4:50:46 - loss: 0.4728 - accuracy: 0.8466\n",
      " training set -> batch:148 loss:0.33628231287002563 and acc: 0.8462145328521729\n",
      "148/782 [====>.........................] - ETA: 4:50:03 - loss: 0.4719 - accuracy: 0.8462\n",
      " training set -> batch:149 loss:0.266239732503891 and acc: 0.8469626307487488\n",
      "149/782 [====>.........................] - ETA: 4:49:20 - loss: 0.4705 - accuracy: 0.8470\n",
      " training set -> batch:150 loss:0.38435474038124084 and acc: 0.8473076820373535\n",
      "\n",
      " validation set -> batch:150 val loss:0.29986974131315947 and val acc: 0.8740000128746033\n",
      "150/782 [====>.........................] - ETA: 5:08:19 - loss: 0.4699 - accuracy: 0.8473\n",
      " training set -> batch:151 loss:0.3668345808982849 and acc: 0.873062014579773\n",
      "151/782 [====>.........................] - ETA: 5:07:25 - loss: 0.4692 - accuracy: 0.8731\n",
      " training set -> batch:152 loss:0.21864554286003113 and acc: 0.8759398460388184\n",
      "152/782 [====>.........................] - ETA: 5:06:32 - loss: 0.4676 - accuracy: 0.8759\n",
      " training set -> batch:153 loss:0.1923888921737671 and acc: 0.8777372241020203\n",
      "153/782 [====>.........................] - ETA: 5:05:38 - loss: 0.4658 - accuracy: 0.8777\n",
      " training set -> batch:154 loss:0.38518622517585754 and acc: 0.875\n",
      "154/782 [====>.........................] - ETA: 5:04:45 - loss: 0.4653 - accuracy: 0.8750\n",
      " training set -> batch:155 loss:0.4615745544433594 and acc: 0.8741379380226135\n",
      "155/782 [====>.........................] - ETA: 5:03:52 - loss: 0.4652 - accuracy: 0.8741\n",
      " training set -> batch:156 loss:0.2854722738265991 and acc: 0.875\n",
      "156/782 [====>.........................] - ETA: 5:02:59 - loss: 0.4641 - accuracy: 0.8750\n",
      " training set -> batch:157 loss:0.3932705223560333 and acc: 0.8741829991340637\n",
      "157/782 [=====>........................] - ETA: 5:02:11 - loss: 0.4636 - accuracy: 0.8742\n",
      " training set -> batch:158 loss:0.2218445986509323 and acc: 0.8757961988449097\n",
      "158/782 [=====>........................] - ETA: 5:01:20 - loss: 0.4621 - accuracy: 0.8758\n",
      " training set -> batch:159 loss:0.463798850774765 and acc: 0.8726708292961121\n",
      "159/782 [=====>........................] - ETA: 5:00:29 - loss: 0.4621 - accuracy: 0.8727\n",
      " training set -> batch:160 loss:0.3313624858856201 and acc: 0.8719696998596191\n",
      "160/782 [=====>........................] - ETA: 4:59:38 - loss: 0.4613 - accuracy: 0.8720\n",
      " training set -> batch:161 loss:0.3045271933078766 and acc: 0.8720414042472839\n",
      "161/782 [=====>........................] - ETA: 4:58:47 - loss: 0.4603 - accuracy: 0.8720\n",
      " training set -> batch:162 loss:0.411141574382782 and acc: 0.8699421882629395\n",
      "162/782 [=====>........................] - ETA: 4:57:58 - loss: 0.4600 - accuracy: 0.8699\n",
      " training set -> batch:163 loss:0.28778308629989624 and acc: 0.8707627058029175\n",
      "163/782 [=====>........................] - ETA: 4:57:08 - loss: 0.4590 - accuracy: 0.8708\n",
      " training set -> batch:164 loss:0.3098292350769043 and acc: 0.8701657652854919\n",
      "164/782 [=====>........................] - ETA: 4:56:18 - loss: 0.4581 - accuracy: 0.8702\n",
      " training set -> batch:165 loss:0.371213436126709 and acc: 0.8689188957214355\n",
      "165/782 [=====>........................] - ETA: 4:55:28 - loss: 0.4575 - accuracy: 0.8689\n",
      " training set -> batch:166 loss:0.30557262897491455 and acc: 0.8683862686157227\n",
      "166/782 [=====>........................] - ETA: 4:54:39 - loss: 0.4566 - accuracy: 0.8684\n",
      " training set -> batch:167 loss:0.33851221203804016 and acc: 0.8691709637641907\n",
      "167/782 [=====>........................] - ETA: 4:53:50 - loss: 0.4559 - accuracy: 0.8692\n",
      " training set -> batch:168 loss:0.29726099967956543 and acc: 0.8692893385887146\n",
      "168/782 [=====>........................] - ETA: 4:53:02 - loss: 0.4550 - accuracy: 0.8693\n",
      " training set -> batch:169 loss:0.3251224160194397 and acc: 0.8687810897827148\n",
      "169/782 [=====>........................] - ETA: 4:52:14 - loss: 0.4542 - accuracy: 0.8688\n",
      " training set -> batch:170 loss:0.4671679139137268 and acc: 0.8664634227752686\n",
      "170/782 [=====>........................] - ETA: 4:51:30 - loss: 0.4543 - accuracy: 0.8665\n",
      " training set -> batch:171 loss:0.2036992907524109 and acc: 0.8672248721122742\n",
      "171/782 [=====>........................] - ETA: 4:50:42 - loss: 0.4528 - accuracy: 0.8672\n",
      " training set -> batch:172 loss:0.33042746782302856 and acc: 0.8673709034919739\n",
      "172/782 [=====>........................] - ETA: 4:49:56 - loss: 0.4521 - accuracy: 0.8674\n",
      " training set -> batch:173 loss:0.2542653977870941 and acc: 0.8680875301361084\n",
      "173/782 [=====>........................] - ETA: 4:49:10 - loss: 0.4509 - accuracy: 0.8681\n",
      " training set -> batch:174 loss:0.33610787987709045 and acc: 0.8682126402854919\n",
      "174/782 [=====>........................] - ETA: 4:48:24 - loss: 0.4503 - accuracy: 0.8682\n",
      " training set -> batch:175 loss:0.3016253709793091 and acc: 0.8677777647972107\n",
      "175/782 [=====>........................] - ETA: 4:47:38 - loss: 0.4494 - accuracy: 0.8678\n",
      " training set -> batch:176 loss:0.38868263363838196 and acc: 0.8673580884933472\n",
      "176/782 [=====>........................] - ETA: 4:46:52 - loss: 0.4491 - accuracy: 0.8674\n",
      " training set -> batch:177 loss:0.2913665473461151 and acc: 0.8680257797241211\n",
      "177/782 [=====>........................] - ETA: 4:46:06 - loss: 0.4482 - accuracy: 0.8680\n",
      " training set -> batch:178 loss:0.21205538511276245 and acc: 0.8691983222961426\n",
      "178/782 [=====>........................] - ETA: 4:45:21 - loss: 0.4469 - accuracy: 0.8692\n",
      " training set -> batch:179 loss:0.47003108263015747 and acc: 0.8677386045455933\n",
      "179/782 [=====>........................] - ETA: 4:44:36 - loss: 0.4470 - accuracy: 0.8677\n",
      " training set -> batch:180 loss:0.407877117395401 and acc: 0.8658163547515869\n",
      "180/782 [=====>........................] - ETA: 4:43:51 - loss: 0.4468 - accuracy: 0.8658\n",
      " training set -> batch:181 loss:0.4638098478317261 and acc: 0.8649598360061646\n",
      "181/782 [=====>........................] - ETA: 4:43:06 - loss: 0.4469 - accuracy: 0.8650\n",
      " training set -> batch:182 loss:0.21334253251552582 and acc: 0.8661067485809326\n",
      "182/782 [=====>........................] - ETA: 4:42:21 - loss: 0.4456 - accuracy: 0.8661\n",
      " training set -> batch:183 loss:0.28609806299209595 and acc: 0.8662451505661011\n",
      "183/782 [======>.......................] - ETA: 4:41:37 - loss: 0.4447 - accuracy: 0.8662\n",
      " training set -> batch:184 loss:0.18437740206718445 and acc: 0.867337167263031\n",
      "184/782 [======>.......................] - ETA: 4:40:54 - loss: 0.4433 - accuracy: 0.8673\n",
      " training set -> batch:185 loss:0.21609564125537872 and acc: 0.8683962225914001\n",
      "185/782 [======>.......................] - ETA: 4:40:11 - loss: 0.4421 - accuracy: 0.8684\n",
      " training set -> batch:186 loss:0.40875545144081116 and acc: 0.8671003580093384\n",
      "186/782 [======>.......................] - ETA: 4:39:28 - loss: 0.4419 - accuracy: 0.8671\n",
      " training set -> batch:187 loss:0.3709339499473572 and acc: 0.8658424615859985\n",
      "187/782 [======>.......................] - ETA: 4:38:44 - loss: 0.4415 - accuracy: 0.8658\n",
      " training set -> batch:188 loss:0.17780271172523499 and acc: 0.8664259910583496\n",
      "188/782 [======>.......................] - ETA: 4:38:01 - loss: 0.4401 - accuracy: 0.8664\n",
      " training set -> batch:189 loss:0.2683260142803192 and acc: 0.8665480613708496\n",
      "189/782 [======>.......................] - ETA: 4:37:19 - loss: 0.4392 - accuracy: 0.8665\n",
      " training set -> batch:190 loss:0.5044658780097961 and acc: 0.8644737005233765\n",
      "190/782 [======>.......................] - ETA: 4:36:36 - loss: 0.4396 - accuracy: 0.8645\n",
      " training set -> batch:191 loss:0.16995613276958466 and acc: 0.8659169673919678\n",
      "191/782 [======>.......................] - ETA: 4:35:53 - loss: 0.4381 - accuracy: 0.8659\n",
      " training set -> batch:192 loss:0.22852914035320282 and acc: 0.8668941855430603\n",
      "192/782 [======>.......................] - ETA: 4:35:10 - loss: 0.4371 - accuracy: 0.8669\n",
      " training set -> batch:193 loss:0.269856333732605 and acc: 0.8670033812522888\n",
      "193/782 [======>.......................] - ETA: 4:34:28 - loss: 0.4362 - accuracy: 0.8670\n",
      " training set -> batch:194 loss:0.3479805588722229 and acc: 0.8671096563339233\n",
      "194/782 [======>.......................] - ETA: 4:33:46 - loss: 0.4357 - accuracy: 0.8671\n",
      " training set -> batch:195 loss:0.2857883870601654 and acc: 0.8672131299972534\n",
      "195/782 [======>.......................] - ETA: 4:33:03 - loss: 0.4350 - accuracy: 0.8672\n",
      " training set -> batch:196 loss:0.31380707025527954 and acc: 0.8677184581756592\n",
      "196/782 [======>.......................] - ETA: 4:32:22 - loss: 0.4343 - accuracy: 0.8677\n",
      " training set -> batch:197 loss:0.26723742485046387 and acc: 0.8670127987861633\n",
      "197/782 [======>.......................] - ETA: 4:31:43 - loss: 0.4335 - accuracy: 0.8670\n",
      " training set -> batch:198 loss:0.22563442587852478 and acc: 0.8671135902404785\n",
      "198/782 [======>.......................] - ETA: 4:31:02 - loss: 0.4324 - accuracy: 0.8671\n",
      " training set -> batch:199 loss:0.2953163981437683 and acc: 0.8668224215507507\n",
      "199/782 [======>.......................] - ETA: 4:30:21 - loss: 0.4318 - accuracy: 0.8668\n",
      " training set -> batch:200 loss:0.4114982485771179 and acc: 0.8665384650230408\n",
      "\n",
      " validation set -> batch:200 val loss:0.28904584888368845 and val acc: 0.875\n",
      "200/782 [======>.......................] - ETA: 4:43:24 - loss: 0.4317 - accuracy: 0.8665\n",
      " training set -> batch:201 loss:0.3507424592971802 and acc: 0.873062014579773\n",
      "201/782 [======>.......................] - ETA: 4:42:38 - loss: 0.4313 - accuracy: 0.8731\n",
      " training set -> batch:202 loss:0.300281822681427 and acc: 0.8721804618835449\n",
      "202/782 [======>.......................] - ETA: 4:41:53 - loss: 0.4306 - accuracy: 0.8722\n",
      " training set -> batch:203 loss:0.2700367271900177 and acc: 0.8740875720977783\n",
      "203/782 [======>.......................] - ETA: 4:41:07 - loss: 0.4298 - accuracy: 0.8741\n",
      " training set -> batch:204 loss:0.2105904370546341 and acc: 0.8741135001182556\n",
      "204/782 [======>.......................] - ETA: 4:40:21 - loss: 0.4287 - accuracy: 0.8741\n",
      " training set -> batch:205 loss:0.21506617963314056 and acc: 0.876724123954773\n",
      "205/782 [======>.......................] - ETA: 4:39:36 - loss: 0.4277 - accuracy: 0.8767\n",
      " training set -> batch:206 loss:0.2677270770072937 and acc: 0.8766778707504272\n",
      "206/782 [======>.......................] - ETA: 4:38:51 - loss: 0.4269 - accuracy: 0.8767\n",
      " training set -> batch:207 loss:0.19136783480644226 and acc: 0.8782680034637451\n",
      "207/782 [======>.......................] - ETA: 4:38:07 - loss: 0.4258 - accuracy: 0.8783\n",
      " training set -> batch:208 loss:0.20073296129703522 and acc: 0.8789808750152588\n",
      "208/782 [======>.......................] - ETA: 4:37:22 - loss: 0.4247 - accuracy: 0.8790\n",
      " training set -> batch:209 loss:0.37527555227279663 and acc: 0.8781055808067322\n",
      "209/782 [=======>......................] - ETA: 4:36:37 - loss: 0.4245 - accuracy: 0.8781\n",
      " training set -> batch:210 loss:0.26234620809555054 and acc: 0.8780303001403809\n",
      "210/782 [=======>......................] - ETA: 4:35:53 - loss: 0.4237 - accuracy: 0.8780\n",
      " training set -> batch:211 loss:0.3097226023674011 and acc: 0.8772189617156982\n",
      "211/782 [=======>......................] - ETA: 4:35:09 - loss: 0.4232 - accuracy: 0.8772\n",
      " training set -> batch:212 loss:0.3359565734863281 and acc: 0.8771676421165466\n",
      "212/782 [=======>......................] - ETA: 4:34:27 - loss: 0.4227 - accuracy: 0.8772\n",
      " training set -> batch:213 loss:0.18049518764019012 and acc: 0.8792372941970825\n",
      "213/782 [=======>......................] - ETA: 4:33:43 - loss: 0.4216 - accuracy: 0.8792\n",
      " training set -> batch:214 loss:0.6039735078811646 and acc: 0.8756905794143677\n",
      "214/782 [=======>......................] - ETA: 4:33:00 - loss: 0.4225 - accuracy: 0.8757\n",
      " training set -> batch:215 loss:0.3684280514717102 and acc: 0.8756756782531738\n",
      "215/782 [=======>......................] - ETA: 4:32:17 - loss: 0.4222 - accuracy: 0.8757\n",
      " training set -> batch:216 loss:0.15212884545326233 and acc: 0.8769841194152832\n",
      "216/782 [=======>......................] - ETA: 4:31:34 - loss: 0.4210 - accuracy: 0.8770\n",
      " training set -> batch:217 loss:0.34472593665122986 and acc: 0.8762953281402588\n",
      "217/782 [=======>......................] - ETA: 4:30:52 - loss: 0.4206 - accuracy: 0.8763\n",
      " training set -> batch:218 loss:0.2642843425273895 and acc: 0.875634491443634\n",
      "218/782 [=======>......................] - ETA: 4:30:10 - loss: 0.4199 - accuracy: 0.8756\n",
      " training set -> batch:219 loss:0.2366226315498352 and acc: 0.8762437701225281\n",
      "219/782 [=======>......................] - ETA: 4:29:27 - loss: 0.4190 - accuracy: 0.8762\n",
      " training set -> batch:220 loss:0.3246341645717621 and acc: 0.8762195110321045\n",
      "220/782 [=======>......................] - ETA: 4:28:45 - loss: 0.4186 - accuracy: 0.8762\n",
      " training set -> batch:221 loss:0.3972224295139313 and acc: 0.8755980730056763\n",
      "221/782 [=======>......................] - ETA: 4:28:02 - loss: 0.4185 - accuracy: 0.8756\n",
      " training set -> batch:222 loss:0.14438986778259277 and acc: 0.8767605423927307\n",
      "222/782 [=======>......................] - ETA: 4:27:19 - loss: 0.4173 - accuracy: 0.8768\n",
      " training set -> batch:223 loss:0.1863185167312622 and acc: 0.8778801560401917\n",
      "223/782 [=======>......................] - ETA: 4:26:37 - loss: 0.4163 - accuracy: 0.8779\n",
      " training set -> batch:224 loss:0.15472453832626343 and acc: 0.8789592981338501\n",
      "224/782 [=======>......................] - ETA: 4:25:55 - loss: 0.4151 - accuracy: 0.8790\n",
      " training set -> batch:225 loss:0.2817980647087097 and acc: 0.878333330154419\n",
      "225/782 [=======>......................] - ETA: 4:25:15 - loss: 0.4145 - accuracy: 0.8783\n",
      " training set -> batch:226 loss:0.19404569268226624 and acc: 0.8788209557533264\n",
      "226/782 [=======>......................] - ETA: 4:24:33 - loss: 0.4135 - accuracy: 0.8788\n",
      " training set -> batch:227 loss:0.2886199355125427 and acc: 0.8782188892364502\n",
      "227/782 [=======>......................] - ETA: 4:23:51 - loss: 0.4130 - accuracy: 0.8782\n",
      " training set -> batch:228 loss:0.304710328578949 and acc: 0.8776371479034424\n",
      "228/782 [=======>......................] - ETA: 4:23:10 - loss: 0.4125 - accuracy: 0.8776\n",
      " training set -> batch:229 loss:0.3840305209159851 and acc: 0.8765560388565063\n",
      "229/782 [=======>......................] - ETA: 4:22:29 - loss: 0.4124 - accuracy: 0.8766\n",
      " training set -> batch:230 loss:0.5229910016059875 and acc: 0.8744897842407227\n",
      "230/782 [=======>......................] - ETA: 4:21:47 - loss: 0.4128 - accuracy: 0.8745\n",
      " training set -> batch:231 loss:0.2829442620277405 and acc: 0.8734939694404602\n",
      "231/782 [=======>......................] - ETA: 4:21:06 - loss: 0.4123 - accuracy: 0.8735\n",
      " training set -> batch:232 loss:0.3568764626979828 and acc: 0.872035562992096\n",
      "232/782 [=======>......................] - ETA: 4:20:26 - loss: 0.4120 - accuracy: 0.8720\n",
      " training set -> batch:233 loss:0.19636784493923187 and acc: 0.8730545043945312\n",
      "233/782 [=======>......................] - ETA: 4:19:45 - loss: 0.4111 - accuracy: 0.8731\n",
      " training set -> batch:234 loss:0.3734026551246643 and acc: 0.8721264600753784\n",
      "234/782 [=======>......................] - ETA: 4:19:05 - loss: 0.4110 - accuracy: 0.8721\n",
      " training set -> batch:235 loss:0.18553060293197632 and acc: 0.8735849261283875\n",
      "235/782 [========>.....................] - ETA: 4:18:25 - loss: 0.4100 - accuracy: 0.8736\n",
      " training set -> batch:236 loss:0.20715442299842834 and acc: 0.8740706443786621\n",
      "236/782 [========>.....................] - ETA: 4:17:45 - loss: 0.4091 - accuracy: 0.8741\n",
      " training set -> batch:237 loss:0.20008710026741028 and acc: 0.8740842342376709\n",
      "237/782 [========>.....................] - ETA: 4:17:05 - loss: 0.4083 - accuracy: 0.8741\n",
      " training set -> batch:238 loss:0.3294350504875183 and acc: 0.874097466468811\n",
      "238/782 [========>.....................] - ETA: 4:16:28 - loss: 0.4079 - accuracy: 0.8741\n",
      " training set -> batch:239 loss:0.2921079397201538 and acc: 0.8741103410720825\n",
      "239/782 [========>.....................] - ETA: 4:15:47 - loss: 0.4074 - accuracy: 0.8741\n",
      " training set -> batch:240 loss:0.27131137251853943 and acc: 0.8745614290237427\n",
      "240/782 [========>.....................] - ETA: 4:15:08 - loss: 0.4069 - accuracy: 0.8746\n",
      " training set -> batch:241 loss:0.2655223608016968 and acc: 0.875\n",
      "241/782 [========>.....................] - ETA: 4:14:28 - loss: 0.4063 - accuracy: 0.8750\n",
      " training set -> batch:242 loss:0.3210352957248688 and acc: 0.875\n",
      "242/782 [========>.....................] - ETA: 4:13:48 - loss: 0.4059 - accuracy: 0.8750\n",
      " training set -> batch:243 loss:0.17768734693527222 and acc: 0.875420868396759\n",
      "243/782 [========>.....................] - ETA: 4:13:09 - loss: 0.4050 - accuracy: 0.8754\n",
      " training set -> batch:244 loss:0.338609516620636 and acc: 0.8758305907249451\n",
      "244/782 [========>.....................] - ETA: 4:12:30 - loss: 0.4047 - accuracy: 0.8758\n",
      " training set -> batch:245 loss:0.4532456398010254 and acc: 0.8754098415374756\n",
      "245/782 [========>.....................] - ETA: 4:11:51 - loss: 0.4049 - accuracy: 0.8754\n",
      " training set -> batch:247 loss:0.4015883803367615 and acc: 0.8753993511199951\n",
      "247/782 [========>.....................] - ETA: 4:10:33 - loss: 0.4050 - accuracy: 0.8754\n",
      " training set -> batch:248 loss:0.2414441704750061 and acc: 0.8761829733848572\n",
      "248/782 [========>.....................] - ETA: 4:09:54 - loss: 0.4043 - accuracy: 0.8762\n",
      " training set -> batch:249 loss:0.23633939027786255 and acc: 0.8761682510375977\n",
      "249/782 [========>.....................] - ETA: 4:09:16 - loss: 0.4037 - accuracy: 0.8762\n",
      " training set -> batch:250 loss:0.2644374370574951 and acc: 0.8757692575454712\n",
      "\n",
      " validation set -> batch:250 val loss:0.2751237200573087 and val acc: 0.8820000290870667\n",
      "250/782 [========>.....................] - ETA: 4:18:54 - loss: 0.4031 - accuracy: 0.8758\n",
      " training set -> batch:251 loss:0.2212129682302475 and acc: 0.8817829489707947\n",
      "251/782 [========>.....................] - ETA: 4:18:12 - loss: 0.4024 - accuracy: 0.8818\n",
      " training set -> batch:252 loss:0.28401392698287964 and acc: 0.8815789222717285\n",
      "252/782 [========>.....................] - ETA: 4:17:32 - loss: 0.4019 - accuracy: 0.8816\n",
      " training set -> batch:253 loss:0.2196013629436493 and acc: 0.8841241002082825\n",
      "253/782 [========>.....................] - ETA: 4:16:52 - loss: 0.4012 - accuracy: 0.8841\n",
      " training set -> batch:254 loss:0.24087627232074738 and acc: 0.8847517967224121\n",
      "254/782 [========>.....................] - ETA: 4:16:11 - loss: 0.4006 - accuracy: 0.8848\n",
      " training set -> batch:255 loss:0.276641845703125 and acc: 0.8853448033332825\n",
      "255/782 [========>.....................] - ETA: 4:15:30 - loss: 0.4001 - accuracy: 0.8853\n",
      " training set -> batch:256 loss:0.25120052695274353 and acc: 0.8850671052932739\n",
      "256/782 [========>.....................] - ETA: 4:14:50 - loss: 0.3995 - accuracy: 0.8851\n",
      " training set -> batch:257 loss:0.43106794357299805 and acc: 0.883169949054718\n",
      "257/782 [========>.....................] - ETA: 4:14:09 - loss: 0.3996 - accuracy: 0.8832\n",
      " training set -> batch:258 loss:0.34887945652008057 and acc: 0.8837579488754272\n",
      "258/782 [========>.....................] - ETA: 4:13:28 - loss: 0.3994 - accuracy: 0.8838\n",
      " training set -> batch:259 loss:0.29560428857803345 and acc: 0.8827639818191528\n",
      "259/782 [========>.....................] - ETA: 4:12:47 - loss: 0.3990 - accuracy: 0.8828\n",
      " training set -> batch:260 loss:0.3430701494216919 and acc: 0.8818181753158569\n",
      "260/782 [========>.....................] - ETA: 4:12:06 - loss: 0.3988 - accuracy: 0.8818\n",
      " training set -> batch:261 loss:0.31168702244758606 and acc: 0.88165682554245\n",
      "261/782 [=========>....................] - ETA: 4:11:26 - loss: 0.3985 - accuracy: 0.8817\n",
      " training set -> batch:262 loss:0.23105159401893616 and acc: 0.8815028667449951\n",
      "262/782 [=========>....................] - ETA: 4:10:46 - loss: 0.3978 - accuracy: 0.8815\n",
      " training set -> batch:263 loss:0.46138250827789307 and acc: 0.8792372941970825\n",
      "263/782 [=========>....................] - ETA: 4:10:05 - loss: 0.3981 - accuracy: 0.8792\n",
      " training set -> batch:264 loss:0.210470512509346 and acc: 0.8798342347145081\n",
      "264/782 [=========>....................] - ETA: 4:09:25 - loss: 0.3974 - accuracy: 0.8798\n",
      " training set -> batch:265 loss:0.24460725486278534 and acc: 0.8804054260253906\n",
      "265/782 [=========>....................] - ETA: 4:08:45 - loss: 0.3968 - accuracy: 0.8804\n",
      " training set -> batch:266 loss:0.31434738636016846 and acc: 0.8796296119689941\n",
      "266/782 [=========>....................] - ETA: 4:08:07 - loss: 0.3965 - accuracy: 0.8796\n",
      " training set -> batch:267 loss:0.3010115921497345 and acc: 0.8795337080955505\n",
      "267/782 [=========>....................] - ETA: 4:07:27 - loss: 0.3961 - accuracy: 0.8795\n",
      " training set -> batch:268 loss:0.20749840140342712 and acc: 0.8807106614112854\n",
      "268/782 [=========>....................] - ETA: 4:06:48 - loss: 0.3954 - accuracy: 0.8807\n",
      " training set -> batch:269 loss:0.24445179104804993 and acc: 0.8805969953536987\n",
      "269/782 [=========>....................] - ETA: 4:06:09 - loss: 0.3949 - accuracy: 0.8806\n",
      " training set -> batch:270 loss:0.19140180945396423 and acc: 0.8817073106765747\n",
      "270/782 [=========>....................] - ETA: 4:05:30 - loss: 0.3941 - accuracy: 0.8817\n",
      " training set -> batch:271 loss:0.24062350392341614 and acc: 0.8809808492660522\n",
      "271/782 [=========>....................] - ETA: 4:04:52 - loss: 0.3935 - accuracy: 0.8810\n",
      " training set -> batch:272 loss:0.19168603420257568 and acc: 0.8814554214477539\n",
      "272/782 [=========>....................] - ETA: 4:04:13 - loss: 0.3928 - accuracy: 0.8815\n",
      " training set -> batch:273 loss:0.16074545681476593 and acc: 0.8830645084381104\n",
      "273/782 [=========>....................] - ETA: 4:03:35 - loss: 0.3920 - accuracy: 0.8831\n",
      " training set -> batch:274 loss:0.10149221122264862 and acc: 0.8840497732162476\n",
      "274/782 [=========>....................] - ETA: 4:02:56 - loss: 0.3909 - accuracy: 0.8840\n",
      " training set -> batch:275 loss:0.08137775957584381 and acc: 0.8861111402511597\n",
      "275/782 [=========>....................] - ETA: 4:02:18 - loss: 0.3898 - accuracy: 0.8861\n",
      " training set -> batch:276 loss:0.14710217714309692 and acc: 0.8870087265968323\n",
      "276/782 [=========>....................] - ETA: 4:01:40 - loss: 0.3889 - accuracy: 0.8870\n",
      " training set -> batch:277 loss:0.4422285556793213 and acc: 0.8857296109199524\n",
      "277/782 [=========>....................] - ETA: 4:01:02 - loss: 0.3891 - accuracy: 0.8857\n",
      " training set -> batch:278 loss:0.4472249746322632 and acc: 0.8855485320091248\n",
      "278/782 [=========>....................] - ETA: 4:00:25 - loss: 0.3893 - accuracy: 0.8855\n",
      " training set -> batch:279 loss:0.369011789560318 and acc: 0.884854793548584\n",
      "279/782 [=========>....................] - ETA: 3:59:48 - loss: 0.3892 - accuracy: 0.8849\n",
      " training set -> batch:280 loss:0.32731640338897705 and acc: 0.8852040767669678\n",
      "280/782 [=========>....................] - ETA: 3:59:11 - loss: 0.3890 - accuracy: 0.8852\n",
      " training set -> batch:281 loss:0.08978542685508728 and acc: 0.8860442042350769\n",
      "281/782 [=========>....................] - ETA: 3:58:33 - loss: 0.3879 - accuracy: 0.8860\n",
      " training set -> batch:282 loss:0.20062988996505737 and acc: 0.8863636255264282\n",
      "282/782 [=========>....................] - ETA: 3:57:55 - loss: 0.3873 - accuracy: 0.8864\n",
      " training set -> batch:283 loss:0.24154220521450043 and acc: 0.887159526348114\n",
      "283/782 [=========>....................] - ETA: 3:57:17 - loss: 0.3868 - accuracy: 0.8872\n",
      " training set -> batch:284 loss:0.21829387545585632 and acc: 0.8874521255493164\n",
      "284/782 [=========>....................] - ETA: 3:56:40 - loss: 0.3862 - accuracy: 0.8875\n",
      " training set -> batch:285 loss:0.36036625504493713 and acc: 0.8872641324996948\n",
      "285/782 [=========>....................] - ETA: 3:56:03 - loss: 0.3861 - accuracy: 0.8873\n",
      " training set -> batch:286 loss:0.5004711747169495 and acc: 0.886617124080658\n",
      "286/782 [=========>....................] - ETA: 3:55:25 - loss: 0.3865 - accuracy: 0.8866\n",
      " training set -> batch:287 loss:0.3156958520412445 and acc: 0.8855311274528503\n",
      "287/782 [==========>...................] - ETA: 3:54:49 - loss: 0.3862 - accuracy: 0.8855\n",
      " training set -> batch:288 loss:0.21667727828025818 and acc: 0.8862816095352173\n",
      "288/782 [==========>...................] - ETA: 3:54:11 - loss: 0.3856 - accuracy: 0.8863\n",
      " training set -> batch:289 loss:0.14075340330600739 and acc: 0.8874555230140686\n",
      "289/782 [==========>...................] - ETA: 3:53:34 - loss: 0.3848 - accuracy: 0.8875\n",
      " training set -> batch:290 loss:0.19009041786193848 and acc: 0.8885964751243591\n",
      "290/782 [==========>...................] - ETA: 3:52:57 - loss: 0.3841 - accuracy: 0.8886\n",
      " training set -> batch:291 loss:0.22751668095588684 and acc: 0.8892733454704285\n",
      "291/782 [==========>...................] - ETA: 3:52:20 - loss: 0.3836 - accuracy: 0.8893\n",
      " training set -> batch:292 loss:0.29454660415649414 and acc: 0.88950514793396\n",
      "292/782 [==========>...................] - ETA: 3:51:44 - loss: 0.3833 - accuracy: 0.8895\n",
      " training set -> batch:293 loss:0.2436898648738861 and acc: 0.8897306323051453\n",
      "293/782 [==========>...................] - ETA: 3:51:07 - loss: 0.3828 - accuracy: 0.8897\n",
      " training set -> batch:294 loss:0.2126387655735016 and acc: 0.8895348906517029\n",
      "294/782 [==========>...................] - ETA: 3:50:31 - loss: 0.3822 - accuracy: 0.8895\n",
      " training set -> batch:295 loss:0.31833648681640625 and acc: 0.8889344334602356\n",
      "295/782 [==========>...................] - ETA: 3:49:54 - loss: 0.3820 - accuracy: 0.8889\n",
      " training set -> batch:296 loss:0.14961525797843933 and acc: 0.8899676203727722\n",
      "296/782 [==========>...................] - ETA: 3:49:18 - loss: 0.3812 - accuracy: 0.8900\n",
      " training set -> batch:297 loss:0.37708956003189087 and acc: 0.889776349067688\n",
      "297/782 [==========>...................] - ETA: 3:48:42 - loss: 0.3812 - accuracy: 0.8898\n",
      " training set -> batch:298 loss:0.35621628165245056 and acc: 0.8888012766838074\n",
      "298/782 [==========>...................] - ETA: 3:48:05 - loss: 0.3811 - accuracy: 0.8888\n",
      " training set -> batch:299 loss:0.3438633680343628 and acc: 0.8886292576789856\n",
      "299/782 [==========>...................] - ETA: 3:47:29 - loss: 0.3810 - accuracy: 0.8886\n",
      " training set -> batch:300 loss:0.31678980588912964 and acc: 0.888076901435852\n",
      "\n",
      " validation set -> batch:300 val loss:0.2709265863522887 and val acc: 0.8920000195503235\n",
      "300/782 [==========>...................] - ETA: 3:54:28 - loss: 0.3808 - accuracy: 0.8881\n",
      " training set -> batch:301 loss:0.17405448853969574 and acc: 0.8914728760719299\n",
      "301/782 [==========>...................] - ETA: 3:53:50 - loss: 0.3801 - accuracy: 0.8915\n",
      " training set -> batch:302 loss:0.10035042464733124 and acc: 0.8947368264198303\n",
      "302/782 [==========>...................] - ETA: 3:53:11 - loss: 0.3792 - accuracy: 0.8947\n",
      " training set -> batch:303 loss:0.3137964606285095 and acc: 0.8932482004165649\n",
      "303/782 [==========>...................] - ETA: 3:52:32 - loss: 0.3789 - accuracy: 0.8932\n",
      " training set -> batch:304 loss:0.30590665340423584 and acc: 0.8918439745903015\n",
      "304/782 [==========>...................] - ETA: 3:51:54 - loss: 0.3787 - accuracy: 0.8918\n",
      " training set -> batch:305 loss:0.37400415539741516 and acc: 0.8913792967796326\n",
      "305/782 [==========>...................] - ETA: 3:51:16 - loss: 0.3787 - accuracy: 0.8914\n",
      " training set -> batch:306 loss:0.222815603017807 and acc: 0.8934563994407654\n",
      "306/782 [==========>...................] - ETA: 3:50:38 - loss: 0.3782 - accuracy: 0.8935\n",
      " training set -> batch:307 loss:0.18652009963989258 and acc: 0.8946078419685364\n",
      "307/782 [==========>...................] - ETA: 3:50:01 - loss: 0.3776 - accuracy: 0.8946\n",
      " training set -> batch:308 loss:0.3201162815093994 and acc: 0.8949044346809387\n",
      "308/782 [==========>...................] - ETA: 3:49:23 - loss: 0.3774 - accuracy: 0.8949\n",
      " training set -> batch:309 loss:0.2933748960494995 and acc: 0.8936335444450378\n",
      "309/782 [==========>...................] - ETA: 3:48:45 - loss: 0.3771 - accuracy: 0.8936\n",
      " training set -> batch:310 loss:0.229105144739151 and acc: 0.8924242258071899\n",
      "310/782 [==========>...................] - ETA: 3:48:08 - loss: 0.3766 - accuracy: 0.8924\n",
      " training set -> batch:311 loss:0.2610388398170471 and acc: 0.8942307829856873\n",
      "311/782 [==========>...................] - ETA: 3:47:30 - loss: 0.3763 - accuracy: 0.8942\n",
      " training set -> batch:312 loss:0.17079459130764008 and acc: 0.8945086598396301\n",
      "312/782 [==========>...................] - ETA: 3:46:53 - loss: 0.3756 - accuracy: 0.8945\n",
      " training set -> batch:313 loss:0.2523382008075714 and acc: 0.8940678238868713\n",
      "313/782 [===========>..................] - ETA: 3:46:15 - loss: 0.3752 - accuracy: 0.8941\n",
      " training set -> batch:314 loss:0.2202230840921402 and acc: 0.894336998462677\n",
      "314/782 [===========>..................] - ETA: 3:45:38 - loss: 0.3747 - accuracy: 0.8943\n",
      " training set -> batch:315 loss:0.23783540725708008 and acc: 0.8939189314842224\n",
      "315/782 [===========>..................] - ETA: 3:45:02 - loss: 0.3743 - accuracy: 0.8939\n",
      " training set -> batch:316 loss:0.23400476574897766 and acc: 0.8941798806190491\n",
      "316/782 [===========>..................] - ETA: 3:44:25 - loss: 0.3738 - accuracy: 0.8942\n",
      " training set -> batch:317 loss:0.4245387315750122 and acc: 0.8931347131729126\n",
      "317/782 [===========>..................] - ETA: 3:43:48 - loss: 0.3740 - accuracy: 0.8931\n",
      " training set -> batch:318 loss:0.35908716917037964 and acc: 0.8921319842338562\n",
      "318/782 [===========>..................] - ETA: 3:43:11 - loss: 0.3739 - accuracy: 0.8921\n",
      " training set -> batch:319 loss:0.19438832998275757 and acc: 0.893034815788269\n",
      "319/782 [===========>..................] - ETA: 3:42:35 - loss: 0.3734 - accuracy: 0.8930\n",
      " training set -> batch:320 loss:0.17597037553787231 and acc: 0.8932926654815674\n",
      "320/782 [===========>..................] - ETA: 3:41:59 - loss: 0.3728 - accuracy: 0.8933\n",
      " training set -> batch:321 loss:0.13820965588092804 and acc: 0.8947368264198303\n",
      "321/782 [===========>..................] - ETA: 3:41:23 - loss: 0.3720 - accuracy: 0.8947\n",
      " training set -> batch:322 loss:0.2883691191673279 and acc: 0.8937793374061584\n",
      "322/782 [===========>..................] - ETA: 3:40:46 - loss: 0.3718 - accuracy: 0.8938\n",
      " training set -> batch:323 loss:0.3074965476989746 and acc: 0.8928571343421936\n",
      "323/782 [===========>..................] - ETA: 3:40:10 - loss: 0.3716 - accuracy: 0.8929\n",
      " training set -> batch:324 loss:0.2239260971546173 and acc: 0.8930995464324951\n",
      "324/782 [===========>..................] - ETA: 3:39:34 - loss: 0.3711 - accuracy: 0.8931\n",
      " training set -> batch:325 loss:0.3432476222515106 and acc: 0.8927778005599976\n",
      "325/782 [===========>..................] - ETA: 3:38:57 - loss: 0.3710 - accuracy: 0.8928\n",
      " training set -> batch:326 loss:0.20625658333301544 and acc: 0.8930131196975708\n",
      "326/782 [===========>..................] - ETA: 3:38:21 - loss: 0.3705 - accuracy: 0.8930\n",
      " training set -> batch:327 loss:0.21928180754184723 and acc: 0.8932403326034546\n",
      "327/782 [===========>..................] - ETA: 3:37:45 - loss: 0.3701 - accuracy: 0.8932\n",
      " training set -> batch:328 loss:0.4095359444618225 and acc: 0.8913502097129822\n",
      "328/782 [===========>..................] - ETA: 3:37:09 - loss: 0.3702 - accuracy: 0.8914\n",
      " training set -> batch:329 loss:0.2096511721611023 and acc: 0.8915975093841553\n",
      "329/782 [===========>..................] - ETA: 3:36:34 - loss: 0.3697 - accuracy: 0.8916\n",
      " training set -> batch:330 loss:0.12837347388267517 and acc: 0.8923469185829163\n",
      "330/782 [===========>..................] - ETA: 3:35:59 - loss: 0.3690 - accuracy: 0.8923\n",
      " training set -> batch:331 loss:0.3297121822834015 and acc: 0.8910642862319946\n",
      "331/782 [===========>..................] - ETA: 3:35:24 - loss: 0.3688 - accuracy: 0.8911\n",
      " training set -> batch:332 loss:0.44535014033317566 and acc: 0.8888339996337891\n",
      "332/782 [===========>..................] - ETA: 3:34:49 - loss: 0.3691 - accuracy: 0.8888\n",
      " training set -> batch:333 loss:0.1502336710691452 and acc: 0.8895914554595947\n",
      "333/782 [===========>..................] - ETA: 3:34:14 - loss: 0.3684 - accuracy: 0.8896\n",
      " training set -> batch:334 loss:0.5821690559387207 and acc: 0.8874521255493164\n",
      "334/782 [===========>..................] - ETA: 3:33:39 - loss: 0.3691 - accuracy: 0.8875\n",
      " training set -> batch:335 loss:0.31280726194381714 and acc: 0.8872641324996948\n",
      "335/782 [===========>..................] - ETA: 3:33:04 - loss: 0.3689 - accuracy: 0.8873\n",
      " training set -> batch:336 loss:0.3033052980899811 and acc: 0.8861523866653442\n",
      "336/782 [===========>..................] - ETA: 3:32:29 - loss: 0.3687 - accuracy: 0.8862\n",
      " training set -> batch:337 loss:0.2973390519618988 and acc: 0.8859890103340149\n",
      "337/782 [===========>..................] - ETA: 3:31:53 - loss: 0.3685 - accuracy: 0.8860\n",
      " training set -> batch:338 loss:0.3633430600166321 and acc: 0.8853790760040283\n",
      "338/782 [===========>..................] - ETA: 3:31:18 - loss: 0.3685 - accuracy: 0.8854\n",
      " training set -> batch:339 loss:0.3802100419998169 and acc: 0.8847864866256714\n",
      "339/782 [============>.................] - ETA: 3:30:43 - loss: 0.3685 - accuracy: 0.8848\n",
      " training set -> batch:340 loss:0.2329537570476532 and acc: 0.8859649300575256\n",
      "340/782 [============>.................] - ETA: 3:30:08 - loss: 0.3681 - accuracy: 0.8860\n",
      " training set -> batch:341 loss:0.19384971261024475 and acc: 0.8866782188415527\n",
      "341/782 [============>.................] - ETA: 3:29:33 - loss: 0.3676 - accuracy: 0.8867\n",
      " training set -> batch:342 loss:0.18622782826423645 and acc: 0.8873720169067383\n",
      "342/782 [============>.................] - ETA: 3:28:58 - loss: 0.3671 - accuracy: 0.8874\n",
      " training set -> batch:343 loss:0.5296710729598999 and acc: 0.8867844939231873\n",
      "343/782 [============>.................] - ETA: 3:28:23 - loss: 0.3675 - accuracy: 0.8868\n",
      " training set -> batch:344 loss:0.3938632607460022 and acc: 0.8857973217964172\n",
      "344/782 [============>.................] - ETA: 3:27:48 - loss: 0.3676 - accuracy: 0.8858\n",
      " training set -> batch:345 loss:0.2406548708677292 and acc: 0.8856557607650757\n",
      "345/782 [============>.................] - ETA: 3:27:13 - loss: 0.3672 - accuracy: 0.8857\n",
      " training set -> batch:346 loss:0.22834233939647675 and acc: 0.8859223127365112\n",
      "346/782 [============>.................] - ETA: 3:26:39 - loss: 0.3668 - accuracy: 0.8859\n",
      " training set -> batch:347 loss:0.09007540345191956 and acc: 0.8873801827430725\n",
      "347/782 [============>.................] - ETA: 3:26:05 - loss: 0.3660 - accuracy: 0.8874\n",
      " training set -> batch:348 loss:0.2671007812023163 and acc: 0.886829674243927\n",
      "348/782 [============>.................] - ETA: 3:25:30 - loss: 0.3658 - accuracy: 0.8868\n",
      " training set -> batch:349 loss:0.2920648455619812 and acc: 0.8862928152084351\n",
      "349/782 [============>.................] - ETA: 3:24:55 - loss: 0.3655 - accuracy: 0.8863\n",
      " training set -> batch:350 loss:0.4294011890888214 and acc: 0.8853846192359924\n",
      "\n",
      " validation set -> batch:350 val loss:0.297285970300436 and val acc: 0.8840000033378601\n",
      "350/782 [============>.................] - ETA: 3:30:03 - loss: 0.3657 - accuracy: 0.8854\n",
      " training set -> batch:351 loss:0.19184309244155884 and acc: 0.8846899271011353\n",
      "351/782 [============>.................] - ETA: 3:29:28 - loss: 0.3652 - accuracy: 0.8847\n",
      " training set -> batch:352 loss:0.198508158326149 and acc: 0.8862782120704651\n",
      "352/782 [============>.................] - ETA: 3:28:51 - loss: 0.3648 - accuracy: 0.8863\n",
      " training set -> batch:353 loss:0.30397289991378784 and acc: 0.8868613243103027\n",
      "353/782 [============>.................] - ETA: 3:28:15 - loss: 0.3646 - accuracy: 0.8869\n",
      " training set -> batch:354 loss:0.12087251245975494 and acc: 0.8891844153404236\n",
      "354/782 [============>.................] - ETA: 3:27:39 - loss: 0.3639 - accuracy: 0.8892\n",
      " training set -> batch:355 loss:0.3337196409702301 and acc: 0.8879310488700867\n",
      "355/782 [============>.................] - ETA: 3:27:03 - loss: 0.3638 - accuracy: 0.8879\n",
      " training set -> batch:356 loss:0.4032062292098999 and acc: 0.8859060406684875\n",
      "356/782 [============>.................] - ETA: 3:26:27 - loss: 0.3639 - accuracy: 0.8859\n",
      " training set -> batch:357 loss:0.1737467348575592 and acc: 0.8880718946456909\n",
      "357/782 [============>.................] - ETA: 3:25:51 - loss: 0.3634 - accuracy: 0.8881\n",
      " training set -> batch:358 loss:0.11160039901733398 and acc: 0.8901273608207703\n",
      "358/782 [============>.................] - ETA: 3:25:16 - loss: 0.3627 - accuracy: 0.8901\n",
      " training set -> batch:359 loss:0.39364922046661377 and acc: 0.8889751434326172\n",
      "359/782 [============>.................] - ETA: 3:24:40 - loss: 0.3628 - accuracy: 0.8890\n",
      " training set -> batch:360 loss:0.44048595428466797 and acc: 0.8863636255264282\n",
      "360/782 [============>.................] - ETA: 3:24:04 - loss: 0.3630 - accuracy: 0.8864\n",
      " training set -> batch:361 loss:0.19615447521209717 and acc: 0.8860946893692017\n",
      "361/782 [============>.................] - ETA: 3:23:29 - loss: 0.3625 - accuracy: 0.8861\n",
      " training set -> batch:362 loss:0.42309263348579407 and acc: 0.8836705088615417\n",
      "362/782 [============>.................] - ETA: 3:22:53 - loss: 0.3627 - accuracy: 0.8837\n",
      " training set -> batch:363 loss:0.3420380651950836 and acc: 0.8827683329582214\n",
      "363/782 [============>.................] - ETA: 3:22:18 - loss: 0.3626 - accuracy: 0.8828\n",
      " training set -> batch:364 loss:0.2906413674354553 and acc: 0.8819060921669006\n",
      "364/782 [============>.................] - ETA: 3:21:43 - loss: 0.3624 - accuracy: 0.8819\n",
      " training set -> batch:365 loss:0.34916195273399353 and acc: 0.8810811042785645\n",
      "365/782 [=============>................] - ETA: 3:21:07 - loss: 0.3624 - accuracy: 0.8811\n",
      " training set -> batch:366 loss:0.23965275287628174 and acc: 0.8816137313842773\n",
      "366/782 [=============>................] - ETA: 3:20:31 - loss: 0.3621 - accuracy: 0.8816\n",
      " training set -> batch:367 loss:0.5807288885116577 and acc: 0.878238320350647\n",
      "367/782 [=============>................] - ETA: 3:19:56 - loss: 0.3627 - accuracy: 0.8782\n",
      " training set -> batch:368 loss:0.37200021743774414 and acc: 0.8775380849838257\n",
      "368/782 [=============>................] - ETA: 3:19:20 - loss: 0.3627 - accuracy: 0.8775\n",
      " training set -> batch:369 loss:0.43492045998573303 and acc: 0.8756219148635864\n",
      "369/782 [=============>................] - ETA: 3:18:45 - loss: 0.3629 - accuracy: 0.8756\n",
      " training set -> batch:370 loss:0.32789385318756104 and acc: 0.875\n",
      "370/782 [=============>................] - ETA: 3:18:11 - loss: 0.3628 - accuracy: 0.8750\n",
      " training set -> batch:371 loss:0.22306987643241882 and acc: 0.8767942786216736\n",
      "371/782 [=============>................] - ETA: 3:17:38 - loss: 0.3624 - accuracy: 0.8768\n",
      " training set -> batch:372 loss:0.23166754841804504 and acc: 0.8779342770576477\n",
      "372/782 [=============>................] - ETA: 3:17:05 - loss: 0.3621 - accuracy: 0.8779\n",
      " training set -> batch:373 loss:0.39986270666122437 and acc: 0.8755760192871094\n",
      "373/782 [=============>................] - ETA: 3:16:33 - loss: 0.3622 - accuracy: 0.8756\n",
      " training set -> batch:374 loss:0.316961407661438 and acc: 0.875\n",
      "374/782 [=============>................] - ETA: 3:15:59 - loss: 0.3620 - accuracy: 0.8750\n",
      " training set -> batch:375 loss:0.2515626549720764 and acc: 0.875\n",
      "375/782 [=============>................] - ETA: 3:15:24 - loss: 0.3618 - accuracy: 0.8750\n",
      " training set -> batch:376 loss:0.45230287313461304 and acc: 0.873908281326294\n",
      "376/782 [=============>................] - ETA: 3:14:49 - loss: 0.3620 - accuracy: 0.8739\n",
      " training set -> batch:377 loss:0.21639278531074524 and acc: 0.875\n",
      "377/782 [=============>................] - ETA: 3:14:14 - loss: 0.3616 - accuracy: 0.8750\n",
      " training set -> batch:378 loss:0.26468196511268616 and acc: 0.8744725584983826\n",
      "378/782 [=============>................] - ETA: 3:13:39 - loss: 0.3614 - accuracy: 0.8745\n",
      " training set -> batch:379 loss:0.2694224417209625 and acc: 0.875\n",
      "379/782 [=============>................] - ETA: 3:13:05 - loss: 0.3611 - accuracy: 0.8750\n",
      " training set -> batch:380 loss:0.2188212126493454 and acc: 0.8755102157592773\n",
      "380/782 [=============>................] - ETA: 3:12:31 - loss: 0.3607 - accuracy: 0.8755\n",
      " training set -> batch:381 loss:0.2956788241863251 and acc: 0.8744980096817017\n",
      "381/782 [=============>................] - ETA: 3:11:56 - loss: 0.3606 - accuracy: 0.8745\n",
      " training set -> batch:382 loss:0.3066323399543762 and acc: 0.875\n",
      "382/782 [=============>................] - ETA: 3:11:22 - loss: 0.3604 - accuracy: 0.8750\n",
      " training set -> batch:383 loss:0.21623316407203674 and acc: 0.8754863739013672\n",
      "383/782 [=============>................] - ETA: 3:10:48 - loss: 0.3600 - accuracy: 0.8755\n",
      " training set -> batch:384 loss:0.39922913908958435 and acc: 0.8735632300376892\n",
      "384/782 [=============>................] - ETA: 3:10:14 - loss: 0.3601 - accuracy: 0.8736\n",
      " training set -> batch:385 loss:0.12895488739013672 and acc: 0.8754717111587524\n",
      "385/782 [=============>................] - ETA: 3:09:39 - loss: 0.3595 - accuracy: 0.8755\n",
      " training set -> batch:386 loss:0.1329798698425293 and acc: 0.8763940334320068\n",
      "386/782 [=============>................] - ETA: 3:09:06 - loss: 0.3590 - accuracy: 0.8764\n",
      " training set -> batch:387 loss:0.08207027614116669 and acc: 0.8782051205635071\n",
      "387/782 [=============>................] - ETA: 3:08:32 - loss: 0.3582 - accuracy: 0.8782\n",
      " training set -> batch:388 loss:0.4042973816394806 and acc: 0.8777076005935669\n",
      "388/782 [=============>................] - ETA: 3:07:58 - loss: 0.3584 - accuracy: 0.8777\n",
      " training set -> batch:389 loss:0.29922035336494446 and acc: 0.878113865852356\n",
      "389/782 [=============>................] - ETA: 3:07:24 - loss: 0.3582 - accuracy: 0.8781\n",
      " training set -> batch:390 loss:0.23971694707870483 and acc: 0.8780701756477356\n",
      "390/782 [=============>................] - ETA: 3:06:50 - loss: 0.3579 - accuracy: 0.8781\n",
      " training set -> batch:391 loss:0.36715370416641235 and acc: 0.8771626353263855\n",
      "391/782 [==============>...............] - ETA: 3:06:16 - loss: 0.3579 - accuracy: 0.8772\n",
      " training set -> batch:392 loss:0.11692366003990173 and acc: 0.8784129619598389\n",
      "392/782 [==============>...............] - ETA: 3:05:42 - loss: 0.3573 - accuracy: 0.8784\n",
      " training set -> batch:393 loss:0.137007474899292 and acc: 0.8792087435722351\n",
      "393/782 [==============>...............] - ETA: 3:05:08 - loss: 0.3568 - accuracy: 0.8792\n",
      " training set -> batch:394 loss:0.27492672204971313 and acc: 0.8791528344154358\n",
      "394/782 [==============>...............] - ETA: 3:04:34 - loss: 0.3565 - accuracy: 0.8792\n",
      " training set -> batch:395 loss:0.27713650465011597 and acc: 0.8795081973075867\n",
      "395/782 [==============>...............] - ETA: 3:04:01 - loss: 0.3563 - accuracy: 0.8795\n",
      " training set -> batch:396 loss:0.19315743446350098 and acc: 0.8802589178085327\n",
      "396/782 [==============>...............] - ETA: 3:03:27 - loss: 0.3559 - accuracy: 0.8803\n",
      " training set -> batch:397 loss:0.2569271922111511 and acc: 0.8793929815292358\n",
      "397/782 [==============>...............] - ETA: 3:02:54 - loss: 0.3557 - accuracy: 0.8794\n",
      " training set -> batch:398 loss:0.2172698974609375 and acc: 0.8793375492095947\n",
      "398/782 [==============>...............] - ETA: 3:02:20 - loss: 0.3553 - accuracy: 0.8793\n",
      " training set -> batch:399 loss:0.3878128230571747 and acc: 0.8785046935081482\n",
      "399/782 [==============>...............] - ETA: 3:01:47 - loss: 0.3554 - accuracy: 0.8785\n",
      " training set -> batch:400 loss:0.12871012091636658 and acc: 0.879230797290802\n",
      "\n",
      " validation set -> batch:400 val loss:0.2668914347887039 and val acc: 0.8949999809265137\n",
      "400/782 [==============>...............] - ETA: 3:05:52 - loss: 0.3549 - accuracy: 0.8792\n",
      " training set -> batch:401 loss:0.12634243071079254 and acc: 0.8963178396224976\n",
      "401/782 [==============>...............] - ETA: 3:05:18 - loss: 0.3543 - accuracy: 0.8963\n",
      " training set -> batch:402 loss:0.18900778889656067 and acc: 0.8984962701797485\n",
      "402/782 [==============>...............] - ETA: 3:04:44 - loss: 0.3539 - accuracy: 0.8985\n",
      " training set -> batch:403 loss:0.13549603521823883 and acc: 0.900547444820404\n",
      "403/782 [==============>...............] - ETA: 3:04:09 - loss: 0.3533 - accuracy: 0.9005\n",
      " training set -> batch:404 loss:0.2474483996629715 and acc: 0.9007092118263245\n",
      "404/782 [==============>...............] - ETA: 3:03:36 - loss: 0.3531 - accuracy: 0.9007\n",
      " training set -> batch:405 loss:0.12287546694278717 and acc: 0.9017241597175598\n",
      "405/782 [==============>...............] - ETA: 3:03:02 - loss: 0.3525 - accuracy: 0.9017\n",
      " training set -> batch:406 loss:0.24191932380199432 and acc: 0.9018456339836121\n",
      "406/782 [==============>...............] - ETA: 3:02:27 - loss: 0.3522 - accuracy: 0.9018\n",
      " training set -> batch:407 loss:0.3987107276916504 and acc: 0.8995097875595093\n",
      "407/782 [==============>...............] - ETA: 3:01:53 - loss: 0.3523 - accuracy: 0.8995\n",
      " training set -> batch:408 loss:0.21278037130832672 and acc: 0.8996815085411072\n",
      "408/782 [==============>...............] - ETA: 3:01:20 - loss: 0.3520 - accuracy: 0.8997\n",
      " training set -> batch:409 loss:0.4170275330543518 and acc: 0.8975155353546143\n",
      "409/782 [==============>...............] - ETA: 3:00:46 - loss: 0.3522 - accuracy: 0.8975\n",
      " training set -> batch:410 loss:0.358161598443985 and acc: 0.8969696760177612\n",
      "410/782 [==============>...............] - ETA: 3:00:12 - loss: 0.3522 - accuracy: 0.8970\n",
      " training set -> batch:411 loss:0.3520633578300476 and acc: 0.8971893787384033\n",
      "411/782 [==============>...............] - ETA: 2:59:38 - loss: 0.3522 - accuracy: 0.8972\n",
      " training set -> batch:412 loss:0.18084970116615295 and acc: 0.8981214165687561\n",
      "412/782 [==============>...............] - ETA: 2:59:03 - loss: 0.3518 - accuracy: 0.8981\n",
      " training set -> batch:413 loss:0.4174971580505371 and acc: 0.8947740197181702\n",
      "413/782 [==============>...............] - ETA: 2:58:29 - loss: 0.3519 - accuracy: 0.8948\n",
      " training set -> batch:414 loss:0.3328930139541626 and acc: 0.8936464190483093\n",
      "414/782 [==============>...............] - ETA: 2:57:56 - loss: 0.3519 - accuracy: 0.8936\n",
      " training set -> batch:415 loss:0.35488197207450867 and acc: 0.8932432532310486\n",
      "415/782 [==============>...............] - ETA: 2:57:22 - loss: 0.3519 - accuracy: 0.8932\n",
      " training set -> batch:416 loss:0.23015189170837402 and acc: 0.8935185074806213\n",
      "416/782 [==============>...............] - ETA: 2:56:48 - loss: 0.3516 - accuracy: 0.8935\n",
      " training set -> batch:417 loss:0.2106490582227707 and acc: 0.8931347131729126\n",
      "417/782 [==============>...............] - ETA: 2:56:14 - loss: 0.3512 - accuracy: 0.8931\n",
      " training set -> batch:418 loss:0.2091653048992157 and acc: 0.8927664756774902\n",
      "418/782 [===============>..............] - ETA: 2:55:40 - loss: 0.3509 - accuracy: 0.8928\n",
      " training set -> batch:419 loss:0.20407280325889587 and acc: 0.8936567306518555\n",
      "419/782 [===============>..............] - ETA: 2:55:07 - loss: 0.3506 - accuracy: 0.8937\n",
      " training set -> batch:420 loss:0.30931517481803894 and acc: 0.8939024209976196\n",
      "420/782 [===============>..............] - ETA: 2:54:33 - loss: 0.3505 - accuracy: 0.8939\n",
      " training set -> batch:421 loss:0.19312505424022675 and acc: 0.8947368264198303\n",
      "421/782 [===============>..............] - ETA: 2:54:00 - loss: 0.3501 - accuracy: 0.8947\n",
      " training set -> batch:422 loss:0.15743309259414673 and acc: 0.8961267471313477\n",
      "422/782 [===============>..............] - ETA: 2:53:26 - loss: 0.3496 - accuracy: 0.8961\n",
      " training set -> batch:423 loss:0.25905969738960266 and acc: 0.8951612710952759\n",
      "423/782 [===============>..............] - ETA: 2:52:53 - loss: 0.3494 - accuracy: 0.8952\n",
      " training set -> batch:424 loss:0.21742868423461914 and acc: 0.8953620195388794\n",
      "424/782 [===============>..............] - ETA: 2:52:19 - loss: 0.3491 - accuracy: 0.8954\n",
      " training set -> batch:425 loss:0.4139780104160309 and acc: 0.8938888907432556\n",
      "425/782 [===============>..............] - ETA: 2:51:46 - loss: 0.3493 - accuracy: 0.8939\n",
      " training set -> batch:426 loss:0.20927542448043823 and acc: 0.8941047787666321\n",
      "426/782 [===============>..............] - ETA: 2:51:12 - loss: 0.3489 - accuracy: 0.8941\n",
      " training set -> batch:427 loss:0.18370528519153595 and acc: 0.8937768340110779\n",
      "427/782 [===============>..............] - ETA: 2:50:39 - loss: 0.3485 - accuracy: 0.8938\n",
      " training set -> batch:428 loss:0.21232318878173828 and acc: 0.8945147395133972\n",
      "428/782 [===============>..............] - ETA: 2:50:06 - loss: 0.3482 - accuracy: 0.8945\n",
      " training set -> batch:429 loss:0.1804482638835907 and acc: 0.8952282071113586\n",
      "429/782 [===============>..............] - ETA: 2:49:33 - loss: 0.3478 - accuracy: 0.8952\n",
      " training set -> batch:430 loss:0.6358891725540161 and acc: 0.8923469185829163\n",
      "430/782 [===============>..............] - ETA: 2:48:59 - loss: 0.3485 - accuracy: 0.8923\n",
      " training set -> batch:431 loss:0.4303855299949646 and acc: 0.8905622363090515\n",
      "431/782 [===============>..............] - ETA: 2:48:26 - loss: 0.3487 - accuracy: 0.8906\n",
      " training set -> batch:432 loss:0.14769145846366882 and acc: 0.8913043737411499\n",
      "432/782 [===============>..............] - ETA: 2:47:53 - loss: 0.3482 - accuracy: 0.8913\n",
      " training set -> batch:433 loss:0.1821688860654831 and acc: 0.8920233249664307\n",
      "433/782 [===============>..............] - ETA: 2:47:19 - loss: 0.3478 - accuracy: 0.8920\n",
      " training set -> batch:434 loss:0.3101630210876465 and acc: 0.8912835121154785\n",
      "434/782 [===============>..............] - ETA: 2:46:46 - loss: 0.3478 - accuracy: 0.8913\n",
      " training set -> batch:435 loss:0.10597892105579376 and acc: 0.8929245471954346\n",
      "435/782 [===============>..............] - ETA: 2:46:13 - loss: 0.3472 - accuracy: 0.8929\n",
      " training set -> batch:436 loss:0.16457802057266235 and acc: 0.8935873508453369\n",
      "436/782 [===============>..............] - ETA: 2:45:40 - loss: 0.3468 - accuracy: 0.8936\n",
      " training set -> batch:437 loss:0.357879638671875 and acc: 0.8928571343421936\n",
      "437/782 [===============>..............] - ETA: 2:45:08 - loss: 0.3468 - accuracy: 0.8929\n",
      " training set -> batch:438 loss:0.47067075967788696 and acc: 0.8925992846488953\n",
      "438/782 [===============>..............] - ETA: 2:44:35 - loss: 0.3471 - accuracy: 0.8926\n",
      " training set -> batch:439 loss:0.24429011344909668 and acc: 0.892793595790863\n",
      "439/782 [===============>..............] - ETA: 2:44:02 - loss: 0.3469 - accuracy: 0.8928\n",
      " training set -> batch:440 loss:0.152255579829216 and acc: 0.8938596248626709\n",
      "440/782 [===============>..............] - ETA: 2:43:29 - loss: 0.3464 - accuracy: 0.8939\n",
      " training set -> batch:441 loss:0.2019200176000595 and acc: 0.8944636583328247\n",
      "441/782 [===============>..............] - ETA: 2:42:57 - loss: 0.3461 - accuracy: 0.8945\n",
      " training set -> batch:442 loss:0.2536623477935791 and acc: 0.894197940826416\n",
      "442/782 [===============>..............] - ETA: 2:42:24 - loss: 0.3459 - accuracy: 0.8942\n",
      " training set -> batch:443 loss:0.33640483021736145 and acc: 0.8943602442741394\n",
      "443/782 [===============>..............] - ETA: 2:41:51 - loss: 0.3459 - accuracy: 0.8944\n",
      " training set -> batch:444 loss:0.3344876766204834 and acc: 0.8941029906272888\n",
      "444/782 [================>.............] - ETA: 2:41:19 - loss: 0.3458 - accuracy: 0.8941\n",
      " training set -> batch:445 loss:0.15952229499816895 and acc: 0.8942623138427734\n",
      "445/782 [================>.............] - ETA: 2:40:46 - loss: 0.3454 - accuracy: 0.8943\n",
      " training set -> batch:446 loss:0.18293225765228271 and acc: 0.8952265381813049\n",
      "446/782 [================>.............] - ETA: 2:40:13 - loss: 0.3450 - accuracy: 0.8952\n",
      " training set -> batch:447 loss:0.2835696339607239 and acc: 0.894568681716919\n",
      "447/782 [================>.............] - ETA: 2:39:41 - loss: 0.3449 - accuracy: 0.8946\n",
      " training set -> batch:448 loss:0.19616712629795074 and acc: 0.8955047130584717\n",
      "448/782 [================>.............] - ETA: 2:39:08 - loss: 0.3446 - accuracy: 0.8955\n",
      " training set -> batch:449 loss:0.28375595808029175 and acc: 0.8944703936576843\n",
      "449/782 [================>.............] - ETA: 2:38:36 - loss: 0.3444 - accuracy: 0.8945\n",
      " training set -> batch:450 loss:0.1651488095521927 and acc: 0.8949999809265137\n",
      "\n",
      " validation set -> batch:450 val loss:0.2894021449610591 and val acc: 0.8939999938011169\n",
      "450/782 [================>.............] - ETA: 2:41:29 - loss: 0.3440 - accuracy: 0.8950\n",
      " training set -> batch:451 loss:0.1588757187128067 and acc: 0.8963178396224976\n",
      "451/782 [================>.............] - ETA: 2:40:56 - loss: 0.3436 - accuracy: 0.8963\n",
      " training set -> batch:452 loss:0.2568742334842682 and acc: 0.896616518497467\n",
      "452/782 [================>.............] - ETA: 2:40:22 - loss: 0.3434 - accuracy: 0.8966\n",
      " training set -> batch:453 loss:0.25156891345977783 and acc: 0.8959854245185852\n",
      "453/782 [================>.............] - ETA: 2:39:49 - loss: 0.3432 - accuracy: 0.8960\n",
      " training set -> batch:454 loss:0.2009393572807312 and acc: 0.896276593208313\n",
      "454/782 [================>.............] - ETA: 2:39:15 - loss: 0.3429 - accuracy: 0.8963\n",
      " training set -> batch:455 loss:0.22370845079421997 and acc: 0.8965517282485962\n",
      "455/782 [================>.............] - ETA: 2:38:42 - loss: 0.3427 - accuracy: 0.8966\n",
      " training set -> batch:456 loss:0.34464770555496216 and acc: 0.8959731459617615\n",
      "456/782 [================>.............] - ETA: 2:38:10 - loss: 0.3427 - accuracy: 0.8960\n",
      " training set -> batch:457 loss:0.15565651655197144 and acc: 0.8962418437004089\n",
      "457/782 [================>.............] - ETA: 2:37:37 - loss: 0.3423 - accuracy: 0.8962\n",
      " training set -> batch:458 loss:0.3928315043449402 and acc: 0.8957006335258484\n",
      "458/782 [================>.............] - ETA: 2:37:04 - loss: 0.3424 - accuracy: 0.8957\n",
      " training set -> batch:459 loss:0.12139784544706345 and acc: 0.89673912525177\n",
      "459/782 [================>.............] - ETA: 2:36:31 - loss: 0.3419 - accuracy: 0.8967\n",
      " training set -> batch:460 loss:0.31489109992980957 and acc: 0.8954545259475708\n",
      "460/782 [================>.............] - ETA: 2:35:58 - loss: 0.3418 - accuracy: 0.8955\n",
      " training set -> batch:461 loss:0.17238447070121765 and acc: 0.8964496850967407\n",
      "461/782 [================>.............] - ETA: 2:35:25 - loss: 0.3415 - accuracy: 0.8964\n",
      " training set -> batch:462 loss:0.08366180211305618 and acc: 0.8981214165687561\n",
      "462/782 [================>.............] - ETA: 2:34:52 - loss: 0.3409 - accuracy: 0.8981\n",
      " training set -> batch:463 loss:0.19751183688640594 and acc: 0.8990113139152527\n",
      "463/782 [================>.............] - ETA: 2:34:19 - loss: 0.3406 - accuracy: 0.8990\n",
      " training set -> batch:464 loss:0.44734740257263184 and acc: 0.8970994353294373\n",
      "464/782 [================>.............] - ETA: 2:33:46 - loss: 0.3408 - accuracy: 0.8971\n",
      " training set -> batch:465 loss:0.09070219099521637 and acc: 0.8986486196517944\n",
      "465/782 [================>.............] - ETA: 2:33:13 - loss: 0.3403 - accuracy: 0.8986\n",
      " training set -> batch:466 loss:0.3238897919654846 and acc: 0.8981481194496155\n",
      "466/782 [================>.............] - ETA: 2:32:40 - loss: 0.3402 - accuracy: 0.8981\n",
      " training set -> batch:467 loss:0.17931777238845825 and acc: 0.8976684212684631\n",
      "467/782 [================>.............] - ETA: 2:32:07 - loss: 0.3399 - accuracy: 0.8977\n",
      " training set -> batch:468 loss:0.12636937201023102 and acc: 0.8991116881370544\n",
      "468/782 [================>.............] - ETA: 2:31:34 - loss: 0.3394 - accuracy: 0.8991\n",
      " training set -> batch:469 loss:0.22297513484954834 and acc: 0.8992537260055542\n",
      "469/782 [================>.............] - ETA: 2:31:02 - loss: 0.3392 - accuracy: 0.8993\n",
      " training set -> batch:470 loss:0.2965956926345825 and acc: 0.8987804651260376\n",
      "470/782 [=================>............] - ETA: 2:30:29 - loss: 0.3391 - accuracy: 0.8988\n",
      " training set -> batch:471 loss:0.1327015459537506 and acc: 0.9001196026802063\n",
      "471/782 [=================>............] - ETA: 2:29:56 - loss: 0.3387 - accuracy: 0.9001\n",
      " training set -> batch:472 loss:0.3221617341041565 and acc: 0.9008215665817261\n",
      "472/782 [=================>............] - ETA: 2:29:24 - loss: 0.3386 - accuracy: 0.9008\n",
      " training set -> batch:473 loss:0.652573823928833 and acc: 0.8980414867401123\n",
      "473/782 [=================>............] - ETA: 2:28:51 - loss: 0.3393 - accuracy: 0.8980\n",
      " training set -> batch:474 loss:0.2162991166114807 and acc: 0.8993212580680847\n",
      "474/782 [=================>............] - ETA: 2:28:18 - loss: 0.3390 - accuracy: 0.8993\n",
      " training set -> batch:475 loss:0.20111621916294098 and acc: 0.9005555510520935\n",
      "475/782 [=================>............] - ETA: 2:27:46 - loss: 0.3387 - accuracy: 0.9006\n",
      " training set -> batch:476 loss:0.20536001026630402 and acc: 0.9012008905410767\n",
      "476/782 [=================>............] - ETA: 2:27:13 - loss: 0.3385 - accuracy: 0.9012\n",
      " training set -> batch:477 loss:0.2616458535194397 and acc: 0.9002146124839783\n",
      "477/782 [=================>............] - ETA: 2:26:40 - loss: 0.3383 - accuracy: 0.9002\n",
      " training set -> batch:478 loss:0.35564136505126953 and acc: 0.899789035320282\n",
      "478/782 [=================>............] - ETA: 2:26:08 - loss: 0.3383 - accuracy: 0.8998\n",
      " training set -> batch:479 loss:0.15701612830162048 and acc: 0.9004149436950684\n",
      "479/782 [=================>............] - ETA: 2:25:36 - loss: 0.3380 - accuracy: 0.9004\n",
      " training set -> batch:480 loss:0.1127462387084961 and acc: 0.9020408391952515\n",
      "480/782 [=================>............] - ETA: 2:25:04 - loss: 0.3375 - accuracy: 0.9020\n",
      " training set -> batch:481 loss:0.24312357604503632 and acc: 0.9021084308624268\n",
      "481/782 [=================>............] - ETA: 2:24:32 - loss: 0.3373 - accuracy: 0.9021\n",
      " training set -> batch:482 loss:0.10902699828147888 and acc: 0.9031620621681213\n",
      "482/782 [=================>............] - ETA: 2:24:00 - loss: 0.3368 - accuracy: 0.9032\n",
      " training set -> batch:483 loss:0.1449992060661316 and acc: 0.9046692848205566\n",
      "483/782 [=================>............] - ETA: 2:23:28 - loss: 0.3364 - accuracy: 0.9047\n",
      " training set -> batch:484 loss:0.11977560818195343 and acc: 0.9061302542686462\n",
      "484/782 [=================>............] - ETA: 2:22:56 - loss: 0.3360 - accuracy: 0.9061\n",
      " training set -> batch:485 loss:0.21676157414913177 and acc: 0.9061321020126343\n",
      "485/782 [=================>............] - ETA: 2:22:23 - loss: 0.3357 - accuracy: 0.9061\n",
      " training set -> batch:486 loss:0.14994777739048004 and acc: 0.9065985083580017\n",
      "486/782 [=================>............] - ETA: 2:21:51 - loss: 0.3354 - accuracy: 0.9066\n",
      " training set -> batch:487 loss:0.07727250456809998 and acc: 0.9075091481208801\n",
      "487/782 [=================>............] - ETA: 2:21:19 - loss: 0.3348 - accuracy: 0.9075\n",
      " training set -> batch:488 loss:0.27332228422164917 and acc: 0.9079422354698181\n",
      "488/782 [=================>............] - ETA: 2:20:47 - loss: 0.3347 - accuracy: 0.9079\n",
      " training set -> batch:489 loss:0.3871763348579407 and acc: 0.9074733257293701\n",
      "489/782 [=================>............] - ETA: 2:20:15 - loss: 0.3348 - accuracy: 0.9075\n",
      " training set -> batch:490 loss:0.623916506767273 and acc: 0.9048245549201965\n",
      "490/782 [=================>............] - ETA: 2:19:43 - loss: 0.3354 - accuracy: 0.9048\n",
      " training set -> batch:491 loss:0.4339469075202942 and acc: 0.9026816487312317\n",
      "491/782 [=================>............] - ETA: 2:19:11 - loss: 0.3356 - accuracy: 0.9027\n",
      " training set -> batch:492 loss:0.4428047835826874 and acc: 0.9018771052360535\n",
      "492/782 [=================>............] - ETA: 2:18:39 - loss: 0.3358 - accuracy: 0.9019\n",
      " training set -> batch:493 loss:0.10669675469398499 and acc: 0.9023569226264954\n",
      "493/782 [=================>............] - ETA: 2:18:07 - loss: 0.3353 - accuracy: 0.9024\n",
      " training set -> batch:494 loss:0.20041996240615845 and acc: 0.9028239250183105\n",
      "494/782 [=================>............] - ETA: 2:17:35 - loss: 0.3351 - accuracy: 0.9028\n",
      " training set -> batch:495 loss:0.54396653175354 and acc: 0.9012295007705688\n",
      "495/782 [=================>............] - ETA: 2:17:03 - loss: 0.3355 - accuracy: 0.9012\n",
      " training set -> batch:496 loss:0.1722743809223175 and acc: 0.901294469833374\n",
      "496/782 [==================>...........] - ETA: 2:16:32 - loss: 0.3352 - accuracy: 0.9013\n",
      " training set -> batch:497 loss:0.5021037459373474 and acc: 0.899760365486145\n",
      "497/782 [==================>...........] - ETA: 2:16:00 - loss: 0.3355 - accuracy: 0.8998\n",
      " training set -> batch:498 loss:0.26730242371559143 and acc: 0.8994479775428772\n",
      "498/782 [==================>...........] - ETA: 2:15:28 - loss: 0.3354 - accuracy: 0.8994\n",
      " training set -> batch:499 loss:0.3580450415611267 and acc: 0.8987538814544678\n",
      "499/782 [==================>...........] - ETA: 2:14:57 - loss: 0.3354 - accuracy: 0.8988\n",
      " training set -> batch:500 loss:0.2376096546649933 and acc: 0.8992307782173157\n",
      "\n",
      " validation set -> batch:500 val loss:0.2953124148771167 and val acc: 0.8740000128746033\n",
      "500/782 [==================>...........] - ETA: 2:17:11 - loss: 0.3352 - accuracy: 0.8992\n",
      " training set -> batch:501 loss:0.2394879162311554 and acc: 0.875\n",
      "501/782 [==================>...........] - ETA: 2:16:41 - loss: 0.3350 - accuracy: 0.8750\n",
      " training set -> batch:502 loss:0.28643518686294556 and acc: 0.8731203079223633\n",
      "502/782 [==================>...........] - ETA: 2:16:10 - loss: 0.3349 - accuracy: 0.8731\n",
      " training set -> batch:503 loss:0.24233394861221313 and acc: 0.8740875720977783\n",
      "503/782 [==================>...........] - ETA: 2:15:40 - loss: 0.3347 - accuracy: 0.8741\n",
      " training set -> batch:504 loss:0.2467714250087738 and acc: 0.8758864998817444\n",
      "504/782 [==================>...........] - ETA: 2:15:15 - loss: 0.3346 - accuracy: 0.8759\n",
      " training set -> batch:505 loss:0.31219035387039185 and acc: 0.875\n",
      "505/782 [==================>...........] - ETA: 2:14:47 - loss: 0.3345 - accuracy: 0.8750\n",
      " training set -> batch:506 loss:0.2910025715827942 and acc: 0.8741610646247864\n",
      "506/782 [==================>...........] - ETA: 2:14:17 - loss: 0.3344 - accuracy: 0.8742\n",
      " training set -> batch:507 loss:0.13249559700489044 and acc: 0.8766340017318726\n",
      "507/782 [==================>...........] - ETA: 2:13:47 - loss: 0.3340 - accuracy: 0.8766\n",
      " training set -> batch:508 loss:0.27531349658966064 and acc: 0.8765923380851746\n",
      "508/782 [==================>...........] - ETA: 2:13:17 - loss: 0.3339 - accuracy: 0.8766\n",
      " training set -> batch:509 loss:0.1846197247505188 and acc: 0.8781055808067322\n",
      "509/782 [==================>...........] - ETA: 2:12:47 - loss: 0.3336 - accuracy: 0.8781\n",
      " training set -> batch:510 loss:0.2984972596168518 and acc: 0.8787878751754761\n",
      "510/782 [==================>...........] - ETA: 2:12:16 - loss: 0.3336 - accuracy: 0.8788\n",
      " training set -> batch:511 loss:0.20019640028476715 and acc: 0.8794378638267517\n",
      "511/782 [==================>...........] - ETA: 2:11:46 - loss: 0.3333 - accuracy: 0.8794\n",
      " training set -> batch:512 loss:0.24975517392158508 and acc: 0.8793352842330933\n",
      "512/782 [==================>...........] - ETA: 2:11:16 - loss: 0.3331 - accuracy: 0.8793\n",
      " training set -> batch:513 loss:0.1306597888469696 and acc: 0.880649745464325\n",
      "513/782 [==================>...........] - ETA: 2:10:45 - loss: 0.3327 - accuracy: 0.8806\n",
      " training set -> batch:514 loss:0.1469098925590515 and acc: 0.8819060921669006\n",
      "514/782 [==================>...........] - ETA: 2:10:15 - loss: 0.3324 - accuracy: 0.8819\n",
      " training set -> batch:515 loss:0.4115847051143646 and acc: 0.8804054260253906\n",
      "515/782 [==================>...........] - ETA: 2:09:45 - loss: 0.3325 - accuracy: 0.8804\n",
      " training set -> batch:516 loss:0.2156982570886612 and acc: 0.8802909851074219\n",
      "516/782 [==================>...........] - ETA: 2:09:15 - loss: 0.3323 - accuracy: 0.8803\n",
      " training set -> batch:517 loss:0.3639853298664093 and acc: 0.8801813721656799\n",
      "517/782 [==================>...........] - ETA: 2:08:45 - loss: 0.3324 - accuracy: 0.8802\n",
      " training set -> batch:518 loss:0.19734737277030945 and acc: 0.8819797039031982\n",
      "518/782 [==================>...........] - ETA: 2:08:15 - loss: 0.3321 - accuracy: 0.8820\n",
      " training set -> batch:519 loss:0.061782002449035645 and acc: 0.8843283653259277\n",
      "519/782 [==================>...........] - ETA: 2:07:45 - loss: 0.3316 - accuracy: 0.8843\n",
      " training set -> batch:520 loss:0.36480826139450073 and acc: 0.8817073106765747\n",
      "520/782 [==================>...........] - ETA: 2:07:15 - loss: 0.3317 - accuracy: 0.8817\n",
      " training set -> batch:521 loss:0.11326675862073898 and acc: 0.8833732008934021\n",
      "521/782 [==================>...........] - ETA: 2:06:44 - loss: 0.3312 - accuracy: 0.8834\n",
      " training set -> batch:522 loss:0.18540024757385254 and acc: 0.8826290965080261\n",
      "522/782 [===================>..........] - ETA: 2:06:14 - loss: 0.3310 - accuracy: 0.8826\n",
      " training set -> batch:523 loss:0.07489075511693954 and acc: 0.8847926259040833\n",
      "523/782 [===================>..........] - ETA: 2:05:44 - loss: 0.3305 - accuracy: 0.8848\n",
      " training set -> batch:524 loss:0.26574474573135376 and acc: 0.8851810097694397\n",
      "524/782 [===================>..........] - ETA: 2:05:14 - loss: 0.3303 - accuracy: 0.8852\n",
      " training set -> batch:525 loss:0.2974987030029297 and acc: 0.8855555653572083\n",
      "525/782 [===================>..........] - ETA: 2:04:44 - loss: 0.3303 - accuracy: 0.8856\n",
      " training set -> batch:526 loss:0.4335213303565979 and acc: 0.8837336301803589\n",
      "526/782 [===================>..........] - ETA: 2:04:14 - loss: 0.3305 - accuracy: 0.8837\n",
      " training set -> batch:527 loss:0.27321332693099976 and acc: 0.883583664894104\n",
      "527/782 [===================>..........] - ETA: 2:03:44 - loss: 0.3304 - accuracy: 0.8836\n",
      " training set -> batch:528 loss:0.3335353136062622 and acc: 0.8823839426040649\n",
      "528/782 [===================>..........] - ETA: 2:03:14 - loss: 0.3304 - accuracy: 0.8824\n",
      " training set -> batch:529 loss:0.11660101264715195 and acc: 0.8832987546920776\n",
      "529/782 [===================>..........] - ETA: 2:02:44 - loss: 0.3300 - accuracy: 0.8833\n",
      " training set -> batch:530 loss:0.3090555965900421 and acc: 0.8826530575752258\n",
      "530/782 [===================>..........] - ETA: 2:02:14 - loss: 0.3299 - accuracy: 0.8827\n",
      " training set -> batch:531 loss:0.5016221404075623 and acc: 0.8820281028747559\n",
      "531/782 [===================>..........] - ETA: 2:01:45 - loss: 0.3303 - accuracy: 0.8820\n",
      " training set -> batch:532 loss:0.21046197414398193 and acc: 0.8833991885185242\n",
      "532/782 [===================>..........] - ETA: 2:01:14 - loss: 0.3300 - accuracy: 0.8834\n",
      " training set -> batch:533 loss:0.1682436615228653 and acc: 0.8842412233352661\n",
      "533/782 [===================>..........] - ETA: 2:00:44 - loss: 0.3297 - accuracy: 0.8842\n",
      " training set -> batch:534 loss:0.4928441047668457 and acc: 0.8840996026992798\n",
      "534/782 [===================>..........] - ETA: 2:00:14 - loss: 0.3300 - accuracy: 0.8841\n",
      " training set -> batch:535 loss:0.3740634322166443 and acc: 0.8834905624389648\n",
      "535/782 [===================>..........] - ETA: 1:59:44 - loss: 0.3301 - accuracy: 0.8835\n",
      " training set -> batch:536 loss:0.09657269716262817 and acc: 0.8852230310440063\n",
      "536/782 [===================>..........] - ETA: 1:59:14 - loss: 0.3297 - accuracy: 0.8852\n",
      " training set -> batch:537 loss:0.2532269060611725 and acc: 0.8850732445716858\n",
      "537/782 [===================>..........] - ETA: 1:58:44 - loss: 0.3295 - accuracy: 0.8851\n",
      " training set -> batch:538 loss:0.2641288638114929 and acc: 0.8853790760040283\n",
      "538/782 [===================>..........] - ETA: 1:58:14 - loss: 0.3294 - accuracy: 0.8854\n",
      " training set -> batch:539 loss:0.34512460231781006 and acc: 0.8843416571617126\n",
      "539/782 [===================>..........] - ETA: 1:57:44 - loss: 0.3294 - accuracy: 0.8843\n",
      " training set -> batch:540 loss:0.32876092195510864 and acc: 0.8833333253860474\n",
      "540/782 [===================>..........] - ETA: 1:57:14 - loss: 0.3294 - accuracy: 0.8833\n",
      " training set -> batch:541 loss:0.3105466365814209 and acc: 0.882785439491272\n",
      "541/782 [===================>..........] - ETA: 1:56:45 - loss: 0.3294 - accuracy: 0.8828\n",
      " training set -> batch:542 loss:0.3289191424846649 and acc: 0.8826791644096375\n",
      "542/782 [===================>..........] - ETA: 1:56:15 - loss: 0.3294 - accuracy: 0.8827\n",
      " training set -> batch:543 loss:0.13623981177806854 and acc: 0.8842592835426331\n",
      "543/782 [===================>..........] - ETA: 1:55:45 - loss: 0.3290 - accuracy: 0.8843\n",
      " training set -> batch:544 loss:0.4422403573989868 and acc: 0.8841361999511719\n",
      "544/782 [===================>..........] - ETA: 1:55:15 - loss: 0.3293 - accuracy: 0.8841\n",
      " training set -> batch:545 loss:0.2219364047050476 and acc: 0.8844262361526489\n",
      "545/782 [===================>..........] - ETA: 1:54:45 - loss: 0.3291 - accuracy: 0.8844\n",
      " training set -> batch:546 loss:0.17977558076381683 and acc: 0.8851132392883301\n",
      "546/782 [===================>..........] - ETA: 1:54:15 - loss: 0.3288 - accuracy: 0.8851\n",
      " training set -> batch:547 loss:0.1598449945449829 and acc: 0.8861821293830872\n",
      "547/782 [===================>..........] - ETA: 1:53:45 - loss: 0.3285 - accuracy: 0.8862\n",
      " training set -> batch:548 loss:0.21978099644184113 and acc: 0.886435329914093\n",
      "548/782 [====================>.........] - ETA: 1:53:15 - loss: 0.3283 - accuracy: 0.8864\n",
      " training set -> batch:549 loss:0.26950278878211975 and acc: 0.8862928152084351\n",
      "549/782 [====================>.........] - ETA: 1:52:45 - loss: 0.3282 - accuracy: 0.8863\n",
      " training set -> batch:550 loss:0.29893431067466736 and acc: 0.8861538171768188\n",
      "\n",
      " validation set -> batch:550 val loss:0.24496001284569502 and val acc: 0.9100000262260437\n",
      "550/782 [====================>.........] - ETA: 1:54:32 - loss: 0.3281 - accuracy: 0.8862\n",
      " training set -> batch:551 loss:0.18664351105690002 and acc: 0.9098837375640869\n",
      "551/782 [====================>.........] - ETA: 1:54:01 - loss: 0.3279 - accuracy: 0.9099\n",
      " training set -> batch:552 loss:0.2367546260356903 and acc: 0.9097744226455688\n",
      "552/782 [====================>.........] - ETA: 1:53:31 - loss: 0.3277 - accuracy: 0.9098\n",
      " training set -> batch:553 loss:0.14096638560295105 and acc: 0.9114963412284851\n",
      "553/782 [====================>.........] - ETA: 1:53:00 - loss: 0.3274 - accuracy: 0.9115\n",
      " training set -> batch:554 loss:0.25606414675712585 and acc: 0.9104610085487366\n",
      "554/782 [====================>.........] - ETA: 1:52:29 - loss: 0.3272 - accuracy: 0.9105\n",
      " training set -> batch:555 loss:0.3341417610645294 and acc: 0.9086207151412964\n",
      "555/782 [====================>.........] - ETA: 1:51:59 - loss: 0.3272 - accuracy: 0.9086\n",
      " training set -> batch:556 loss:0.15084907412528992 and acc: 0.9093959927558899\n",
      "556/782 [====================>.........] - ETA: 1:51:28 - loss: 0.3269 - accuracy: 0.9094\n",
      " training set -> batch:557 loss:0.3732556402683258 and acc: 0.9076797366142273\n",
      "557/782 [====================>.........] - ETA: 1:50:58 - loss: 0.3270 - accuracy: 0.9077\n",
      " training set -> batch:558 loss:0.21941420435905457 and acc: 0.9068471193313599\n",
      "558/782 [====================>.........] - ETA: 1:50:27 - loss: 0.3268 - accuracy: 0.9068\n",
      " training set -> batch:559 loss:0.2386544644832611 and acc: 0.9068322777748108\n",
      "559/782 [====================>.........] - ETA: 1:49:57 - loss: 0.3267 - accuracy: 0.9068\n",
      " training set -> batch:560 loss:0.1719363033771515 and acc: 0.907575786113739\n",
      "560/782 [====================>.........] - ETA: 1:49:26 - loss: 0.3264 - accuracy: 0.9076\n",
      " training set -> batch:561 loss:0.43129318952560425 and acc: 0.9060651063919067\n",
      "561/782 [====================>.........] - ETA: 1:48:56 - loss: 0.3266 - accuracy: 0.9061\n",
      " training set -> batch:562 loss:0.25125032663345337 and acc: 0.9039017558097839\n",
      "562/782 [====================>.........] - ETA: 1:48:25 - loss: 0.3264 - accuracy: 0.9039\n",
      " training set -> batch:563 loss:0.22063106298446655 and acc: 0.9046609997749329\n",
      "563/782 [====================>.........] - ETA: 1:47:54 - loss: 0.3262 - accuracy: 0.9047\n",
      " training set -> batch:564 loss:0.12389715015888214 and acc: 0.9060773253440857\n",
      "564/782 [====================>.........] - ETA: 1:47:24 - loss: 0.3259 - accuracy: 0.9061\n",
      " training set -> batch:565 loss:0.06684962660074234 and acc: 0.9074324369430542\n",
      "565/782 [====================>.........] - ETA: 1:46:54 - loss: 0.3254 - accuracy: 0.9074\n",
      " training set -> batch:566 loss:0.22464706003665924 and acc: 0.9080687761306763\n",
      "566/782 [====================>.........] - ETA: 1:46:23 - loss: 0.3253 - accuracy: 0.9081\n",
      " training set -> batch:567 loss:0.29963070154190063 and acc: 0.9067357778549194\n",
      "567/782 [====================>.........] - ETA: 1:45:53 - loss: 0.3252 - accuracy: 0.9067\n",
      " training set -> batch:568 loss:0.45261940360069275 and acc: 0.904187798500061\n",
      "568/782 [====================>.........] - ETA: 1:45:23 - loss: 0.3254 - accuracy: 0.9042\n",
      " training set -> batch:569 loss:0.22399434447288513 and acc: 0.9042288661003113\n",
      "569/782 [====================>.........] - ETA: 1:44:52 - loss: 0.3253 - accuracy: 0.9042\n",
      " training set -> batch:570 loss:0.3789437413215637 and acc: 0.9030487537384033\n",
      "570/782 [====================>.........] - ETA: 1:44:22 - loss: 0.3253 - accuracy: 0.9030\n",
      " training set -> batch:571 loss:0.20757277309894562 and acc: 0.9025119543075562\n",
      "571/782 [====================>.........] - ETA: 1:43:52 - loss: 0.3251 - accuracy: 0.9025\n",
      " training set -> batch:572 loss:0.24866998195648193 and acc: 0.9014084339141846\n",
      "572/782 [====================>.........] - ETA: 1:43:21 - loss: 0.3250 - accuracy: 0.9014\n",
      " training set -> batch:573 loss:0.9917569160461426 and acc: 0.89573734998703\n",
      "573/782 [====================>.........] - ETA: 1:42:51 - loss: 0.3262 - accuracy: 0.8957\n",
      " training set -> batch:574 loss:0.4611145853996277 and acc: 0.8936651349067688\n",
      "574/782 [=====================>........] - ETA: 1:42:20 - loss: 0.3264 - accuracy: 0.8937\n",
      " training set -> batch:575 loss:0.11082565039396286 and acc: 0.8955555558204651\n",
      "575/782 [=====================>........] - ETA: 1:41:50 - loss: 0.3260 - accuracy: 0.8956\n",
      " training set -> batch:576 loss:0.22992974519729614 and acc: 0.8957423567771912\n",
      "576/782 [=====================>........] - ETA: 1:41:20 - loss: 0.3259 - accuracy: 0.8957\n",
      " training set -> batch:577 loss:0.31021052598953247 and acc: 0.8948497772216797\n",
      "577/782 [=====================>........] - ETA: 1:40:49 - loss: 0.3258 - accuracy: 0.8948\n",
      " training set -> batch:578 loss:0.32683277130126953 and acc: 0.8939873576164246\n",
      "578/782 [=====================>........] - ETA: 1:40:19 - loss: 0.3258 - accuracy: 0.8940\n",
      " training set -> batch:579 loss:0.29846101999282837 and acc: 0.8941908478736877\n",
      "579/782 [=====================>........] - ETA: 1:39:48 - loss: 0.3258 - accuracy: 0.8942\n",
      " training set -> batch:580 loss:0.2843461036682129 and acc: 0.8943877816200256\n",
      "580/782 [=====================>........] - ETA: 1:39:18 - loss: 0.3257 - accuracy: 0.8944\n",
      " training set -> batch:581 loss:0.2365429699420929 and acc: 0.8945783376693726\n",
      "581/782 [=====================>........] - ETA: 1:38:48 - loss: 0.3256 - accuracy: 0.8946\n",
      " training set -> batch:582 loss:0.08681806921958923 and acc: 0.8962450623512268\n",
      "582/782 [=====================>........] - ETA: 1:38:18 - loss: 0.3252 - accuracy: 0.8962\n",
      " training set -> batch:583 loss:0.19261308014392853 and acc: 0.8968871831893921\n",
      "583/782 [=====================>........] - ETA: 1:37:47 - loss: 0.3249 - accuracy: 0.8969\n",
      " training set -> batch:584 loss:0.23407524824142456 and acc: 0.8965517282485962\n",
      "584/782 [=====================>........] - ETA: 1:37:17 - loss: 0.3248 - accuracy: 0.8966\n",
      " training set -> batch:585 loss:0.37091195583343506 and acc: 0.8957546949386597\n",
      "585/782 [=====================>........] - ETA: 1:36:47 - loss: 0.3249 - accuracy: 0.8958\n",
      " training set -> batch:586 loss:0.24870990216732025 and acc: 0.8959107995033264\n",
      "586/782 [=====================>........] - ETA: 1:36:17 - loss: 0.3247 - accuracy: 0.8959\n",
      " training set -> batch:587 loss:0.15219104290008545 and acc: 0.8969780206680298\n",
      "587/782 [=====================>........] - ETA: 1:35:47 - loss: 0.3244 - accuracy: 0.8970\n",
      " training set -> batch:588 loss:0.1553788185119629 and acc: 0.8966606259346008\n",
      "588/782 [=====================>........] - ETA: 1:35:17 - loss: 0.3241 - accuracy: 0.8967\n",
      " training set -> batch:589 loss:0.2921571731567383 and acc: 0.8967971801757812\n",
      "589/782 [=====================>........] - ETA: 1:34:47 - loss: 0.3241 - accuracy: 0.8968\n",
      " training set -> batch:590 loss:0.21385455131530762 and acc: 0.8973684310913086\n",
      "590/782 [=====================>........] - ETA: 1:34:18 - loss: 0.3239 - accuracy: 0.8974\n",
      " training set -> batch:591 loss:0.2802578806877136 and acc: 0.8970588445663452\n",
      "591/782 [=====================>........] - ETA: 1:33:49 - loss: 0.3238 - accuracy: 0.8971\n",
      " training set -> batch:592 loss:0.22547182440757751 and acc: 0.8976109027862549\n",
      "592/782 [=====================>........] - ETA: 1:33:19 - loss: 0.3237 - accuracy: 0.8976\n",
      " training set -> batch:593 loss:0.18097041547298431 and acc: 0.8981481194496155\n",
      "593/782 [=====================>........] - ETA: 1:32:50 - loss: 0.3234 - accuracy: 0.8981\n",
      " training set -> batch:594 loss:0.12609277665615082 and acc: 0.8990863561630249\n",
      "594/782 [=====================>........] - ETA: 1:32:21 - loss: 0.3231 - accuracy: 0.8991\n",
      " training set -> batch:595 loss:0.30016106367111206 and acc: 0.8983606696128845\n",
      "595/782 [=====================>........] - ETA: 1:31:51 - loss: 0.3230 - accuracy: 0.8984\n",
      " training set -> batch:596 loss:0.2825147807598114 and acc: 0.8976536989212036\n",
      "596/782 [=====================>........] - ETA: 1:31:21 - loss: 0.3230 - accuracy: 0.8977\n",
      " training set -> batch:597 loss:0.33380454778671265 and acc: 0.8973641991615295\n",
      "597/782 [=====================>........] - ETA: 1:30:50 - loss: 0.3230 - accuracy: 0.8974\n",
      " training set -> batch:598 loss:0.1360730230808258 and acc: 0.897870659828186\n",
      "598/782 [=====================>........] - ETA: 1:30:20 - loss: 0.3227 - accuracy: 0.8979\n",
      " training set -> batch:599 loss:0.18795141577720642 and acc: 0.8979750871658325\n",
      "599/782 [=====================>........] - ETA: 1:29:50 - loss: 0.3225 - accuracy: 0.8980\n",
      " training set -> batch:600 loss:0.10651129484176636 and acc: 0.8988461494445801\n",
      "\n",
      " validation set -> batch:600 val loss:0.26960218977183104 and val acc: 0.8880000114440918\n",
      "600/782 [======================>.......] - ETA: 1:30:59 - loss: 0.3221 - accuracy: 0.8988\n",
      " training set -> batch:601 loss:0.24518686532974243 and acc: 0.8895348906517029\n",
      "601/782 [======================>.......] - ETA: 1:30:28 - loss: 0.3220 - accuracy: 0.8895\n",
      " training set -> batch:602 loss:0.19875407218933105 and acc: 0.8919172883033752\n",
      "602/782 [======================>.......] - ETA: 1:29:57 - loss: 0.3218 - accuracy: 0.8919\n",
      " training set -> batch:603 loss:0.22117707133293152 and acc: 0.8923357725143433\n",
      "603/782 [======================>.......] - ETA: 1:29:27 - loss: 0.3216 - accuracy: 0.8923\n",
      " training set -> batch:604 loss:0.3907499313354492 and acc: 0.8891844153404236\n",
      "604/782 [======================>.......] - ETA: 1:28:56 - loss: 0.3217 - accuracy: 0.8892\n",
      " training set -> batch:605 loss:0.2206282764673233 and acc: 0.8870689868927002\n",
      "605/782 [======================>.......] - ETA: 1:28:25 - loss: 0.3215 - accuracy: 0.8871\n",
      " training set -> batch:606 loss:0.35274630784988403 and acc: 0.8859060406684875\n",
      "606/782 [======================>.......] - ETA: 1:27:55 - loss: 0.3216 - accuracy: 0.8859\n",
      " training set -> batch:607 loss:0.18120849132537842 and acc: 0.8872548937797546\n",
      "607/782 [======================>.......] - ETA: 1:27:24 - loss: 0.3214 - accuracy: 0.8873\n",
      " training set -> batch:608 loss:0.19188277423381805 and acc: 0.8885350227355957\n",
      "608/782 [======================>.......] - ETA: 1:26:53 - loss: 0.3212 - accuracy: 0.8885\n",
      " training set -> batch:609 loss:0.5345406532287598 and acc: 0.8866459727287292\n",
      "609/782 [======================>.......] - ETA: 1:26:23 - loss: 0.3215 - accuracy: 0.8866\n",
      " training set -> batch:610 loss:0.35335594415664673 and acc: 0.8863636255264282\n",
      "610/782 [======================>.......] - ETA: 1:25:52 - loss: 0.3216 - accuracy: 0.8864\n",
      " training set -> batch:611 loss:0.30328038334846497 and acc: 0.8860946893692017\n",
      "611/782 [======================>.......] - ETA: 1:25:22 - loss: 0.3215 - accuracy: 0.8861\n",
      " training set -> batch:612 loss:0.240009143948555 and acc: 0.8865606784820557\n",
      "612/782 [======================>.......] - ETA: 1:24:51 - loss: 0.3214 - accuracy: 0.8866\n",
      " training set -> batch:613 loss:0.27362310886383057 and acc: 0.8877118825912476\n",
      "613/782 [======================>.......] - ETA: 1:24:20 - loss: 0.3213 - accuracy: 0.8877\n",
      " training set -> batch:614 loss:0.2615387439727783 and acc: 0.8874309659004211\n",
      "614/782 [======================>.......] - ETA: 1:23:49 - loss: 0.3212 - accuracy: 0.8874\n",
      " training set -> batch:615 loss:0.27463144063949585 and acc: 0.887837827205658\n",
      "615/782 [======================>.......] - ETA: 1:23:19 - loss: 0.3211 - accuracy: 0.8878\n",
      " training set -> batch:616 loss:0.2417946457862854 and acc: 0.8882275223731995\n",
      "616/782 [======================>.......] - ETA: 1:22:48 - loss: 0.3210 - accuracy: 0.8882\n",
      " training set -> batch:617 loss:0.2262008786201477 and acc: 0.8879533410072327\n",
      "617/782 [======================>.......] - ETA: 1:22:17 - loss: 0.3209 - accuracy: 0.8880\n",
      " training set -> batch:618 loss:0.41556236147880554 and acc: 0.8864213228225708\n",
      "618/782 [======================>.......] - ETA: 1:21:47 - loss: 0.3210 - accuracy: 0.8864\n",
      " training set -> batch:619 loss:0.26479774713516235 and acc: 0.8861940503120422\n",
      "619/782 [======================>.......] - ETA: 1:21:16 - loss: 0.3209 - accuracy: 0.8862\n",
      " training set -> batch:620 loss:0.19709134101867676 and acc: 0.8871951103210449\n",
      "620/782 [======================>.......] - ETA: 1:20:46 - loss: 0.3207 - accuracy: 0.8872\n",
      " training set -> batch:621 loss:0.19882993400096893 and acc: 0.8875598311424255\n",
      "621/782 [======================>.......] - ETA: 1:20:15 - loss: 0.3205 - accuracy: 0.8876\n",
      " training set -> batch:622 loss:0.30734845995903015 and acc: 0.8867371082305908\n",
      "622/782 [======================>.......] - ETA: 1:19:45 - loss: 0.3205 - accuracy: 0.8867\n",
      " training set -> batch:623 loss:0.12264514714479446 and acc: 0.888248860836029\n",
      "623/782 [======================>.......] - ETA: 1:19:14 - loss: 0.3202 - accuracy: 0.8882\n",
      " training set -> batch:624 loss:0.173907071352005 and acc: 0.889140248298645\n",
      "624/782 [======================>.......] - ETA: 1:18:43 - loss: 0.3200 - accuracy: 0.8891\n",
      " training set -> batch:625 loss:0.3636489510536194 and acc: 0.8888888955116272\n",
      "625/782 [======================>.......] - ETA: 1:18:13 - loss: 0.3200 - accuracy: 0.8889\n",
      " training set -> batch:626 loss:0.08324281871318817 and acc: 0.8908296823501587\n",
      "626/782 [=======================>......] - ETA: 1:17:42 - loss: 0.3196 - accuracy: 0.8908\n",
      " training set -> batch:627 loss:0.1406472772359848 and acc: 0.8916308879852295\n",
      "627/782 [=======================>......] - ETA: 1:17:12 - loss: 0.3194 - accuracy: 0.8916\n",
      " training set -> batch:628 loss:0.11764407902956009 and acc: 0.892405092716217\n",
      "628/782 [=======================>......] - ETA: 1:16:42 - loss: 0.3190 - accuracy: 0.8924\n",
      " training set -> batch:629 loss:0.3230544626712799 and acc: 0.8921161890029907\n",
      "629/782 [=======================>......] - ETA: 1:16:11 - loss: 0.3190 - accuracy: 0.8921\n",
      " training set -> batch:630 loss:0.172516331076622 and acc: 0.893367350101471\n",
      "630/782 [=======================>......] - ETA: 1:15:41 - loss: 0.3188 - accuracy: 0.8934\n",
      " training set -> batch:631 loss:0.20369315147399902 and acc: 0.8940762877464294\n",
      "631/782 [=======================>......] - ETA: 1:15:11 - loss: 0.3186 - accuracy: 0.8941\n",
      " training set -> batch:632 loss:0.2819655239582062 and acc: 0.893774688243866\n",
      "632/782 [=======================>......] - ETA: 1:14:41 - loss: 0.3186 - accuracy: 0.8938\n",
      " training set -> batch:633 loss:0.22506387531757355 and acc: 0.8944552540779114\n",
      "633/782 [=======================>......] - ETA: 1:14:10 - loss: 0.3184 - accuracy: 0.8945\n",
      " training set -> batch:634 loss:0.3287171721458435 and acc: 0.8941571116447449\n",
      "634/782 [=======================>......] - ETA: 1:13:40 - loss: 0.3184 - accuracy: 0.8942\n",
      " training set -> batch:635 loss:0.5052981376647949 and acc: 0.8929245471954346\n",
      "635/782 [=======================>......] - ETA: 1:13:10 - loss: 0.3187 - accuracy: 0.8929\n",
      " training set -> batch:636 loss:0.22164054214954376 and acc: 0.893122673034668\n",
      "636/782 [=======================>......] - ETA: 1:12:39 - loss: 0.3186 - accuracy: 0.8931\n",
      " training set -> batch:637 loss:0.244347482919693 and acc: 0.8933150172233582\n",
      "637/782 [=======================>......] - ETA: 1:12:09 - loss: 0.3185 - accuracy: 0.8933\n",
      " training set -> batch:638 loss:0.21964871883392334 and acc: 0.8930505514144897\n",
      "638/782 [=======================>......] - ETA: 1:11:39 - loss: 0.3183 - accuracy: 0.8931\n",
      " training set -> batch:639 loss:0.09588615596294403 and acc: 0.8941280841827393\n",
      "639/782 [=======================>......] - ETA: 1:11:08 - loss: 0.3180 - accuracy: 0.8941\n",
      " training set -> batch:640 loss:0.2815221846103668 and acc: 0.8929824829101562\n",
      "640/782 [=======================>......] - ETA: 1:10:38 - loss: 0.3179 - accuracy: 0.8930\n",
      " training set -> batch:641 loss:0.12521283328533173 and acc: 0.8940311670303345\n",
      "641/782 [=======================>......] - ETA: 1:10:08 - loss: 0.3176 - accuracy: 0.8940\n",
      " training set -> batch:642 loss:0.196943461894989 and acc: 0.894197940826416\n",
      "642/782 [=======================>......] - ETA: 1:09:37 - loss: 0.3174 - accuracy: 0.8942\n",
      " training set -> batch:643 loss:0.32924729585647583 and acc: 0.8935185074806213\n",
      "643/782 [=======================>......] - ETA: 1:09:07 - loss: 0.3174 - accuracy: 0.8935\n",
      " training set -> batch:644 loss:0.2470134198665619 and acc: 0.8941029906272888\n",
      "644/782 [=======================>......] - ETA: 1:08:37 - loss: 0.3173 - accuracy: 0.8941\n",
      " training set -> batch:645 loss:0.09045427292585373 and acc: 0.8950819969177246\n",
      "645/782 [=======================>......] - ETA: 1:08:07 - loss: 0.3170 - accuracy: 0.8951\n",
      " training set -> batch:646 loss:0.19212070107460022 and acc: 0.8952265381813049\n",
      "646/782 [=======================>......] - ETA: 1:07:36 - loss: 0.3168 - accuracy: 0.8952\n",
      " training set -> batch:647 loss:0.3138761520385742 and acc: 0.8957667946815491\n",
      "647/782 [=======================>......] - ETA: 1:07:06 - loss: 0.3168 - accuracy: 0.8958\n",
      " training set -> batch:648 loss:0.17423054575920105 and acc: 0.8966876864433289\n",
      "648/782 [=======================>......] - ETA: 1:06:36 - loss: 0.3166 - accuracy: 0.8967\n",
      " training set -> batch:649 loss:0.34118449687957764 and acc: 0.8964174389839172\n",
      "649/782 [=======================>......] - ETA: 1:06:06 - loss: 0.3166 - accuracy: 0.8964\n",
      " training set -> batch:650 loss:0.311615526676178 and acc: 0.8957692384719849\n",
      "\n",
      " validation set -> batch:650 val loss:0.24990058969706297 and val acc: 0.9020000100135803\n",
      "650/782 [=======================>......] - ETA: 1:06:44 - loss: 0.3166 - accuracy: 0.8958\n",
      " training set -> batch:651 loss:0.13997122645378113 and acc: 0.9040697813034058\n",
      "651/782 [=======================>......] - ETA: 1:06:13 - loss: 0.3163 - accuracy: 0.9041\n",
      " training set -> batch:652 loss:0.3218960165977478 and acc: 0.9041353464126587\n",
      "652/782 [========================>.....] - ETA: 1:05:42 - loss: 0.3163 - accuracy: 0.9041\n",
      " training set -> batch:653 loss:0.1551538109779358 and acc: 0.9051094651222229\n",
      "653/782 [========================>.....] - ETA: 1:05:12 - loss: 0.3161 - accuracy: 0.9051\n",
      " training set -> batch:654 loss:0.21345245838165283 and acc: 0.9042553305625916\n",
      "654/782 [========================>.....] - ETA: 1:04:41 - loss: 0.3159 - accuracy: 0.9043\n",
      " training set -> batch:655 loss:0.12972711026668549 and acc: 0.9051724076271057\n",
      "655/782 [========================>.....] - ETA: 1:04:10 - loss: 0.3156 - accuracy: 0.9052\n",
      " training set -> batch:656 loss:0.23891907930374146 and acc: 0.9052013158798218\n",
      "656/782 [========================>.....] - ETA: 1:03:39 - loss: 0.3155 - accuracy: 0.9052\n",
      " training set -> batch:657 loss:0.15928897261619568 and acc: 0.9060457348823547\n",
      "657/782 [========================>.....] - ETA: 1:03:09 - loss: 0.3153 - accuracy: 0.9060\n",
      " training set -> batch:658 loss:0.24949300289154053 and acc: 0.9052547812461853\n",
      "658/782 [========================>.....] - ETA: 1:02:38 - loss: 0.3152 - accuracy: 0.9053\n",
      " training set -> batch:659 loss:0.09380649030208588 and acc: 0.907608687877655\n",
      "659/782 [========================>.....] - ETA: 1:02:07 - loss: 0.3148 - accuracy: 0.9076\n",
      " training set -> batch:660 loss:0.2709311246871948 and acc: 0.907575786113739\n",
      "660/782 [========================>.....] - ETA: 1:01:36 - loss: 0.3148 - accuracy: 0.9076\n",
      " training set -> batch:661 loss:0.44926291704177856 and acc: 0.9045857787132263\n",
      "661/782 [========================>.....] - ETA: 1:01:06 - loss: 0.3150 - accuracy: 0.9046\n",
      " training set -> batch:662 loss:0.20283187925815582 and acc: 0.9046242833137512\n",
      "662/782 [========================>.....] - ETA: 1:00:35 - loss: 0.3148 - accuracy: 0.9046\n",
      " training set -> batch:663 loss:0.28658294677734375 and acc: 0.9046609997749329\n",
      "663/782 [========================>.....] - ETA: 1:00:04 - loss: 0.3148 - accuracy: 0.9047\n",
      " training set -> batch:664 loss:0.2700344920158386 and acc: 0.9033148884773254\n",
      "664/782 [========================>.....] - ETA: 59:34 - loss: 0.3147 - accuracy: 0.9033  \n",
      " training set -> batch:665 loss:0.342525452375412 and acc: 0.9013513326644897\n",
      "665/782 [========================>.....] - ETA: 59:03 - loss: 0.3147 - accuracy: 0.9014\n",
      " training set -> batch:666 loss:0.2682257294654846 and acc: 0.9014550447463989\n",
      "666/782 [========================>.....] - ETA: 58:32 - loss: 0.3147 - accuracy: 0.9015\n",
      " training set -> batch:667 loss:0.1475711166858673 and acc: 0.9028497338294983\n",
      "667/782 [========================>.....] - ETA: 58:02 - loss: 0.3144 - accuracy: 0.9028\n",
      " training set -> batch:668 loss:0.35656869411468506 and acc: 0.9016497731208801\n",
      "668/782 [========================>.....] - ETA: 57:31 - loss: 0.3145 - accuracy: 0.9016\n",
      " training set -> batch:669 loss:0.22142966091632843 and acc: 0.9017412662506104\n",
      "669/782 [========================>.....] - ETA: 57:00 - loss: 0.3144 - accuracy: 0.9017\n",
      " training set -> batch:670 loss:0.4392331540584564 and acc: 0.9006097316741943\n",
      "670/782 [========================>.....] - ETA: 56:30 - loss: 0.3145 - accuracy: 0.9006\n",
      " training set -> batch:671 loss:0.22347159683704376 and acc: 0.9001196026802063\n",
      "671/782 [========================>.....] - ETA: 55:59 - loss: 0.3144 - accuracy: 0.9001\n",
      " training set -> batch:672 loss:0.4209536910057068 and acc: 0.8984741568565369\n",
      "672/782 [========================>.....] - ETA: 55:29 - loss: 0.3146 - accuracy: 0.8985\n",
      " training set -> batch:673 loss:0.21188104152679443 and acc: 0.899193525314331\n",
      "673/782 [========================>.....] - ETA: 54:58 - loss: 0.3144 - accuracy: 0.8992\n",
      " training set -> batch:674 loss:0.11333275586366653 and acc: 0.9004524946212769\n",
      "674/782 [========================>.....] - ETA: 54:27 - loss: 0.3141 - accuracy: 0.9005\n",
      " training set -> batch:675 loss:0.19747886061668396 and acc: 0.9005555510520935\n",
      "675/782 [========================>.....] - ETA: 53:57 - loss: 0.3139 - accuracy: 0.9006\n",
      " training set -> batch:676 loss:0.25675225257873535 and acc: 0.9006550312042236\n",
      "676/782 [========================>.....] - ETA: 53:26 - loss: 0.3139 - accuracy: 0.9007\n",
      " training set -> batch:677 loss:0.3268115520477295 and acc: 0.9002146124839783\n",
      "677/782 [========================>.....] - ETA: 52:55 - loss: 0.3139 - accuracy: 0.9002\n",
      " training set -> batch:678 loss:0.20765340328216553 and acc: 0.9003164768218994\n",
      "678/782 [=========================>....] - ETA: 52:25 - loss: 0.3137 - accuracy: 0.9003\n",
      " training set -> batch:679 loss:0.2292441427707672 and acc: 0.9009336233139038\n",
      "679/782 [=========================>....] - ETA: 51:54 - loss: 0.3136 - accuracy: 0.9009\n",
      " training set -> batch:680 loss:0.1213933527469635 and acc: 0.9020408391952515\n",
      "680/782 [=========================>....] - ETA: 51:24 - loss: 0.3133 - accuracy: 0.9020\n",
      " training set -> batch:681 loss:0.12209060788154602 and acc: 0.9026104211807251\n",
      "681/782 [=========================>....] - ETA: 50:53 - loss: 0.3130 - accuracy: 0.9026\n",
      " training set -> batch:682 loss:0.13635104894638062 and acc: 0.9031620621681213\n",
      "682/782 [=========================>....] - ETA: 50:23 - loss: 0.3128 - accuracy: 0.9032\n",
      " training set -> batch:683 loss:0.22524696588516235 and acc: 0.9032101035118103\n",
      "683/782 [=========================>....] - ETA: 49:52 - loss: 0.3126 - accuracy: 0.9032\n",
      " training set -> batch:684 loss:0.284597247838974 and acc: 0.9032567143440247\n",
      "684/782 [=========================>....] - ETA: 49:22 - loss: 0.3126 - accuracy: 0.9033\n",
      " training set -> batch:685 loss:0.31099700927734375 and acc: 0.902830183506012\n",
      "685/782 [=========================>....] - ETA: 48:51 - loss: 0.3126 - accuracy: 0.9028\n",
      " training set -> batch:686 loss:0.31570762395858765 and acc: 0.9019516706466675\n",
      "686/782 [=========================>....] - ETA: 48:21 - loss: 0.3126 - accuracy: 0.9020\n",
      " training set -> batch:687 loss:0.15681448578834534 and acc: 0.9024725556373596\n",
      "687/782 [=========================>....] - ETA: 47:51 - loss: 0.3124 - accuracy: 0.9025\n",
      " training set -> batch:688 loss:0.3412235677242279 and acc: 0.9011732935905457\n",
      "688/782 [=========================>....] - ETA: 47:20 - loss: 0.3124 - accuracy: 0.9012\n",
      " training set -> batch:689 loss:0.21229535341262817 and acc: 0.9012455344200134\n",
      "689/782 [=========================>....] - ETA: 46:50 - loss: 0.3123 - accuracy: 0.9012\n",
      " training set -> batch:690 loss:0.17690594494342804 and acc: 0.9013158082962036\n",
      "690/782 [=========================>....] - ETA: 46:20 - loss: 0.3121 - accuracy: 0.9013\n",
      " training set -> batch:691 loss:0.14501023292541504 and acc: 0.9022491574287415\n",
      "691/782 [=========================>....] - ETA: 45:49 - loss: 0.3118 - accuracy: 0.9022\n",
      " training set -> batch:692 loss:0.1953943818807602 and acc: 0.9027303457260132\n",
      "692/782 [=========================>....] - ETA: 45:19 - loss: 0.3117 - accuracy: 0.9027\n",
      " training set -> batch:693 loss:0.4861181080341339 and acc: 0.9015151262283325\n",
      "693/782 [=========================>....] - ETA: 44:48 - loss: 0.3119 - accuracy: 0.9015\n",
      " training set -> batch:694 loss:0.2813605070114136 and acc: 0.9015780687332153\n",
      "694/782 [=========================>....] - ETA: 44:18 - loss: 0.3119 - accuracy: 0.9016\n",
      " training set -> batch:695 loss:0.05353090539574623 and acc: 0.9028688669204712\n",
      "695/782 [=========================>....] - ETA: 43:48 - loss: 0.3115 - accuracy: 0.9029\n",
      " training set -> batch:696 loss:0.2501008212566376 and acc: 0.9021035432815552\n",
      "696/782 [=========================>....] - ETA: 43:17 - loss: 0.3114 - accuracy: 0.9021\n",
      " training set -> batch:697 loss:0.2490818202495575 and acc: 0.9021565318107605\n",
      "697/782 [=========================>....] - ETA: 42:47 - loss: 0.3113 - accuracy: 0.9022\n",
      " training set -> batch:698 loss:0.45387136936187744 and acc: 0.9006308913230896\n",
      "698/782 [=========================>....] - ETA: 42:16 - loss: 0.3115 - accuracy: 0.9006\n",
      " training set -> batch:699 loss:0.3984784483909607 and acc: 0.8999221324920654\n",
      "699/782 [=========================>....] - ETA: 41:46 - loss: 0.3117 - accuracy: 0.8999\n",
      " training set -> batch:700 loss:0.18278908729553223 and acc: 0.9003846049308777\n",
      "\n",
      " validation set -> batch:700 val loss:0.23569630272686481 and val acc: 0.9079999923706055\n",
      "700/782 [=========================>....] - ETA: 41:54 - loss: 0.3115 - accuracy: 0.9004\n",
      " training set -> batch:701 loss:0.1575353443622589 and acc: 0.9098837375640869\n",
      "701/782 [=========================>....] - ETA: 41:23 - loss: 0.3112 - accuracy: 0.9099\n",
      " training set -> batch:702 loss:0.1577996015548706 and acc: 0.9107142686843872\n",
      "702/782 [=========================>....] - ETA: 40:53 - loss: 0.3110 - accuracy: 0.9107\n",
      " training set -> batch:703 loss:0.22729796171188354 and acc: 0.9114963412284851\n",
      "703/782 [=========================>....] - ETA: 40:22 - loss: 0.3109 - accuracy: 0.9115\n",
      " training set -> batch:704 loss:0.211181178689003 and acc: 0.9095744490623474\n",
      "704/782 [==========================>...] - ETA: 39:51 - loss: 0.3108 - accuracy: 0.9096\n",
      " training set -> batch:705 loss:0.11349819600582123 and acc: 0.9112069010734558\n",
      "705/782 [==========================>...] - ETA: 39:20 - loss: 0.3105 - accuracy: 0.9112\n",
      " training set -> batch:706 loss:0.1303035020828247 and acc: 0.9127516746520996\n",
      "706/782 [==========================>...] - ETA: 38:49 - loss: 0.3102 - accuracy: 0.9128\n",
      " training set -> batch:707 loss:0.1937963366508484 and acc: 0.9133986830711365\n",
      "707/782 [==========================>...] - ETA: 38:18 - loss: 0.3101 - accuracy: 0.9134\n",
      " training set -> batch:708 loss:0.3656739592552185 and acc: 0.9140127301216125\n",
      "708/782 [==========================>...] - ETA: 37:47 - loss: 0.3101 - accuracy: 0.9140\n",
      " training set -> batch:709 loss:0.4362421929836273 and acc: 0.9130434989929199\n",
      "709/782 [==========================>...] - ETA: 37:16 - loss: 0.3103 - accuracy: 0.9130\n",
      " training set -> batch:710 loss:0.1062721386551857 and acc: 0.9136363863945007\n",
      "710/782 [==========================>...] - ETA: 36:46 - loss: 0.3100 - accuracy: 0.9136\n",
      " training set -> batch:711 loss:0.09066644310951233 and acc: 0.915680468082428\n",
      "711/782 [==========================>...] - ETA: 36:15 - loss: 0.3097 - accuracy: 0.9157\n",
      " training set -> batch:712 loss:0.41177207231521606 and acc: 0.913294792175293\n",
      "712/782 [==========================>...] - ETA: 35:44 - loss: 0.3099 - accuracy: 0.9133\n",
      " training set -> batch:713 loss:0.36531656980514526 and acc: 0.9117231369018555\n",
      "713/782 [==========================>...] - ETA: 35:13 - loss: 0.3099 - accuracy: 0.9117\n",
      " training set -> batch:714 loss:0.20247948169708252 and acc: 0.9116021990776062\n",
      "714/782 [==========================>...] - ETA: 34:42 - loss: 0.3098 - accuracy: 0.9116\n",
      " training set -> batch:715 loss:0.263641893863678 and acc: 0.9108108282089233\n",
      "715/782 [==========================>...] - ETA: 34:11 - loss: 0.3097 - accuracy: 0.9108\n",
      " training set -> batch:716 loss:0.28688907623291016 and acc: 0.9100528955459595\n",
      "716/782 [==========================>...] - ETA: 33:40 - loss: 0.3097 - accuracy: 0.9101\n",
      " training set -> batch:717 loss:0.21046768128871918 and acc: 0.909326434135437\n",
      "717/782 [==========================>...] - ETA: 33:09 - loss: 0.3096 - accuracy: 0.9093\n",
      " training set -> batch:718 loss:0.17027176916599274 and acc: 0.9098984599113464\n",
      "718/782 [==========================>...] - ETA: 32:38 - loss: 0.3094 - accuracy: 0.9099\n",
      " training set -> batch:719 loss:0.13829489052295685 and acc: 0.9110696315765381\n",
      "719/782 [==========================>...] - ETA: 32:08 - loss: 0.3091 - accuracy: 0.9111\n",
      " training set -> batch:720 loss:0.2982124090194702 and acc: 0.910365879535675\n",
      "720/782 [==========================>...] - ETA: 31:37 - loss: 0.3091 - accuracy: 0.9104\n",
      " training set -> batch:721 loss:0.3139868676662445 and acc: 0.9090909361839294\n",
      "721/782 [==========================>...] - ETA: 31:06 - loss: 0.3091 - accuracy: 0.9091\n",
      " training set -> batch:722 loss:0.27780836820602417 and acc: 0.9090375304222107\n",
      "722/782 [==========================>...] - ETA: 30:35 - loss: 0.3091 - accuracy: 0.9090\n",
      " training set -> batch:723 loss:0.4818577170372009 and acc: 0.9078341126441956\n",
      "723/782 [==========================>...] - ETA: 30:04 - loss: 0.3093 - accuracy: 0.9078\n",
      " training set -> batch:724 loss:0.16593965888023376 and acc: 0.9078054428100586\n",
      "724/782 [==========================>...] - ETA: 29:33 - loss: 0.3091 - accuracy: 0.9078\n",
      " training set -> batch:725 loss:0.0647113174200058 and acc: 0.9094444513320923\n",
      "725/782 [==========================>...] - ETA: 29:03 - loss: 0.3088 - accuracy: 0.9094\n",
      " training set -> batch:726 loss:0.32671207189559937 and acc: 0.9088428020477295\n",
      "726/782 [==========================>...] - ETA: 28:32 - loss: 0.3088 - accuracy: 0.9088\n",
      " training set -> batch:727 loss:0.11031774431467056 and acc: 0.9104077219963074\n",
      "727/782 [==========================>...] - ETA: 28:01 - loss: 0.3085 - accuracy: 0.9104\n",
      " training set -> batch:728 loss:0.33518046140670776 and acc: 0.9098101258277893\n",
      "728/782 [==========================>...] - ETA: 27:30 - loss: 0.3086 - accuracy: 0.9098\n",
      " training set -> batch:729 loss:0.389187753200531 and acc: 0.908713698387146\n",
      "729/782 [==========================>...] - ETA: 26:59 - loss: 0.3087 - accuracy: 0.9087\n",
      " training set -> batch:730 loss:0.23443135619163513 and acc: 0.9086734652519226\n",
      "730/782 [===========================>..] - ETA: 26:29 - loss: 0.3086 - accuracy: 0.9087\n",
      " training set -> batch:731 loss:0.20617996156215668 and acc: 0.9086345434188843\n",
      "731/782 [===========================>..] - ETA: 25:58 - loss: 0.3084 - accuracy: 0.9086\n",
      " training set -> batch:732 loss:0.23390424251556396 and acc: 0.907608687877655\n",
      "732/782 [===========================>..] - ETA: 25:27 - loss: 0.3083 - accuracy: 0.9076\n",
      " training set -> batch:733 loss:0.12304849922657013 and acc: 0.9085603356361389\n",
      "733/782 [===========================>..] - ETA: 24:56 - loss: 0.3081 - accuracy: 0.9086\n",
      " training set -> batch:734 loss:0.20199064910411835 and acc: 0.9090038537979126\n",
      "734/782 [===========================>..] - ETA: 24:26 - loss: 0.3079 - accuracy: 0.9090\n",
      " training set -> batch:735 loss:0.2815330922603607 and acc: 0.9080188870429993\n",
      "735/782 [===========================>..] - ETA: 23:55 - loss: 0.3079 - accuracy: 0.9080\n",
      " training set -> batch:736 loss:0.29416507482528687 and acc: 0.9065985083580017\n",
      "736/782 [===========================>..] - ETA: 23:24 - loss: 0.3079 - accuracy: 0.9066\n",
      " training set -> batch:737 loss:0.3168194890022278 and acc: 0.906593382358551\n",
      "737/782 [===========================>..] - ETA: 22:53 - loss: 0.3079 - accuracy: 0.9066\n",
      " training set -> batch:738 loss:0.24697694182395935 and acc: 0.9061371684074402\n",
      "738/782 [===========================>..] - ETA: 22:22 - loss: 0.3078 - accuracy: 0.9061\n",
      " training set -> batch:739 loss:0.19969920814037323 and acc: 0.9061387777328491\n",
      "739/782 [===========================>..] - ETA: 21:52 - loss: 0.3077 - accuracy: 0.9061\n",
      " training set -> batch:740 loss:0.10526706278324127 and acc: 0.9070175290107727\n",
      "740/782 [===========================>..] - ETA: 21:21 - loss: 0.3074 - accuracy: 0.9070\n",
      " training set -> batch:741 loss:0.2775910794734955 and acc: 0.9070069193840027\n",
      "741/782 [===========================>..] - ETA: 20:50 - loss: 0.3074 - accuracy: 0.9070\n",
      " training set -> batch:742 loss:0.3674439787864685 and acc: 0.9069966077804565\n",
      "742/782 [===========================>..] - ETA: 20:19 - loss: 0.3074 - accuracy: 0.9070\n",
      " training set -> batch:743 loss:0.2068113386631012 and acc: 0.9069865345954895\n",
      "743/782 [===========================>..] - ETA: 19:49 - loss: 0.3073 - accuracy: 0.9070\n",
      " training set -> batch:744 loss:0.1550188511610031 and acc: 0.9078072905540466\n",
      "744/782 [===========================>..] - ETA: 19:18 - loss: 0.3071 - accuracy: 0.9078\n",
      " training set -> batch:745 loss:0.09308341145515442 and acc: 0.9086065292358398\n",
      "745/782 [===========================>..] - ETA: 18:47 - loss: 0.3068 - accuracy: 0.9086\n",
      " training set -> batch:746 loss:0.21223482489585876 and acc: 0.9085760712623596\n",
      "746/782 [===========================>..] - ETA: 18:16 - loss: 0.3067 - accuracy: 0.9086\n",
      " training set -> batch:747 loss:0.14802241325378418 and acc: 0.9089456796646118\n",
      "747/782 [===========================>..] - ETA: 17:46 - loss: 0.3065 - accuracy: 0.9089\n",
      " training set -> batch:748 loss:0.2541850209236145 and acc: 0.9085173606872559\n",
      "748/782 [===========================>..] - ETA: 17:15 - loss: 0.3064 - accuracy: 0.9085\n",
      " training set -> batch:749 loss:0.1331777274608612 and acc: 0.90887850522995\n",
      "749/782 [===========================>..] - ETA: 16:44 - loss: 0.3062 - accuracy: 0.9089\n",
      " training set -> batch:750 loss:0.36441054940223694 and acc: 0.9084615111351013\n",
      "\n",
      " validation set -> batch:750 val loss:0.22816242976114154 and val acc: 0.9120000004768372\n",
      "750/782 [===========================>..] - ETA: 16:26 - loss: 0.3062 - accuracy: 0.9085\n",
      " training set -> batch:751 loss:0.21948716044425964 and acc: 0.911821722984314\n",
      "751/782 [===========================>..] - ETA: 15:55 - loss: 0.3061 - accuracy: 0.9118\n",
      " training set -> batch:752 loss:0.2622162401676178 and acc: 0.9116541147232056\n",
      "752/782 [===========================>..] - ETA: 15:24 - loss: 0.3061 - accuracy: 0.9117\n",
      " training set -> batch:753 loss:0.20096316933631897 and acc: 0.9114963412284851\n",
      "753/782 [===========================>..] - ETA: 14:53 - loss: 0.3059 - accuracy: 0.9115\n",
      " training set -> batch:754 loss:0.2066924273967743 and acc: 0.9104610085487366\n",
      "754/782 [===========================>..] - ETA: 14:22 - loss: 0.3058 - accuracy: 0.9105\n",
      " training set -> batch:755 loss:0.23542918264865875 and acc: 0.9120689630508423\n",
      "755/782 [===========================>..] - ETA: 13:51 - loss: 0.3057 - accuracy: 0.9121\n",
      " training set -> batch:756 loss:0.10571848601102829 and acc: 0.9135906100273132\n",
      "756/782 [============================>.] - ETA: 13:20 - loss: 0.3054 - accuracy: 0.9136\n",
      " training set -> batch:757 loss:0.24538150429725647 and acc: 0.9117646813392639\n",
      "757/782 [============================>.] - ETA: 12:49 - loss: 0.3054 - accuracy: 0.9118\n",
      " training set -> batch:758 loss:0.2348644733428955 and acc: 0.912420392036438\n",
      "758/782 [============================>.] - ETA: 12:18 - loss: 0.3053 - accuracy: 0.9124\n",
      " training set -> batch:759 loss:0.4208790063858032 and acc: 0.9107142686843872\n",
      "759/782 [============================>.] - ETA: 11:47 - loss: 0.3054 - accuracy: 0.9107\n",
      " training set -> batch:760 loss:0.27635693550109863 and acc: 0.9083333611488342\n",
      "760/782 [============================>.] - ETA: 11:16 - loss: 0.3054 - accuracy: 0.9083\n",
      " training set -> batch:761 loss:0.16265134513378143 and acc: 0.909023642539978\n",
      "761/782 [============================>.] - ETA: 10:46 - loss: 0.3052 - accuracy: 0.9090\n",
      " training set -> batch:762 loss:0.13127073645591736 and acc: 0.910404622554779\n",
      "762/782 [============================>.] - ETA: 10:15 - loss: 0.3050 - accuracy: 0.9104\n",
      " training set -> batch:763 loss:0.16790790855884552 and acc: 0.9110169410705566\n",
      "763/782 [============================>.] - ETA: 9:44 - loss: 0.3048 - accuracy: 0.9110 \n",
      " training set -> batch:764 loss:0.15668538212776184 and acc: 0.9122928380966187\n",
      "764/782 [============================>.] - ETA: 9:13 - loss: 0.3046 - accuracy: 0.9123\n",
      " training set -> batch:765 loss:0.1776961386203766 and acc: 0.9128378629684448\n",
      "765/782 [============================>.] - ETA: 8:42 - loss: 0.3044 - accuracy: 0.9128\n",
      " training set -> batch:766 loss:0.11797010153532028 and acc: 0.9133597612380981\n",
      "766/782 [============================>.] - ETA: 8:11 - loss: 0.3042 - accuracy: 0.9134\n",
      " training set -> batch:767 loss:0.2174454629421234 and acc: 0.9132124185562134\n",
      "767/782 [============================>.] - ETA: 7:40 - loss: 0.3041 - accuracy: 0.9132\n",
      " training set -> batch:768 loss:0.2624490559101105 and acc: 0.913705587387085\n",
      "768/782 [============================>.] - ETA: 7:09 - loss: 0.3040 - accuracy: 0.9137\n",
      " training set -> batch:769 loss:0.23780906200408936 and acc: 0.913557231426239\n",
      "769/782 [============================>.] - ETA: 6:39 - loss: 0.3039 - accuracy: 0.9136\n",
      " training set -> batch:770 loss:0.14787492156028748 and acc: 0.9146341681480408\n",
      "770/782 [============================>.] - ETA: 6:08 - loss: 0.3037 - accuracy: 0.9146\n",
      " training set -> batch:771 loss:0.13019561767578125 and acc: 0.915669858455658\n",
      "771/782 [============================>.] - ETA: 5:37 - loss: 0.3035 - accuracy: 0.9157\n",
      " training set -> batch:772 loss:0.4035683870315552 and acc: 0.9149060845375061\n",
      "772/782 [============================>.] - ETA: 5:06 - loss: 0.3036 - accuracy: 0.9149\n",
      " training set -> batch:773 loss:0.2284858226776123 and acc: 0.9153226017951965\n",
      "773/782 [============================>.] - ETA: 4:36 - loss: 0.3035 - accuracy: 0.9153\n",
      " training set -> batch:774 loss:0.39541536569595337 and acc: 0.9140271544456482\n",
      "774/782 [============================>.] - ETA: 4:05 - loss: 0.3037 - accuracy: 0.9140\n",
      " training set -> batch:775 loss:0.21650834381580353 and acc: 0.9127777814865112\n",
      "775/782 [============================>.] - ETA: 3:34 - loss: 0.3035 - accuracy: 0.9128\n",
      " training set -> batch:776 loss:0.2940766215324402 and acc: 0.9110261797904968\n",
      "776/782 [============================>.] - ETA: 3:03 - loss: 0.3035 - accuracy: 0.9110\n",
      " training set -> batch:777 loss:0.3259516656398773 and acc: 0.9104077219963074\n",
      "777/782 [============================>.] - ETA: 2:33 - loss: 0.3036 - accuracy: 0.9104\n",
      " training set -> batch:778 loss:0.31094324588775635 and acc: 0.9103375673294067\n",
      "778/782 [============================>.] - ETA: 2:02 - loss: 0.3036 - accuracy: 0.9103\n",
      " training set -> batch:779 loss:0.24796292185783386 and acc: 0.908713698387146\n",
      "779/782 [============================>.] - ETA: 1:31 - loss: 0.3035 - accuracy: 0.9087\n",
      " training set -> batch:780 loss:0.4064849019050598 and acc: 0.9071428775787354\n",
      "780/782 [============================>.] - ETA: 1:01 - loss: 0.3036 - accuracy: 0.9071\n",
      " training set -> batch:781 loss:0.27640023827552795 and acc: 0.9066265225410461\n",
      "781/782 [============================>.] - ETA: 30s - loss: 0.3036 - accuracy: 0.9066 \n",
      " training set -> batch:782 loss:0.17975029349327087 and acc: 0.9070000052452087\n",
      "782/782 [==============================] - 24243s 31s/step - loss: 0.3034 - accuracy: 0.9070 - val_loss: 0.2289 - val_accuracy: 0.9080\n",
      "\n",
      "execution time: 6:44:09\n"
     ]
    }
   ],
   "source": [
    "# time the function\n",
    "start_time = time.time()\n",
    "\n",
    "# making the transformation here since inside model.fit it creates a lot of warnings\n",
    "data_train = data_feature_extraction(train_dataset, model.name)\n",
    "data_val = data_feature_extraction(valid_dataset, model.name)\n",
    "histories_per_step = History_per_step(data_val, 50)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(data_train, \n",
    "                    epochs=1, \n",
    "                    steps_per_epoch=782, #STEP_EPOCH_TRAIN,\n",
    "                    validation_data=data_val,\n",
    "                    validation_steps=16,\n",
    "                    callbacks=[tensorboard_callback,\n",
    "                               *checkpoint_callback,\n",
    "                               histories_per_step])\n",
    "\n",
    "# print execution time\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "print('\\nexecution time: {}'.format(timedelta(seconds=round(elapsed_time_secs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# in case of crash, restart training from checkpoint:\n",
    "\n",
    "# load the model\n",
    "#new_model = load_model(checkpoint_prefix)\n",
    "\n",
    "#assert_allclose(model.predict(x_train),\n",
    "#                new_model.predict(x_train),\n",
    "#                1e-5)\n",
    "\n",
    "# fit the model\n",
    "#checkpoint = ModelCheckpoint(checkpoint_prefix, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "#callbacks_list = [checkpoint]\n",
    "#new_model.fit(data_train, \n",
    "#                    epochs=1, \n",
    "#                    steps_per_epoch=782, #STEP_EPOCH_TRAIN,\n",
    "#                    validation_data=data_val,\n",
    "#                    validation_steps=16,\n",
    "#                    callbacks=[tensorboard_callback,\n",
    "#                               *checkpoint_callback,\n",
    "#                               histories_per_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "savemodel_path = os.path.join(checkpoint_dir, 'saved_model')\n",
    "os.makedirs(savemodel_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/.conda-env/env_multilingual_class/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/.conda-env/env_multilingual_class/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/vera_luechinger/save_model/saved_model/tf_bert_classification/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /home/vera_luechinger/save_model/saved_model/tf_bert_classification/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.join(savemodel_path,model.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: tf_bert_classification\n",
      "  assets\n",
      "  variables\n",
      "  saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "# check the saved model\n",
    "print('Model: {}'.format(model.name))\n",
    "for i in os.listdir(os.path.join(savemodel_path,model.name)):\n",
    "        print(' ',i)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Loading a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard model: tf_bert_classification\n",
      "standard model: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "# redo these in case of loading the model\n",
    "data_train = data_feature_extraction(train_dataset, model.name)\n",
    "data_val = data_feature_extraction(valid_dataset, model.name)\n",
    "histories_per_step = History_per_step(data_val, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(checkpoint_dir + '/saved_model/tf_bert_classification')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'History_per_step' object has no attribute 'steps'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-e9aeb7c81331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m mm.plot_acc_loss(steps_loss_train=histories_per_step.steps, loss_train=histories_per_step.losses,\n\u001b[0m\u001b[1;32m      2\u001b[0m                  \u001b[0msteps_acc_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistories_per_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistories_per_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                  \u001b[0msteps_loss_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistories_per_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistories_per_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  steps_acc_eval=histories_per_step.val_steps, accuracy_eval=histories_per_step.val_accuracies)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'History_per_step' object has no attribute 'steps'"
     ]
    }
   ],
   "source": [
    "mm.plot_acc_loss(steps_loss_train=histories_per_step.steps, loss_train=histories_per_step.losses,\n",
    "                 steps_acc_train=histories_per_step.steps, accuracy_train=histories_per_step.accuracies,\n",
    "                 steps_loss_eval=histories_per_step.val_steps, loss_eval=histories_per_step.val_losses,\n",
    "                 steps_acc_eval=histories_per_step.val_steps, accuracy_eval=histories_per_step.val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Get more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['loss']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFBertForSequenceClassification' object has no attribute 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-6fd66ed6bf74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFBertForSequenceClassification' object has no attribute 'epoch'"
     ]
    }
   ],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFBertForSequenceClassification' object has no attribute 'params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-9b49a96a85ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFBertForSequenceClassification' object has no attribute 'params'"
     ]
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFBertForSequenceClassification' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-b2dcc5d67f0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFBertForSequenceClassification' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# dir(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Exploration of the model's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,\n",
    "                          'model.png',\n",
    "                          show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': <tf.Tensor 'attention_mask_4:0' shape=(None, 512) dtype=int32>,\n",
       " 'input_ids': <tf.Tensor 'input_ids_4:0' shape=(None, 512) dtype=int32>,\n",
       " 'token_type_ids': <tf.Tensor 'token_type_ids_4:0' shape=(None, 512) dtype=int32>}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'tf_bert_classification_4/Identity:0' shape=(None, 2) dtype=float32>]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.saving.saved_model.load.TFBertMainLayer at 0x7fc129a60350>,\n",
       " <tensorflow.python.keras.saving.saved_model.load.Dropout at 0x7fc129a60710>,\n",
       " <tensorflow.python.keras.saving.saved_model.load.Dense at 0x7fc129a60e90>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert [] []\n",
      "dropout_189 [] []\n",
      "classifier [] []\n"
     ]
    }
   ],
   "source": [
    "# _inbound_nodes and inbound_nodes give the same !\n",
    "# to see method available: dir(model.layers[2])\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer._inbound_nodes, layer._outbound_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Validation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard model: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "# get probablility for each classes\n",
    "if model.name=='custom_tf_bert_classification':\n",
    "        print('custom model: {}'.format(model.name))\n",
    "        y_pred = tf.nn.softmax(model.predict(valid_dataset))\n",
    "elif model.name=='tf_bert_classification':\n",
    "        print('standard model: {}'.format(model.name))\n",
    "        y_pred = tf.squeeze(tf.nn.softmax(model.predict(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1000, 2), dtype=float32, numpy=\n",
       "array([[0.05090828, 0.94909173],\n",
       "       [0.01116403, 0.988836  ],\n",
       "       [0.58249706, 0.4175029 ],\n",
       "       ...,\n",
       "       [0.85302615, 0.14697383],\n",
       "       [0.95922637, 0.04077363],\n",
       "       [0.9674078 , 0.03259215]], dtype=float32)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(model.predict(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000, 2])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# get predicted classes\n",
    "y_pred_argmax = tf.math.argmax(y_pred, axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1000])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred_argmax).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Extracting true classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# extracting and flatten true classes\n",
    "y_true_tf=valid_dataset.map(pp.label_extraction).flat_map(lambda x: valid_dataset.from_tensor_slices(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_true=list(y_true_tf.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true), len(y_pred_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Model performanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.92      0.89      0.91       503\n",
      "         pos       0.90      0.92      0.91       497\n",
      "\n",
      "    accuracy                           0.91      1000\n",
      "   macro avg       0.91      0.91      0.91      1000\n",
      "weighted avg       0.91      0.91      0.91      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_argmax, target_names=info.features[\"label\"].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on the  dataset:\n",
      "   Metric             \n",
      "accuracy...........   0.9080\n",
      "recall.............   0.9215\n",
      "auc................   0.9081\n",
      "precision (p=0.5)..   0.8963\n",
      "precision (avg)....   0.8649\n",
      "precision (micro)..   0.9080\n",
      "precision (macro)..   0.9083\n",
      "f1.................    0.9087\n",
      "r2.................    0.6320\n"
     ]
    }
   ],
   "source": [
    "mm.print_metrics(y_true, y_pred_argmax, mode='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.plot_confusion_matrix(confusion_matrix(y_true, y_pred_argmax), info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnoAAAH+CAYAAAALY6NfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nOzdd3hUZdrH8e8dCDWEXmyAJYgoUgQBWRZUVBBZbEsAFfRlEWXtYsGG6CruWlhFxUV3LUhTbAiiqyhYkY4oCtGlI70nhITkef84k8kkTBrM5KT8Ptc1V855znPO3BNK7jzVnHOIiIiISNkT43cAIiIiIhIdSvREREREyigleiIiIiJllBI9ERERkTJKiZ6IiIhIGaVET0RERKSMUqInIiWambU2s9lmtsvMnJk97HdM0RL4fK8Vof4aM5sT4RiuDcTRLZLPFRF/KNETKSfMrFvgB3joa7+ZLTaz282sYj73/tHM3jazTWaWZmZbzewjM7u0gPdsZmYvmtkvZpZsZgfMbJWZjTez9oWIuSLwDpAAPAhcA7xbxI9eqpnZwwV9n0VE8pLnf+wiUmZNBj4CDGgEDASeAU4Drs9d2cweA+4D1gL/BlYH7hsAvGdmE4DrnHMZue4bDIwDUgPvuRQ4BDQDrgCGmNnpzrkV+cR6UuB1p3Pu+SP9wKXcSOB14P0w104FtOq9iORJiZ5I+bPYOfdm1omZvQj8AvzFzO53zm0LuTYYL8n7DOjjnEsJufYPvMRvILAGeCjkWndgPLACuMg5tyk0ADMbAdxciFgbBb7uLMoHLIiZGVDdObc/ks8tbs65g37HICIlm7puRco551wyMA+vhe/krHIzqwT8DdgPDAhN8gL3HQKGAuuA4WZWP+Ty3wPPS8yd5GXd65wbk19rXmDs2dzA6ash3c1NA9erm9loM/vNzA6a2WYze8PMmuR6TlaX9bVm9lczW4HXyjg8v+9L1ng5MzvPzL4zsxQz22Bm9wSu1zazfwe6sVPMbIaZHZvrGa+ZWdgWt4LG45lZ05B7B4V2uYfUKdIYPTOrZGZ3m9nSQMx7zGyhmd1UwH01zOxvZva9mW0PfL9/NbMnzKxarrpmZreZ2Q9mts/M9prZysD3Kjak3jlmNivw55ZqZhsDwwE6FvbziEjB1KInIpCd4IW2nHXGa1GbGNrKF8o5l2pmb+K1+l0MvG5mJwJtga8K6JYtyGPAN4Fnjwe+CpRvC4zd+yQQ4zTgabxxfDcCF5pZO+fchlzPuw2oC7wMbAbWFyKGNkDvwPu/AfQFnjCzVGAQXkvmw8ApwC2BOt2L/lHD2oY3JnEC3mcffzQPCyTunwDdgP8Cb+IlvC2By4H8usaPA/6CN15yEl4XfFfgbrzv0UUhdR8AHgE+BF4CMoATgT8BlYF0MzsV+BTvz+FZYAve37XOQCu8XzxEJAKU6ImUP9XMrB7ZY/RuwPthvcA5tyqk3hmBr4sLeF7W9Za57lt6NEE65z41s3S8RO+7XN3NQ/CSgiedc3eHlH8GzABG4yVJoRoDzZ1zW4sQRkugk3Pu+8Dz/403VnEM8Lxz7paQ9wa43cxOdc6tLMJ7hBVoaX0zMAbyf6Gf/wjdhpfkjXbO3Rd6wcwK6t35H3CCcy49pOwFM3sUeMDMznbOzQ+UXwb87Jz7U65n3BtyfBFQDegfcp+IRIG6bkXKn1F4rUVbgR+AYXgzWXP/YI4PfN1TwPOyrtfMdd/eowszX5cBmXgJXZBzbiZegtknTPLyRhGTPPASzO9Dnp8GzMdLkp/LVTerxTGhiO9RXK4CduG1tuXgnMvM70bnXFpWkmdmFQPd1vXwxm4CdAipvgc4zsz+kM8js/7O9DGzKoX9ACJSdEr0RMqf8cAFeF2t9+B11x6P140XKitRq0n+cieEWffVOLow83UisMk5tyvMtZ8C710vV/mqMHUL8r8wZVnvuTqP8rpH8D4RYWY1zaxRrleFwOUE4BfnXO4/58I+e5iZ/QAcxPs7sw2YE7hcO6TqfXh/l74KjLubaGYDAl3HWabgJYn3ATvN7HMzuyf3+EoROXpK9ETKnyTn3GfOuVnOuX/gjUFrjzeeKtSPga9tC3he1vXlue5rc9SR5s2O4J6UgqscJiOvC7mXkwkRGlteEzGiNWzmWeD3XK8TCoqnIGZ2B/BC4HlDgV54vyxcG6gS/FninPsOb8znlcB7QGtgIrDUzOoE6hx0zl2A1xI4Gu/7/Ajwi5lddiQxikh4GqMnUs45574NjAMbaGbPOee+DVz6Fm+QfB8zq+ec25773kC329V4LTizAs9bbWZLgM5m1tw590sUwv4N6GFmtZxzu3Nda4HXqnhYvD7YCWBmdZxzoRNdTorS+/0Db5JFqM2Br6uA08ys8hEsy3IN3sSTnqHdvGbWI1zlwLI17wRemNkwvERxMPBkSL35eF3hmNkJwBK8md7vFTE+EcmDWvREBOBRsltVgOAabQ8BcXiTAqqG3hDoEnwRaII3KSJ0/Ns9ga9TzKwRuZhZhcASHC2OMN738f7/Ch3gj5n1xGtJnF7QuLNiktVdnHsm7p1FeMZ+oE5hKjrnVgRaa0NfWV21E/G6WB/IfZ8FZpLkIwOvNTBYL9AqeW/uioGxe7llTdipk0+dDXjdwYX6rCJSOGrRExGcc7+a2RTgKjPr4pz7KlA+3sxOxltGY4WZvYHXstMI6I83K/VNvAkeoc/71Myux9sZY6WZhe6McQrezhgnkz1Dt6hew1ve5B7z1tX7MvDcYXitkPfldWMxmww8Dow3s+bADqAnh48fzM88oHtg/b51gHPOTTmCWJ7F66Z/wLzt5/6L1xJ7Ot4OG/ktCzMNr4t1lpm9izcucwCQHqbuz2Y2D/ge2AQcg7fjShre2DwCMVyIN0N6NV4C2RtojtcqKSIRokRPRLI8hpe8PQKcm1XonLvHzGbh7WRxPd5kgz3AQmCkcy5sN5tz7t9m9jXesh7n4+2gEYO3PMnnQN8jXWfPOZduZhfhtU4l4q0Dtxt4G3jAOVeYNfKizjm318wuxtti7j681rl38bq7w00kCSer2/N+sie4FDnRc86lBZKrO/GStMfxEr0k4NUCbn8SLxkbjJcwbgamBu7L/Wf4NN5En1vwJvJsxUtWRzvnlgXqvI+XAPYFGgIHAnEMwdttRUQixJzTNokiIiIiZZHG6ImIiIiUUUr0RERERMooJXoiIiIiZZQSPREREZEySomeiIiISBlVLpZXqVevnmvatKnfYYiIiIgUaNGiRdudc/Uj8axykeg1bdqUhQsX+h2GiIiISIHMbG2knqWuWxEREZEySomeiIiISBmlRE9ERESkjFKiJyIiIlJGKdETERERKaOU6ImIiIiUUUr0RERERMooJXoiIiIiZZQSPREREZEySomeiIiISBmlRE9ERESkjFKiJyIiIlJGKdETERERKaOKPdEzs1PM7F9mtszMMsxsTiHvq2lmr5rZLjPbY2YTzaxulMMVERERKbUq+vCepwMXA/OASkW4bypwKvAXIBP4O/A+0CXSAYqIiIiUBX4keh865z4AMLNpQL2CbjCzTsBFQFfn3JeBso3A92bW3Tn3WTQDFhERESmNij3Rc85lHsFtPYEtWUle4DnzzWx14JoSPRERkSN06FAmBw6k4xxkZjoqVowhLi58p9u2bcmkpGTXbdQojmrVYg+rl56ewYoV24L1YmNjaNmyYdhnrlmzmzVrdpOZ6cjMdJx0Um1OOql22LqzZiVx4MChYN0//elUqlQ5PJ1Zv34Pn3++Onh+/PHxnH/+SWGfOXv2/9iwYW/w/LzzTuSEE2oeVi8lJZ233/4peF61aix9+54e9plLl25m2bLNwfNWrRrRunWjsHXfeusnDhxID3vtaPnRonckmgO/hCn/OXBNRETCOHQok//9bxfOOZyDChWMhITww5u3bNnP77/vD9Zt1CiOY4+tEbbuokWbOHgwA+ccAO3aHUvlyof/SNm+PYVlyzbjHDjnqFevGm3aHBP2mQsWbGTz5v0EHsnZZx9Ho0Zxh9VLTT3E9Okrg3FWqVKRSy8N/6Ng2bLNLF2a/f6tWzfK8/0nTFjG/v1pwfcfOLBVdrIzZx0MnwNr97I6I4P3/q8F7pjqOAcnnliLK65oEXjDrdD9reAz32pYmVX9Tg1+n/r2PZ1TTw10ZN3xOUxYAcC+TMcTnevjzqiPc464uErcf/8fs4Or/3zw8O2DB5l15Sns35/G/v1pDB7cJvv93/gR7pwTrHt86j42H0gnM9P7Xv3++53Z39Pzp8IP2wCYeTCNS/ftC97Xu3czpk/vH/YzDXKpzNqRHDyfMaM/vXo1O+wzbcnIoPWu3cF6xx5bg40b7wj7mf6TnMKjBw4Ez0eN6sZDD3UN+5muTdnL1pTspCivz7QsLY1r92Z/pl69ErITvVyf6Z8ulRkhn2n69H7ZiV7IZ9qTmcm1O3cF6zVqFJcz0Qv5TO8npzAq5DONHNk1O9HL9ZluO7CP35PTAmeRTfgs6y+fH7K6bp1z3Qqo9ymQ7Jy7NFf5m8BJzrlz8ru/Xbt2buHChUcbroiUcZmZjn37DpKSks6BA4fIyMjMMyn68cetLF26mYyMTDIyHK1aNeSss44NW3fs2O/ZvTuVjAxHRkYmd93Vmfj4yofVW758C889932gnuPMMxtw553h/3sbNmwm3323gb17D7JnTyofftifTp1OYFKvSSR9lHTk3wQR8UUmmXzHd8xnPnvYs8g51y4Szy1Nid5+59xluconAk2dc53D3HM9cD1A48aNz1q7dm3E4hYRLynasyc7eclqAQpn27Zkfv99PxkZmWRmOho2jOP44+PD1p07dw3JyenBuhdccHLYbqGNG/cya9avwUTruONq0KdP+FadF16Yz5Ilm9m3L419+w7y+OPn07p1IyVFIlIiJJPM+7xPEsH/jyKW6JWWrttdQP0w5bWA3WHKcc6NB8aD16IXvdBE/HPoUCZr1+5mw4a97N+fxgUXnEylShUOq7dgwUaeeWYe6ekZpKVl0KpVQx599Lywzxw2bCazZv0arPvqq32yu2VCvH7RBNZ9tjrMEyJraSHrbSug7gkhxx/M+pUPjjykEi+hY30GPHG232GInxbtgtErOT51Hxv3pwWL16+/PfuXrJBuzk/S0rgi/QAxFWMwMy688GTefvvPXr1c3ZyDYw/xaUVHTIxhZvzrX5dw4YUnexdDujl3ZGZybrUMYupWxcyoW7cqn302MDvGkG7O11NTefWsesTEGDExxtVXn8m117b2Lubq5hzcsCK7mtUOvv9LL/Wibt1qh32mpYcOMeYPDaFOVQBatWrIHXd0CvuZxtSqwNJzsrv0b7utQ3YXf8hn2p2Zya2n1YCTvfGDNWtW5rnneob9TO8dPMj7l50cPL/00lO57LLTDvtMX6b/ysUpr5OcvptKleLo3PlmvvhiNJFSWlr0HgGGOOeOyVX+G/C+c+7O/O5X162UFWqBirzSmhT98P12Wt3zffD8X/+6hOuvPytnpTs+Z+frP9Jx9x4ArEE16jStxXffDT78gfWfZ3xqKi8cSMUAa1mfv/ylDX/9a67vTeAH1KV797Il02H1q2In1uKdd/oe3qJ7/lTmL9rEvSkpWJuGWI1KtG9/LKNHd8+uE/ID928pKcyrGoN1Og4zGDHiD3TqFJKiB37g7snM5C/7k7FW9bETa1GzZmXGj+992OcB+OBgGu+kHcQST8PMG3sWHM8W8nkARiQns+uUWljn4zAz/va386gTSBKyPg97DvLzza0Zv3QTZoYZNG9ejyFDcn3vAyZO/IEVK7YF6/bv35IWLQ5vt0hOTmPMmHmYgZlRvXost97aMewzly/fwvffb6RGjUrExVXi5JPr0Lx5+AUs0tIygslT1rOl5Ni+fTtNmjQhJSWFc845h8mTJ9O4cWPMrNx13XYCvgW6OOe+DpS1AxYAFxS0vIoSPSlJMjIy2bnzAPXrVw97feXK7XzwwUpSUtJJTk5j4MBWwZlqo2xUcYZaoNKaJB2xOjWhZQLUf54ZaWlMPniQCkCF/i3o2fOUw2ffBZKIp1IOsNs5KpxZnwoXn8RNN52dM4EAOH8qG5dsZmZ6uvfMu87m2I7HZbeUZAkkRr9mZLDHOeKb16XmB5dTq1aVw1tzQ1oiAHi6Gww8I/xnC2mJAGDbTeHr5Wpd4ZoW8Ez41uGsxIinukG3xuHriJRz48aNY/369YwaNYrYWG+YSqlO9MysGt6CyQB3AvHAyMD5R865FDP7FZjrnBscct/HQDNgONkLJm91zhW4YLISPfGbc442bf7F77/vZ/v2FPplOg7vDC28kc77J9O162vc3bMBvTo1iEygZVHFKsy5+FNWZmRQw4z4k2rRasaVYZdOEBGJttmzZ7N3714uu+yyPOtEMtHzY4xeA+DtXGVZ5ycCa/Diyj3QqB8wBvgP3tZtM4BbohallBvF1R2a9z/poknoWB/mer+4zH0kj9aZMDamxvDd8EXErt1HvRijc2wsfNYXWuVMErds2U/Kos3EXj2TWDNqmlGlVQOYnXj4Q6PZYtQkPmItQd32nEG3o36KiMiRO3ToEKNGjeKxxx4jLi6Otm3b0qRJk6i/rx8LJq8B8h0k4JxrGqZsN3Bd4CVSJCVxbFtEuz1TYmDgt9nnTeJh4cAcVY4DrvxHEmw+mO+jGjaMg+PiocLhkzoO88x5eXfb5ZZXYpfbwDPyThZFREqhDRs2MGDAAL766itiYmIYPnw4xx9/fLG8d2mZdStSKEea0CVcnMCAmQMAb9mQqlUfIy0tI3h97957qXHp+8HZXFktYZMnL2fAgHeD9Ra90pW2p4RfYmTeqn10ut7b3KV25Yq8//NBuHKedzGkJWzduj0899z3VKsWS/XqsdR4dB5XV65EfExM/q1gWbJawsIJ1yoXTqsGhU/MREQkTzNnzmTQoEHs2LGDY445hkmTJtGtW7die39fJ2MUF43RK9s+/HAlW7Yks3VrMun3f55nvWqtGnL3si3B8/79z2DSpCvCDi5P+PAnfv11JwAzRrc/+jFwdWqypkZ9YmNjqF+/OpXunVu0Ls8IdmOKiEjxePrppxk+fDgAF110EW+88QYNGhT886S0j9GTcq44ulGzJiuEmjFjFfSeHDzftSs1++KIU+Gs7H0Vk/p0OvI3D6xflTs5axpaJxpdniIiUqKce+65VK9enQcffJC77rqLmJiYYo9BiZ5EXXGPj2vYJVerV2ASQO0/n5KjePfukETvrPCbZwfVqQm3Lc7uus2vha0rcMcFRQtaRETKhCVLltCmTRsA2rZty+rVq6lfP9yeD8VDXbcSUYVN6hp3P5FzX/4TTZvWCnv9lVcWM2TIh8HzgQNb8frrl4at27XVS3z5Q3aX7GefXeNtXL08CXbuKdoHuHJe/uuCiYiIhJGamspdd93F888/z5QpU0hMLOSY6DDUdSslVl5JXtZkB+ccEyb8wP33f06VCybw9dfXebM8Qy3byiW7t+Hm9MpZPjd8sj73udz/FnbC3J1FD75OTXWTiohIkSUlJZGYmMiSJUuIjY1l165dfocUpERPoiLcGDmA0aO/5v6QCROr3v6Ohi0P7zZt1L5uZAKpUxPO+yT7XJMaREQkgiZPnsz111/P/v37Oemkk5g6dSrt2kWkMS4ilOhJxEzsNTF4/MYbyxg4sNVhdQYPbsN//vYVvx1IB6BLmCQvKGtSw5n1814WJHTB3obVoErFw9aPY1tCkT6HiIhIQVJSUrj11lt55ZVXAOjbty/jx4+nZs2SteuOEj2JiEm9JvHrR78CsAr4YtTcsIlew4ZxtK1RJZjoBXUN+e0nZJPzIqlSMe/140RERCLo0KFDfPHFF1SuXJlnn32W66+/HrN894PwhRI9Kbw562D4HFi7N8dYttAJGKuAScCpu1K91jY4bGJD27gq/LQjhXur5trUPUtRFustyjIlIiIiR8E5R0ZGBhUrViQ+Pp5p06YRExPDmWee6XdoeVKiJ4WXleQF5J5hm1avGpO2pwCQtOuA16XaJP6wx9zdpA73/jWh4CVNRERESoh9+/YxbNgw4uLiGDduHACtW7f2OaqCKdGTwlu7lxTnmLhnL5tsVI5LCRcncFrHxjz+0GwALoyNxTWugYV2pQaWO4kZmXM9O+qUrPEMIiIioZYuXUpiYiKrVq2iWrVqjBgxgsaNS8ekPiV6cpiiLHAcukesc45W7/zIZZc1Z+TIbodXzr2mXZ2a0FITJUREpGRyzvHSSy9x++23c/DgQVq2bMnUqVNLTZIHWjBZAvbtO0hcXCUmXzK5UEleQsf6DHji7CN7s64lZ9q5iIhIOLt372bIkCFMmzYNgKFDhzJmzBiq5jW+PIK0YLJExGef/Y9nn/2eJUt+Z/v2FJa/0ieY5CXExjLglT/BwDMA+GrcbLq0iEAXq7ppRUSkFHj44YeZNm0aNWrU4OWXXz6qnS78pBa9cmzixB+otX4DvTo1YNK980maty14bWTuXSly2ZZRkfrnlfxBqCIiIkdi7969DB48mNGjR3PKKacUfEMEqUVPCq0w4+1yp8AJHcNvvnygajVi2zanYsUY/NueWUREJPJ27NjB448/zmOPPUaVKlWIj4/n7bff9juso6ZErwwryqQKgN/jKnHqI93on2Zw5bzDtguL/qgEERGR4vfNN9/Qr18/NmzYAMDTTz/tc0SRo0SvjMrMdMEkb1e9avxz213ZF+s/D9M6ApC8NZ1VpzTl9NMbUKlShew693QsznBFRESKXWZmJn//+9958MEHycjIoGPHjtxyyy1+hxVRMX4HIJHnnGPo0A+D589uT2HFiuzxd1zYNHhYfdgi2rQ5JmeSJyIiUsZt3bqVnj17ct9995GRkcHdd9/Nl19+SZMmTfwOLaLUolfGZHXXHp+rPGPpSti21ju5vlH2hTA7V4iIiJRlGzdupF27dmzevJl69erxxhtv0LNnT7/DigolemVM7jF5qwJfWx5X5fDKK/Z5Y/BERETKkWOPPZYuXbqwZcsWJk2axHHHHed3SFGjRK8US0/PYNmyLZx+en2qVo3NcW3k0+cE95KdGHohdLHirtGPUUREpCTYtGkTqampnHTSSZgZr776KpUrV6ZixbKdCmmMXinkvljLa01fYM4zn9AueRNV5y+DuQuZ1OnF7EqBJC8HLVYsIiLl0Mcff0yrVq244oorSE1NBaB69eplPskDJXql0v/9eRrXrd3OBR0b5CjPWvA4uA7eLgd3rgDXwGvJ076yIiJSjqSnp3PvvffSs2dPtm/fTv369UlJSfE7rGJV9lPZsmZ5EhckVKXpjv2M6jYzbJUB3w3LPrm0fTEFJiIiUnKsXbuW/v37891331GhQgUeffRR7rnnHmJiylcblxK90mR5Euzck2OrstwSLlarnYiIlG/Tp0/n2muvZdeuXRx//PFMnjyZP/zhD36H5QsleqXIpOs/zZHkfXT2cbz6ah9atNCGZCIiIlk2btzIrl27uOSSS3jttdeoW7eu3yH5pny1X5Yi6ekZjBjxGZs37w+WhSZ5J/c8he++G6wkT0REBDh48GDw+IYbbuCDDz5g+vTp5TrJAzDnnN8xRF27du3cwoUL/Q6j0Nat20Pv3pM544ctNAtzfaQbWewxiYiIlFRTp07lzjvv5IsvviAhofQPYTKzRc65dgXXLJha9EqYH3/cStu2/8ozyUvoWN/bq1ZERKScO3DgAEOHDqVfv35s3LiR1157ze+QShyN0Ssp5qyD4XO4Ye9eXh9+BgtHLAC8xG7AE2dn11u0Cyb97lOQIiIiJcPPP/9MYmIiy5cvp3LlyowZM4YbbrjB77BKHCV6JcGcdUy68E2S0tO5AFg4Ykfw0jH9T+GXhk1pPv03eHKBtzetti0TEZFy7PXXX2fYsGGkpKTQrFkzpk6dSuvWrf0Oq0RSolcSDJ9DUnr6YcXHdmtCp+vPo0qVitC8HtzdwYfgRERESo61a9cydOhQDh48yFVXXcW4ceOoUaOG32GVWEr0SoK1e4OHI+vVhWkdvZOuERmHKSIiUmY0adKEsWPHUrFiRa699lrMzO+QSjQleiXBNS1gzFfecVaSJyIiIjjnGD9+PLVq1SIxMRGAIUOG+BxV6aFEzy+BXS4A6BMPY3Jdr1Oz2EMSEREpSfbu3cuQIUN46623qFGjBueeey4NGjQo+EYJUqLnl6wkD5h07/zs8jo1oWXpXwNIRETkaCxatIjExER+++034uLieOmll5TkHQGto+eDQ7PXBI+t28zgjhdrK1XgwClN/QlKRESkBHDO8dxzz9GpUyd+++03WrduzeLFixkwYIDfoZVKatErDqHdtEDFPL7rQ7+8lqpVY4spKBERkZLn9ttv59lnnwXgpptu4sknn6RKlSo+R1V6qUUvytLTM3IkeaEOLtrFrSF/eTt0OL64whIRESmRBg4cSMOGDXnnnXcYO3askryjpEQvitLSMvjjH1/LLujaDmo1Dr7euXcBtVNTfYtPRETEb5mZmXzyySfB87Zt27J69Wouv/xyH6MqO5ToRdGsWUnMm7cheO6cg1YNvNcnq3MskpxwsSZgiIhI+bJ161YuvvhievTowdSpU4PlVatW9TGqskVj9KJo1arsrcwm3TufpHkzw9Yb6UYWV0giIiIlwty5c+nfvz+///47devWJT4+3u+QyiQlelG0ceM+BgCjuoVP8EAteSIiUr5kZGTw2GOPMWrUKDIzM+nSpQuTJk3i+OM1Tj0alOhFUcPP/kftkPNqzepy1+D22rNWRETKpa1bt9KvXz+++OILzIwHHniAkSNHUjGv5SjkqGmMXhSl/eStj5fQsT4j5/SiQ+O68OQCn6MSERHxR5UqVVi3bh0NGzbkv//9L48++qiSvCjTd7cYDHjibAA6LdkOTbW1mYiIlB/p6elkZGRQpUoV4uPj+eCDD6hbty6NGjXyO7RyQYlepOVaHDnLwe0ZVG5aE57qVvwxiYiI+GD9+vX069ePM888k3HjxgFw+umn+xxV+aKu20gLtzhynZpUvqIDLBwI3RoXf0wiIiLF7MMPP6R169Z8++23fMEp1nAAACAASURBVPjhh+zYsaPgmyTilOhF0px12ccf7M0+bqmZtSIiUj6kpaVxxx138Kc//YmdO3fSq1cvli5dSt26df0OrVxSohdJw+f4HYGIiIhv/ve//9G5c2fGjBlDxYoVeeqpp5g+fTr16tXzO7RyS2P0Imjn6t3UCRw//Ow3mK/RiIiIFK/HHnuMhQsX0qRJE6ZOnUqHDlpOzG9K9CLo5xvPpHPg2DIzAS2ILCIi5ceYMWOoVq0ajzzyCLVr1y74Bok6dd1G0C8nHr59y4CZA3yIREREJPpWrlzJ1VdfzYEDBwCIj49n7NixSvJKECV6R2POOmj3BtR/HpYnMfgUrxVv0r3zfQ5MREQkuiZMmMBZZ53FxIkTGT16tN/hSB6U6B2N4XNg7V4YcWqOZVWS5nk7YjT6o5ZSERGRsiU5OZnrrruOgQMHkpycTL9+/Rg+fLjfYUkeNEbvaKwNLKFyVqCJuk7NwFIqMwEYOvc6f+ISERGJgh9//JG+ffvy888/U6VKFcaOHcvgwYMx0/TDkkqJ3tF4uQPUDvnLrfXyRESkjPrll19o3749qampnHbaabz11lucccYZfoclBVCidySytjkLTfJW7IOu/oUkIiISTaeeeiq9e/cmLi6OsWPHUr16db9DkkJQolcYeexfC5CxaBebX1vPcc91L+agREREomvx4sXUqFGDhIQEzIyJEycSGxvrd1hSBJqMURh57F97qHNbukxbx7mpySyrXan44xIREYkC5xzPP/88nTp1om/fvqSmpgIoySuF1KJXFF3b5Th95KEv+O67DQB06PAKzz3XkyFD2voRmYiISETs2rWLwYMH89577wHQqVMnnyOSo6EWvYIsT8rzUpUq2XnywYMZDB06g/Xr9xZHVCIiIhE3b9482rRpw3vvvUd8fDxvv/02L774IlWqVPE7NDlCSvQKktVtW6dmdtkbP8IbP3J11co5qn755bU0blwTERGR0ua5556jS5curF27lvbt27NkyRKuvPJKv8OSo6Su28IKXTrlzjkANAbOja3IvIowceLlrH/iG0b98TU/ohMRETkqsbGxHDp0iDvuuIPRo0dTqZLGnpcFSvSO0tPVq1N32XU0blyTUZe/FSxPuFhr6omISMm2a9eu4L60N9xwA2eddRZnn322z1FJJCnRy0/I+LwZM1aRkFCHE0+sTejvOD8np5DU5J85bhvpRhZTgCIiIkWXkZHB6NGjefrpp5k/f35w+RQleWWPEr38BMbnzfxuK71HeNua9ehxCrOuaRGskjTmqxy3qCVPRERKss2bN3P11Vcze/ZszIzZs2eTkKCfXWWVEr1CuGTEAgAGAM0+/pVRH/96WB214omISEn32WefcdVVV7F161YaNGjAhAkTuPDCC/0OS6Ko2GfdmlkLM5ttZilmtsnMHjGzCoW4r52Z/dfMdpjZTjP7zMw6FEfMWZrlUa5WPBERKckOHTrEAw88wIUXXsjWrVs577zzWLp0qZK8cqBYW/TMrDbwGbAC6AOcDDyNl3A+kM99JwTuWwwMDBTfBfzXzM50zq2NZtxDhrQlKWknzFkDqPVORERKl6SkJJ566inMjIcffpj777+fChUKbGORMqC4u25vAKoClzvn9gKfmlk88LCZ/SNQFk4voEbgvt0AZvYtsB24GBgXzaDHj+8NwCgbFc23ERERiYrTTjuN8ePH07hxY7p16+Z3OFKMirvrtifwSa6Ebgpe8tc1n/tigUPA/pCy/YEyi3SQ4UzqNSn75Pyp3ktERKQESktLY/jw4UyZMiVYNnDgQCV55VBxJ3rNgV9CC5xz64CUwLW8vBOo87SZNTCzBsAYYBfwdpRizSHpI2+plYTYWPhhm/cSEREpYVavXk2XLl14+umn+etf/8q+ffv8Dkl8VNyJXm1gd5jyXYFrYTnnNgHnAlcAWwKvy4GLnHNhMy4zu97MFprZwm3bIpeUDagZH7FniYiIRNI777xDmzZtmD9/Po0bN+bDDz+kRo0afoclPvJjr1sXpszyKPcumh0DTAMW4XX/9gwczzSzxmHfxLnxzrl2zrl29evXL3qUIYsl5/BZX7iwKTRRwiciIiVDamoqN910E1deeSV79uyhT58+LFmyhHPOOcfv0MRnxT0ZYxdQK0x5TcK39GW5Cy/WK51z6QBm9jmQBAwHbolwnMHFkl2d+JyDAD9ZDSt3wlPdIv6WIiIiR+Kaa65h2rRpxMbG8tRTT3HzzTdjVixD2KWEK+5E7xdyjcULLJ1SnVxj93JpDvyUleQBOOfSzOwnvCVaoqZet/fp0OE4ggv23d3Be4mIiJQQ9913HytWrOD111+nXbt2focjJUhxd93OAi4ys9ABA4nAAWBuPvetBc4ws+A2s2ZWGTgDWBOFOIN27jzArFmH74QhIiLil5SUFN58883geZs2bVi+fLmSPDlMcSd6LwEHgXfNrLuZXQ88DDwTuuSKmf1qZv8Oue8V4FjgPTPrZWaXAO8DxwDjIx3kli37C64kIiLig59++omzzz6ba665hqlTs5f6ionxY9i9lHTF+rfCObcLOB+oAHwIjMJbJiX3VhMVA3Wy7lsE9MBbNHkC8AZQDbjAObcs0nGuXLkjeHzyybU5oUH1SL+FiIhIkTjn+M9//kP79u356aefaN68OaeddprfYUkJV9xj9HDOrQDOK6BO0zBls4HZUQorhz/+sQnM9ZZk+fWVS0m58gOeJNm7uGyr97VVg+IIRUREhH379nHjjTcyceJEAAYNGsTzzz9PXFycz5FJSVfsiV6pM3wO1UJnLnV/y1taZeHAvO8RERGJkKSkJC655BJWrVpFtWrVePHFFxk0aJDfYUkpoUQv1PKk4LIqQTUrw5n14fPs7lwtrSIiIsWlYcOGZGRk0LJlS9566y2aN89vIymRnJTohcqd5NWpCbMTvWMb5X19+0/QLewazSIiIhGxe/duKleuTNWqVYmPj+eTTz7h2GOPpWrVqn6HJqWMpuiE07Wd92qZAMCkXpOyrynJExGRKJo/fz5t2rThjjvuCJadfPLJSvLkiCjRK4Skj7zt0BIuTvA5EhERKaucczzzzDN07tyZNWvWsGDBAlJSUvwOS0o5dd1mCdnbNj5+NNVjKxCXnM7tnZsEywfMHOBHZCIiUsbt2LGDa6+9lhkzZgBw22238cQTT1C5cmWfI5PSTolelsD4vJ+3pLFvXxr7AsX70jL8i0lERMq8r7/+mv79+7NhwwZq167Nq6++Sp8+ffwOS8oIJXq5PPjWhhzn1WMrkOpTLCIiUvaNGzeODRs20KlTJ6ZMmULjxhoLLpGjMXq5TJ58BUNCmsqbLtrqYzQiIlLWjRs3jtGjRzN37lwleRJxSvRyiY2twL/+1YtRl7WgWaWKXBQb63dIIiJShsyePZsePXqQmur1F8XHx3PvvfcSq583EgVK9CDHRAwAG9SSh979M4t23EXs9pt9CkpERMqSQ4cO8dBDD3HBBRfwySefMG7cOL9DknJAY/Qge6HkOjWDRZN6TQouqyIiInI0Nm7cyIABA/jyyy8xMx566CFuvlkNCRJ9SvRCtcxeJy93kqc19ERE5Eh89NFHDBo0iO3bt9OoUSMmTpzIeeed53dYUk4o0SvASDfS7xBERKSUmjdvHr169QLgggsuYMKECTRs2NDnqKQ8UaIXRo4tz0RERI5Qhw4dGDBgAC1btuTuu+8mJkZD46V4KdEL0bjxGGptSuaKDG+R5ATNgBIRkSJ6//33adGiBc2aNcPMePPNNzEzv8OSckq/WoRYv34vLTOyd8IYUDPex2hERKQ0OXjwILfeeiuXXXYZiYmJHDx4EEBJnvhKLXq5NAt8VWueiIgU1q+//kpiYiKLFy8mNjaWQYMGUalSJb/DElGiF6pBg+qwNRkItOY1UYueiIjkb8qUKVx//fXs27ePE088kalTp9K+fXu/wxIB1HWbw5Ytw7NPmsTDU918i0VEREq+W2+9lf79+7Nv3z6uvPJKlixZoiRPShS16OVl4UC/IxARkRKuefPmVK5cmX/+858MHTpU4/GkxFGiJyIiUgRr1qyhadOmANxwww1cdNFFnHTSSf4GJZIHdd2KiIgUwv79+xk4cCAtW7YkKcnbPcnMlORJiaZEL9QbP/odgYiIlEDLli2jXbt2TJgwgczMTFasWOF3SCKFUu4TvcWLfw8e/2vwBz5GIiIiJY1zjpdeeokOHTqwcuVKzjjjDBYsWECfPn38Dk2kUMp9onfWWeODx5sPHQK0hp6IiMCePXtITEzkxhtv5ODBgwwZMoTvv/+eFi1a+B2aSKGV68kYmZkubLl2xBARkTVr1jB9+nTi4uIYP348/fv39zskkSJTohfONfptTUSkPHLOBZdIadWqFRMmTKB169YkJCT4HJnIkSnXXbfOOdq0aQTApHvnZ1945jyfIhIREb/s3LmTSy+9lMmTJwfL/vznPyvJk1KtXLfoxcZWYPHioUzq9CJJ87YBkHCx/kGLiJQ333zzDf3792f9+vUsXbqUK664QnvVSplQrlv0soQmeQNmDvA5GhERKS6ZmZk88cQTdO3alfXr19OhQwfmzp2rJE/KDCV6IZTkiYiUH1u3bqVnz56MGDGCjIwM7rrrLr766qvgrhciZUG57roFYHmS3xGIiIgP+vbty9y5c6lbty5vvPEGF198sd8hiUScWvR27vE7AhER8cEzzzxD9+7dWbp0qZI8KbOU6IU6f2r2S0REypRNmzYxduzY4Hnbtm359NNPOf74432MSiS6ynWi99RT3+Y4X7r4d/hhm/cSEZEy45NPPqF169bccsstTJs2ze9wRIpNuU70FizYlON8WfPaPkUiIiLRkJ6ezogRI+jRowfbtm2je/fudOnSxe+wRIpNuU701q3LOT6v8eAzfYpEREQibd26dXTr1o0nnniCmJgY/va3v/Hxxx/TsGFDv0MTKTbletbtYYleozjvoIn2uhURKc3mz59Pjx492LVrF8cddxyTJ09WS56US0Vq0TOzODNrb2aXm1nNQJlFJ7Tom5lr3bzjG8Z5Sd5T3fwJSEREIqJ58+bUqVOHXr16sXTpUiV5Um4VqkUvkMyNAm4D4gAHtAcWA7PM7Fvn3CNRizJKWlfYl+O8cvtjYOFAn6IREZGjsXr1aho1akTVqlWJj4/nq6++omHDhsTElOtRSlLOFfZv/6N4Sd49QAsgtBXvfeBPEY6reGgNPRGRMuGtt96idevW3HHHHcGyY445RkmelHuF/RdwHTDCOTcOyL2VxK/AKRGNKtqWJ8HchYeXL9ta/LGIiMgRO3DgADfeeCOJiYns3buXbdu2cejQIb/DEikxCpvo1QFW5nGtIqVtUkdoS96iXdnH3d8q/lhEROSI/PLLL3Ts2JGXXnqJSpUq8cILL/D2229TsWLp+pEkEk2F/dewArgY+CzMtQuBpRGLqDjduQLW7vU7ChERKaIJEyZw4403kpycTEJCAlOnTqVNmzZ+hyVS4hS2RW80cIuZPQ90wZuMcZqZ3Q/8NXC91JmTeYiMlvWyC7SsiohIieecY+bMmSQnJzNgwAAWLVqkJE8kD4Vq0XPOTTOz/wOeAIYFiicA24AhzrmZUYov4pxzwZkkLy9Zz9zQi1pWRUSkxMrMzCQmJgYzY/z48fTu3ZsBAwZQilf5Eom6Qk9Hcs69ARwPtAa6A22BYwPlpcYPP2wJHjcLKU+4OAG6NS7+gEREJF/OOV5++WU6d+7MgQMHAIiPj+eqq65SkidSgMKuo3c38IZzbjPwQ65rDYFBzrl/RCG+iFu8+HdanZSzbKQb6U8wIiKSr7179zJ06FCmTJkCwDvvvMPVV1/tc1QipUdRxujl1dx1PKVojF7z5vUKriQiIr5btGgRbdu2ZcqUKcTFxfHmm28qyRMposImeoY3ASOcY4HdkQkn+jp1OsHvEEREJB/OOZ577jk6derEb7/9RqtWrVi0aBFXXXWV36GJlDp5dt2a2VVA1r8qB/zTzHJvJVEFb6zenKhEJyIi5c6sWbO49dZbARg2bBhPP/00VapU8TkqkdIpvzF6mUBG4NhynWfZBbwAPBv50IrRHZ97X585z984RESEnj178pe//IWLLrqIK6+80u9wREq1PBM959xkYDKAmU0G7nfO/a+4Aou2SffOzz6ZsML7qkRPRKTYZWZmMmbMGHr37k2zZs0wM15++WW/wxIpEwo1Rs85178sJXkASfO2AZAQG+tzJCIi5de2bdu45JJLGD58OImJiWRk5O44EpGjUegNAc3sOKA/3vJzhw2WcM4NjGBcxWZATe2GISLihy+//JL+/fuzadMm6tSpwyOPPEKFChX8DkukTCnsOnqtgK+A7UAT4BegNtAI+B1YG60AIy0xcRpThzUNni+/rQ0tT6jpX0AiIuVMRkYGo0ePZuTIkWRmZtK5c2cmT57MCSdoVQSRSCtsi95TwAxgIJAGXOOcW2xm5wGvAQ9GJ7zI+/XXnUDT4PnBi06Edsf6Fo+ISHninKNPnz7MnDkTM+O+++5j1KhRVKxY6A4mESmCwq6j1wZ4A2/mLQS6bp1znwOPAk9GPrTo2L8/Lcd59eoaoyciUlzMjF69etGgQQM+/vhjHnvsMSV5IlFU2EQvBkh1zmUC24DQ9vXVwKmRDixakpNzJnpxcZV8ikREpHw4dOgQS5cuDZ7fcMMNrFixggsvvNDHqETKh8Imej8DWTvEfg/camYnBPa5vR1YE4XYouLbbwfnOG/QoLpPkYiIlH3r16/n3HPPpUuXLiQlJQFeq17dunV9jkykfChsovdvsve6vR9vkNsaYBPQDbg7wnFFTePGOSdeVK6sLgMRkWiYMWMGrVu35uuvvyY+Pp4dO3b4HZJIuVOoLMc595+Q4+Vm1gLoAlQFvnHObYxSfNFX/3nv67ab/I1DRKSMSEtLY8SIETzzzDOAt9PF66+/Tv369X2OTKT8OaLmLOfcbuDDrHMza+Cc2xqxqEREpFRavXo1iYmJLFiwgIoVK/L4449z5513EhNT2A4kEYmko+q3NLNmwJ3ANUC1iEQkIiKl1p49e/jhhx9o3LgxU6ZMoVOnTn6HJFKu5ZvomdnleGvnnYA3u/bvzrkFZnYq8DjQB9gPjIl2oCIiUjIdOnQouERK69atee+99+jYsSO1a9f2OTIRybMt3cwGAtOAM4D1eLNu55jZX4ClwHnAw0AT59z90Q/16O3ceYBPP/0tZ+G2mzQ+T0TkCK1atYp27doxefLkYFnPnj2V5ImUEPkNmrgNmAw0c85d6pxrCzwC/AtYBiQ45/7mnNtTDHFGxJIlv3PhhW/6HYaISJkwceJE2rZty7Jly3jyySfJzMws+CYRKVb5JXqnAK8GFknOMh4w4BHn3PaoRhYFp7s9uDm9/A5DRKRUS05OZvDgwVx99dUkJyfTr18/5syZowkXIiVQfmP04oC9ucqyzjdHJ5zoahSb4XcIIiKl2k8//UTfvn1ZsWIFVapUYezYsQwePBgz8zs0EQmjoFm37cwsLuQ8BnBAezOrFVoxsO9tgQJr8I0FOgG7gVeAUc65ArOwwOSQEXjjBlOABcAVzrnkwrx3lo73LKVnUW4QERGcc1x11VWsWLGC0047jalTp9KyZUu/wxKRfBSU6D2fR/m4XOcOqFDQm5lZbeAzYAXejN2TgafxEsgHCrj3L4F4/gHcBdTGmxBS5CVi5g3ryKjv3/FO3vgRBp5R1EeIiJQ7Zsarr77KuHHjGDNmDNWrawtJkZIuvyTptCi83w14u2lc7pzbC3xqZvHAw2b2j0DZYcysHt4SLjc7514OufTeEUVx55ycx0r0RETCWrp0KTNmzOCBB7zfxdu0acP48eN9jkpECivPRM85tzIK79cT+CRXQjcF+DvQlZDdNnLpG/j6ehRiEhGRXJxzvPjii9xxxx2kpaXRqlUrevfu7XdYIlJExT1FqjnwS2iBc24d3ni75vnc1wFYCQw2sw1mlm5m35vZOdELVUSkfNq9ezd//vOfuemmm0hLS2Po0KF0797d77BE5AgUd6JXG28CRm67Atfy0gg4FW8c3z1AbyAZ+NjMGoa7wcyuN7OFZrZw27ZtOS9e0yL8sYhIOTd//nzatGnDO++8Q40aNZgyZQovvfQSVatW9Ts0ETkCfix65MKUWR7lWWLwlnsZ7Jyb6Jz7GLgUyADCbmvhnBvvnGvnnGtXv379nBefOS/8sYhIOfbRRx/RuXNn1qxZw1lnncWSJUtITEz0OywROQrFnejtAmqFKa9J+Ja+LDsDX+dkFQTG+S0Citwk93//90FRbxERKfP+8Ic/0LRpU2677Ta++eYbTj75ZL9DEpGjVOSlSY7SL+Qai2dmJwDVyTV2L5ef8Vr8cq/IaUCR99x59dWlPFzUm0REyqDvv/+eM888k6pVqxIfH8/ixYupUaOG32GJSIQUukXPzOqY2Sgzm2lmP5jZaYHyG82sXSEfMwu4yMxC/xdJBA4Ac/O5bwZeUnduSDw1gbPw9t0VEZEiyMzM5PHHH6dz587cfvvtwXIleSJlS6ESPTNrC/wKXIfXxXo63np4ACfhLWBcGC8BB4F3zay7mV0PPAw8E7rkipn9amb/zjp3zi0EPgD+bWaDzKwXMB1IB14o5HuLiAiwZcsWevTowf33309GRga1atXCufyGSYtIaVXYFr1/At8BpwCDyNmF+h3QsTAPcc7tAs7H20XjQ2AU3kLII3NVrcjhO21cDbwPPANMw0vyzgs8s0heeUVrQYlI+TR79mxatWrFp59+Sv369Zk1axZPPPGE9qoVKaMKm+i1A55zzqVx+OzY7UDYJU7Ccc6tcM6d55yr6pw7xjn3YO59bp1zTZ1z1+Yq2++cu9E5Vzdwb3fn3PLCvm+owZOSsk/On3okjxARKVUyMzN56KGHuOCCC9iyZQvdunVj6dKl9OjRw+/QRCSKCpvo7QPq5HHtRGBbHtdKph+2hT8WESmjzIxVq1YBMHLkSD777DOOPfZYn6MSkWgrbKI3A28/2hNCypyZ1QLuwOtSFRGREiY1NRXwEr3x48czZ84cHn74YSpUyD06RkTKosImevfgjYn7Bfg0UPYs3rZkAA9GOK7ouqu93xGIiERVeno6d999N506deLAgQMAxMfH88c//tHnyESkOBUq0XPObccbp3c33qzbr/EWMf4b0NE5l99ixyXP3R2yj5vE+xeHiEgUrFmzhi5duvDkk0+yfPlyvvzyS79DEhGfFHrBZOdcKt5SJqVyOZPMTBfMajMyQtZYfqqbH+GIiETFu+++y+DBg9m9ezcnnHACkydPpnPnzn6HJSI+Kew6ep+Y2XWBMXml0oYNwWX6qF798ewL3Rr7EI2ISGSlpqZy8803c8UVV7B792569+7NkiVLlOSJlHOFHaOXDowDNpvZh2Y2wMziohhXxK1cuT14fPBgRj41RURKn/fff5/nn3+e2NhYxowZwwcffEDdunX9DktEfFaorlvn3CWBLccuB/oCrwHpZjYLmAp8GOjaLbFWrdrBBWcUesc3EZFSJTExkYULF5KYmEj79ppwJiKeQmc+zrk9zrlXnXM9gWOA24FawERga5Tii5idOw8EjxvWrZpPTRGRku/AgQPccsstwbXxzIynnnpKSZ6I5HBETVzOuR3AImAJ3uzb6pEMKhoefLBr8HizVfMxEhGRo/Pzzz9z9tlnM3bsWAYNGqR9akUkT0VK9MzsTDN7zMx+BeYDfYCXgTOjEZyIiOT0+uuv065dO3788UeaNWvGSy+9pH1qRSRPhRqjZ2YPA4lAM2Ad8BYw1Tm3OHqhRdGZ9eHzHX5HISJSaPv372fYsGFMmDABgKuvvppx48YRF1eq5sWJSDEr7Dp6Q4C3geucc/OiGI+IiORy6NAh/vCHP7Bs2TKqVavGCy+8wKBBg9SSJyIFKmyid7zTIBAREV9UrFiRoUOH8uKLLzJ16lRatGjhd0giUkrkOUbPzGJynlpMfq9iiDVyZif6HYGISL727NmTY+uyG264gQULFijJE5EiyS9BSzezswPHh/AWTc7vVaLNmpUUPH7kkbk+RiIikr8FCxbQtm1bevXqRVKS93+XmVGlShWfIxOR0ia/rtthwP9Cjkt11+3MmUn0/HMDAFaOnEMzn+MREcnNOcezzz7L3XffTXp6Om3atCEmpnR1mIhIyZJnouec+1fI8UvFE07xyEryEi5O8DUOEZEsO3fu5LrrrmP69OkA3HzzzTz55JNUrlzZ58hEpDQr1K+KZrbCzFrmca2Fma2IbFiR5zbsO6xswMwBPkQiIpLT999/T+vWrZk+fTq1atXi3Xff5bnnnlOSJyJHrbCzbpsDee0bFgeU+Kaxnj/u8jsEEZGwKleuzNatW+nQoQNTpkyhadOmfockImVEnomemVXDS+Ky1DazBrmqVQGuADZGIbaIumTPIb9DEBEJ2rdvHzVq1ACgdevWfP7557Rv357Y2FifIxORsiS/rtu7gM3A73gTMT4KHIe+VgfqjYtumBHwdDe/IxARAeCLL77g1FNPZfLkycGyc845R0meiERcfl23bwE/AhY4vg9IylUnDfjFOZe7vORpUxl2pvodhYiUYxkZGTz66KM88sgjOOeYNGkS/fr10w4XIhI1+c26/Rn4GcDMegLfOef2FldgEbdzj/e1Tk1/4xCRcmnTpk1cddVVzJkzBzPjoYce4sEHH1SSJyJRVajJGM65T6IdSLFpWeLnjYhIGfPxxx9zzTXXsH37dho2bMjEiRM5//zz/Q5LRMqB/CZjrAN6O+eWmdl6Clgw2TnXONLBiYiUdunp6dx6661s376d7t278+abb9KwYUO/wxKRciK/Fr2JwPaQ41K9M0aW4457huv9DkJEyo3Y2FimTJnCrFmzuPfee7XThYgUq/zG6I0IiEtq+gAAIABJREFUOb63eMKJvk2bDl84WUQkkqZPn84333zD3//+dwDatGlDmzZtfI5KRMqjwi6YfBgzOwlvN7FFzrltkQtJRKR0SktL45577uGf//wnAD169ODcc8/1OSoRKc8KleiZ2VjAnHM3Bc4vA6YG7t9jZhc55+ZHL0wRkZLtt99+o1+/fixcuJCKFSvyxBNP0LVrV7/DEpFyrrCDRXoD34WcPw68A5wEzAUei3BcUTHp3vk87HcQIlLmvPXWW7Rt25aFCxfStGlTvv76a+68806NxxMR3xX2f6GGwDoAMzsZOBUY7ZxbA7wItI1KdBGWNC+7hznhYi2zIiJH77XXXiMxMZG9e/dy+eWXs2TJEjp06OB3WCIiQOHH6O0C6geOuwNbnXM/BM4dUKr27RnpRvodgoiUEZdffjlPPfUUw4YN48Ybb9QCyCJSohQ20fsv8LCZ1QbuBqaFXDv9/9m777Aozq4N4PdIE1h6VRAQRYIaBWtssRAlxhYsGIiAYIstGo1GE6Nie6MxamKMSmzYWxK7sRCxRfxU1KgY0QAWlCBKW6Tunu8PdMLKgqDAAHt+17WXzMwzM/fsUo7PlAdAfDnnYoyxKuvXX39Fz549oa+vD2NjY1y5cgXa2q99bxtjjFWY0p66nYSCcW+nAYgC8HWhZR8BOF7OuRhjrMrJzMxEUFAQBgwYgEmTJonzuchjjFVVpR0C7SkAv2KWvVOuiRhjrAq6du0afHx88Pfff0NfXx+tWrWSOhJjjL1Smf4bKgiCJYC2AMwBPAVwnoiSS15LepmZuSrTCQnpsLMzligNY6w6ISKsWbMGn376KbKzs9G4cWPs3LkTTZo0kToaY4y9UqlO3QqCUEsQhCUAEgDsBxD2/N8EQRC+E6r41cdpaTkq06GhlyRKwhirTnJzc+Hn54eRI0ciOzsbw4YNw4ULF7jIY4xVG6W9Ru9rAOMAzAPwFgCz5//Oez5/RoWkKyd5eUqVaeu4dImSMMaqEx0dHRARZDIZNm/ejDVr1sDAwEDqWIwxVmqlPXUbDGAmEX1TaF4agLmCIOQBGA1gbnmHKy/5+aqFnuWeOxIlYYxVdUSElJQUmJubQxAEhIaGIjExEY0aNZI6GmOMlVlZHphc3PnOS8+XV1kymepj/pz4afWMMTWePn2K/v37o2vXrsjKygIAGBsbc5HHGKu2Slvx3AEwsJhlA58vr7JsbGQq0211qtXznRljleDcuXPw8PDAnj17cPfuXdy4cUPqSIwx9sZKe+r2fwA2CYJgh4KHJf8LwBrAIAA9AfhXTLwK4t9Y6gSMsSpCqVRi8eLF+PLLL6FQKNCmTRts374d9evXlzoaY4y9sdI+R2+LIAjpAOYAWAtAQMHQZ1cB9COiAxUXsQIs6SZ1AsZYFfD48WMEBgbi8OHDAIDJkydjwYIF0NXVlTgZY4yVj1I/R4+I9gPYLwiCLgBbAIlElPuK1RhjrMo6cOAADh8+DHNzc4SFhaF3795SR2KMsXJVYqH3vKjrDsAJQCKACCJ6AuBexUdjjLGKNXToUCQkJCAwMBD16tWTOg5jjJW7Ym/GEATBEcA1FDwYeTmAXQBiBEHoWknZGGOsXCUmJuLDDz9ETEwMAEAQBMyYMYOLPMZYjVXSXbeLAOihoEfPHEBLAH8DCK2EXOUqISFDZfrPP+9LlIQxJpVjx46hefPm2Lt3L8aPHy91HMYYqxQlFXodAHxFROFElEpElwEMA+AsCIJt5cQrH1lZeSrTycnPJErCGKts+fn5+Oqrr+Dl5YWkpCR069YNGzZskDoWY4xVipIKvToo+ny82yi447ZOhSWqDDNOS52AMVYJ7t+/j65du2LBggUQBAFz5szB0aNHUadO9f4VxhhjpVXSzRgCAGUJy6uveB7rlrGaLjs7G+3atUNCQgLq1q2LrVu3onPnzlLHYoyxSvWqx6vsFwRB3SNUDj0f41ZERA7lF6t82dkZqUy30yn1U2UYY9VU7dq18dVXX2H//v0ICwuDlZWV1JEYY6zSlVTxLKy0FBVMX191yDMrHuuWsRopLi4Ot27dwvvvvw8A+OSTTzBq1CjU4p95xpiGKrbQI6LplRmkUh33kToBY6yc7d69G8OHD4dCoUBUVBRcXFwgCAIEQZA6GmOMSUYz/5vb3FrqBIyxcpKdnY2xY8di0KBBSEtLw3vvvQdLS0upYzHGWJXAF6sxxqqtmJgY+Pj44OrVq9DV1cXixYsxbtw47sVjjLHnNKbQ2zrt/6SOwBgrR3v37sXHH3+MzMxMNGjQADt27EDLli2ljsUYY1WKRpy6zctT4nbkYwBA3c6OyMxUdyMxY6w6cXR0RH5+Pj766CNERUVxkccYY2poRKEXH58qfj3y5F1ERMRLF4Yx9toePnwofu3u7o7Lly9j69atMDY2ljAVY4xVXWUq9ARBaCAIwiBBECYJgmD9fF49QRAMKiZexRDi0qSOwBgrAyLC2rVr0bBhQ2zbtk2c7+bmxtfjMcZYCUpV6AmCoC8IwkYAfwPYBuBbAPbPFy8DMLtC0pUTIlKZ1vmKh0BjrLrIyMjAkCFDMHz4cGRlZeHcuXNSR2KMsWqjtD163wHoDqAvABMUDI/2wkEAPcs5V7mqZ6knfm1fqxYMuQeAsWrh8uXLaNGiBbZu3QpDQ0Ns3LgRP/zwg9SxGGOs2ijtXbeDAEwmosOCIGi9tCwOgGP5xipf+jr/1bP3zc0kTMIYKw0iwk8//YRJkyYhNzcXzZo1w86dO+Hq6ip1NMYYq1ZK26NnCODfEpYpyydOJWhmVfBijFVZWVlZ+P7775Gbm4vRo0cjMjKSizzGGHsNpe3RuwTAD8ARNcv6AzhfbokqWvhgqRMwxl7BwMAAO3bswO3bt+Hjw0MWMsbY6yptoTcTwBFBECwA7AJAAN4TBGE0CgrArhWUjzGmAZRKJZYuXYq4uDj8+OOPAAAPDw94eHhInIwxxqq3Up26JaITAN4HYA1gHQpuxvgGQAsAHxAR3wbHGHstycnJ6Nu3Lz7//HOsWLECly9fljoSY4zVGKUeAo2I/gDQRhAEEwAWAFKIKKXCkjHGarzTp0/D19cXCQkJMDMzw4YNG7gXjzHGylGZR8YgojQiiq2uRd7XX/8BuZyHQGNMSkqlEvPnz0eXLl2QkJCAdu3a4cqVK+jbt6/U0RhjrEYp7QOTN77qVdodCoLQWBCEcEEQngmC8FAQhDlqHtlS0vq1BEG4JAgCCYLQu7TrvTBv3mnk5OSXdTXGWDn69ttvMWPGDCiVSkybNg0nT56Eg4OD1LEYY6zGKe2pWxc188wBOANIRsGz9F5JEAQzAMcBRAPoB6ABCh7GXAvAjFJmGQ7ArpRt1dKddQb4scebbIIx9gZGjx6NvXv3YubMmXj//feljsMYYzVWaW/GaKfm5QrgLQCPAMwp5f4+AaAPoD8RHSOiVQBCAEwSBOGVo5I/LxTnA/iqlPtTS3f7rTdZnTFWRvn5+fjhhx+QlZUFADA2NsbZs2e5yGOMsQpW5mv0CiOifwD8D8DiUq7SE8ARIkovNG87Coq/zqVYfy6AswDCy5KzsBADfei87sqMsTJLSEiAp6cnJkyYgEmTJonzBR6KkDHGKtwbFXrP5aD0Q6C9BeDvwjOI6B6AZ8+XFUsQhGYAggB8/hoZRTMNDFCL/8AwVikOHTqE5s2b49SpU6hTpw4//JgxxipZqa7REwTBWc1sXQBuKOjRiyrl/swApKqZn/J8WUmWA1hBRHcEQXAq5f6K+q7La6/KGCudvLw8fPnll1i8uKCz38vLCxs3boS1tbXEyRhjTLOU9maMOygYDeNlAoBrAEaWYZ/FbUfd/IKFgvARAFcAfUq7E0EQRr7I1bKR238LApqWdhOMsdeQkZGB7t274/z589DS0sL8+fMxZcoU1KpVHicQGGOMlUVpC72eauZlA3jw/Dq90koBYKpmvgnU9/RBEAQdAN8CWAigliAIpgBe3LhhKAiCERFlvLweEYUCCAWAVq6Niy0iGWPlSyaTwcnJCQ8fPsT27dvRvn17qSMxxpjGemWhJwiCHoCmAI4S0bU33N/feOlaPEEQ6gEwxEvX7hViCMAewJLnr8K2A/gHQMM3zMUYewPZ2dl48uQJ7OzsIAgCQkNDkZ+fD3Nzc6mjMcaYRntloUdEOYIgzAFwsRz2dxjAlJd64QYDyAJwsph15AC6vjTPFsA2AF8C+KMccjHGXtPt27cxePBgKBQKREZGQl9fH8bGr3xaEmOMsUpQ2otmLgFoXg77W4WCu3R/FQThvefX0c0GsKTwI1cEQbgjCMJaACCifCKKKPwCEPm86TUiOl+WAO3arS2Hw2CMAcC2bdvQokULXL58GXK5HAkJCVJHYowxVkhpC70JAMYKgjBcEIS6giBoPR+KTHyVZiPPx8f1BKAFYD8KHpa8FMCsl5pqP29T7tLSsitis4xplGfPnmHEiBHw8/ODXC6Hj48PoqKi0LAhX0XBGGNVSWlvxrj0/N/VJbQpVWFGRNEAur2ijdMrlsej4E7dMtONSQGsfgQej3ud1RnTeNHR0fDx8cGNGzegp6eH77//HiNHjuQHIDPGWBVU2kJvDEp4/El1ost/ixh7I2fOnMGNGzfg6uqKnTt3olmzZlJHYowxVoxiCz1BEN4FEEVE8udj0tYIv/NF4oyVGRGJPXYjRoyAQqGAv78/ZDKZxMkYY4yVpKRr604AaFxZQSqLOT+0lbEyuXLlClq3bo2YmBgABWPUjh49mos8xhirBkqqemrmSc7H4/j6PMZKgYiwcuVKvPPOO7h06RJCQkKkjsQYY6yMSnuNHmNMg6SlpWHEiBHYtWsXAGDkyJFYtmyZxKkYY4yV1asKvQ8EQXjrFW0AAES0sRzyMMYkduHCBQwePBhxcXGQyWT4+eef8dFHH0kdizHG2Gt4VaE3s5TbIQBc6DFWzaWmpsLT0xMZGRnw8PDAjh074OLiInUsxhhjr+lVhV5XlM/QZ1VGVNQjtGhRR+oYjFVJpqamWLhwIW7evIlvv/0Wenp6UkdijDH2Bl5V6GURUWalJKkkP048gnWnhkodg7Eq4+zZs0hKSoK3tzcAYPTo0RInYowxVl407lkjuhcSpY7AWJWgVCrxzTffoHPnzggICEBsbKzUkRhjjJUzjbvrVqeGPjWGsbJISkqCv78/jh49CgAYM2YM6tWrJ3Eqxhhj5a3YQo+IamRvn4d2qYbkZazGOnHiBPz8/JCYmAhLS0ts3LgRPXv2lDoWY4yxClAji7mSBI9oIXUExiSzcuVKeHp6IjExEe+++y6uXLnCRR5jjNVgGlfoYUk3qRMwJpl27dqhdu3amDlzJsLDw2FnZyd1JMYYYxVI467RY0zTXL9+HU2bNgUAuLu7IzY2Fra2thKnYowxVhk0r0ePMQ2Rl5eHadOm4e2338a2bdvE+VzkMcaY5uAePcZqoLt378LX1xfnzp2DlpYWEhP5sUKMMaaJuNBjrIbZu3cvgoKCkJKSAnt7e2zbtg0dO3aUOhZjjDEJaNyp299/vyN1BMYqRE5ODiZOnIgPP/wQKSkp6N27N65cucJFHmOMaTCNK/QefRYudQTGKkRubi4OHToEHR0dLFmyBPv27YOFhYXUsRhjjElI407d6j6QSx2BsXKlVCpRq1YtGBkZYdeuXcjJyUGbNm2kjsUYY6wK0LxCj0dAYzVEVlYWJk6cCABYvXo1AKB58+ZSRmKMMVbFaNyp2046OlJHYOyN3bx5E23btkVoaCjCwsIQGxsrdSTGGGNVkMYVerZ/fCR1BMbeSFhYGFq1aoVr167BxcUFkZGRcHZ2ljoWY4yxKkjjCj00t5Y6AWOvRS6XIzAwEEOHDsWzZ8/w8ccf49KlS3B3d5c6GmOMsSpK8wo9xqqpOXPmYOPGjdDX18e6deuwadMmGBkZSR2LMcZYFaZxN2MwVl3NmDEDt27dwoIFC9CkSROp4zDGGKsGuEePsSoqPT0d06ZNQ1ZWFgDA2NgYe/fu5SKPMcZYqWlcj152dj5q19a4w2bVzKVLlzB48GD8888/yMrKwvfffy91JMYYY9WQxvXoRe+5JXUExopFRPjhhx/Qrl07/PPPP3B3d8e4ceOkjsUYY6ya0rhCD6OPSZ2AMbWePn2K/v37Y8KECcjLy8O4ceNw7tw5uLi4SB2NMcZYNcXnMBmrAv7991+0adMG9+7dg4mJCdauXYsBAwZIHYsxxlg1p3GFnp7UARhTw9raGu3atYOtrS22b9+O+vXrSx2JMcZYDaBxhV6TFnWkjsAYAODx48fIyMiAs7MzBEHAmjVroKurC11dXamjMcYYqyE07xq98MFSJ2AMJ0+ehLu7O7y9vcXHp8hkMi7yGGOMlSvNK/QYk5BCocCcOXPQrVs3PHz4EMbGxsjIyJA6FmOMsRqKCz3GKsmjR4/QvXt3zJo1C0SEr776CidOnIC1NY+/zBhjrGJo3DV6jEnh6NGjGDJkCB4/fgxra2ts3rwZ3bt3lzoWY4yxGo579BirBHFxcXj8+DE8PT1x9epVLvIYY4xVCo3r0XvwIB329sZSx2AaIC8vDzo6OgCAkSNHwsLCAt7e3tDS0pI4GWOMMU2hcT16j6dFSB2BaYD9+/ejYcOGiImJAQAIgoCBAwdykccYY6xSaVyhh4OxUidgNVhubi4mTZqEvn374t69e1i9erXUkRhjjGkwjTt1y1hFiY2NxeDBg3Hx4kVoa2vjm2++wWeffSZ1LMYYYxpM4wq9urU0rxOTVbxdu3Zh+PDhSE9Ph6OjI7Zv34533nlH6liMMcY0nMZVPTb1TaWOwGqYhIQE+Pv7Iz09Hd7e3rh8+TIXeYwxxqoEjevRw+IuUidgNYydnR2WL1+OnJwcjB07FoIgSB2JMcYYA6CJhV4XB6kTsBpg8+bN0NXVhY+PDwBgxIgREidijDHGitK8Qo+xN5CZmYnx48dj/fr1kMlk6NixI+rWrSt1LMYYY0wtLvQYK6UbN27Ax8cH0dHRqF27NpYuXYo6depIHYsxxhgrFhd6jL0CEWHdunUYP348srKy4Obmhp07d6Jp06ZSR2OMMcZKpHF33V69mih1BFbNfPnllxg+fDiysrIQFBSECxcucJHHGGOsWtC4Qk+pJKkjsGpm8ODBsLCwwMaNG7Fu3ToYGhpKHYkxxhgrFT51y9hLiAgnTpxAt27dAADu7u6Ij4+HTCaTOBljjDFWNhrXo8dYSVJSUjBgwAB4enpi27Zt4nwu8hhjjFVHGtej16yZjdQRWBV1/vx5fPTRR4iPj4exsTH09PSkjsQYY4y9EY3r0dPS0rhDZq+gVCqxePFidOzYEfHx8WjdujUuX76M/v37Sx2NMcYYeyNc9TCN9vTpU/Tt2xdTpkxBfn4+PvvsM5w5cwbOzs5SR2OMMcbemMadumWsMB0dHdy6dQtmZmbYsGED+vbtK3UkxhhjrNxwocc0jkKhQH5+PvT09GBkZIQ9e/bAyMgIDg48DjJjjLGahU/dMo2SmJgILy8vfPrpp+K8Jk2acJHHGGOsRtK4Qi8lJUvqCEwix48fR/PmzREeHo7ffvsNSUlJUkdijDHGKpTGFXp376ZJHYFVsvz8fMyYMQM9evRAUlISunbtiqtXr8La2lrqaIwxxliF4mv0WI324MED+Pr64syZM6hVqxZmz56Nr776ClpaWlJHY4wxxiocF3qsRps7dy7OnDmDOnXqYOvWrejSpYvUkRhjjLFKo3GFnqlpbakjsEr07bffgogwb948PlXLGGNM42jcNXpOTqZSR2AVKD4+HsHBwcjKKrjpxtjYGKGhoVzkMcYY00ga16PHaq5ff/0VwcHBSEtLg62tLRYsWCB1JMYYY0xSGtejx2qe7OxsjB8/HgMGDEBaWhr69euHzz//XOpYjDHGmOS4R49Va7dv38bgwYNx+fJl6OjoYPHixRg/fjwEQZA6GmOMMSY5LvRYtfXPP/+gRYsWkMvlcHZ2xo4dO9CqVSupYzHGGGNVRqUXeoIgNAawHEA7AKkA1gAIISJFCeu0BjAGQCcAdQHcB7AVwEIiyq7w0KxKcnZ2Rq9evUBECA0NhYmJidSRWAVRKpV48OABMjMzpY7CGGNvTEdHB9bW1jA2Nq7wfVVqoScIghmA4wCiAfQD0ADAdyi4VnBGCasOft52IYDbAJoBmPv83wFlyXDnzlM0bGhe5uysaoiOjoa2tjYaNWoEQRAQFhYGXV1dPlVbwyUnJ0MQBLi6uqJWLb60mDFWfRERsrKykJCQAAAVXuxVdo/eJwD0AfQnonQAxwRBMAYwWxCERc/nqbOQiB4Xmo4QBCEbwGpBEByJ6G5pA2Rn5792eCYdIsL69esxbtw4uLi4IDIyEvr6+tDT05M6GqsEqampcHJy4iKPMVbtCYIAAwMD2NnZ4eHDhxVe6FX2b82eAI68VNBtR0Hx17m4lV4q8l64/PzfMj0gzdBQpyzNWRWQkZEBf39/DBs2DFlZWXB3d4dSqZQ6FqtECoUCOjr8s8sYqzn09fWRl5dX4fup7ELvLQB/F55BRPcAPHu+rCzaA1ACuPWqhk8T/ruuRybTLeNumJSuXLmCVq1aYcuWLTAwMMCGDRsQFhYGQ0NDqaOxSsan5xljNUll/U6r7ELPDAU3YLws5fmyUhEEwRbAVwA2lXC6V5SdWVAxG7lZ8RBo1cjPP/+Md955BzExMXj77bdx8eJFBAYGSh2LMcYYqzakuOCF1MwTiplftKEg6ALYCUAO4LMS2o0UBOGiIAgXX8ybFD0GOjpaZYzLpKJUKpGTk4NRo0bh/PnzcHNzkzoSY+VOJpPh3LlzUsdgGio7OxsuLi64deuVJ8dYCdq1a4fw8HCpY6hV2YVeCgB1g82aQH1PnwqhoJ9zI4AmAD4gopTi2hJRKBG1IiJ+sFo1kp7+XwftyJEjcfr0aaxatQr6+voSpmKs4sjlcrRr107qGDXO0KFDoaOjA5lMBmNjY7i5ueGnn34q0i46OhoDBw6EhYUFDAwM0KRJEyxZsqTIdcDp6emYOnUqXFxcYGhoCDs7O/Tq1avK/nEvre+//x7t2rWDq6ur1FHKzbNnzxAcHAwzMzOYmpqK13cXR6FQYO7cuahfvz5kMhk6deqEv/76S1y+ZcsWyGQylZeWlhb69u0rtpk9ezY++6zYvidJVXah9zdeuhZPEIR6AAzx0rV7xViKgsey9COi0rRn1QQRYcmSJXByckJMTAyAgusXOnbsKHEyxqqmyriIuzxImTMwMBByuRypqamYN28exo0bh4iICHH5X3/9hbZt28LKygrXr19Hamoqli1bhiVLliAoKEhsJ5fL0bFjR5w+fRpbt25FSkoK/vnnH4wcORK7d++ulGOpiPdRoVDgxx9/xIgRI157G1Xx+3DChAn4+++/8ffffyMmJgY3b97EpEmTim2/ZMkSbN68GeHh4Xj69Ck6deoELy8vZGRkAAA+/vhjyOVy8ZWQkIDatWtjyJAh4ja6d++OlJQU/PHHHxV+fGVGRJX2AjAdwFMARoXmfY6CmzGMS7GuAsCAsu63DurQbMwmVjUlJydT7969CQWn72nJkiVSR2JVTHR0dNGZlstVX8UJu6ba7rPw4tt2267a9sq/Zcrp6OhIc+fOpS5dupChoSE1bdqUrl69Slu3bqUGDRqQsbExDRs2jPLy8sR1ANDp06fF6YiICOrYsSOZmZmRhYUFDR06lIiITpw4QVpaWrRx40aqX78+yWQyIir4+fH39ydbW1uysbGhgIAAevLkSbEZMzMzydvbm2xsbMjIyIg8PDzo6NGjRESUl5dHtra2tGfPHpV1AgICKCgoSJwODQ2lJk2akLGxMbm7u9ORI0fEZbNmzaKuXbvS5MmTydramt5//30iIho6dCjZ29uTTCYjNzc32rJli8o+Dhw4QG5ubmRoaEi9evWiiRMnUufOncXlycnJFBwcTPb29mRpaUmDBg2ixMTEYo8zMDCQhg0bpjLP0tKSvv32W3Ha09OTunTpUmTdEydOqHwuc+fOJXNz8xLfV3WuXr1KXl5eZGlpSWZmZvTee+8REVFcXBwBoPv374tt169fTw0aNBCnHR0dKSQkhLp06UIGBga0ZcsW0tPTo8uXL6vs491336WQkBAiKvj85s+fTy4uLmRiYkLt27enixcvFpsvMjKSDA0NVb4f79+/L2Y2Njamjh07qmyjuM/37t27NGDAALK1tSVbW1saMWIEpaeni+tNnz6d6tevT4aGhuTs7ExLly4t03tZWs+ePaPatWvT8ePHxXnHjx8nfX19ysrKUrtO69atadmyZeJ0bm4u6ejoUFhYmNr2y5cvJxsbG8rNzVWZHxgYSOPHjy9TXrW/24gIwEUqp9qrsnv0VgHIAfCrIAjvCYIwEsBsAEuo0E0VgiDcEQRhbaFpPwALUHDaNkEQhHcKvawq9xBYeTpz5gzc3d1x4MABmJqa4rfffquy3d+MlUZYWBh++uknpKSkoHnz5vD29saJEydw9epVXLt2Dfv27cPOnTvVrvvXX3/By8sLw4YNw6NHj3D//n0EBASIyxUKBQ4fPozLly/j33//BVDQ25CSkoLo6GjcvHkTycnJ8Pf3LzafUqlE//79cfv2bTx58gS+vr4YMGAAHj9+DG1tbfj7+2P9+vVie7lcjl9++UXs4QoNDcXChQuxZcsWpKSkYP78+ejfvz/u3LkjrnPq1CnUqVMH9+/fxy+//AIA6NixI65cuYLU1FTMnDkTQ4cORXR0NICC4Qz79++Pr7/+Gqmpqfjss8+wdq34JwBEhA8//BCCIOD69eu4e/cujIyM4OfnV6rPRKFQYMeOHUhOThZPUWZlZSEiIkKlV+aFLl26wN7eHocPHwYAHDp0CD179oS5eekftv/o0SN07twZnTt3Rnx8PBITE/HFF1+awg4JAAAgAElEQVSUen2g4Ia0JUuWQC6Xw9vbG3379sWGDRvE5bGxsTh79qx4k9rMmTOxd+9e/P7773jy5AmCg4Ph5eWFlBT1VzlFRUWhUaNG0Nb+75G6SqUSY8aMwd27d5GYmIgWLVqgf//+Kj13L3++2dnZ6NatGxo3bozY2FhER0fjwYMHmDBhgrhO48aNcebMGWRkZODnn3/G9OnTceTIkWKPvXfv3jA1NS32tXXrVrXr3bp1C9nZ2WjZsqU4r0WLFsjKyhLPFr1MqVS+6FASERGuXLmitv3q1asRHBxc5JFPb7/9NqKiooo9JsmUV8VY2heAxgD+AJAF4BEKRrjQeqlNPIANhaY34Hlvj5rX0Fft80WP3tNT98pUabOKo1AoaMGCBaSlpUUAqF27dhQfHy91LFZFVacevUWLFonTBw8eJACUlJQkzhs0aBBNnDhRnEahnqPRo0fTwIED1W77RS/T3bt3xXkJCQkEgGJiYsR5f//9NwGghw8fljq3hYUFHTx4kIgK3msdHR3699+CY1+7di25uLiIbZs0aVKkp6N37940d+5cIiro8alfv/4r99myZUtasWIFERX0mHXq1Ell+ZAhQ8QevQsXLpC+vj5lZ2eLy5OTk4v0ihUWGBhIurq6ZGJiQtra2iQIAs2ZM0dc/uDBAwJAhw8fVrt+mzZtaPjw4URE1LBhQ5o6deorj6mwhQsXUqtWrdQuK0uPXmGHDh0iS0tLsSfp66+/FnsJlUolyWQyOnnypMo6TZs2pU2bNqnNMX/+fJVeU3XS09MJAN24cYOI1H++u3btImdnZ5V5Fy9eJF1dXcrPz1e73QEDBtCUKVNK3PfrOHXqFAEgpVIpzlMoFEV6zgubPXs2NWzYkGJiYigrK4umTp1KgiAU6REmIjpz5gzVqlWLYmNjiywLDQ0lNze3MuWtiT16IKJoIupGRPpEVIeIvqaXxrklIiciGlpoeigRCcW8NpR2336em8rvQNgbuXPnDkJCQqBQKPDFF1/g5MmTcHR0lDoWY2+sTp064tcGBgbQ0tKClZWVyrwX1/68LD4+Ho0aNSp227Vq1UK9evXE6fv37wMA6tevL85r0KCBuOzli8iBgp6s8ePHw9nZGcbGxjA1NUVKSgoePy54Lr2bmxtatGiBzZs3AwDWr1+vcr1aXFwcxo4dq9K7cuLECXE4JwBwcnJSya1UKjFz5ky4urrCxMQEpqamuHr1qrjPhISEIj//hafj4uKQk5MDGxsbcZ8NGjRA7dq1ce/evWLfL39/f6SmpiItLQ1jxoxBeHg48vMLRkcyNzeHlpaWSu7CHj58KH5uVlZWxbYrzqs+y9J4+X3s0aMHdHV1sX//fhARNm7ciODgYAAFwwTK5XL06dNH5bOJjY3FgwcP1G7fzMxM5Qa4F9sJCAiAg4MDjI2Nxe+3F5+VulxxcXG4d++eyn49PT0hCAISExMBAD/88APefvtt8QaJ/fv3q2yzvBgZGQEA0tLSxHkvvi5uBIpp06bB29sbPXr0gIODA4CCnwNLS8sibVevXo0ePXqo/My9kJ6eXqZe38pS2UOgSUrGD1ytMho1aoTVq1fDxsYG77//vtRxWHX0eFzp2gU0LXiVRvjg189TDpycnHD79u1ilwuCoPKQ1Rd/hOPj49GwYUMABafzXixr06YNPv74Y5VtLFmyBCdPnkR4eDicnJwgCAIsLS1VTl0FBQVhxYoV6Nu3LyIjI7F9+3ZxmaOjI0JCQjBo0KBic748VN22bduwZs0aHD16FI0bN0atWrXQqlUrcZ92dnY4evSoyjqFCzhHR0cYGhri6dOnrzUMnoGBAZYsWYImTZpgxYoVmDBhAvT19fHuu+9i69atGDZsmEr7U6dO4cGDB+jZsycA4IMPPsCyZcuQkpICM7PSPfLVycmp2Bs1XhTdmZn/Pcz/4cOHRdq9fKxaWloICAjAhg0bYGJigrS0NHh7ewMALC0tYWhoiOPHj6N169alyujh4YGYmBgoFApoaRU8emz69Ol49OgRzp8/jzp16iAjIwPGxsYq3x8v53J0dESjRo1w48YNtfs5e/YsvvjiC4SHh6Nt27bQ0tLCwIEDi5wuLaxnz544ffp0sctXr15d5HsbAFxdXVG7dm1ERUWhW7duAIDLly9DX1+/2MJbT08PixYtwqJFiwAUFLs//PADunTpotLu6dOn2LVrl8rPQ2HXr1+Hh4dHsZmlolEDR3KhJx2FQoFZs2Zh27Zt4rzAwEAu8hgrZNSoUdi3bx82bdqE3Nxc8Tqy4tStWxc9evTA5MmTkZqaipSUFEyePBk9e/ZU6VksLD09HXp6erCwsEBubi7mzJmD1FTVp1t99NFHuHPnDj799FN0794ddnZ24rLPPvsMs2fPxpUrV0BUMDj7mTNn8PffxT8IIT09Hdra2rCysoJSqcS6detw9epVcbmvry/Onz+PnTt3QqFQICIiAnv27BGXt2rVCu7u7pgwYQKePHkCoKCHqbg/uOro6upi5syZmDdvntij+t133+H8+fMYN24cEhMTkZubi/DwcAwZMgR+fn7o1KkTgIK7OOvWrYvevXvj4sWLyMvLQ05ODg4ePIgxY8ao3d+QIUNw69YtLFy4EM+ePUNeXp74KBZLS0s4Ojpi3bp1UCgUuHbtGn7++edSHUdQUBAOHz6MhQsXwtfXF7VrFwwCIAgCJkyYgM8//1z8z4JcLseRI0fUFpEA0Lp1a5iamqo8xzE9PR0GBgYwMzODXC4v1XWFvXv3Rl5eHhYsWICMjAwQERISEvDbb7+J23zRsy0IAg4ePChe/1icw4cPq9zp+vJLXZEHFAwrNmTIEMycORNJSUlISkrCzJkzERAQIL5XL0tMTER8fDyAgp7woUOHol27dvDy8lJpFxYWBktLS/Tu3bvINogI4eHh+PDDD1/1dlU6jSr0bGx42CwpJCQkwNPTE3PmzMHo0aOL/FFhjBVo3rw5Dh06hJUrV8La2hoODg7YtKnkS042b94MIyMjvPXWW3jrrbdgamqKjRs3Ftt+0qRJMDU1Rd26ddGgQQMYGBgUORVnYmICb29vHD58WDw1+MKIESMwdepUBAUFwczMDA4ODpg7d26Jj9kIDAxE27Zt0bBhQ9jZ2SE6OlosooCC0827du3CrFmzYGJigsWLF8Pf3x96enoACnqQ9uzZA6VSiZYtW8LIyAht27YtsQhWx8/PD+bm5vjuu+8AFPRoRUZG4uHDh2jcuDFMTU0xbtw4jB8/XuU9NDIywpkzZ9ChQwcMHjwYJiYmcHZ2xsqVK+Hj46N2X3Xr1kVERASOHTsGe3t72NjYYOHCheLysLAwHDhwACYmJpg0aVKRXsXiNGrUCG3atMGxY8eKfDYhISHo168f+vXrB2NjY7i4uGDVqlXFjg2upaWFcePGYc2aNSrbSEpKgoWFBZo1a4b27duLvX3FMTAwQHh4OKKjo/HWW2/BxMQEnp6e4s0MXl5e8Pf3R5s2bWBpaYndu3eLPZEV4fvvv0ejRo3El6urK5YuXSouX7BgAZo0aSJOP3jwAN27d4eBgQFatWoFJycn7Nu3r8gQZaGhoRg+fLja9+PYsWPicVc1QkldpzVFXaEujcIozKJZUkfROIcPH0ZAQACSk5Nha2uLzZs3V8kfBFa13bx5k0dG0TC+vr4wMjJCaGio1FFqtKysLDRr1gwHDhyoUQ9Nrmzt27fHnDlz8N5775VpveJ+twmCcInKacAHjbpGj1WevLw8zJgxQ7zmoXv37ti0aRNsbGwkTsYYq4r279+Pjh07wsjICAcPHsQvv/xS4uM3WPnQ19cv8bpQVjp//vmn1BGKxYUeqxDBwcHYvHkztLS0MG/ePEydOvW1LqJmjGmGkydPIigoCNnZ2XBwcMCqVavQtWtXqWMxVu1xoccqxOTJkxEZGYkNGzagQ4cOUsdhjFVxixcvxuLFi6WOwViNw10srFzk5ORgx44d4rS7uztu3rzJRR5jjDEmIe7RY2/szp07GDx4MKKioqBUKuHr6wsAKsPqMMYYY6zyaVSP3t3hh6SOUONs374dLVq0QFRUFOrXry8+lZ8xxhhj0tOoQu/f3bekjlBjZGVlYdSoUfD19UVGRgYGDhyIqKgotGnTRupojDHGGHtOo86t8cgY5SM+Ph59+vTB9evXoaenh6VLl+KTTz4p8nBJxhhjjElLo3r0DLkQKRcWFhbIzs5Go0aNEBkZidGjR3ORxxhjNQARoX379uJwbez1DB48GGvXrpU6BgANK/QsvuksdYRqSy6XIysrC0DBcECHDh3CpUuX4O7uLnEyxhh7tdmzZ0NbWxsymQxGRkZwdnbG7Nmz8fLoUA8ePEBQUBBsbW2hr6+Phg0bYsaMGcjOzlZpl5ubKw6lZWhoCFtbW3Tt2hW7d++uzMMqdzt37oS2tnaNGsFIoVBgypQpsLKygpGREQYMGIDk5OQS11m1ahUaNWoEmUwGDw8PleH2Tp8+DZlMpvLS1tZGs2bNxDYhISH48ssvxb+bUtKoQk82kouS13H16lW0atUKEydOFOe5uLhAJpNJmIoxVphCoSh2TNOqpKQxcStaly5dIJfLkZ6ejrCwMCxatAhhYWHi8oSEBLRp0wapqak4d+4cMjIysGXLFvz222/o1asXFAoFgIL3ulevXti0aROWL1+O5ORkPHjwAF9//TV++eWXSjmWinofly1bhhEjRrz2+lJ+vsX55ptvsHfvXpw/fx4PHjwAAPj7+xfbfteuXfj666+xc+dOpKWlYdSoUejVqxfu3bsHAOjUqRPkcrn4Sk9Ph52dHYYMGSJu46233kLDhg2xbdu2ij240iCiGv+qgzo0G7OJlY1SqaSVK1eSnp4eAaAmTZpQenq61LGYBoqOjpY6Qqk4OjrS3LlzqUuXLmRoaEhNmzalq1ev0tatW6lBgwZkbGxMw4YNo7y8PHGdoUOHkr29PclkMnJzc6MtW7aobPPq1avk5eVFlpaWZGZmRu+99x4REcXFxREAWrNmDbm5uZGuri49evSIMjMz6dNPPyV7e3uysLCgfv360d27d0vMXVKGli1b0rJly1Taz5w5k7p27SpO//bbb9SiRQsyMTGht956izZv3iwuW79+PTVo0IAWLVpEdnZ21LhxYyIimj59OtWvX58MDQ3J2dmZli5dqrKPyMhIatGiBclkMurQoQOFhISQo6OjuDwzM5MmT55MTk5OZGZmRl5eXnT79u1ij3HWrFnk6empMq9Vq1Y0duxYcXrYsGHk4uKi8vkQEcXExJCOjg5t2rSJiIg2bdpEurq6FBMTU+z+1ImLi6OBAweSra0tmZiYUPv27Sk5OZmIiADQ6dOnxbYnTpwgLS0tcbpz5840YcIE6tevHxkZGdHcuXPJ1taW9uzZo7KPgIAACgoKEqdDQ0OpSZMmZGxsTO7u7nTkyJFi8yUmJhIAevjwoTgvMzOTvL29ycbGhoyMjMjDw4OOHj0qLi/u801OTqbg4GCyt7cnS0tLGjRoECUmJorrLVu2jFxdXUkmk1G9evVo2rRplJ+fX6b3s7QcHBxozZo14vSdO3cIAMXFxaltP2jQIJo4caLKPCcnJwoJCVHbfv/+/aSrq0tJSUkq82fNmkV9+vQpMVtxv9sAXKRyqoEkL8Iq48WFXtmlpqaSj48PASAANHz4cMrMzJQ6FtNQ6n4ZArNVXsVZvfqiSrsRI/YV27ZFi9UqbS9eTChTTkdHR2rYsCFFR0dTbm4uffzxx+Ts7EwjRowguVxOd+/eJSsrK5VCas2aNZScnEz5+fm0bds20tHRoRs3bhAR0cOHD8nU1JQWLFhAcrmccnJy6NixY0T0X6HXrVs3evToEeXk5FB+fj6NHDmS2rRpQw8ePCC5XE7Dhg2jZs2alfhHtKQMK1asoObNm4ttlUolOTk50caNG4mI6OjRo2Rubk6nTp0ihUJB58+fJ1NTUzp58iQRFRQCWlpaNHHiRHr27Jn4e2TTpk2UkJBASqWSwsPDqXbt2vT7778TUcHvH3Nzc1q0aBHl5uZSVFQU1a1bV6XQ8/X1pV69elFiYiLl5OTQzJkzydXVlXJzc9UeY+FCT6FQ0B9//EG1a9emH374QWxTp04dmjFjhtr1O3bsSH5+fuK+O3ToUOz7qU5mZibVr1+fxowZQ6mpqZSXl0d//vmn+J/n0hR6RkZGFB4eTkqlkjIzM2nKlCnUr18/sU1GRgYZGhrSqVOniIho9erV1KBBA7py5QopFAo6ePAgGRoaFlsQHzp0iMzMzFTmZWRk0KZNmyg9PZ1yc3Np0aJFZGRkJBY16j5fpVJJHTt2pGHDhlFqaiplZmZScHAwdevWTdzu7t27KTY2lpRKJUVFRZG1tTWtWrWq2Pdv9OjRZGJiUuzrf//7n9r1UlNTCQBdvnxZZb6xsTHt3btX7ToDBgygCRMmqMxzdHQkb29vte179epFvr6+Rebv3r2b7Ozsij0mIi70uNCTyIULF8jZ2ZkAkEwmK9LDwFhlq06F3qJFi8TpgwcPEgCV/+mr6y0orGXLlrRixQoiIlq4cCG1atVKbbsXhd6LgoqooICpXbu2So9LRkYG6ejo0J9//lnq4yic4enTp6Snp0dRUVFERBQeHk7GxsZiwdarV68iPR3jxo2jYcOGEVFBIVC7dm3Kzs4ucZ8DBgygKVOmEFFBEejg4EBKpVJcPmPGDLHQe/z4MQFQ6alUKBRkbGysUiwVNmvWLNLW1iYTExPS1dUlADR69GiV3jttbW1auXKl2vV9fHzE3tT33nuPfHx8Sjyel+3YsYNsbW2L9Ba+UJpCr3BPHVHBz4WOjg79+++/RES0du1acnFxEZc3adKEwsLCVNbp3bs3zZ07V22GLVu2qBTTxbGwsKCDBw8SkfrP98KFC6Svr68yLzk5mQDQ/fv31W5z8uTJNGjQoFfuu6zu3btHACg2NlZlvoODg9hD+7INGzaQhYUFXbhwgXJzc2n58uUkCEKRHuEX269VqxZFREQUWXb06FHS19cvMV9lFHoadY0eK53ly5cjNjYWHh4eiIqKgp+fn9SRGKs26tSpI35tYGAALS0tWFlZqczLyMgAACiVSsycOROurq4wMTGBqakprl69isePHwMoeJRRo0aNStyfk5OT+PXjx4+RnZ0NZ2dncZ5MJoO1tTXu379f5CLye/fuvTKDmZkZPvzwQ6xfvx4AsH79enz00UcwMDAAAMTFxWHhwoUwNTUVXxs2bMDDhw9V3hM9PT2V3D/88APefvttmJmZwdTUFPv37xf3mZCQAAcHB5W7+R0dHcWv4+LiAADNmjUT92lubo68vDzcv3+/2Peqc+fOSE1NRUZGBhYsWICIiAg8e/ZMXG5lZYWEhAS16z58+FD8HEtqV5z4+Hg4Ozu/0YhBhT9rAHBzc0OLFi2wefNmAAWfTVBQkLg8Li4OY8eOVflsTpw4UWx2MzMzpKenq8zLysrC+PHj4ezsDGNjY5iamiIlJUX8rICin29cXBxycnJgY2Mj7rdBgwaoXbu2eJ3btm3b0Lp1a1hYWMDExAQrVqxQ2WZ5MTIyAgCkpaWpzE9NTYWxsbHadQICAjBlyhR8/PHHsLW1RVRUFDw9PWFpaVmk7c8//wxXV1d07lz0Zs/09HSYm5uXw1G8GS70WBHLly/H7Nmzce7cObi4uEgdh7Eaa9u2bVizZg1++eUXpKSkIDU1Fc2bNy843YKCP+y3b98ucRu1av33a9zKygp6enpiIQQU3DGflJSEevXqFbmI3MHB4ZUZACAoKAhbt25FcnIyfv31V5ViwtHREbNnz0Zqaqr4ysjIwKFD/41EVDgjAJw9exZffPEFVq9ejeTkZKSmpqJPnz7iPu3s7HDv3j2VDC8KhBf7BIDbt2+r7PfZs2fiEIwl0dXVxfTp02FlZYVZs2aJ899//33s3LkT+fn5Ku3/+ecfnD9/Hj179gQAfPDBB7hw4QLu3Lnzyn294OTkhLi4OPGGjpcZGhoiMzNTnC5cKL/w8vsIFHw2GzZswJ07dxAZGYmAgABxmaOjI9atW6fyHsnlcqxcuVJtBg8PD6SkpCAxMVGct2TJEpw8eRLh4eFIS0tDamoqzMzMVD6bl3M5OjrC0NAQT58+Vdl3VlYW2rdvj/v372PIkCGYMWMGHj16hLS0NIwdO1Zlmy/75JNPitzpWvi1YMECteuZmprCwcEBUVFR4rzY2Fikp6er3CVbmCAI+OKLL3Dr1i08efIEq1atws2bN9GlSxeVdvn5+Vi7di1GjRqldjvXr1+Hh4dHscdUacqra7Aqv8RTt5bLS+xC1VR//vkn9e7dm549eyZ1FMbUqk43YxQ+HfTy6TciosDAQPG05k8//UT16tWjxMREysvLo7Vr15K2tjbNmjWLiIgSEhLI2NiYvvnmG8rMzKTc3Fw6fvw4Ef136vblU2EjRoygd955hxISEigzM5NGjhxJb7/9drHX6L0qA1HBaVF7e3vq2bMnubm5qax/5MgRqlu3Lp06dYry8/MpJyeHLl68SBcuXCCi/y7WL+zQoUNkaGhIMTExpFAo6MCBA2RgYECBgYFERJSSkkJmZma0ePFiys3NpStXrpC9vb3KaUU/Pz8aOHAgPXjwQFzn119/pYyMDLXHqe5mjFOnTpGuri7Fx8cTUcFpOBsbGxowYADFxcVRfn4+/d///R81bdqU3n33XfG0a35+Pnl6elLjxo3pxIkTlJWVRfn5+RQREaH2Wi0iIrlcTo6OjjR+/HhKTU2l/Px8OnfunHiNXufOncnX15dycnIoLi6OWrZsWeTUrbpTrqmpqaSvr089e/aknj17qiwLDQ2lxo0b0+XLl0mpVNKzZ8/o9OnTdPPmTbUZiYjatm2r8j08depUatWqFaWlpVF2djaFhISQlpYWrV+/nojUf74KhYI6depE48aNE282SUpKom3bthFRwc8zADp79iwplUo6d+4cWVtbU+fOnYvN9SbmzZtHjRo1otjYWEpLS6OBAweSl5dXse1TU1MpOjqalEolJSUlUXBwMLm5uRX5G/nrr7+Svr4+PX36VO12OnTooHITiDp86pZVKKVSiUWLFqFTp044cOAAvv/+e6kjMaZRAgMD0bZtWzRs2BB2dnaIjo5Gp06dxOV169ZFREQEjh07Bnt7e9jY2GDhwoUlbnPp0qVo1aoVWrduDQcHBzx69Aj79u2DlpbWa2UACnpsAgICcPjwYQQHB6ss69GjB0JDQzFlyhRYWlqiTp06+OyzzyCXy4vN6OXlBX9/f7Rp0waWlpbYvXs3vL29xeWmpqY4ePAgtmzZAjMzM4wdOxZDhw5VOT344pRZly5dYGRkhLfffhu7du0q08PbO3XqhE6dOom9evXq1cP//d//wcDAAG3btoWhoSEGDx6MPn364PfffxdPu2ppaeHQoUPw8/PDmDFjYG5uDjs7O4SEhGDQoEFq92VoaIg//vgD9+/fh4uLCywsLDBlyhTxcSQ//vgj7ty5A3Nzc/j4+GDo0KGlOgYTExN4e3ur/WxGjBiBqVOnIigoCGZmZnBwcMDcuXNLfATKxIkTsWbNGnF60qRJMDU1Rd26ddGgQQMYGBgUOYX8slq1amHPnj1QKpVo2bIljIyM0LZtW/FZdG5ubggJCUG/fv1gamqKb775plQ9sa9r2rRp6NOnD1q3bg07OzsoFArxdDcAbNmyReVxYenp6Rg0aBCMjIzg6uqK3NxcnDhxAvr6+irbXb16NQYPHgwzM7Mi+7x16xZu375dJS59EqiErtKaoq5Ql0ZhFGZZWgCPx0kdp0pISkpCQEAAjhw5AgCYPHkyFixYAF1dXYmTMVbUzZs34ebmJnUMJqHp06fj0qVLOHr0qNRRajSigpEx5s+fj27dukkdp9ry9fWFp6cnhg8fXmK74n63CYJwiYhalUcWjRrrlhWIiIiAn58fHj16BHNzc4SFhaF3795Sx2KMMdGxY8fQtGlT2NjY4OzZswgNDcXixYuljlXjCYKAc+fOSR2j2qsSD0p+TrMKPe7NE+8eUiqV6NixI7Zt2wZ7e3upYzHGmIpr167B398f6enpqFu3LqZMmYLAwECpYzFW7WhWocfg4eGBwYMHi+M8vsmt/owxVlEmTZqESZMmSR2DsWqP/8prgKNHj8LR0RGurq4QBAGbN29We5s+Y4wxxmoW/mtfg+Xn5+PLL7+El5cXfHx8kJWVBUD9s5gYY4wxVvNwj14Ndf/+ffj6+uLs2bOoVasWfHx8+I5axhhjTMNwoVcD7du3D0FBQXj69Cns7OywdetWvPvuu1LHYowxxlgl06xzeBuvS52gwk2fPh39+vXD06dP8cEHH+DKlStc5DHGGGMaSrMKvckRUieocE5OTtDW1sbixYuxf/9+tYMwM8YYY0wzaFahV0M9ePBA/HrkyJG4fv06Jk+ezDddMMaqpPHjx8PS0hIymQxJSUlSx3kjN27cgKura4nDirGSPX78GI6OjkhOTpY6So3ElUA1lp2djTFjxsDNzQ0xMTEACp5q7urqKnEyxjRTly5doKenB5lMBhMTE7i7u2PXrl1F2p07dw7vv/8+TExMIJPJ0LJlS4SFhRVp9+jRI4wePRqOjo4wNDSEg4MDfHx8cOnSpco4nArx559/Yt26dbh58ybkcjmsra2ljvRGPv/8c3zxxRfQ0dGROkq5uXPnDt577z0YGhrC3t4e3333XYntnzx5gsDAQNja2sLExAR+fn5ISUkRl3/yySeQyWQqL0EQsGTJEgCAlZUV/Pz8EBISUqHHpak0q9Dzbyx1gnJz69YttG3bFitXrkRubi4uX74sdSTGGICvv/4acrkcT548wdChQ+Hn54c7d+6Iy48ePYquXbuiXbt2iI2NRVJSEr744gtMnDgRs2bNEts9fPgQrVu3xv3793Ho0CGkp1zgxG0AACAASURBVKcjOjoaffr0wa+//lrhx0FEyM/PL/ftxsbGok6dOrCysnqt9Ssq1+u4desWzp49i48++ui1t1HVegIVCgX69OkDNzc3PH78GPv27cPChQuxY8eOYtcJCAiAXC7H7du3ERcXhydPnsDf319cvmrVKsjlcvH122+/QVtbW+V9Cw4Oxvr165Genl6hx6eRiKjGv+qgDs3GbKopNm7cSIaGhgSAGjZsSFFRUVJHYqxCRUdHSx2hVDp37kxz584Vp+VyOQGgXbt2ifMaNmxIQ4cOLbLu+vXrSUtLi+Li4oiIaNiwYdSoUSPKzc0tU4aIiAjq2LEjmZmZkYWFhbivEydOkJaWlkrbWbNmkaenpzgNgJYtW0YtW7ak2rVr0+nTp0lHR4eSkpLENkqlkpycnCgsLIyIiDIzM2ny5Mnk5OREZmZm5OXlRbdv31abbeHChaSnp0eCIJChoSF17dqViIji4+Opb9++ZGFhQfb29jRhwgR69uxZsbnOnTtXZNuzZs2ibt260fTp08nKyoqsrKxo5syZKm2uXbtGPXr0IAsLC6pXrx5NmzZN5f2NjIykFi1akEwmow4dOlBISAg5OjoW+15/88035OXlpTLv+PHj1KZNGzI1NSVLS0saPHgw/fvvv+Lyzp0704QJE6hfv35kZGRE//vf/4iI6NSpU9ShQwcyMzMjZ2dnWrx4MSmVSvE99vb2JhsbGzIyMiIPDw86evRosbnexB9//EH6+vqUkZEhzpsxYwZ16dJFbXu5XE6CINCVK1fEeREREQSA4uPj1a4zYMAA8vb2LjLf0dGRfvnllzc8guqluN9tAC5SOdVA/HiVaiQzMxPjxo3Dhg0bAAC+vr5YvXo1jIyMpA3GWGU7ebFy99e5VZlXyc3NxcqVKwEAjRo1AgDExMTgzp07WLVqVZH2fn5+GDZsGI4dO4YRI0bg0KFDCA4OLtMpwb/++gteXl5YtWoVfH19oVQqERkZWabca9euxW+//QYnJyfk5+fD3d0dW7ZswcSJEwEAERERePLkCQYOHAgAGD58ONLT0xEZGQkzMzPMnz8fvXv3xrVr14pknzp1KqytrTFv3jyxlzM/Px+9evVChw4dcPfuXaSmpuLDDz/E559/jhUrVhSbS51Tp05h0KBBePjwIS5duoSOHTuiR48e6NChA5KSktC5c2csWLAA+/fvx+PHj9GvXz/o6+tj5syZSEtLwwcffIBp06Zh4sSJuH79Onr37l3i+x8VFYXGjVXPFOnp6eHHH3+Eh4cHkpOT4ePjgwkTJqgMcr9u3f+3d+dxXVX548df73BB+bAJ4oIgirmkqWRaNk3m0pimo7mmY+Y2mU59dVwns59ZOpMt5nzHCgul3DC1zExr3DLNsVIpG1MTv+6KCygigoFwfn/cD5/4wIdFVpH38/G4D7zn3nPv+36OwJtzzr13EZ9++ilr1qwhJSWFn3/+me7du7N06VJ69OhBTEwM3bp1o2bNmgwdOpSMjAz69OnDhx9+iLu7O/PmzaNv37783//9X649oz4+PrnGDdb/leDg4Bzl+/bto3HjxthsNkfZPffc49QWWWVNKDJlZGQ4jlW/fn2n/c+dO8dnn33G559/nuNYd999N9HR0fTp0yfP2NXNqVhDt+Xc0aNHiYqKolq1akRERLBs2TJN8pS6xcyePRsfHx+qVavG9OnTiYiIoGXLloA16RwgMDAwR70qVarg7+/vuDnh4sWLLvfLS3h4OD179mTYsGFUrVqVatWq0bFjx5s6xqRJkwgNDcXNzY2qVasyfPhwIiMjHdsjIyMZOHAg1atXJy4ujqioKN555x1q1apFlSpVmDFjBrGxsXz33XcFOt/3339PTEwMc+fOxcPDg8DAQGbNmsWiRYuckofscbnSuHFjnnnmGSpVqsR9991H69at2bPH+qNg8eLFtGrVitGjR1OlShUCAwN5/vnnWbx4MQDr1q3DZrMxadIkKleuTFhYGCNGjMgz9suXL+Pl5eVU9uCDD9K2bVsqVapE7dq1mTJlClu2bHHap1+/fnTq1AkRoXr16rz77rv079+fXr164ebmRtOmTXn22WcdsdlsNoYMGYKnpyeVK1dm8uTJVKlShd27d+caW0JCQp6LqyQP4OrVq3h7ezuV+fj45DqkarPZePjhh3nppZdISEjg4sWL/P3vfwdwWWfhwoUEBQXxyCOP5Njm5eXFpUuXcr0mVTjao1eO3H333SxevJi77rqLFi1alHU4SpWdQvSwlZYXXniB6dOnc/nyZUaOHMnWrVsZOXIkgKP35cyZMzRt2tSpXmpqKnFxcY59atasyZkzZ27q3MePHycsLKxI8YeEhDitDxo0iAkTJhAdHc2dd97Jxx9/zObNmwE4duwYgCORzZSWlsapU6cKdL5Tp04REBCAh4eHoyw0NJTr169z8eJFx80a2eNypU6dOk7rHh4eXL161RHrzp07nXq6jDGkp6cDVpsEBwcjIo7t2XujsvP19c2RzOzdu5dp06axb98+kpOTMcaQlJTktE/2azl27Bhbt251mnuZkZFBUFAQACkpKUyZMoX169cTFxfHHXfcwdWrVx1/OBQnT09Prly54lSWkJCQI6HNaunSpUyYMIFmzZrh7u7OxIkT2bx5c47He2VkZPD+++8zZswYp885U2JiIg0aNCieC1EO2qN3C0tMTGTw4MEsX77cUTZgwABN8pQqB3x9fYmIiGDDhg2sXbsWgDvvvJOGDRs6fU9nWrFiBSLi6Ono3r07q1evvqnJ+iEhIcTExLjcZrPZSE9P59dff3WUnT17Nsd+2R/L5OPjQ+/evfnggw9YuXIlwcHBtG/fHvgtEYqJiXHqLUpOTmbQoEEFijkoKIgLFy6QnJzsKDt69Cju7u5OiUJRHxdVv359unTp4hTnlStXHElYYGAgJ0+edOpFPHnyZJ7HDAsL48CBA05lTzzxBPfccw+HDx8mMTHRacg2t2upX78+I0aMcIotMTGRn3/+GYC5c+fy9ddfs2XLFq5cuUJCQgK+vr5OsWaX/S7X7Etu19aqVSsOHz7MtWvXHGU//PADrVq1yvVcgYGBfPTRR8TGxnLs2DEaNGiAu7s7999/v9N+X375JbGxsbn2lO7fv7/If6ionDTRu0VFR0fTpk0boqKimDRpEtevXy/rkJRSN6lGjRpMmDCBadOmkZGRgYgwf/58li5dyqxZs7h06RIpKSmsXr2a8ePHM3XqVEePxsyZM0lKSqJfv34cPHiQ9PR0rl27RlRUFNOnT3d5vtGjR/PZZ5+xZMkSUlNTSUlJYdu2bQA0adIEm81GREQEGRkZfPPNN6xevbpA1zF8+HCWL1/Oe++9x/Dhwx3lAQEBDB48mLFjxzp6HxMSElizZk2OXqzctGvXjkaNGjFx4kSSk5M5e/YsL774IsOHDy/WZ4EOHTqUPXv2sGjRIq5fv05GRgZHjx7lyy+/BKBHjx5cvXqVuXPnkpaWxr59+5yGrF3p1asXu3btIiUlxVGWmJiIt7c3np6enDx5kldffTXf2MaOHcuKFStYt24daWlp3LhxgwMHDvD11187jlm1alX8/PxITU3l5ZdfJiEhIc9jZr3L1dWS29DtQw89RP369Zk2bRopKSn8+OOPLFiwgNGjR+d6rl9++YVLly6RkZHB7t27GT9+PH/7299yzBNcsGABffr0cTmv8MiRI1y8eJEuXbrk93Gpm1RhEr07/Tygc+63h98qjDHMnz+f9u3bc+TIEVq2bMlXX32Fu7t7WYemlCqEcePGERsb65hv1a1bN7Zs2cL27dsJCQnB39+f2bNn88YbbzB79mxHvcDAQHbv3k2dOnX4wx/+gJeXF82aNePTTz+lb9++Ls/VqlUrNmzYwLvvvktAQADBwcEsWbIEsIbkIiMjefPNN/H29uaf//wnTz31VIGuoUuXLlSvXp29e/cydOhQp23vv/8+TZo04eGHH8bT05O7776bVatWuRyac6VSpUp8/vnnnD59muDgYNq1a8d9993HG2+8UaD6BVW7dm2++uorPv30U0JCQvD19eXxxx/n6NGjgNVzuX79epYtW4avry9/+ctfHHMdc9OsWTPat2/v9OiR9957j4iICDw9PenTpw/9+/fPN7YWLVrw+eefM2/ePOrUqUNAQADDhg1zDM1OmDABHx8f6tatS2hoKNWrVy/QUHZhuLm5sW7dOvbv34+fnx/du3dn8uTJTo9C6datG88884xjffv27TRv3hybzcbgwYN59tlnnR4VBNbQ+Pr1653qZbVo0SKGDRuWY36gKjrJq+v3dlFX6pqz/tOslYvPlm0wecic07NmzRoAxowZw5tvvkm1atXKODKlytbBgwdp1qxZWYehKpjnn3+evXv3snHjxlz32b9/P/369XN5l7EqmLi4ONq0acOePXsK/XzF8iq3n20istcYUyyTkStMj155MHDgQNasWYOXlxcrV67knXfe0SRPKaVKyaZNm4iNjSUjI4MdO3bw3nvv5TvXsEWLFhw6dEiTvCLw9/fnxIkTFS7JKy2a6N1CXnvtNR588EF++OGHAnX3K6WUKj7//e9/CQsLw2azMXz4cCZPnlzg4W2lblUVZ+j2xx+tlVa3znsV4+LiWLlyJWPHjnWUGWMKPLdFqYpCh26VUrej0hi6rTjP0buFEjywJq8OHjyYM2fOUKNGDcdEV03ylFJKKVVcdOi2lKWnpzNr1iw6duzImTNneOCBB3jggQfKOiyllFJK3YYqTo/eLeDcuXMMGTLE8Tqc559/npkzZ+okXqWUUkqVCE30SsmPP/5I165duXDhAjVr1mTJkiV07dq1rMNSSiml1G1ME71SEhoaipeXF82bN2fZsmU53smolFJKKVXcKs4cvX0XrKUUnT592vH+Rk9PT7Zt28amTZs0yVNKVWjPPfcc/v7+2Gw2Llwo3Z/LxcUYwwMPPOCYiqMKZ+DAgSxcuLCsw7itVZxEr8tKaykl69evp3Xr1vz1r391lAUGBuLm5lZqMSilStfDDz9M1apVsdlseHt707p1a1atWpVjv127dvHoo4/i7e2NzWajTZs2fPjhhzn2i42NZcyYMdSvXx8PDw+Cg4MZMGAAe/fuLY3LKRH/+c9/WLRoEQcPHiQpKYmAgFvriQgFtXLlSipVqkTnzp3LOpRik56ezuTJk6lZsyaenp707duXuLi4POuEh4fTuHFjbDYbYWFhjncrA+zYsQObzea0VKpUiZYtWzr2mTlzpuO9uqpkVJxEr5SkpqYyadIkevToQXx8PCdPniQ1NbWsw1JKlZIXX3yRpKQk4uPjGTZsGIMHD+bIkSOO7Rs3bqRjx460b9+eo0ePcuHCBaZOncr48eOd3g969uxZ2rZty6lTp9iwYQOJiYkcOHCAnj178sknn5T4dRhjuHHjRrEf9+jRo9SpU6fQb0Eoqbhu1rx58/jzn/9c6PppaWnFGE3xePXVV1m7di3fffcdp0+fBuDJJ5/Mdf9Vq1bx4osvsnLlSq5cucLo0aN57LHHOHnyJAC///3vSUpKciyJiYkEBgYyZMgQxzGaNm1Ko0aNiIqKKtmLq8iMMbf9Uoc6xvj/y1pK0NGjR027du0MYNzc3MycOXNMenp6iZ5TqYrgwIEDZR1CgXTo0MG88sorjvWkpCQDmFWrVjnKGjVqZIYNG5ajbmRkpHFzczPHjh0zxhgzcuRI07hxY5OamnpTMWzbts08+OCDxtfX1/j5+TnO9dVXXxk3NzenfWfMmGE6d+7sWAfMvHnzTJs2bYy7u7vZsWOHqVy5srlw4YJjn4yMDBMSEmI+/PBDY4wx165dMxMnTjQhISHG19fXdO3a1cTExLiMbc6cOaZq1apGRIyHh4fp2LGjMcaY48ePmz/+8Y/Gz8/P1KtXz4wbN84kJyfnGteuXbtyHHvGjBmmU6dOZvz48aZGjRomMDDQ/OMf/8jx2bRr1854eXmZJk2amPDwcMe2S5cumX79+pkaNWoYLy8v07x5c7N9+3aX13Hu3DkDmLNnzzrKrl27Zh5//HFTq1Yt4+npacLCwszGjRsd2yMjI01oaKh57bXXTGBgoLnrrruMMcbExcWZESNGmHr16hl/f3/Tv39/c+7cOUe9efPmmSZNmhibzWaCgoLM3/72N3Pjxg2XcRVVcHCwiYiIcKwfOXLEAI7/k9n179/fjB8/3qksJCTEzJw50+X+69atM1WqVHH6/2SM1XY9e/YsWvDlVG4/24A9pphyoIrTo9eyprWUkI8//piwsDC+//57goOD2bFjB1OmTOGOOyrOR6yU+k1qairvvvsuAI0bNwbg8OHDHDlyxKlHI9PgwYMxxrBp0yYANmzYQP/+/W/q8Us//fQTXbt2ZeTIkcTGxnLq1CmGDh16U3EvXLiQjz76iKSkJNq2bUvr1q1ZtmyZY/u2bduIj4+nX79+AIwaNYpDhw7x7bffcu7cOe677z569OjhssdqypQphIeH07BhQ5KSkti6dSs3btzgscceo3bt2pw4cYJvv/2WnTt3MmnSpFzjCgsLcxn79u3bqVWrFrGxsaxdu5a5c+c6eoqOHTvGo48+yjPPPEN8fDwffPABzz//vGNo/fXXXyc5OZkTJ06QkJDAJ598Qr169VyeJzo6Gl9fX6f51hkZGfTp04eYmBji4+MZNGgQffv25eLFi459jh8/ztmzZ4mJiWH37t0YY+jduzciwv79+zlx4gSenp4MHjzYUadevXp88cUXJCYmsnbtWhYtWkRERESu7Td27Fh8fHxyXV599VWX9a5cucLJkydp06aNoyzzJsKffvrJZZ2MjAxMtrdrGWP4MfNNVNmEh4fTt2/fHL25d999N9HR0blekyqainPX7ZaBJXr4jz/+mCtXrtC7d28WLlxIjRo1SvR8SlVkM2VmqZ5vhpmR/052s2fP5o033uDq1atUrlyZiIgIx5ykzF/6gYGBOepVqVIFf39/x80JFy9edLlfXsLDw+nZsyfDhg1zlHXs2PGmjjFp0iRCQ0MBcHNzY/jw4YSHhzN+/HgAIiMjGThwINWrVycuLo6oqChOnDhBrVq1AJgxYwbz5s3ju+++48EHH8z3fN9//z0xMTF89913eHh44OHhwaxZs+jduzfz5893vC0oe1yu1KlTh6lTpyIitGnThqeffprIyEgGDRpEVFQU99xzD8OHDwfg/vvvZ/To0URERNC/f3+qVKlCfHw8v/zyC2FhYY7k3JXLly/j5eXlVGaz2ZwS+MmTJzNnzhx2795N9+7dAahcuTKvvvoqVatWBWDPnj3s3buXzZs3O8pee+01/P39OX36NPXq1aNv376OY4aFhfHkk0+yZcsWRo8e7TK2d955h3feeSf3DzwXiYmJAHh7ezuV+/j4OLZl17NnTyZOnMiQIUNo1aoVCxYs4OTJkzRq1CjHvqdOneKLL75g69atObZ5eXlx6dKlm45ZFYx2NxVB1r9kwsPDiYiI4JNPPtEkT6kK7IUXXiAhIYG4uDi6d+/u9IstsyfjzJkzOeqlpqYSFxfn2KdmzZou98vL8ePH80xQCiIkJMRpfdCgQRw+fJjo6GiuXr3Kxx9/zIgRIwCrlwygZcuWjh6jGjVqkJaWxqlTpwp0vlOnThEQEICHh4ejLDQ0lOvXrzv1hmWPy5X69es7vUYyJCTEMdfs1KlTNGzY0Gn/0NBQR5yTJ0+mc+fOPPXUU9SsWZOnnnqK8+fPuzyPr69vjuQnJSWF5557joYNG+Ll5YWPjw+XL192uoY6deo4EjqwPr9ff/2VWrVqOT6/0NBQ3N3dHfPcoqKiaNu2LX5+fnh7e/P22287HbO4eHp6AlbPXlYJCQk5ktpMQ4cOZfLkyfzpT3+idu3aREdH07lzZ/z9/XPs+/7779OkSRM6dOiQY1tiYqL+3ixBFadHr5gtX76cd955h02bNlGtWjW8vLwYOXJkWYelVIVwMz1sZcXX15eIiAhCQ0NZu3YtvXr14s4776Rhw4YsX748x92aK1asQER45JFHAOjevTurV69mxowZBR6+DQkJISYmxuU2m81Geno6v/76qyPZOHv2bI79sk838fHxoXfv3nzwwQe0atWK4OBg2rdvD1iJFUBMTEyhb64ICgriwoULJCcnU716dcC6YcPd3d0pYSjINJgTJ05gjHEke8ePH3cMvwYFBbFhwwan/Y8ePUpQUBAAHh4ezJ49m9mzZzveYjR58mQWL16c4zxhYWFcvnyZc+fOUbt2bQDmzp3L119/zZYtWwgJCUFE8Pf3d+oQyH4NmXdTX7p0yeX1nTp1iiFDhvDJJ5/QrVs3qlSpwqRJk9izZ0+un8EzzzzD0qVLc90+bdo0pk2blqPcx8eH4OBgoqOjad26tePzSUxMdLpLNisRYerUqUydOhWw/lhp2LAh06dPd9rvxo0bLFy4kClTprg8zv79+3MdjldFpz16Nyk5OZlRo0bxpz/9iZ07dzrNXVFKqaxq1KjBhAkTmDZtGhkZGYgI8+fPZ+nSpcyaNYtLly6RkpLC6tWrGT9+PFOnTqVBgwaA9diJpKQk+vXrx8GDB0lPT+fatWtERUXl+EWaafTo0Xz22WcsWbKE1NRUUlJSHI+7aNKkCTabjYiICDIyMvjmm29YvXp1ga5j+PDhLF++nPfee88x9AkQEBDA4MGDGTt2rKP3MSEhgTVr1pCUlFSgY7dr145GjRoxceJEkpOTOXv2LC+++CLDhw+/6TnOsbGxvP7666SlpfHDDz/w/vvv89RTTwFWz+TevXtZvHgxN27c4Pvvv2fBggWOP9DXrVvn+JxtNhvu7u5UquS6L6R27drcd999bN682VGWmJhI1apV8fPzIzU1lZdffpmEhIQ847333ntp3bo148aNIz4+HrCG7FesWAFAUlISGRkZ1KxZk8qVK/Ptt9+yZMmSPI8ZHh7udKdr9sVVkpfp6aefZs6cORw7dozExESmTp1K165dc+1NvXLlCgcPHsQYw8WLFxkzZgxeXl6OzzzTunXruHz5cq7zRTdt2kTv3r3zvC5VBMV1V8etvNShTl43vRTYzz//bJo3b24A4+7ubhYsWGAyMjKK5dhKqdyV17tujTHmypUrxtfX10RGRjrKduzYYR555BHj6elpqlevblq3bm0WLlyY43hnz541o0ePNvXq1TPVq1c3QUFBZsCAASY6OjrXGLZs2WLat29vvL29jb+/vxkxYoRj26pVq0yDBg2MzWYz/fr1M+PHj89x1+2OHTtyHDM9Pd0EBQUZNzc3Exsb67Tt2rVr5oUXXjCNGjUyNpvN1KtXzwwaNMgkJSW5jC/z7tOsjh49anr06GH8/PxMYGCgee6558y1a9fyjSurGTNmmI4dOzruuq1bt66ZPXu208/orVu3mrZt2xovLy/TuHFjM3/+fMe2t956y4SGhprq1asbPz8/069fP3P+/PlczxcVFWU6dOjgWD937pzp0qWL8fDwMIGBgeb11183oaGhjnZ3dd3GGBMfH2/Gjh1r6tevb2w2m2nQoIEZPXq0Y/vMmTONv7+/8fLyMr169TLjxo1zOm9xunHjhpk4caLx8/MzNpvNPP744+bixYuO7UuXLjUeHh6O9ZMnT5rmzZsbDw8P4+vra4YMGeJ0x3Cmrl27urzT3BhjDh06ZAICApzusq5ISuOuWzHZ7pi5HdWVuubsX5fC3E6Fqm+M4YMPPuAvf/kLKSkpNG3alI8++ijX7mylVPE6ePAgzZo1K+sw1C3spZde4ptvvnHqZStJxlhvxpg9ezadOhXud4uyelo7d+7MqFGjyjqUMpHbzzYR2WuMubc4zlFx5ugtOVDoRG/r1q2OycdDhw7l7bffxmazFWd0SimlyhERYdeuXWUdRrmnD0oueRUn0SuCTp06MWLECB566KEccw+UUkoppW5Vmui5YIxhwYIFdOzYkSZNmiAi+tJlpZS6hb300ktlHYJSt6SKc9ftmw8XaLeEhAQGDBjAmDFjGDBgwC35PkKllFJKqYKoOD16Q1vku8vu3bsZOHAgx44dw9PTk2nTpt3U64eUUkoppW4lFadHLw/GGN566y1+97vfcezYMdq0aUN0dDQDB5bsa9OUUgVXEZ4QoJSqODIyMkrlPBU+0TPG8MQTTzBhwgTS0tIYN24cO3fudPmuPqVU2XB3dyc+Pl6TPaVUuWeMITU1lTNnzji9+q+kVJyh21yICJ06dWLjxo1ERkbq07mVugXVq1eP06dPl8g7PpVSqrRVqlQJb29vl+8FLm6l/sBkEbkL+BfQHkgAIoCZxpj0fOp5A/OA3lg9kZ8D/2OMic/vnHWlrjlrfnunY0ZGBgcOHKBFC2venrG/viUgIKBwF6WUUkopVUyK84HJpTp0KyK+wGbAAL2Al4GJwMwCVP8IeBgYBQwD2gKf3mwM58+f59FHH+X+++/n8OHDmXFpkqeUUkqp205pD90+A1QD+hhjEoFNIuIFvCQir9nLchCR9kBXoIMxZru97AzwnYh0Mcbk/86bmvPZsqIZQ4YM4dy5c/j7+xMbG0vjxo2L69qUUkoppW4ppX0zRjfg39kSuhVYyV+HfOqdz0zyAIwx3wPH7Nvy9f+ureeRRx7h3LlzdOjQgX379tGhQ16nVEoppZQq30o70WsKHMpaYIw5CSTbtxW4nt3BfOoBEE88r6R8CcCMGTPYsmULdevWLWjMSimllFLlUmkP3fpi3YCR3WX7tsLUa5jfSVNJpbZ4sWzzGjp16lSgQJVSSimlyruyeLyKq9t8JZfyQtcTkaeBp+2rv54zifs7d+5c4CDVLcUfiCvrIFShaNuVb9p+5Ze2XfnWpLgOVNqJ3mXAx0W5N6577LLWq+mi3Ce3esaY94D3AERkT3HdpqxKn7Zf+aVtV75p+5Vf2nblm4jsKa5jlfYcvUNkm1MnZU6sqAAADhlJREFUIkGAB67n4OVazy63uXtKKaWUUhVeaSd6XwBdRcQzS9lAIAX4Op96tUXkwcwCEbkXa37eFyURqFJKKaVUeVfaiV448CvwiYh0sc+jewmYm/WRKyJyREQWZq4bY3YB/wYWi0gfEekNLAO+KdAz9OxDuKrc0vYrv7Ttyjdtv/JL2658K7b2K6tXoM3H+RVoL2V9BZqIHAe2GWOGZSnzAd4CHsf5FWg62VQppZRSyoVST/SUUkoppVTpKO2h22IlIneJyBYRSRaRsyLysoi4FaCet4hEishlEbkiIstExK80Yla/KUz7iUhbe9sdsdf7RURmiIh7acWtCv+9l6X+HSKyV0SMiPQoyVhVTkVpP/v0md0ikiIi8SLypYh4lHTM6jdF+N13r4hstLfbJRHZLCL3lUbMyiIijURkgYjsE5F0EdlWwHqFzlvK4jl6xUJEfIHNwAGgFxAKvImVvE7Pp/pHWM+oGQVkAHOAT4Hfl1S8ylkR2m+gfd85QAzQEnjF/rVvCYas7Ir4vZdpFBBYIgGqPBWl/URkFNbUm9eAyVgPs+9EOf5dUt4Utv3sT7jYDEQDQ+3Fk4GNItLSGHOiJONWDs2B7sC3QJWbqFf4vMUYUy4X4Hms5+t5ZSmbgvU6Na886rXHesjyQ1nK2tnLupT1dVWUpQjtV9NF2dP29qtf1tdVEZbCtl2WfX2Bi8BIe7v1KOtrqkhLEb73/IGrwJ/L+hoq8lKE9nsGSAd8spT52svGlPV1VZQFuCPLv1dj3Y+QX50i5S3leei2G/Bvk+VuXWAFUA3okE+988aY7ZkFxpjvgWP2bap0FKr9jDEXXRT/YP8aUHzhqTwU9nsv0yvATmBLCcSm8lfY9htg//phSQWmCqSw7VcZuAEkZSlLspdJcQepXDPGZBSiWpHylvKc6OV4WLIx5iTWXzWuHq6caz27g/nUU8WrsO3nygNYXdm/FE9oKh+FbjsRaQkMByaVWHQqP4Vtv/uwvsdGishpEUkTke9E5IGSC1W5UNj2+9i+z5siEiAiAVhPsrgMrCqhWFXxKFLeUp4TPV9cv/7ssn1bcddTxatY2kFEagMvAEuy/YWrSk5R2u5fwNvGmCPFHpUqqMK2X22sOULTgalAT+Aa8KWI1CruIFWuCtV+xpizQEesuczn7UsfoGsuIyXq1lGk35flOdEDa3w6O8mlvDjqqeJVpHYQkSrASqzhh78WY1wqfzfddiLyBFaiMKukglIFVpjvvTsAGzDSGLPMGPMl0BtrjtezxR+iykNhvv/qYM0J24s13NfN/u/1IhJcEkGqYlXo35flOdG7DPi4KPfGdeabXz2ffOqp4lXY9gNARARYjP0OJmPM5eINT+XhpttORCoDr2PdKXaH/QHoXvbNHtlei6hKVmG/9y7Zv27LLLD3ou8F7iqu4FS+Ctt+k7Huju5njPnSnqj3xUrUdSrFra1IeUt5TvQOkW1s2n77uAeux7JzrWeX2xi4KhmFbb9Mb2E9WqCXMUbbrXQVpu08gHrAXKwfWpeBffZtK/jthhpV8gr7vXcQq/cg+8R9wZojq0pHYduvKfCzMSYts8AYkwr8jPWIFnXrKlLeUp4TvS+Artl6AgYCKcDX+dSrLSIPZhaIyL1AQ/s2VToK236IyPPAc8AQY8w3JReiykVh2i4Ja35Q1mWQfds04E8lE6pyobDfe59jJXUdMwtExBtow29Juyp5hW2/E0AL+5QXAESkKtACOF4CcariU7S8payfKVOEZ9H4ArHAJqAL1rPUkoBZ2fY7AizMVvYlcBRrImpvrDvJdpT1NVWkpbDtBwzG6lWIBO7PtuR4xp4ut07buThOCPocvXLVflgPaI0FngIew0osLgK+ZX1dFWUpws/ONkAasN7edj3sSUIa0Kqsr6uiLEB1oJ992YXVo5q5Xt1V29nLCp23lPlFF/EDuwvYivWXTCzW87ncsu1zHPggW5mPPVFIABKB5YB/WV9PRVsK037AB/bkwNUyrKyvqaIshf3ey7ZdE71y1n5YN2O8C8Tb624G7i7r66loSxHarzOwHWu+5SWsRP3hsr6eirRk+bnnagnJo+0KnbeI/QBKKaWUUuo2U57n6CmllFJKqTxooqeUUkopdZvSRE8ppZRS6jaliZ5SSiml1G1KEz2llFJKqduUJnpKKaWUUrcpTfSUUvkSkZdExLhYNt/kcb4RkRUlFWeW88zKFucZEVklIg1L4Dznsqw3tX9WXtn2G2WPw704z59LTI2yXftVEflRREYU8nhPiMjQ4o5TKVU6KpV1AEqpcuMK8KiLslvVJaw3AID1Ls9ZwGYRaWGMSS6mc4QDn2RZbwrMACKwHmqaaS2wH/i1mM5bEH8FvgW8sN5ksVBEko0xN5toP4H1oOTFxRyfUqoUaKKnlCqoG8aYb8s6iJuQliXeb0XkDPAV0BVYUxwnMMacBk4XYL+LWK8KK02HMq/f3vN6LzAUKPEeVaXUrUOHbpVSxUJEJovIHhFJFJHzIrJWRELzqRMsIqtF5KKIpIjIERF5Kds+HURku4gki0i8iCwQEVshQtxr/xqS5dhPiMh+EflVRE6KyMsi4pZlu6+ILBKRWBG5LiInRCQ8y3bH0K2IdOG3BPKUfdj0iH2bY+hWLKdE5O8uPo9PReSrLOt+IvK+iFywn/8bEWl7sxdujMnA6lEMyna+4SKyU0Qu2ZctInJPlu1LgV5A5yxDwdOzbO8jInvtscWKyKsioh0ISt1C9BtSKVVgLn6Jp5vf3qNYD/hf4CTgDYwBvhGRxsaYq7kccingBozCGupsCNyZ5XwPYb28/WPgH0AA8Kr9+E/cZPgh9q+ZiVl3IArr/ZGTgNbAy0AN4Fn7vv/E6gkbB5zHSpQezOX43wNTgTnAH7F68K5n38kYY0RkJTAQmJblWr2whsbH29fdsd5n6gFMtB/vL1jDz3caYy7c5PUHA8eyldXHen/0UaAKMATYISJ3GWNOYA1DBwHVgP+x1zllj28wsATr3bfPY7XbP+z7/O0mY1NKlZSyfsGvLrrocusvwEu4fgl3l1z2dwOqA9eAwVnKvwFWZFm/DnTL47y7gE3Zyv4AZABN86g3Cyuhq2RfmmC9zP0KUMu+zx4Xx54G3ADq2NcPAWPyO0+W9d72z6Vetv1G2cvd7ett7ev3ZtnnSSAN+4vKgdH2z6dhln2qYL3w/B95xNTIfuzu9muvgZUoXgd+l0e9O+z7HwGmZSn/FNjsYt/TwPvZyp8GkgHfsv4/q4suuliLDt0qpQrqClaCknX5LnOjiDwgIptFJB4rWbqGlew1zuOYPwJzROQpEck+rGgD7gNWikilzAUrYcsA2uQTby2sxCkNK2ELAvobY86LSGWsHrxV2ep8hJWk3p8lvqkiMkZE7qSYGGN2Y/WiDcxSPBDYaoyJs693AXYDJ7NcewbW9d9bgNOsx7r2eOANYIIxZmfWHUSkuX24+DyQbt8/lLzbDKAZEEjOttmK1ft3VwHiU0qVAk30lFIFdcMYsyfbchVARBoA/8ZKFp4GfoeVCF4C8nqkSD+sZOqfWAlNtIh0tG/zAwR4j98StjQgBSsZC8p5OCfx9hjuBQKNMQ2MMRvt2wLsxzifrU7meg371zHA51g9modF5LCI9M/nvAX1ETDAPmfPF6unMuuNEv5Yw8Rp2ZYnyf/awRpqbQv0wErI3xKRFpkbRcQb2AjUxbpD9/f2/feTd5tlxoa9ftbYYuzlBYlPKVUKdI6eUqo4dAOqAr2NMSkAIlIF8MmrkrHuWh1qvwGiHdYcuc/svXuX7btNx0oiszuTT0w3jDF7ctl2ASspDchWXsv+9ZI9vsvAsyLyHNASaw5elIj8ZIz5JZ/z5+cjrLlt92P1kBmc7wa+hPV4lOdc1M0x98+FmMzrF5FdWEOy/wB62rf/DivJ62CMOZJZSUTybLMssQGMAP7rYvvRAhxDKVUKNNFTShWHaliJ040sZU9QwFEDY0w6sEtEXsYamgw2xvwkIruBxsaY2cUZrDEmTUR+APoD72fZNADrOr7Ntr8B9onIVGAQ1pw/V4leqv1rvg9GNsbsE5FDWEO2zYB/G2MSsuyyBXgFOJ5lOLdQjDGXROR1YLaINDfG/IzVZpDl2X72m1/qZaueSs7rOYA1BzLEGBNZlNiUUiVLEz2lVHHYArwGRIpIJHA31nBgYm4VRMQPWId15+ZhrMRjEnCW35KoKcBGEQHrztskrDtFHwOmGmP+rwgxzwDWi0gE1ly9VlhDtOHGmFh7jLuAlcDPWMPITwNXsebOuXLI/nWM/c7aa8aY/XnE8BEwFvAFhmXbFol1Q8Y2EXkTq5fMH6sH8JQx5n8LfKWWt7E+z0nAcOA/WDdORIjIG1h35c7A+vyzX1N3EemF1Yt6xhgTKyKTsNrbB6vHNQ3rrunHgV7GmNJ8OLRSKhc6R08pVWTGmB+BkcADWHPaBgB9sZKi3CRj9QyNx0r4IrESwz9kJgnGmG1AB6A21qNY1gGTgRMU8QHExpgNwGCsxGkd1py217AepZJpF9bw5CdY8+d8se4Sjs3lmEexhnf7Azux7ljNywqgJlaStDbbsVKwrv0rrJ69TVhzGRtgPcrlphhjEoF/AYNFJNB+Df2x5tNlXv/T5HwEy3xgM9ZjWHZjtTPGmGVYSV0brET5Y+AZe2xpNxufUqpkiDUioZRSSimlbjfao6eUUkopdZvSRE8ppZRS6jaliZ5SSiml1G1KEz2llFJKqduUJnpKKaWUUrcpTfSUUkoppW5TmugppZRSSt2mNNFTSimllLpNaaKnlFJKKXWb+v9HaVzbhIiW7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.roc_curves(to_categorical(y_true), y_pred.numpy(), info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Making new prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: This classic has so many great one liners and unintentionally hilarious scenes that I don't even know where to start. If you want advice on dating, its here. Just totally ignore the person you want, and then spout out classic lines like \"Chicken's good...I like Chicken\", and before you know it you will be having a one nighter in a basement (it's a NICE basement) with a woman who is 35 years younger than you. Bronson does it all in this film. He buys a car for no good reason just so he can murder two gang members...paying with \"CASH\"......chunnng.... He buys an ice cream, simply because \"this is America, isn't it\", and ends up wasting someone named \"the giggler   he laughs when he runs\" just because he stole his camera. By the way, this \"giggler\" is so fast that Bronson's regular pistol can't even catch up to him, he needs to order a special one just to get this elusive creep. He gets cleaned up just so he can eat a REALLY smelly meal (stuffed cabbage) in a rat trap with a couple of old people who like to wear heavy clothing in 90 degree weather. He goes into the dentistry business. He always seems to find a crow bar when he needs one (and its the same one!). And last, but not least, he always seems to have a rocket launcher at his disposal just in case he needs to blow away Richie Cunningham's older brother Chuck who is now strung out and in dire need of a makeover. Anyway, this will all make sense once you have seen this classic...all I can say is enjoy! \"I owed you that one DUDE\"\n",
      "label:    pos\n"
     ]
    }
   ],
   "source": [
    "# getting a random example from the test data\n",
    "for i in valid_data_np.shuffle(100).take(1):\n",
    "    example = i['sentence']\n",
    "    print('sentence: {}\\nlabel:    {}'.format(i['sentence'].numpy().decode(), info.features[\"label\"].names[i['label'].numpy()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce classique a tant de grandes scènes et une liners involontairement hilarantes que je ne sais même pas par où commencer. Si vous voulez des conseils sur la datation, son ici. Il suffit de ne pas tenir compte tout à fait la personne que vous voulez, et le bec puis des lignes classiques comme « le bon de poulet ... Je comme le poulet », et avant que vous le savez vous allez avoir une nuit blanche dans un sous-sol (il est un sous-sol de NICE) avec une femme qui est 35 ans plus jeune que vous. Bronson fait tout dans ce film. Il achète une voiture sans raison juste pour qu'il puisse assassiner deux membres de gangs ... de payer avec « CASH » ...... chunnng .... Il achète une crème glacée, tout simplement parce que «c'est l'Amérique, ISN » t il » et finit par quelqu'un débilitante nommé « le Giggler il rit quand il court » juste parce qu'il a volé sa caméra. Soit dit en passant, ce « Giggler » est si rapide que le pistolet régulier de Bronson ne peut même rattraper lui, il a besoin de commander un spécial juste pour obtenir ce fluage insaisissable. Il se nettoie juste pour qu'il puisse manger un repas très mauvais (chou farci) dans un piège à rat avec un couple de personnes âgées qui aiment porter des vêtements lourds dans 90 degrés. Il va dans les affaires de la dentisterie. Il semble toujours trouver un bar crow quand il a besoin d'un (et son le même!). Et enfin, mais pas moins, il semble toujours avoir un lance-roquettes à sa disposition au cas où il a besoin de souffler le frère aîné de Richie Cunningham Chuck qui est maintenant égrènent et en grand besoin d'une cure de jouvence. Quoi qu'il en soit, tout cela va du sens une fois que vous avez vu ce classique ... tout ce que je peux dire est profiter! « Je vous devais que l'on Dude » \n",
      " Dieses klassische hat so viele große Einzeiler und unbeabsichtigt urkomisch Szenen, die ich weiß nicht einmal, wo ich anfangen soll. Wenn Sie wollen Beratung über Dating, seine hier. Nur völlig ignorieren die Person, die Sie wollen, und dann klassische Linien wie auszustoßen „Chicken ist gut ... Ich mag Chicken“, und bevor Sie es wissen, werden Sie ein ein nighter in einem Keller mit (es ist ein schöner Keller) mit einer Frau die ist 35 Jahre jünger als Sie. Bronson macht alles in diesem Film. Er kauft ein Auto ohne guten Grund nur so er zwei Bandenmitglieder ermorden kann ... mit „BAR“ zu zahlen ...... chunnng .... Er kauft ein Eis, einfach weil „dies ist Amerika, isn‘ t it“, und Enden verschwenden jemand mit dem Namen‚die giggler er lacht, wenn er läuft‘, nur weil er seine Kamera gestohlen hat. By the way, diese „giggler“ so schnell ist, dass Bronson regelmäßigen Pistole nicht einmal auf ihn aufholen kann, braucht er nur ein besonderes zu bestellen diese schwer fassbare Kriechen zu bekommen. Er wird aufgeräumt, nur damit er essen wirklich stinkende Essen (Kohlrouladen) in einer Rattenfalle mit ein paar alten Menschen, die schwere Kleidung in 90-Grad-Wetter tragen mögen. Er geht in die Zahnmedizin-Geschäft. Er scheint immer einen Kuhfuß zu finden, wenn er braucht (und seine das gleiche!). Und last, but not least, er scheint immer einen Raketenwerfer haben zu seiner Verfügung für den Fall, er muss Richie Cunningham älterer Bruder wegzublasen Chuck, der jetzt aufgereiht und dringend eine Renovierung. Auf jeden Fall wird das alles Sinn machen, wenn Sie diese klassischen gesehen hat ... alles, was ich sagen kann ist, genießen! „Ich schuldete dir das GECK“\n"
     ]
    }
   ],
   "source": [
    "# conda install googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "# translate some example in some languages\n",
    "# original entry:\n",
    "# Role reversal remake of 1942's \"The Major and the Minor\" has Jerry Lewis stepping into the part originally played by Ginger Rogers, but unfortunately \n",
    "# this anemic outing is missing a lot more than just Ginger. Lewis attempts to pass for a child when boarding a train; he's successful, but the deception \n",
    "# leads to a string of comic and romantic confusions. Sidney Sheldon adapted the screenplay, tossing in musical moments for Dean Martin (playing yet \n",
    "# another in his stable of second bananas) and a jewel robbery subplot (which is dire). Diana Lynn, who played the wily teenager in the original film, \n",
    "# plays Lewis' love interest here. She's cute; Jerry isn't. *1 2 from ****\n",
    "#text_en = [\"Role reversal remake of 1942's 'The Major and the Minor' has Jerry Lewis stepping into the part originally played by Ginger Rogers, but unfortunately this anemic outing is missing a lot more than just Ginger. Lewis attempts to pass for a child when boarding a train; he's successful, but the deception leads to a string of comic and romantic confusions. Sidney Sheldon adapted the screenplay, tossing in musical moments for Dean Martin (playing yet another in his stable of second bananas) and a jewel robbery subplot (which is dire). Diana Lynn, who played the wily teenager in the original film, plays Lewis' love interest here. She's cute; Jerry isn't. *1 2 from ****\"]\n",
    "#text_de = [\"Das Rollentausch-Remake von 'The Major and the Minor' aus dem Jahr 1942 lässt Jerry Lewis in die Rolle von Ginger Rogers eintreten, aber leider fehlt diesem anämischen Ausflug viel mehr als nur Ginger. Lewis versucht beim Einsteigen in einen Zug als Kind zu gelten. Er ist erfolgreich, aber die Täuschung führt zu einer Reihe von komischen und romantischen Verwirrungen. Sidney Sheldon adaptierte das Drehbuch und warf musikalische Momente für Dean Martin (der noch einen weiteren in seinem Stall mit zweiten Bananen spielt) und eine Nebenhandlung über Juwelenraub (was schrecklich ist) ein. Diana Lynn, die im Originalfilm den schlauen Teenager spielte, spielt hier Lewis' Liebesinteresse. Sie ist süß; Jerry aber nicht. * 1 2 von ****\"]\n",
    "#text_fr = [\"Le remake d'inversion des rôles de 'The Major and the Minor' de 1942 a Jerry Lewis entrer dans le rôle initialement joué par Ginger Rogers, mais malheureusement cette sortie anémique manque beaucoup plus que juste Ginger. Lewis tente de passer pour un enfant à bord d'un train; il a réussi, mais la tromperie mène à une chaîne de confusions comiques et romantiques. Sidney Sheldon a adapté le scénario, jetant des moments musicaux pour Dean Martin (jouant encore un autre dans son écurie de secondes bananes) et une intrigue secondaire de vol de bijoux (ce qui est terrible). Diana Lynn, qui a joué l'adolescente astucieuse dans le film original, joue l'intérêt amoureux de Lewis ici. Elle est mignonne; Jerry ne l'est pas. * 1 2 de ****\"]\n",
    "translator = Translator()\n",
    "text_en = example.numpy().decode()\n",
    "#test = translator.translate(\"Hello, this is a test.\", dest='fr')\n",
    "#print(test.text)\n",
    "#print(text_en)\n",
    "text_fr = translator.translate(text_en, dest='fr').text\n",
    "text_de = translator.translate(text_en, dest='de').text\n",
    "print(text_fr, '\\n', text_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ce classique a tellement de super doublures et de scènes involontairement hilarantes que je ne sais même pas par où commencer. Si vous voulez des conseils sur les rencontres, c&#39;est ici. Ignorez totalement la personne que vous voulez, puis lancez des lignes classiques comme &quot;Chicken&#39;s good ... I like Chicken&quot;, et avant de vous en rendre compte, vous aurez une nuit dans un sous-sol (c&#39;est un joli sous-sol) avec une femme qui a 35 ans de moins que toi. Bronson fait tout dans ce film. Il achète une voiture sans raison valable juste pour pouvoir assassiner deux membres de gangs ... en payant avec &quot;CASH&quot; ...... chunnng .... Il achète une glace, simplement parce que &quot;c&#39;est l&#39;Amérique, n&#39;est-ce pas&quot; t it &quot;, et finit par gaspiller quelqu&#39;un nommé&quot; le fou rire qu&#39;il rit quand il court &quot;juste parce qu&#39;il a volé son appareil photo. Soit dit en passant, ce &quot;glousseur&quot; est si rapide que le pistolet ordinaire de Bronson ne peut même pas le rattraper, il doit en commander un spécial juste pour obtenir ce fluage insaisissable. Il est nettoyé juste pour pouvoir manger un repas VRAIMENT malodorant (chou farci) dans un piège à rats avec quelques personnes âgées qui aiment porter des vêtements lourds par temps de 90 degrés. Il se lance dans le domaine de la dentisterie. Il semble toujours trouver un corbeau quand il en a besoin (et c&#39;est le même!). Enfin et surtout, il semble toujours avoir un lance-roquettes à sa disposition au cas où il aurait besoin de faire exploser le frère aîné de Richie Cunningham, Chuck, qui est maintenant enfermé et a désespérément besoin d&#39;une cure de jouvence. Quoi qu&#39;il en soit, tout cela aura du sens une fois que vous aurez vu ce classique ... tout ce que je peux dire c&#39;est en profiter! &quot;Je te devais ce mec&quot; \n",
      " Dieser Klassiker hat so viele großartige Einzeiler und ungewollt witzige Szenen, dass ich nicht einmal weiß, wo ich anfangen soll. Wenn Sie Ratschläge zum Dating wünschen, finden Sie diese hier. Ignorieren Sie einfach die Person, die Sie wollen, und sprechen Sie dann klassische Zeilen wie &quot;Huhn ist gut ... ich mag Huhn&quot; aus, und bevor Sie es wissen, haben Sie eine Nacht in einem Keller (es ist ein schöner Keller) mit einer Frau Wer ist 35 Jahre jünger als Sie. Bronson macht alles in diesem Film. Er kauft ein Auto ohne guten Grund, nur damit er zwei Gangmitglieder ermorden kann ... mit &quot;CASH&quot; bezahlen ... chunnng ... Er kauft ein Eis, einfach weil &quot;das ist Amerika, nicht wahr&quot; t it &quot;, und am Ende verschwendet jemand namens&quot; der Giggler, den er lacht, wenn er rennt &quot;, nur weil er seine Kamera gestohlen hat. Übrigens ist dieser &quot;Giggler&quot; so schnell, dass Bronsons normale Pistole ihn nicht einmal einholen kann. Er muss eine spezielle bestellen, um dieses schwer fassbare Kriechen zu bekommen. Er wird aufgeräumt, nur damit er mit ein paar alten Leuten, die bei 90-Grad-Wetter gerne schwere Kleidung tragen, eine WIRKLICH stinkende Mahlzeit (Kohlrouladen) in einer Rattenfalle essen kann. Er steigt in die Zahnmedizin ein. Er scheint immer eine Brechstange zu finden, wenn er eine braucht (und es ist dieselbe!). Und zu guter Letzt scheint er immer einen Raketenwerfer zur Verfügung zu haben, nur für den Fall, dass er Richie Cunninghams älteren Bruder Chuck wegblasen muss, der jetzt aufgereiht ist und dringend eine Überarbeitung braucht. Wie auch immer, das alles wird Sinn machen, wenn Sie diesen Klassiker gesehen haben ... alles was ich sagen kann ist genießen! &quot;Ich schuldete dir diesen einen DUDE&quot;\n"
     ]
    }
   ],
   "source": [
    "# conda install google-cloud-translate\n",
    "from google.cloud import translate_v2 as translate\n",
    "translate_client = translate.Client()\n",
    "\n",
    "#result = translate_client.translate(\n",
    "#    text_en, target_language='de')\n",
    "\n",
    "#print(u'Text: {}'.format(result['input']))\n",
    "#print(u'Translation: {}'.format(result['translatedText']))\n",
    "#print(u'Detected source language: {}'.format(\n",
    "#    result['detectedSourceLanguage']))\n",
    "\n",
    "text_fr = translate_client.translate(text_en, target_language='fr')['translatedText']\n",
    "text_de = translate_client.translate(text_en, target_language='de')['translatedText']\n",
    "\n",
    "print(text_fr, '\\n', text_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def made_prediction(text, max_length):\n",
    "    print('example of input:\\n\\n{}\\n \\nlength:{}\\n'.format(text[0], len(text)))\n",
    "    #print(‘text:{} length:{}\\n’.format(text, len(text)))\n",
    "    # get probablility for each classes\n",
    "    tokens=tokenizer.batch_encode_plus(text, return_tensors=\"tf\", pad_to_max_length=True, max_length=max_length)\n",
    "    digits=model.predict(tokens)\n",
    "    if model.name=='custom_tf_bert_classification':\n",
    "        print('custom model: {}'.format(model.name))\n",
    "        y_single_pred = tf.nn.softmax(digits)\n",
    "    elif model.name=='tf_bert_classification':\n",
    "        print('standard model: {}'.format(model.name))\n",
    "        temp=tf.nn.softmax(digits)\n",
    "        y_single_pred = tf.squeeze(temp)\n",
    "    return y_single_pred.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15, 2), dtype=float32, numpy=\n",
       "array([[0.5642907 , 0.43570933],\n",
       "       [0.5827053 , 0.4172947 ],\n",
       "       [0.45373687, 0.54626316],\n",
       "       [0.48502573, 0.51497424],\n",
       "       [0.48259225, 0.5174077 ],\n",
       "       [0.45373687, 0.54626316],\n",
       "       [0.48502573, 0.51497424],\n",
       "       [0.48259225, 0.5174077 ],\n",
       "       [0.49669424, 0.5033058 ],\n",
       "       [0.48259225, 0.5174077 ],\n",
       "       [0.5642907 , 0.43570933],\n",
       "       [0.4460985 , 0.55390143],\n",
       "       [0.48502576, 0.51497424],\n",
       "       [0.5642907 , 0.4357093 ],\n",
       "       [0.48759958, 0.5124004 ]], dtype=float32)>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens=tokenizer.batch_encode_plus(\"This is a test.\", return_tensors=\"tf\",  pad_to_max_length=True, max_length=SEQUENCE_LENGTH)\n",
    "digits = tf.nn.softmax(model.predict(tokens))\n",
    "digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "made_prediction() missing 1 required positional argument: 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-a349d2a78b6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_single_pred_en\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmade_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_single_pred_de\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmade_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_de\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_single_pred_fr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmade_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_fr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: made_prediction() missing 1 required positional argument: 'max_length'"
     ]
    }
   ],
   "source": [
    "y_single_pred_en=made_prediction(text_en)\n",
    "y_single_pred_de=made_prediction(text_de)\n",
    "y_single_pred_fr=made_prediction(text_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5642907 , 0.43570933],\n",
       "       [0.5827053 , 0.4172947 ],\n",
       "       [0.45373687, 0.54626316],\n",
       "       ...,\n",
       "       [0.58410066, 0.4158993 ],\n",
       "       [0.4460985 , 0.55390143],\n",
       "       [0.43300384, 0.5669962 ]], dtype=float32)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_single_pred_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.58410066, 0.4158993 ],\n",
       "       [0.45373687, 0.54626316],\n",
       "       [0.4460985 , 0.55390143],\n",
       "       ...,\n",
       "       [0.5686786 , 0.43132132],\n",
       "       [0.5642907 , 0.43570933],\n",
       "       [0.513251  , 0.48674905]], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_single_pred_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5537481 , 0.44625193],\n",
       "       [0.4460985 , 0.55390143],\n",
       "       [0.48259225, 0.5174077 ],\n",
       "       ...,\n",
       "       [0.5686786 , 0.43132132],\n",
       "       [0.5642907 , 0.43570933],\n",
       "       [0.513251  , 0.48674905]], dtype=float32)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_single_pred_fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-bb42a11be964>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_single_pred_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "info.features[\"label\"].names[tf.math.argmax(y_single_pred_en).numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "info.features[\"label\"].names[tf.math.argmax(y_single_pred_de).numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "info.features[\"label\"].names[tf.math.argmax(y_single_pred_fr).numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Model interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of input:\n",
      "\n",
      "T\n",
      " \n",
      "length:5000\n",
      "\n",
      "standard model: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "exp_en = explainer.explain_instance(text_en[0], made_prediction, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of input:\n",
      "\n",
      "D\n",
      " \n",
      "length:5000\n",
      "\n",
      "standard model: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "# will do 5'000 permutations/selections of the original sentence and made prediction\n",
    "exp_de = explainer.explain_instance(text_de[0], made_prediction, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example of input:\n",
      "\n",
      "C\n",
      " \n",
      "length:5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_fr = explainer.explain_instance(text_fr[0], made_prediction, num_features=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_en.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_de.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_fr.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "fig = exp_de.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_de.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_de.save_to_file('ex_de.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame    \n",
    "IFrame(src=\"ex_de.html\", width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<iframe src=ex_de.html width=900 style=\"background: #FFFFFF;\" height=350></iframe>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "fig = exp_en.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_en.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_en.save_to_file('ex_en.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame    \n",
    "IFrame(src=\"ex_en.html\", width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<iframe src=ex_en.html width=900 style=\"background: #FFFFFF;\" height=350></iframe>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "fig = exp_fr.as_pyplot_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_fr.show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "exp_fr.save_to_file('ex_fr.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame    \n",
    "IFrame(src=\"ex_fr.html\", width=900, height=350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML('<iframe src=ex_fr.html width=900 style=\"background: #FFFFFF;\" height=350></iframe>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Online Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#client = Client()\n",
    "#client.get_bucket('multilingual_text_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
