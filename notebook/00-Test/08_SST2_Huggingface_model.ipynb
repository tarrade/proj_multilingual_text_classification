{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model` \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    "    TFBertForSequenceClassification,\n",
    "    glue_convert_examples_to_features,\n",
    "    glue_processors\n",
    ")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    checkpoint_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import preprocessing.preprocessing as pp\n",
    "import utils.model_metrics as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);\n",
    "importlib.reload(mm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading a data from Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/home/fabien_tarrade/data/glue/sst2/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/fabien_tarrade/data/glue/sst2/1.0.0\n"
     ]
    }
   ],
   "source": [
    "data, info = tensorflow_datasets.load(name='glue/sst2',\n",
    "                                      data_dir=data_dir,\n",
    "                                      with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking baics info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    version=1.0.0,\n",
       "    description='GLUE, the General Language Understanding Evaluation benchmark\n",
       "(https://gluebenchmark.com/) is a collection of resources for training,\n",
       "evaluating, and analyzing natural language understanding systems.\n",
       "\n",
       "            The Stanford Sentiment Treebank consists of sentences from movie reviews and\n",
       "            human annotations of their sentiment. The task is to predict the sentiment of a\n",
       "            given sentence. We use the two-way (positive/negative) class split, and use only\n",
       "            sentence-level labels.',\n",
       "    homepage='https://nlp.stanford.edu/sentiment/index.html',\n",
       "    features=FeaturesDict({\n",
       "        'idx': tf.int32,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'sentence': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=70042,\n",
       "    splits={\n",
       "        'test': 1821,\n",
       "        'train': 67349,\n",
       "        'validation': 872,\n",
       "    },\n",
       "    supervised_keys=None,\n",
       "    citation=\"\"\"@inproceedings{socher2013recursive,\n",
       "                  title={Recursive deep models for semantic compositionality over a sentiment treebank},\n",
       "                  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},\n",
       "                  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},\n",
       "                  pages={1631--1642},\n",
       "                  year={2013}\n",
       "                }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "      ['negative', 'positive']\n",
      "\n",
      "Number of label:\n",
      "      2\n",
      "\n",
      "Structure of the data:\n",
      "      dict_keys(['sentence', 'label', 'idx'])\n",
      "\n",
      "Number of entries:\n",
      "   Train dataset: 67349\n",
      "   Test dataset:  1821\n",
      "   Valid dataset: 872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_dataset(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking baics info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'train': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'validation': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   {'idx': TensorShape([]), 'label': TensorShape([]), 'sentence': TensorShape([])}\n",
      "\n",
      "# Output types of one entry:\n",
      "   {'idx': tf.int32, 'label': tf.int64, 'sentence': tf.string}\n",
      "\n",
      "# Output typesof one entry:\n",
      "   {'idx': <class 'tensorflow.python.framework.ops.Tensor'>, 'label': <class 'tensorflow.python.framework.ops.Tensor'>, 'sentence': <class 'tensorflow.python.framework.ops.Tensor'>}\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (67349,)\n",
      "   ---> 67349 entries\n",
      "   ---> 1 dim\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [idx       / label     / sentence ]\n",
      "           [()        / ()        / ()       ]\n",
      "           [int32     / int64     / bytes    ]\n",
      "\n",
      "\n",
      "# Examples of data:\n",
      "{'idx': 16399,\n",
      " 'label': 0,\n",
      " 'sentence': b'for the uninitiated plays better on video with the sound '}\n",
      "{'idx': 1680,\n",
      " 'label': 0,\n",
      " 'sentence': b'like a giant commercial for universal studios , where much of th'\n",
      "             b'e action takes place '}\n",
      "{'idx': 47917,\n",
      " 'label': 1,\n",
      " 'sentence': b'company once again dazzle and delight us '}\n",
      "{'idx': 17307,\n",
      " 'label': 1,\n",
      " 'sentence': b\"'s no surprise that as a director washington demands and receive\"\n",
      "             b's excellent performances , from himself and from newcomer derek '\n",
      "             b'luke '}\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:           67349/  1821/   872\n",
      "Batch size:                32/    32/    64\n",
      "Step per epoch:          2105/    57/    29\n",
      "Total number of batch:   6315/   171/    87\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "BATCH_SIZE_VALID = 64\n",
    "EPOCH = 2\n",
    "\n",
    "# extract parameters\n",
    "size_train_dataset = info.splits['train'].num_examples\n",
    "size_test_dataset = info.splits['test'].num_examples\n",
    "size_valid_dataset = info.splits['validation'].num_examples\n",
    "number_label = info.features[\"label\"].num_classes\n",
    "\n",
    "# computer parameter\n",
    "STEP_EPOCH_TRAIN = math.ceil(size_train_dataset/BATCH_SIZE_TRAIN)\n",
    "STEP_EPOCH_TEST = math.ceil(size_test_dataset/BATCH_SIZE_TEST)\n",
    "STEP_EPOCH_VALID = math.ceil(size_test_dataset/BATCH_SIZE_VALID)\n",
    "\n",
    "\n",
    "print('Dataset size:          {:6}/{:6}/{:6}'.format(size_train_dataset, size_test_dataset, size_valid_dataset))\n",
    "print('Batch size:            {:6}/{:6}/{:6}'.format(BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, BATCH_SIZE_VALID))\n",
    "print('Step per epoch:        {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN, STEP_EPOCH_TEST, STEP_EPOCH_VALID))\n",
    "print('Total number of batch: {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN*(EPOCH+1), STEP_EPOCH_TEST*(EPOCH+1), STEP_EPOCH_VALID*(EPOCH+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tokenizer and prepare data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>\n",
      "tf.Tensor(67349, shape=(), dtype=int64)\n",
      "tf.Tensor(1821, shape=(), dtype=int64)\n",
      "tf.Tensor(872, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# recap of input dataset\n",
    "print(data['train'])\n",
    "print(tf.data.experimental.cardinality(data['train']))\n",
    "print(tf.data.experimental.cardinality(data['test']))\n",
    "print(tf.data.experimental.cardinality(data['validation']))\n",
    "# super slow since looping over all data\n",
    "#print(len(list(data['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare data for BERT\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')\n",
    "test_dataset = glue_convert_examples_to_features(data['test'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "67349\n"
     ]
    }
   ],
   "source": [
    "# recap of pre processing dataset\n",
    "print(train_dataset)\n",
    "print(tf.data.experimental.cardinality(train_dataset))\n",
    "print(tf.data.experimental.cardinality(test_dataset))\n",
    "print(tf.data.experimental.cardinality(valid_dataset))\n",
    "# super slow since looping over all data\n",
    "print(len(list(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# set shuffle and batch size\n",
    "train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE_TRAIN).repeat(EPOCH+1)\n",
    "test_dataset = test_dataset.shuffle(100).batch(BATCH_SIZE_TEST).repeat(EPOCH+1)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE_VALID) #.repeat(EPOCH+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   ({'input_ids': TensorShape([None, None]), 'attention_mask': TensorShape([None, None]), 'token_type_ids': TensorShape([None, None])}, TensorShape([None]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   ({'input_ids': <class 'tensorflow.python.framework.ops.Tensor'>, 'attention_mask': <class 'tensorflow.python.framework.ops.Tensor'>, 'token_type_ids': <class 'tensorflow.python.framework.ops.Tensor'>}, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (6315, 2)\n",
      "   ---> 6315 batches\n",
      "   ---> 2 dim\n",
      "        label\n",
      "           shape: (32,)\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [input_ids       / attention_mask  / token_type_ids ]\n",
      "           [(32, 128)       / (32, 128)       / (32, 128)      ]\n",
      "           [ndarray         / ndarray         / ndarray        ]\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(train_dataset,print_example=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_ids     ---->    attention_mask    token_type_ids    modified text                 \n",
      "\n",
      "       101     ---->           1                 1          [ C L S ]                     \n",
      "     18027     ---->           1                 1          c o m p a s s i o n           \n",
      "       117     ---->           1                 1          ,                             \n",
      "      9050     ---->           1                 1          s a c r i f i c e             \n",
      "       117     ---->           1                 1          ,                             \n",
      "       102     ---->           1                 1          [ S E P ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n"
     ]
    }
   ],
   "source": [
    "pp.print_detail_tokeniser(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define the callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                         save_weights_only=True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Function for decaying the learning rate.\n",
    "def decay(epoch):\n",
    "    if epoch < 3:\n",
    "        return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "decay_callback = tf.keras.callbacks.LearningRateScheduler(decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Print learning rate at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200401-053154\n"
     ]
    }
   ],
   "source": [
    "# checking existing folders\n",
    "for i in os.listdir(tensorboard_dir):\n",
    "    if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200401-053154\n"
     ]
    }
   ],
   "source": [
    "# clean old TensorBoard directory \n",
    "for i in os.listdir(tensorboard_dir):\n",
    "        if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "            print(i)\n",
    "            shutil.rmtree(tensorboard_dir+'/'+i, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "log_dir=tensorboard_dir+'/'+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1, \n",
    "                                                      embeddings_freq=1,\n",
    "                                                      write_graph=True,\n",
    "                                                      update_freq='batch',\n",
    "                                                      profile_batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Loss and efficiency per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class History_per_step(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, validation_data, N):\n",
    "        self.validation_data = validation_data\n",
    "        self.N = N\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_train_begin(self, validation_data, logs={}):\n",
    "        self.steps = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.val_steps = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('accuracy'))\n",
    "        self.steps.append(self.batch)\n",
    "        print('\\n training set -> batch:{} loss:{} and acc: {}'.format(self.batch,logs.get('loss'),logs.get('accuracy')))\n",
    "        \n",
    "        if self.batch % self.N == 0:\n",
    "            loss_val, acc_val = self.model.evaluate(self.validation_data, verbose=0)\n",
    "            self.val_losses.append(loss_val)\n",
    "            self.val_accuracies.append(acc_val)\n",
    "            self.val_steps.append(self.batch)\n",
    "            print('\\n validation set -> batch:{} val loss:{} and val acc: {}'.format(self.batch,loss_val, acc_val))\n",
    "\n",
    "        self.batch += 1\n",
    "    \n",
    "    def on_test_batch_end(self, batch, logs={}):    \n",
    "        #print('{}\\n'.format(logs))\n",
    "        return\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}): \n",
    "        #print('{}\\n'.format(logs))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checks callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelCheckpoint need to unpack this tuple by adding *\n"
     ]
    }
   ],
   "source": [
    "list_callback = [tensorboard_callback, checkpoint_callback, decay_callback]\n",
    "for cb in list_callback:\n",
    "    if type(cb).__name__=='tuple':\n",
    "        print(cb[0].__class__.__name__, 'need to unpack this tuple by adding *')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Use TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "# Gradient clipping in the optimizer (by setting clipnorm or clipvalue) is currently unsupported when using a distribution strategy\n",
    "# clipnorm=1.0\n",
    "\n",
    "# loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Uses the tf.distribute.MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine\n",
    "strategy_model_1 = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy_model_1.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy_model_1.scope():\n",
    "    # metric\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    \n",
    "    # model\n",
    "    model_1 = TFBertForSequenceClassification.from_pretrained('bert-base-cased',num_labels=number_label)\n",
    "    #model.layers[-1].activation = tf.keras.activations.softmax\n",
    "    model_1._name='tf_bert_classification'\n",
    "    model_1.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Building a custom classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def custom_keras_model(number_classes, bert_model):\n",
    "\n",
    "    # create model\n",
    "    input_layer = tf.keras.Input(shape = (128,), dtype='int64')    \n",
    "    bert_ini = TFBertModel.from_pretrained('bert-base-cased') (input_layer)\n",
    "    # This is because in a bert pretraining progress, there are two tasks: \n",
    "    # masked token prediction and next sentence predition . \n",
    "    # The first needs hidden state of each tokens ( shape: [batch_size, sequence_length, hidden_size]) \n",
    "    # the second needs the embedding of the whole sequence (shape : [batch_size, hidden_size] ) .\n",
    "    bert = bert_ini[1]    \n",
    "    dropout = tf.keras.layers.Dropout(0.1)(bert)\n",
    "    flat = tf.keras.layers.Flatten()(dropout)\n",
    "    classifier = tf.keras.layers.Dense(units=number_classes )(flat) # activation='softmax'               \n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=classifier, name='custom_tf_bert_classification')\n",
    "\n",
    "    return model, bert_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "# Gradient clipping in the optimizer (by setting clipnorm or clipvalue) is currently unsupported when using a distribution strategy\n",
    "# clipnorm=1.0\n",
    "\n",
    "# loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Uses the tf.distribute.MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine\n",
    "strategy_model_2 = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy_model_1.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy_model_2.scope():\n",
    "    # metric\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    \n",
    "    # model\n",
    "    model_2, bert_ini = custom_keras_model(number_label, 'bert-base-cased')\n",
    "    model_2.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'tf_bert_model/Identity:0' shape=(None, 128, 768) dtype=float32>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ini[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'tf_bert_model/Identity_1:0' shape=(None, 768) dtype=float32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ini[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  ((None, 128, 768), (None, 108310272 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Choose the model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's name: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "model=model_1\n",
    "print('model\\'s name: {}'.format(model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9395ff9863b34305\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9395ff9863b34305\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir   {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Final feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def data_feature_extraction(data, name):\n",
    "    if name=='custom_tf_bert_classification':\n",
    "        print('custom model: {}'.format(name))\n",
    "        return data.map(pp.feature_selection)\n",
    "    elif name=='tf_bert_classification':\n",
    "        print('standard model: {}'.format(name))\n",
    "        return data\n",
    "    else:\n",
    "        print('!!! non defined model !!!!')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard model: tf_bert_classification\n",
      "standard model: tf_bert_classification\n",
      "\n",
      " training set -> batch:1 loss:0.731187105178833 and acc: 0.53125\n",
      "   1/2105 [..............................] - ETA: 0s - accuracy: 0.5312 - loss: 0.7312\n",
      " training set -> batch:2 loss:0.7289777994155884 and acc: 0.46875\n",
      "   2/2105 [..............................] - ETA: 48:08 - accuracy: 0.4688 - loss: 0.7290\n",
      " training set -> batch:3 loss:0.7262539863586426 and acc: 0.4583333432674408\n",
      "   3/2105 [..............................] - ETA: 1:02:59 - accuracy: 0.4583 - loss: 0.7263\n",
      " training set -> batch:4 loss:0.721787691116333 and acc: 0.453125\n",
      "   4/2105 [..............................] - ETA: 1:09:39 - accuracy: 0.4531 - loss: 0.7218\n",
      " training set -> batch:5 loss:0.7027885317802429 and acc: 0.518750011920929\n",
      "   5/2105 [..............................] - ETA: 1:14:09 - accuracy: 0.5188 - loss: 0.7028\n",
      " training set -> batch:6 loss:0.6978703141212463 and acc: 0.53125\n",
      "   6/2105 [..............................] - ETA: 1:17:14 - accuracy: 0.5312 - loss: 0.6979\n",
      " training set -> batch:7 loss:0.6907253265380859 and acc: 0.5446428656578064\n",
      "   7/2105 [..............................] - ETA: 1:19:27 - accuracy: 0.5446 - loss: 0.6907\n",
      " training set -> batch:8 loss:0.6867748498916626 and acc: 0.5546875\n",
      "   8/2105 [..............................] - ETA: 1:21:08 - accuracy: 0.5547 - loss: 0.6868\n",
      " training set -> batch:9 loss:0.6832570433616638 and acc: 0.5520833134651184\n",
      "   9/2105 [..............................] - ETA: 1:22:17 - accuracy: 0.5521 - loss: 0.6833\n",
      " training set -> batch:10 loss:0.6778634786605835 and acc: 0.5625\n",
      "\n",
      " validation set -> batch:10 val loss:0.6761344075202942 and val acc: 0.521789014339447\n",
      "  10/2105 [..............................] - ETA: 3:09:55 - accuracy: 0.5625 - loss: 0.6779\n",
      " training set -> batch:11 loss:0.6712081432342529 and acc: 0.5265486836433411\n",
      "  11/2105 [..............................] - ETA: 3:00:54 - accuracy: 0.5265 - loss: 0.6712\n",
      " training set -> batch:12 loss:0.67160964012146 and acc: 0.5277777910232544\n",
      "  12/2105 [..............................] - ETA: 2:53:14 - accuracy: 0.5278 - loss: 0.6716\n",
      " training set -> batch:13 loss:0.6708118319511414 and acc: 0.5289255976676941\n",
      "  13/2105 [..............................] - ETA: 2:46:41 - accuracy: 0.5289 - loss: 0.6708\n",
      " training set -> batch:14 loss:0.6642885208129883 and acc: 0.5360000133514404\n",
      "  14/2105 [..............................] - ETA: 2:41:01 - accuracy: 0.5360 - loss: 0.6643\n",
      " training set -> batch:15 loss:0.6599136590957642 and acc: 0.5416666865348816\n",
      "  15/2105 [..............................] - ETA: 2:36:07 - accuracy: 0.5417 - loss: 0.6599\n",
      " training set -> batch:16 loss:0.6567936539649963 and acc: 0.5451127886772156\n",
      "  16/2105 [..............................] - ETA: 2:31:57 - accuracy: 0.5451 - loss: 0.6568\n",
      " training set -> batch:17 loss:0.6559999585151672 and acc: 0.5465328693389893\n",
      "  17/2105 [..............................] - ETA: 2:28:24 - accuracy: 0.5465 - loss: 0.6560\n",
      " training set -> batch:18 loss:0.653960645198822 and acc: 0.5505319237709045\n",
      "  18/2105 [..............................] - ETA: 2:25:11 - accuracy: 0.5505 - loss: 0.6540\n",
      " training set -> batch:19 loss:0.6491506695747375 and acc: 0.556034505367279\n",
      "  19/2105 [..............................] - ETA: 2:22:14 - accuracy: 0.5560 - loss: 0.6492\n",
      " training set -> batch:20 loss:0.643718957901001 and acc: 0.563758373260498\n",
      "\n",
      " validation set -> batch:20 val loss:0.5185743570327759 and val acc: 0.8119266033172607\n",
      "  20/2105 [..............................] - ETA: 3:10:16 - accuracy: 0.5638 - loss: 0.6437\n",
      " training set -> batch:21 loss:0.5168948173522949 and acc: 0.8119469285011292\n",
      "  21/2105 [..............................] - ETA: 3:05:30 - accuracy: 0.8119 - loss: 0.5169\n",
      " training set -> batch:22 loss:0.5150555968284607 and acc: 0.8087607026100159\n",
      "  22/2105 [..............................] - ETA: 3:01:00 - accuracy: 0.8088 - loss: 0.5151\n",
      " training set -> batch:23 loss:0.5125730037689209 and acc: 0.8099173307418823\n",
      "  23/2105 [..............................] - ETA: 2:56:53 - accuracy: 0.8099 - loss: 0.5126\n",
      " training set -> batch:24 loss:0.5146044492721558 and acc: 0.8100000023841858\n",
      "  24/2105 [..............................] - ETA: 2:53:00 - accuracy: 0.8100 - loss: 0.5146\n",
      " training set -> batch:25 loss:0.513242781162262 and acc: 0.8081395626068115\n",
      "  25/2105 [..............................] - ETA: 2:49:26 - accuracy: 0.8081 - loss: 0.5132\n",
      " training set -> batch:26 loss:0.5137775540351868 and acc: 0.8101503849029541\n",
      "  26/2105 [..............................] - ETA: 2:46:10 - accuracy: 0.8102 - loss: 0.5138\n",
      " training set -> batch:27 loss:0.5129935145378113 and acc: 0.8093065619468689\n",
      "  27/2105 [..............................] - ETA: 2:43:12 - accuracy: 0.8093 - loss: 0.5130\n",
      " training set -> batch:28 loss:0.5125837922096252 and acc: 0.807624101638794\n",
      "  28/2105 [..............................] - ETA: 2:40:24 - accuracy: 0.8076 - loss: 0.5126\n",
      " training set -> batch:29 loss:0.5059766173362732 and acc: 0.8086206912994385\n",
      "  29/2105 [..............................] - ETA: 2:37:46 - accuracy: 0.8086 - loss: 0.5060\n",
      " training set -> batch:30 loss:0.5010983347892761 and acc: 0.8095637559890747\n",
      "\n",
      " validation set -> batch:30 val loss:0.40251556038856506 and val acc: 0.8543577790260315\n",
      "  30/2105 [..............................] - ETA: 3:08:46 - accuracy: 0.8096 - loss: 0.5011\n",
      " training set -> batch:31 loss:0.3990909457206726 and acc: 0.857300877571106\n",
      "  31/2105 [..............................] - ETA: 3:05:26 - accuracy: 0.8573 - loss: 0.3991\n",
      " training set -> batch:32 loss:0.3997829854488373 and acc: 0.8579059839248657\n",
      "  32/2105 [..............................] - ETA: 3:02:22 - accuracy: 0.8579 - loss: 0.3998\n",
      " training set -> batch:33 loss:0.4049012362957001 and acc: 0.8574380278587341\n",
      "  33/2105 [..............................] - ETA: 2:59:29 - accuracy: 0.8574 - loss: 0.4049\n",
      " training set -> batch:34 loss:0.40770161151885986 and acc: 0.8550000190734863\n",
      "  34/2105 [..............................] - ETA: 2:56:41 - accuracy: 0.8550 - loss: 0.4077\n",
      " training set -> batch:35 loss:0.41055476665496826 and acc: 0.8536821603775024\n",
      "  35/2105 [..............................] - ETA: 2:54:06 - accuracy: 0.8537 - loss: 0.4106\n",
      " training set -> batch:36 loss:0.4062100052833557 and acc: 0.8543233275413513\n",
      "  36/2105 [..............................] - ETA: 2:51:38 - accuracy: 0.8543 - loss: 0.4062\n",
      " training set -> batch:37 loss:0.4068875014781952 and acc: 0.8540145754814148\n",
      "  37/2105 [..............................] - ETA: 2:49:15 - accuracy: 0.8540 - loss: 0.4069\n",
      " training set -> batch:38 loss:0.41178369522094727 and acc: 0.8510638475418091\n",
      "  38/2105 [..............................] - ETA: 2:47:00 - accuracy: 0.8511 - loss: 0.4118\n",
      " training set -> batch:39 loss:0.4097490608692169 and acc: 0.8517241477966309\n",
      "  39/2105 [..............................] - ETA: 2:44:52 - accuracy: 0.8517 - loss: 0.4097\n",
      " training set -> batch:40 loss:0.4092889726161957 and acc: 0.8515100479125977\n",
      "\n",
      " validation set -> batch:40 val loss:0.3756301701068878 and val acc: 0.8451834917068481\n",
      "  40/2105 [..............................] - ETA: 3:09:44 - accuracy: 0.8515 - loss: 0.4093\n",
      " training set -> batch:41 loss:0.3700113892555237 and acc: 0.8473451137542725\n",
      "  41/2105 [..............................] - ETA: 3:07:13 - accuracy: 0.8473 - loss: 0.3700\n",
      " training set -> batch:42 loss:0.370402991771698 and acc: 0.8472222089767456\n",
      "  42/2105 [..............................] - ETA: 3:04:49 - accuracy: 0.8472 - loss: 0.3704\n",
      " training set -> batch:43 loss:0.3741503953933716 and acc: 0.8471074104309082\n",
      "  43/2105 [..............................] - ETA: 3:02:30 - accuracy: 0.8471 - loss: 0.3742\n",
      " training set -> batch:44 loss:0.37716618180274963 and acc: 0.8460000157356262\n",
      "  44/2105 [..............................] - ETA: 3:00:17 - accuracy: 0.8460 - loss: 0.3772\n",
      " training set -> batch:45 loss:0.3785105347633362 and acc: 0.8449612259864807\n",
      "  45/2105 [..............................] - ETA: 2:58:13 - accuracy: 0.8450 - loss: 0.3785\n",
      " training set -> batch:46 loss:0.37852662801742554 and acc: 0.8458646535873413\n",
      "  46/2105 [..............................] - ETA: 2:56:13 - accuracy: 0.8459 - loss: 0.3785\n",
      " training set -> batch:47 loss:0.3717914819717407 and acc: 0.8485401272773743\n",
      "  47/2105 [..............................] - ETA: 2:54:17 - accuracy: 0.8485 - loss: 0.3718\n",
      " training set -> batch:48 loss:0.3718707859516144 and acc: 0.8492907881736755\n",
      "  48/2105 [..............................] - ETA: 2:52:23 - accuracy: 0.8493 - loss: 0.3719\n",
      " training set -> batch:49 loss:0.3758036196231842 and acc: 0.8474137783050537\n",
      "  49/2105 [..............................] - ETA: 2:50:33 - accuracy: 0.8474 - loss: 0.3758\n",
      " training set -> batch:50 loss:0.37332355976104736 and acc: 0.8481543660163879\n",
      "\n",
      " validation set -> batch:50 val loss:0.3425641655921936 and val acc: 0.8589449524879456\n",
      "  50/2105 [..............................] - ETA: 3:09:48 - accuracy: 0.8482 - loss: 0.3733\n",
      " training set -> batch:51 loss:0.33590129017829895 and acc: 0.8606194853782654\n",
      "  51/2105 [..............................] - ETA: 3:07:48 - accuracy: 0.8606 - loss: 0.3359\n",
      " training set -> batch:52 loss:0.33626556396484375 and acc: 0.8611111044883728\n",
      "  52/2105 [..............................] - ETA: 3:05:50 - accuracy: 0.8611 - loss: 0.3363\n",
      " training set -> batch:53 loss:0.34211471676826477 and acc: 0.8574380278587341\n",
      "  53/2105 [..............................] - ETA: 3:03:54 - accuracy: 0.8574 - loss: 0.3421\n",
      " training set -> batch:54 loss:0.34173330664634705 and acc: 0.8569999933242798\n",
      "  54/2105 [..............................] - ETA: 3:02:01 - accuracy: 0.8570 - loss: 0.3417\n",
      " training set -> batch:55 loss:0.34015917778015137 and acc: 0.8585271239280701\n",
      "  55/2105 [..............................] - ETA: 3:00:12 - accuracy: 0.8585 - loss: 0.3402\n",
      " training set -> batch:56 loss:0.33328112959861755 and acc: 0.8609022498130798\n",
      "  56/2105 [..............................] - ETA: 2:58:27 - accuracy: 0.8609 - loss: 0.3333\n",
      " training set -> batch:57 loss:0.3350799083709717 and acc: 0.860401451587677\n",
      "  57/2105 [..............................] - ETA: 2:56:47 - accuracy: 0.8604 - loss: 0.3351\n",
      " training set -> batch:58 loss:0.3431514501571655 and acc: 0.8581560254096985\n",
      "  58/2105 [..............................] - ETA: 2:55:17 - accuracy: 0.8582 - loss: 0.3432\n",
      " training set -> batch:59 loss:0.3467848598957062 and acc: 0.857758641242981\n",
      "  59/2105 [..............................] - ETA: 2:53:50 - accuracy: 0.8578 - loss: 0.3468\n",
      " training set -> batch:60 loss:0.3461291491985321 and acc: 0.8582214713096619\n",
      "\n",
      " validation set -> batch:60 val loss:0.34074702858924866 and val acc: 0.8520641922950745\n",
      "  60/2105 [..............................] - ETA: 3:10:05 - accuracy: 0.8582 - loss: 0.3461\n",
      " training set -> batch:61 loss:0.33255329728126526 and acc: 0.855088472366333\n",
      "  61/2105 [..............................] - ETA: 3:08:23 - accuracy: 0.8551 - loss: 0.3326\n",
      " training set -> batch:62 loss:0.3248770236968994 and acc: 0.8589743375778198\n",
      "  62/2105 [..............................] - ETA: 3:06:42 - accuracy: 0.8590 - loss: 0.3249\n",
      " training set -> batch:63 loss:0.3316624164581299 and acc: 0.8584710955619812\n",
      "  63/2105 [..............................] - ETA: 3:05:01 - accuracy: 0.8585 - loss: 0.3317\n",
      " training set -> batch:64 loss:0.33036747574806213 and acc: 0.8579999804496765\n",
      "  64/2105 [..............................] - ETA: 3:03:25 - accuracy: 0.8580 - loss: 0.3304\n",
      " training set -> batch:65 loss:0.3382871448993683 and acc: 0.856589138507843\n",
      "  65/2105 [..............................] - ETA: 3:01:53 - accuracy: 0.8566 - loss: 0.3383\n",
      " training set -> batch:66 loss:0.34252017736434937 and acc: 0.8552631735801697\n",
      "  66/2105 [..............................] - ETA: 3:00:24 - accuracy: 0.8553 - loss: 0.3425\n",
      " training set -> batch:67 loss:0.3422812819480896 and acc: 0.8549270033836365\n",
      "  67/2105 [..............................] - ETA: 2:58:59 - accuracy: 0.8549 - loss: 0.3423\n",
      " training set -> batch:68 loss:0.34533068537712097 and acc: 0.853723406791687\n",
      "  68/2105 [..............................] - ETA: 2:57:35 - accuracy: 0.8537 - loss: 0.3453\n",
      " training set -> batch:69 loss:0.347002774477005 and acc: 0.8525862097740173\n",
      "  69/2105 [..............................] - ETA: 2:56:13 - accuracy: 0.8526 - loss: 0.3470\n",
      " training set -> batch:70 loss:0.34352409839630127 and acc: 0.8548657894134521\n",
      "\n",
      " validation set -> batch:70 val loss:0.34348130226135254 and val acc: 0.8704128265380859\n",
      "  70/2105 [..............................] - ETA: 3:09:18 - accuracy: 0.8549 - loss: 0.3435\n",
      " training set -> batch:71 loss:0.3408336937427521 and acc: 0.8694690465927124\n",
      "  71/2105 [>.............................] - ETA: 3:07:46 - accuracy: 0.8695 - loss: 0.3408\n",
      " training set -> batch:72 loss:0.3430298864841461 and acc: 0.8696581125259399\n",
      "  72/2105 [>.............................] - ETA: 3:06:18 - accuracy: 0.8697 - loss: 0.3430\n",
      " training set -> batch:73 loss:0.3602272570133209 and acc: 0.8667355179786682\n",
      "  73/2105 [>.............................] - ETA: 3:04:53 - accuracy: 0.8667 - loss: 0.3602\n",
      " training set -> batch:74 loss:0.3514764606952667 and acc: 0.8690000176429749\n",
      "  74/2105 [>.............................] - ETA: 3:03:33 - accuracy: 0.8690 - loss: 0.3515\n",
      " training set -> batch:75 loss:0.3630581796169281 and acc: 0.8643410801887512\n",
      "  75/2105 [>.............................] - ETA: 3:02:11 - accuracy: 0.8643 - loss: 0.3631\n",
      " training set -> batch:76 loss:0.3585798442363739 and acc: 0.8656014800071716\n",
      "  76/2105 [>.............................] - ETA: 3:00:52 - accuracy: 0.8656 - loss: 0.3586\n",
      " training set -> batch:77 loss:0.3556405305862427 and acc: 0.8667883276939392\n",
      "  77/2105 [>.............................] - ETA: 2:59:34 - accuracy: 0.8668 - loss: 0.3556\n",
      " training set -> batch:78 loss:0.3521755039691925 and acc: 0.8652482032775879\n",
      "  78/2105 [>.............................] - ETA: 2:58:19 - accuracy: 0.8652 - loss: 0.3522\n",
      " training set -> batch:79 loss:0.35022374987602234 and acc: 0.8663793206214905\n",
      "  79/2105 [>.............................] - ETA: 2:57:03 - accuracy: 0.8664 - loss: 0.3502\n",
      " training set -> batch:80 loss:0.3482297658920288 and acc: 0.8657718300819397\n",
      "\n",
      " validation set -> batch:80 val loss:0.32270553708076477 and val acc: 0.8669725060462952\n",
      "  80/2105 [>.............................] - ETA: 3:08:54 - accuracy: 0.8658 - loss: 0.3482\n",
      " training set -> batch:81 loss:0.3162122368812561 and acc: 0.8694690465927124\n",
      "  81/2105 [>.............................] - ETA: 3:07:33 - accuracy: 0.8695 - loss: 0.3162\n",
      " training set -> batch:82 loss:0.32755789160728455 and acc: 0.8685897588729858\n",
      "  82/2105 [>.............................] - ETA: 3:06:15 - accuracy: 0.8686 - loss: 0.3276\n",
      " training set -> batch:83 loss:0.32407307624816895 and acc: 0.8688016533851624\n",
      "  83/2105 [>.............................] - ETA: 3:05:02 - accuracy: 0.8688 - loss: 0.3241\n",
      " training set -> batch:84 loss:0.328631728887558 and acc: 0.8669999837875366\n",
      "  84/2105 [>.............................] - ETA: 3:03:48 - accuracy: 0.8670 - loss: 0.3286\n",
      " training set -> batch:85 loss:0.32850050926208496 and acc: 0.8662790656089783\n",
      "  85/2105 [>.............................] - ETA: 3:02:34 - accuracy: 0.8663 - loss: 0.3285\n",
      " training set -> batch:86 loss:0.33086544275283813 and acc: 0.8646616339683533\n",
      "  86/2105 [>.............................] - ETA: 3:01:24 - accuracy: 0.8647 - loss: 0.3309\n",
      " training set -> batch:87 loss:0.3359396159648895 and acc: 0.8631386756896973\n",
      "  87/2105 [>.............................] - ETA: 3:00:17 - accuracy: 0.8631 - loss: 0.3359\n",
      " training set -> batch:88 loss:0.33365753293037415 and acc: 0.8643617033958435\n",
      "  88/2105 [>.............................] - ETA: 2:59:09 - accuracy: 0.8644 - loss: 0.3337\n",
      " training set -> batch:89 loss:0.33747124671936035 and acc: 0.8629310131072998\n",
      "  89/2105 [>.............................] - ETA: 2:58:03 - accuracy: 0.8629 - loss: 0.3375\n",
      " training set -> batch:90 loss:0.33464515209198 and acc: 0.8640939593315125\n",
      "\n",
      " validation set -> batch:90 val loss:0.46903547644615173 and val acc: 0.8107798099517822\n",
      "  90/2105 [>.............................] - ETA: 3:08:37 - accuracy: 0.8641 - loss: 0.3346\n",
      " training set -> batch:91 loss:0.46702840924263 and acc: 0.8108407258987427\n",
      "  91/2105 [>.............................] - ETA: 3:07:27 - accuracy: 0.8108 - loss: 0.4670\n",
      " training set -> batch:92 loss:0.44999635219573975 and acc: 0.81517094373703\n",
      "  92/2105 [>.............................] - ETA: 3:06:17 - accuracy: 0.8152 - loss: 0.4500\n",
      " training set -> batch:93 loss:0.4505346715450287 and acc: 0.81611567735672\n",
      "  93/2105 [>.............................] - ETA: 3:05:08 - accuracy: 0.8161 - loss: 0.4505\n",
      " training set -> batch:94 loss:0.43799397349357605 and acc: 0.8199999928474426\n",
      "  94/2105 [>.............................] - ETA: 3:04:03 - accuracy: 0.8200 - loss: 0.4380\n",
      " training set -> batch:95 loss:0.43398770689964294 and acc: 0.8217054009437561\n",
      "  95/2105 [>.............................] - ETA: 3:02:56 - accuracy: 0.8217 - loss: 0.4340\n",
      " training set -> batch:96 loss:0.43552365899086 and acc: 0.8204887509346008\n",
      "  96/2105 [>.............................] - ETA: 3:01:52 - accuracy: 0.8205 - loss: 0.4355\n",
      " training set -> batch:97 loss:0.43462035059928894 and acc: 0.8211678862571716\n",
      "  97/2105 [>.............................] - ETA: 3:00:48 - accuracy: 0.8212 - loss: 0.4346\n",
      " training set -> batch:98 loss:0.4352494180202484 and acc: 0.8209219574928284\n",
      "  98/2105 [>.............................] - ETA: 2:59:44 - accuracy: 0.8209 - loss: 0.4352\n",
      " training set -> batch:99 loss:0.4250665009021759 and acc: 0.8241379261016846\n",
      "  99/2105 [>.............................] - ETA: 2:58:43 - accuracy: 0.8241 - loss: 0.4251\n",
      " training set -> batch:100 loss:0.4195350706577301 and acc: 0.8271812200546265\n",
      "\n",
      " validation set -> batch:100 val loss:0.3000324070453644 and val acc: 0.8704128265380859\n",
      " 100/2105 [>.............................] - ETA: 3:08:12 - accuracy: 0.8272 - loss: 0.4195\n",
      " training set -> batch:101 loss:0.2999595105648041 and acc: 0.8716813921928406\n",
      " 101/2105 [>.............................] - ETA: 3:07:08 - accuracy: 0.8717 - loss: 0.3000\n",
      " training set -> batch:102 loss:0.3051180839538574 and acc: 0.870726466178894\n",
      " 102/2105 [>.............................] - ETA: 3:06:03 - accuracy: 0.8707 - loss: 0.3051\n",
      " training set -> batch:103 loss:0.307312935590744 and acc: 0.8708677887916565\n",
      " 103/2105 [>.............................] - ETA: 3:04:59 - accuracy: 0.8709 - loss: 0.3073\n",
      " training set -> batch:104 loss:0.30933699011802673 and acc: 0.871999979019165\n",
      " 104/2105 [>.............................] - ETA: 3:03:56 - accuracy: 0.8720 - loss: 0.3093\n",
      " training set -> batch:105 loss:0.3058355450630188 and acc: 0.873062014579773\n",
      " 105/2105 [>.............................] - ETA: 3:02:55 - accuracy: 0.8731 - loss: 0.3058\n",
      " training set -> batch:106 loss:0.3204852044582367 and acc: 0.8684210777282715\n",
      " 106/2105 [>.............................] - ETA: 3:01:53 - accuracy: 0.8684 - loss: 0.3205\n",
      " training set -> batch:107 loss:0.3139811158180237 and acc: 0.8704379796981812\n",
      " 107/2105 [>.............................] - ETA: 3:00:53 - accuracy: 0.8704 - loss: 0.3140\n",
      " training set -> batch:108 loss:0.31001272797584534 and acc: 0.8723404407501221\n",
      " 108/2105 [>.............................] - ETA: 2:59:55 - accuracy: 0.8723 - loss: 0.3100\n",
      " training set -> batch:109 loss:0.31176772713661194 and acc: 0.8715517520904541\n",
      " 109/2105 [>.............................] - ETA: 2:58:59 - accuracy: 0.8716 - loss: 0.3118\n",
      " training set -> batch:110 loss:0.31167757511138916 and acc: 0.8708053827285767\n",
      "\n",
      " validation set -> batch:110 val loss:0.2932061553001404 and val acc: 0.8795871734619141\n",
      " 110/2105 [>.............................] - ETA: 3:07:25 - accuracy: 0.8708 - loss: 0.3117\n",
      " training set -> batch:111 loss:0.28372374176979065 and acc: 0.8827433586120605\n",
      " 111/2105 [>.............................] - ETA: 3:06:26 - accuracy: 0.8827 - loss: 0.2837\n",
      " training set -> batch:112 loss:0.27732059359550476 and acc: 0.8846153616905212\n",
      " 112/2105 [>.............................] - ETA: 3:05:28 - accuracy: 0.8846 - loss: 0.2773\n",
      " training set -> batch:113 loss:0.2892618179321289 and acc: 0.8801652789115906\n",
      " 113/2105 [>.............................] - ETA: 3:04:30 - accuracy: 0.8802 - loss: 0.2893\n",
      " training set -> batch:114 loss:0.30169445276260376 and acc: 0.8769999742507935\n",
      " 114/2105 [>.............................] - ETA: 3:03:32 - accuracy: 0.8770 - loss: 0.3017\n",
      " training set -> batch:115 loss:0.3050948977470398 and acc: 0.8740310072898865\n",
      " 115/2105 [>.............................] - ETA: 3:02:36 - accuracy: 0.8740 - loss: 0.3051\n",
      " training set -> batch:116 loss:0.3076200485229492 and acc: 0.8731203079223633\n",
      " 116/2105 [>.............................] - ETA: 3:01:40 - accuracy: 0.8731 - loss: 0.3076\n",
      " training set -> batch:117 loss:0.3055347502231598 and acc: 0.8740875720977783\n",
      " 117/2105 [>.............................] - ETA: 3:00:45 - accuracy: 0.8741 - loss: 0.3055\n",
      " training set -> batch:118 loss:0.30654704570770264 and acc: 0.8732269406318665\n",
      " 118/2105 [>.............................] - ETA: 2:59:51 - accuracy: 0.8732 - loss: 0.3065\n",
      " training set -> batch:119 loss:0.30096882581710815 and acc: 0.8758620619773865\n",
      " 119/2105 [>.............................] - ETA: 2:58:58 - accuracy: 0.8759 - loss: 0.3010\n",
      " training set -> batch:120 loss:0.3005211651325226 and acc: 0.8766778707504272\n",
      "\n",
      " validation set -> batch:120 val loss:0.2924962341785431 and val acc: 0.8784403800964355\n",
      " 120/2105 [>.............................] - ETA: 3:06:09 - accuracy: 0.8767 - loss: 0.3005\n",
      " training set -> batch:121 loss:0.2896924316883087 and acc: 0.877212405204773\n",
      " 121/2105 [>.............................] - ETA: 3:05:14 - accuracy: 0.8772 - loss: 0.2897\n",
      " training set -> batch:122 loss:0.28861114382743835 and acc: 0.8760683536529541\n",
      " 122/2105 [>.............................] - ETA: 3:04:20 - accuracy: 0.8761 - loss: 0.2886\n",
      " training set -> batch:123 loss:0.2946951389312744 and acc: 0.8739669322967529\n",
      " 123/2105 [>.............................] - ETA: 3:03:26 - accuracy: 0.8740 - loss: 0.2947\n",
      " training set -> batch:124 loss:0.2991926968097687 and acc: 0.8740000128746033\n",
      " 124/2105 [>.............................] - ETA: 3:02:33 - accuracy: 0.8740 - loss: 0.2992\n",
      " training set -> batch:125 loss:0.2983004152774811 and acc: 0.873062014579773\n",
      " 125/2105 [>.............................] - ETA: 3:01:41 - accuracy: 0.8731 - loss: 0.2983\n",
      " training set -> batch:126 loss:0.3018795847892761 and acc: 0.8721804618835449\n",
      " 126/2105 [>.............................] - ETA: 3:00:50 - accuracy: 0.8722 - loss: 0.3019\n",
      " training set -> batch:127 loss:0.29539772868156433 and acc: 0.8740875720977783\n",
      " 127/2105 [>.............................] - ETA: 2:59:58 - accuracy: 0.8741 - loss: 0.2954\n",
      " training set -> batch:128 loss:0.29072391986846924 and acc: 0.8758864998817444\n",
      " 128/2105 [>.............................] - ETA: 2:59:07 - accuracy: 0.8759 - loss: 0.2907\n",
      " training set -> batch:129 loss:0.28660649061203003 and acc: 0.8775861859321594\n",
      " 129/2105 [>.............................] - ETA: 2:58:19 - accuracy: 0.8776 - loss: 0.2866\n",
      " training set -> batch:130 loss:0.28697994351387024 and acc: 0.8783556818962097\n",
      "\n",
      " validation set -> batch:130 val loss:0.2887715995311737 and val acc: 0.8807339668273926\n",
      " 130/2105 [>.............................] - ETA: 3:04:57 - accuracy: 0.8784 - loss: 0.2870\n",
      " training set -> batch:131 loss:0.2925124764442444 and acc: 0.8794247508049011\n",
      " 131/2105 [>.............................] - ETA: 3:04:05 - accuracy: 0.8794 - loss: 0.2925\n",
      " training set -> batch:132 loss:0.28998300433158875 and acc: 0.879273533821106\n",
      " 132/2105 [>.............................] - ETA: 3:03:14 - accuracy: 0.8793 - loss: 0.2900\n",
      " training set -> batch:133 loss:0.2862073481082916 and acc: 0.8801652789115906\n",
      " 133/2105 [>.............................] - ETA: 3:02:25 - accuracy: 0.8802 - loss: 0.2862\n",
      " training set -> batch:134 loss:0.2810230255126953 and acc: 0.8809999823570251\n",
      " 134/2105 [>.............................] - ETA: 3:01:36 - accuracy: 0.8810 - loss: 0.2810\n",
      " training set -> batch:135 loss:0.27510347962379456 and acc: 0.8827519416809082\n",
      " 135/2105 [>.............................] - ETA: 3:00:47 - accuracy: 0.8828 - loss: 0.2751\n",
      " training set -> batch:136 loss:0.27864140272140503 and acc: 0.8825187683105469\n",
      " 136/2105 [>.............................] - ETA: 2:59:58 - accuracy: 0.8825 - loss: 0.2786\n",
      " training set -> batch:137 loss:0.28454655408859253 and acc: 0.8804744482040405\n",
      " 137/2105 [>.............................] - ETA: 2:59:11 - accuracy: 0.8805 - loss: 0.2845\n",
      " training set -> batch:138 loss:0.282696932554245 and acc: 0.8803191781044006\n",
      " 138/2105 [>.............................] - ETA: 2:58:24 - accuracy: 0.8803 - loss: 0.2827\n",
      " training set -> batch:139 loss:0.279414564371109 and acc: 0.8801724314689636\n",
      " 139/2105 [>.............................] - ETA: 2:57:38 - accuracy: 0.8802 - loss: 0.2794\n",
      " training set -> batch:140 loss:0.27643683552742004 and acc: 0.880033552646637\n",
      "\n",
      " validation set -> batch:140 val loss:0.26410743594169617 and val acc: 0.8899082541465759\n",
      " 140/2105 [>.............................] - ETA: 3:03:50 - accuracy: 0.8800 - loss: 0.2764\n",
      " training set -> batch:141 loss:0.26689112186431885 and acc: 0.8893805146217346\n",
      " 141/2105 [=>............................] - ETA: 3:03:03 - accuracy: 0.8894 - loss: 0.2669\n",
      " training set -> batch:142 loss:0.27270159125328064 and acc: 0.8878205418586731\n",
      " 142/2105 [=>............................] - ETA: 3:02:15 - accuracy: 0.8878 - loss: 0.2727\n",
      " training set -> batch:143 loss:0.27550825476646423 and acc: 0.8873966932296753\n",
      " 143/2105 [=>............................] - ETA: 3:01:27 - accuracy: 0.8874 - loss: 0.2755\n",
      " training set -> batch:144 loss:0.281240850687027 and acc: 0.8870000243186951\n",
      " 144/2105 [=>............................] - ETA: 3:00:41 - accuracy: 0.8870 - loss: 0.2812\n",
      " training set -> batch:145 loss:0.2920399606227875 and acc: 0.8846899271011353\n",
      " 145/2105 [=>............................] - ETA: 2:59:56 - accuracy: 0.8847 - loss: 0.2920\n",
      " training set -> batch:146 loss:0.2870365083217621 and acc: 0.8853383660316467\n",
      " 146/2105 [=>............................] - ETA: 2:59:10 - accuracy: 0.8853 - loss: 0.2870\n",
      " training set -> batch:147 loss:0.27805057168006897 and acc: 0.8886861205101013\n",
      " 147/2105 [=>............................] - ETA: 2:58:27 - accuracy: 0.8887 - loss: 0.2781\n",
      " training set -> batch:148 loss:0.27644282579421997 and acc: 0.890070915222168\n",
      " 148/2105 [=>............................] - ETA: 2:57:45 - accuracy: 0.8901 - loss: 0.2764\n",
      " training set -> batch:149 loss:0.27827757596969604 and acc: 0.8887931108474731\n",
      " 149/2105 [=>............................] - ETA: 2:57:02 - accuracy: 0.8888 - loss: 0.2783\n",
      " training set -> batch:150 loss:0.2756398022174835 and acc: 0.8884228467941284\n",
      "\n",
      " validation set -> batch:150 val loss:0.2703031897544861 and val acc: 0.8979358077049255\n",
      " 150/2105 [=>............................] - ETA: 3:02:52 - accuracy: 0.8884 - loss: 0.2756\n",
      " training set -> batch:151 loss:0.2913402318954468 and acc: 0.894911527633667\n",
      " 151/2105 [=>............................] - ETA: 3:02:08 - accuracy: 0.8949 - loss: 0.2913\n",
      " training set -> batch:152 loss:0.2928761839866638 and acc: 0.8920940160751343\n",
      " 152/2105 [=>............................] - ETA: 3:01:24 - accuracy: 0.8921 - loss: 0.2929\n",
      " training set -> batch:153 loss:0.2937202751636505 and acc: 0.8904958963394165\n",
      " 153/2105 [=>............................] - ETA: 3:00:40 - accuracy: 0.8905 - loss: 0.2937\n",
      " training set -> batch:154 loss:0.30056747794151306 and acc: 0.8880000114440918\n",
      " 154/2105 [=>............................] - ETA: 2:59:56 - accuracy: 0.8880 - loss: 0.3006\n",
      " training set -> batch:155 loss:0.29591065645217896 and acc: 0.8895348906517029\n",
      " 155/2105 [=>............................] - ETA: 2:59:14 - accuracy: 0.8895 - loss: 0.2959\n",
      " training set -> batch:156 loss:0.30074724555015564 and acc: 0.8890977501869202\n",
      " 156/2105 [=>............................] - ETA: 2:58:33 - accuracy: 0.8891 - loss: 0.3007\n",
      " training set -> batch:157 loss:0.30456098914146423 and acc: 0.8841241002082825\n",
      " 157/2105 [=>............................] - ETA: 2:57:52 - accuracy: 0.8841 - loss: 0.3046\n",
      " training set -> batch:158 loss:0.2963104844093323 and acc: 0.8865247964859009\n",
      " 158/2105 [=>............................] - ETA: 2:57:10 - accuracy: 0.8865 - loss: 0.2963\n",
      " training set -> batch:159 loss:0.30163758993148804 and acc: 0.884482741355896\n",
      " 159/2105 [=>............................] - ETA: 2:56:29 - accuracy: 0.8845 - loss: 0.3016\n",
      " training set -> batch:160 loss:0.298021525144577 and acc: 0.8859060406684875\n",
      "\n",
      " validation set -> batch:160 val loss:0.26678019762039185 and val acc: 0.892201840877533\n",
      " 160/2105 [=>............................] - ETA: 3:01:56 - accuracy: 0.8859 - loss: 0.2980\n",
      " training set -> batch:161 loss:0.2708546221256256 and acc: 0.8893805146217346\n",
      " 161/2105 [=>............................] - ETA: 3:01:15 - accuracy: 0.8894 - loss: 0.2709\n",
      " training set -> batch:162 loss:0.2652375400066376 and acc: 0.8910256624221802\n",
      " 162/2105 [=>............................] - ETA: 3:00:33 - accuracy: 0.8910 - loss: 0.2652\n",
      " training set -> batch:163 loss:0.26164162158966064 and acc: 0.8915289044380188\n",
      " 163/2105 [=>............................] - ETA: 2:59:52 - accuracy: 0.8915 - loss: 0.2616\n",
      " training set -> batch:164 loss:0.25969812273979187 and acc: 0.8920000195503235\n",
      " 164/2105 [=>............................] - ETA: 2:59:10 - accuracy: 0.8920 - loss: 0.2597\n",
      " training set -> batch:165 loss:0.2551344931125641 and acc: 0.893410861492157\n",
      " 165/2105 [=>............................] - ETA: 2:58:30 - accuracy: 0.8934 - loss: 0.2551\n",
      " training set -> batch:166 loss:0.25327935814857483 and acc: 0.8956766724586487\n",
      " 166/2105 [=>............................] - ETA: 2:57:49 - accuracy: 0.8957 - loss: 0.2533\n",
      " training set -> batch:167 loss:0.2530854046344757 and acc: 0.8968977928161621\n",
      " 167/2105 [=>............................] - ETA: 2:57:11 - accuracy: 0.8969 - loss: 0.2531\n",
      " training set -> batch:168 loss:0.2573589086532593 and acc: 0.8971630930900574\n",
      " 168/2105 [=>............................] - ETA: 2:56:33 - accuracy: 0.8972 - loss: 0.2574\n",
      " training set -> batch:169 loss:0.25864285230636597 and acc: 0.8965517282485962\n",
      " 169/2105 [=>............................] - ETA: 2:55:55 - accuracy: 0.8966 - loss: 0.2586\n",
      " training set -> batch:170 loss:0.26667195558547974 and acc: 0.8951342105865479\n",
      "\n",
      " validation set -> batch:170 val loss:0.2942619323730469 and val acc: 0.8795871734619141\n",
      " 170/2105 [=>............................] - ETA: 3:01:01 - accuracy: 0.8951 - loss: 0.2667\n",
      " training set -> batch:171 loss:0.29592424631118774 and acc: 0.8794247508049011\n",
      " 171/2105 [=>............................] - ETA: 3:00:21 - accuracy: 0.8794 - loss: 0.2959\n",
      " training set -> batch:172 loss:0.30584970116615295 and acc: 0.877136766910553\n",
      " 172/2105 [=>............................] - ETA: 2:59:42 - accuracy: 0.8771 - loss: 0.3058\n",
      " training set -> batch:173 loss:0.30608484148979187 and acc: 0.8770661354064941\n",
      " 173/2105 [=>............................] - ETA: 2:59:02 - accuracy: 0.8771 - loss: 0.3061\n",
      " training set -> batch:174 loss:0.3027507960796356 and acc: 0.8769999742507935\n",
      " 174/2105 [=>............................] - ETA: 2:58:23 - accuracy: 0.8770 - loss: 0.3028\n",
      " training set -> batch:175 loss:0.29987356066703796 and acc: 0.8788759708404541\n",
      " 175/2105 [=>............................] - ETA: 2:57:45 - accuracy: 0.8789 - loss: 0.2999\n",
      " training set -> batch:176 loss:0.30670255422592163 and acc: 0.8768796920776367\n",
      " 176/2105 [=>............................] - ETA: 2:57:06 - accuracy: 0.8769 - loss: 0.3067\n",
      " training set -> batch:177 loss:0.30874207615852356 and acc: 0.8759124279022217\n",
      " 177/2105 [=>............................] - ETA: 2:56:28 - accuracy: 0.8759 - loss: 0.3087\n",
      " training set -> batch:178 loss:0.30749914050102234 and acc: 0.8767730593681335\n",
      " 178/2105 [=>............................] - ETA: 2:55:50 - accuracy: 0.8768 - loss: 0.3075\n",
      " training set -> batch:179 loss:0.3093017637729645 and acc: 0.876724123954773\n",
      " 179/2105 [=>............................] - ETA: 2:55:13 - accuracy: 0.8767 - loss: 0.3093\n",
      " training set -> batch:180 loss:0.309906929731369 and acc: 0.8766778707504272\n",
      "\n",
      " validation set -> batch:180 val loss:0.27802136540412903 and val acc: 0.8853210806846619\n",
      " 180/2105 [=>............................] - ETA: 3:00:25 - accuracy: 0.8767 - loss: 0.3099\n",
      " training set -> batch:181 loss:0.28832337260246277 and acc: 0.8849557638168335\n",
      " 181/2105 [=>............................] - ETA: 2:59:49 - accuracy: 0.8850 - loss: 0.2883\n",
      " training set -> batch:182 loss:0.28593599796295166 and acc: 0.8856837749481201\n",
      " 182/2105 [=>............................] - ETA: 2:59:12 - accuracy: 0.8857 - loss: 0.2859\n",
      " training set -> batch:183 loss:0.2820202112197876 and acc: 0.8863636255264282\n",
      " 183/2105 [=>............................] - ETA: 2:58:35 - accuracy: 0.8864 - loss: 0.2820\n",
      " training set -> batch:184 loss:0.28423306345939636 and acc: 0.8870000243186951\n",
      " 184/2105 [=>............................] - ETA: 2:57:58 - accuracy: 0.8870 - loss: 0.2842\n",
      " training set -> batch:185 loss:0.2801814675331116 and acc: 0.8875969052314758\n",
      " 185/2105 [=>............................] - ETA: 2:57:21 - accuracy: 0.8876 - loss: 0.2802\n",
      " training set -> batch:186 loss:0.282660573720932 and acc: 0.8862782120704651\n",
      " 186/2105 [=>............................] - ETA: 2:56:45 - accuracy: 0.8863 - loss: 0.2827\n",
      " training set -> batch:187 loss:0.2796233892440796 and acc: 0.8877737522125244\n",
      " 187/2105 [=>............................] - ETA: 2:56:09 - accuracy: 0.8878 - loss: 0.2796\n",
      " training set -> batch:188 loss:0.2800893187522888 and acc: 0.88741135597229\n",
      " 188/2105 [=>............................] - ETA: 2:55:34 - accuracy: 0.8874 - loss: 0.2801\n",
      " training set -> batch:189 loss:0.2856801748275757 and acc: 0.884482741355896\n",
      " 189/2105 [=>............................] - ETA: 2:54:59 - accuracy: 0.8845 - loss: 0.2857\n",
      " training set -> batch:190 loss:0.28269222378730774 and acc: 0.8850671052932739\n",
      "\n",
      " validation set -> batch:190 val loss:0.25758153200149536 and val acc: 0.9013761281967163\n",
      " 190/2105 [=>............................] - ETA: 2:59:19 - accuracy: 0.8851 - loss: 0.2827\n",
      " training set -> batch:191 loss:0.2601427733898163 and acc: 0.8993362784385681\n",
      " 191/2105 [=>............................] - ETA: 2:58:44 - accuracy: 0.8993 - loss: 0.2601\n",
      " training set -> batch:192 loss:0.25686347484588623 and acc: 0.9006410241127014\n",
      " 192/2105 [=>............................] - ETA: 2:58:08 - accuracy: 0.9006 - loss: 0.2569\n",
      " training set -> batch:193 loss:0.271658331155777 and acc: 0.8966942429542542\n",
      " 193/2105 [=>............................] - ETA: 2:57:33 - accuracy: 0.8967 - loss: 0.2717\n",
      " training set -> batch:194 loss:0.2618979215621948 and acc: 0.8989999890327454\n",
      " 194/2105 [=>............................] - ETA: 2:56:58 - accuracy: 0.8990 - loss: 0.2619\n",
      " training set -> batch:195 loss:0.2638683617115021 and acc: 0.8982558250427246\n",
      " 195/2105 [=>............................] - ETA: 2:56:23 - accuracy: 0.8983 - loss: 0.2639\n",
      " training set -> batch:196 loss:0.26265570521354675 and acc: 0.8994361162185669\n",
      " 196/2105 [=>............................] - ETA: 2:55:50 - accuracy: 0.8994 - loss: 0.2627\n",
      " training set -> batch:197 loss:0.26240867376327515 and acc: 0.900547444820404\n",
      " 197/2105 [=>............................] - ETA: 2:55:16 - accuracy: 0.9005 - loss: 0.2624\n",
      " training set -> batch:198 loss:0.2662683129310608 and acc: 0.8998227119445801\n",
      " 198/2105 [=>............................] - ETA: 2:54:43 - accuracy: 0.8998 - loss: 0.2663\n",
      " training set -> batch:199 loss:0.2651084363460541 and acc: 0.9008620977401733\n",
      " 199/2105 [=>............................] - ETA: 2:54:09 - accuracy: 0.9009 - loss: 0.2651\n",
      " training set -> batch:200 loss:0.26010608673095703 and acc: 0.9010066986083984\n",
      "\n",
      " validation set -> batch:200 val loss:0.2519851624965668 and val acc: 0.9002293348312378\n",
      " 200/2105 [=>............................] - ETA: 2:58:29 - accuracy: 0.9010 - loss: 0.2601\n",
      " training set -> batch:201 loss:0.2434975504875183 and acc: 0.9015486836433411\n",
      " 201/2105 [=>............................] - ETA: 2:57:55 - accuracy: 0.9015 - loss: 0.2435\n",
      " training set -> batch:202 loss:0.2436957061290741 and acc: 0.9027777910232544\n",
      " 202/2105 [=>............................] - ETA: 2:57:21 - accuracy: 0.9028 - loss: 0.2437\n",
      " training set -> batch:203 loss:0.24348478019237518 and acc: 0.9028925895690918\n",
      " 203/2105 [=>............................] - ETA: 2:56:48 - accuracy: 0.9029 - loss: 0.2435\n",
      " training set -> batch:204 loss:0.2380685806274414 and acc: 0.9039999842643738\n",
      " 204/2105 [=>............................] - ETA: 2:56:14 - accuracy: 0.9040 - loss: 0.2381\n",
      " training set -> batch:205 loss:0.23838739097118378 and acc: 0.9040697813034058\n",
      " 205/2105 [=>............................] - ETA: 2:55:41 - accuracy: 0.9041 - loss: 0.2384\n",
      " training set -> batch:206 loss:0.23327858746051788 and acc: 0.9060150384902954\n",
      " 206/2105 [=>............................] - ETA: 2:55:09 - accuracy: 0.9060 - loss: 0.2333\n",
      " training set -> batch:207 loss:0.23739072680473328 and acc: 0.9060218930244446\n",
      " 207/2105 [=>............................] - ETA: 2:54:36 - accuracy: 0.9060 - loss: 0.2374\n",
      " training set -> batch:208 loss:0.238403782248497 and acc: 0.9042553305625916\n",
      " 208/2105 [=>............................] - ETA: 2:54:03 - accuracy: 0.9043 - loss: 0.2384\n",
      " training set -> batch:209 loss:0.2557956278324127 and acc: 0.9008620977401733\n",
      " 209/2105 [=>............................] - ETA: 2:53:31 - accuracy: 0.9009 - loss: 0.2558\n",
      " training set -> batch:210 loss:0.25345006585121155 and acc: 0.9018456339836121\n",
      "\n",
      " validation set -> batch:210 val loss:0.27173998951911926 and val acc: 0.8853210806846619\n",
      " 210/2105 [=>............................] - ETA: 2:57:26 - accuracy: 0.9018 - loss: 0.2535\n",
      " training set -> batch:211 loss:0.2745606005191803 and acc: 0.8849557638168335\n",
      " 211/2105 [==>...........................] - ETA: 2:56:53 - accuracy: 0.8850 - loss: 0.2746\n",
      " training set -> batch:212 loss:0.27971190214157104 and acc: 0.882478654384613\n",
      " 212/2105 [==>...........................] - ETA: 2:56:20 - accuracy: 0.8825 - loss: 0.2797\n",
      " training set -> batch:213 loss:0.26968616247177124 and acc: 0.8853305578231812\n",
      " 213/2105 [==>...........................] - ETA: 2:55:47 - accuracy: 0.8853 - loss: 0.2697\n",
      " training set -> batch:214 loss:0.2672994136810303 and acc: 0.8859999775886536\n",
      " 214/2105 [==>...........................] - ETA: 2:55:15 - accuracy: 0.8860 - loss: 0.2673\n",
      " training set -> batch:215 loss:0.26786091923713684 and acc: 0.8866279125213623\n",
      " 215/2105 [==>...........................] - ETA: 2:54:44 - accuracy: 0.8866 - loss: 0.2679\n",
      " training set -> batch:216 loss:0.26741597056388855 and acc: 0.8881579041481018\n",
      " 216/2105 [==>...........................] - ETA: 2:54:12 - accuracy: 0.8882 - loss: 0.2674\n",
      " training set -> batch:217 loss:0.2721003293991089 and acc: 0.8868613243103027\n",
      " 217/2105 [==>...........................] - ETA: 2:53:40 - accuracy: 0.8869 - loss: 0.2721\n",
      " training set -> batch:218 loss:0.2702319025993347 and acc: 0.88741135597229\n",
      " 218/2105 [==>...........................] - ETA: 2:53:09 - accuracy: 0.8874 - loss: 0.2702\n",
      " training set -> batch:219 loss:0.2713713049888611 and acc: 0.8879310488700867\n",
      " 219/2105 [==>...........................] - ETA: 2:52:38 - accuracy: 0.8879 - loss: 0.2714\n",
      " training set -> batch:220 loss:0.27407464385032654 and acc: 0.8875839114189148\n",
      "\n",
      " validation set -> batch:220 val loss:0.26263290643692017 and val acc: 0.89449542760849\n",
      " 220/2105 [==>...........................] - ETA: 2:56:31 - accuracy: 0.8876 - loss: 0.2741\n",
      " training set -> batch:221 loss:0.2558934688568115 and acc: 0.8960176706314087\n",
      " 221/2105 [==>...........................] - ETA: 2:56:00 - accuracy: 0.8960 - loss: 0.2559\n",
      " training set -> batch:222 loss:0.26503434777259827 and acc: 0.8931623697280884\n",
      " 222/2105 [==>...........................] - ETA: 2:55:29 - accuracy: 0.8932 - loss: 0.2650\n",
      " training set -> batch:223 loss:0.27520596981048584 and acc: 0.8915289044380188\n",
      " 223/2105 [==>...........................] - ETA: 2:54:59 - accuracy: 0.8915 - loss: 0.2752\n",
      " training set -> batch:224 loss:0.27201753854751587 and acc: 0.8920000195503235\n",
      " 224/2105 [==>...........................] - ETA: 2:54:29 - accuracy: 0.8920 - loss: 0.2720\n",
      " training set -> batch:225 loss:0.27266255021095276 and acc: 0.893410861492157\n",
      " 225/2105 [==>...........................] - ETA: 2:53:59 - accuracy: 0.8934 - loss: 0.2727\n",
      " training set -> batch:226 loss:0.274197518825531 and acc: 0.8919172883033752\n",
      " 226/2105 [==>...........................] - ETA: 2:53:29 - accuracy: 0.8919 - loss: 0.2742\n",
      " training set -> batch:227 loss:0.27545803785324097 and acc: 0.8905109763145447\n",
      " 227/2105 [==>...........................] - ETA: 2:52:59 - accuracy: 0.8905 - loss: 0.2755\n",
      " training set -> batch:228 loss:0.27337124943733215 and acc: 0.8918439745903015\n",
      " 228/2105 [==>...........................] - ETA: 2:52:29 - accuracy: 0.8918 - loss: 0.2734\n",
      " training set -> batch:229 loss:0.26935407519340515 and acc: 0.8913792967796326\n",
      " 229/2105 [==>...........................] - ETA: 2:51:59 - accuracy: 0.8914 - loss: 0.2694\n",
      " training set -> batch:230 loss:0.2708428204059601 and acc: 0.8901006579399109\n",
      "\n",
      " validation set -> batch:230 val loss:0.255555659532547 and val acc: 0.8979358077049255\n",
      " 230/2105 [==>...........................] - ETA: 2:55:38 - accuracy: 0.8901 - loss: 0.2708\n",
      " training set -> batch:231 loss:0.2635725140571594 and acc: 0.894911527633667\n",
      " 231/2105 [==>...........................] - ETA: 2:55:07 - accuracy: 0.8949 - loss: 0.2636\n",
      " training set -> batch:232 loss:0.271481454372406 and acc: 0.8931623697280884\n",
      " 232/2105 [==>...........................] - ETA: 2:54:38 - accuracy: 0.8932 - loss: 0.2715\n",
      " training set -> batch:233 loss:0.26727965474128723 and acc: 0.8935950398445129\n",
      " 233/2105 [==>...........................] - ETA: 2:54:08 - accuracy: 0.8936 - loss: 0.2673\n",
      " training set -> batch:234 loss:0.274087518453598 and acc: 0.890999972820282\n",
      " 234/2105 [==>...........................] - ETA: 2:53:37 - accuracy: 0.8910 - loss: 0.2741\n",
      " training set -> batch:235 loss:0.2769451141357422 and acc: 0.8885658979415894\n",
      " 235/2105 [==>...........................] - ETA: 2:53:08 - accuracy: 0.8886 - loss: 0.2769\n",
      " training set -> batch:236 loss:0.27689751982688904 and acc: 0.8890977501869202\n",
      " 236/2105 [==>...........................] - ETA: 2:52:39 - accuracy: 0.8891 - loss: 0.2769\n",
      " training set -> batch:237 loss:0.27508294582366943 and acc: 0.889598548412323\n",
      " 237/2105 [==>...........................] - ETA: 2:52:10 - accuracy: 0.8896 - loss: 0.2751\n",
      " training set -> batch:238 loss:0.27906060218811035 and acc: 0.88741135597229\n",
      " 238/2105 [==>...........................] - ETA: 2:51:40 - accuracy: 0.8874 - loss: 0.2791\n",
      " training set -> batch:239 loss:0.284525066614151 and acc: 0.8853448033332825\n",
      " 239/2105 [==>...........................] - ETA: 2:51:12 - accuracy: 0.8853 - loss: 0.2845\n",
      " training set -> batch:240 loss:0.27809491753578186 and acc: 0.8884228467941284\n",
      "\n",
      " validation set -> batch:240 val loss:0.24743913114070892 and val acc: 0.9059633016586304\n",
      " 240/2105 [==>...........................] - ETA: 2:54:35 - accuracy: 0.8884 - loss: 0.2781\n",
      " training set -> batch:241 loss:0.2437533587217331 and acc: 0.9070796370506287\n",
      " 241/2105 [==>...........................] - ETA: 2:54:06 - accuracy: 0.9071 - loss: 0.2438\n",
      " training set -> batch:242 loss:0.23379534482955933 and acc: 0.9102563858032227\n",
      " 242/2105 [==>...........................] - ETA: 2:53:38 - accuracy: 0.9103 - loss: 0.2338\n",
      " training set -> batch:243 loss:0.23006092011928558 and acc: 0.9111570119857788\n",
      " 243/2105 [==>...........................] - ETA: 2:53:10 - accuracy: 0.9112 - loss: 0.2301\n",
      " training set -> batch:244 loss:0.23694700002670288 and acc: 0.9089999794960022\n",
      " 244/2105 [==>...........................] - ETA: 2:52:42 - accuracy: 0.9090 - loss: 0.2369\n",
      " training set -> batch:245 loss:0.23349912464618683 and acc: 0.9098837375640869\n",
      " 245/2105 [==>...........................] - ETA: 2:52:15 - accuracy: 0.9099 - loss: 0.2335\n",
      " training set -> batch:246 loss:0.23036959767341614 and acc: 0.9107142686843872\n",
      " 246/2105 [==>...........................] - ETA: 2:51:47 - accuracy: 0.9107 - loss: 0.2304\n",
      " training set -> batch:247 loss:0.22652976214885712 and acc: 0.9124087691307068\n",
      " 247/2105 [==>...........................] - ETA: 2:51:19 - accuracy: 0.9124 - loss: 0.2265\n",
      " training set -> batch:248 loss:0.22519584000110626 and acc: 0.9131205677986145\n",
      " 248/2105 [==>...........................] - ETA: 2:50:52 - accuracy: 0.9131 - loss: 0.2252\n",
      " training set -> batch:249 loss:0.2257087528705597 and acc: 0.9129310250282288\n",
      " 249/2105 [==>...........................] - ETA: 2:50:25 - accuracy: 0.9129 - loss: 0.2257\n",
      " training set -> batch:250 loss:0.22599120438098907 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:250 val loss:0.2507006824016571 and val acc: 0.9048165082931519\n",
      " 250/2105 [==>...........................] - ETA: 2:53:45 - accuracy: 0.9136 - loss: 0.2260\n",
      " training set -> batch:251 loss:0.2486739158630371 and acc: 0.9059734344482422\n",
      " 251/2105 [==>...........................] - ETA: 2:53:17 - accuracy: 0.9060 - loss: 0.2487\n",
      " training set -> batch:252 loss:0.24963492155075073 and acc: 0.9070512652397156\n",
      " 252/2105 [==>...........................] - ETA: 2:52:50 - accuracy: 0.9071 - loss: 0.2496\n",
      " training set -> batch:253 loss:0.2388300597667694 and acc: 0.9101239442825317\n",
      " 253/2105 [==>...........................] - ETA: 2:52:22 - accuracy: 0.9101 - loss: 0.2388\n",
      " training set -> batch:254 loss:0.24687477946281433 and acc: 0.9089999794960022\n",
      " 254/2105 [==>...........................] - ETA: 2:51:54 - accuracy: 0.9090 - loss: 0.2469\n",
      " training set -> batch:255 loss:0.2455253303050995 and acc: 0.9089147448539734\n",
      " 255/2105 [==>...........................] - ETA: 2:51:26 - accuracy: 0.9089 - loss: 0.2455\n",
      " training set -> batch:256 loss:0.24371293187141418 and acc: 0.9088345766067505\n",
      " 256/2105 [==>...........................] - ETA: 2:50:59 - accuracy: 0.9088 - loss: 0.2437\n",
      " training set -> batch:257 loss:0.24000656604766846 and acc: 0.9087591171264648\n",
      " 257/2105 [==>...........................] - ETA: 2:50:31 - accuracy: 0.9088 - loss: 0.2400\n",
      " training set -> batch:258 loss:0.24100564420223236 and acc: 0.9095744490623474\n",
      " 258/2105 [==>...........................] - ETA: 2:50:04 - accuracy: 0.9096 - loss: 0.2410\n",
      " training set -> batch:259 loss:0.24264542758464813 and acc: 0.9086207151412964\n",
      " 259/2105 [==>...........................] - ETA: 2:49:38 - accuracy: 0.9086 - loss: 0.2426\n",
      " training set -> batch:260 loss:0.24457593262195587 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:260 val loss:0.28058868646621704 and val acc: 0.8876146674156189\n",
      " 260/2105 [==>...........................] - ETA: 2:52:46 - accuracy: 0.9060 - loss: 0.2446\n",
      " training set -> batch:261 loss:0.2676859200000763 and acc: 0.8915929198265076\n",
      " 261/2105 [==>...........................] - ETA: 2:52:19 - accuracy: 0.8916 - loss: 0.2677\n",
      " training set -> batch:262 loss:0.2643566131591797 and acc: 0.8920940160751343\n",
      " 262/2105 [==>...........................] - ETA: 2:51:52 - accuracy: 0.8921 - loss: 0.2644\n",
      " training set -> batch:263 loss:0.2612720727920532 and acc: 0.8935950398445129\n",
      " 263/2105 [==>...........................] - ETA: 2:51:25 - accuracy: 0.8936 - loss: 0.2613\n",
      " training set -> batch:264 loss:0.2536269724369049 and acc: 0.8960000276565552\n",
      " 264/2105 [==>...........................] - ETA: 2:50:59 - accuracy: 0.8960 - loss: 0.2536\n",
      " training set -> batch:265 loss:0.2518167197704315 and acc: 0.895348846912384\n",
      " 265/2105 [==>...........................] - ETA: 2:50:32 - accuracy: 0.8953 - loss: 0.2518\n",
      " training set -> batch:266 loss:0.2536095082759857 and acc: 0.8947368264198303\n",
      " 266/2105 [==>...........................] - ETA: 2:50:05 - accuracy: 0.8947 - loss: 0.2536\n",
      " training set -> batch:267 loss:0.25031331181526184 and acc: 0.8950729966163635\n",
      " 267/2105 [==>...........................] - ETA: 2:49:40 - accuracy: 0.8951 - loss: 0.2503\n",
      " training set -> batch:268 loss:0.26231440901756287 and acc: 0.8918439745903015\n",
      " 268/2105 [==>...........................] - ETA: 2:49:14 - accuracy: 0.8918 - loss: 0.2623\n",
      " training set -> batch:269 loss:0.2622182369232178 and acc: 0.892241358757019\n",
      " 269/2105 [==>...........................] - ETA: 2:48:48 - accuracy: 0.8922 - loss: 0.2622\n",
      " training set -> batch:270 loss:0.26863178610801697 and acc: 0.8909395933151245\n",
      "\n",
      " validation set -> batch:270 val loss:0.24402153491973877 and val acc: 0.896789014339447\n",
      " 270/2105 [==>...........................] - ETA: 2:51:39 - accuracy: 0.8909 - loss: 0.2686\n",
      " training set -> batch:271 loss:0.2582574486732483 and acc: 0.8938053250312805\n",
      " 271/2105 [==>...........................] - ETA: 2:51:13 - accuracy: 0.8938 - loss: 0.2583\n",
      " training set -> batch:272 loss:0.25399425625801086 and acc: 0.8942307829856873\n",
      " 272/2105 [==>...........................] - ETA: 2:50:47 - accuracy: 0.8942 - loss: 0.2540\n",
      " training set -> batch:273 loss:0.2561911344528198 and acc: 0.8925619721412659\n",
      " 273/2105 [==>...........................] - ETA: 2:50:21 - accuracy: 0.8926 - loss: 0.2562\n",
      " training set -> batch:274 loss:0.2519853413105011 and acc: 0.8949999809265137\n",
      " 274/2105 [==>...........................] - ETA: 2:49:56 - accuracy: 0.8950 - loss: 0.2520\n",
      " training set -> batch:275 loss:0.25198090076446533 and acc: 0.895348846912384\n",
      " 275/2105 [==>...........................] - ETA: 2:49:30 - accuracy: 0.8953 - loss: 0.2520\n",
      " training set -> batch:276 loss:0.2567957043647766 and acc: 0.8956766724586487\n",
      " 276/2105 [==>...........................] - ETA: 2:49:04 - accuracy: 0.8957 - loss: 0.2568\n",
      " training set -> batch:277 loss:0.2562216818332672 and acc: 0.8968977928161621\n",
      " 277/2105 [==>...........................] - ETA: 2:48:39 - accuracy: 0.8969 - loss: 0.2562\n",
      " training set -> batch:278 loss:0.2592391073703766 and acc: 0.896276593208313\n",
      " 278/2105 [==>...........................] - ETA: 2:48:14 - accuracy: 0.8963 - loss: 0.2592\n",
      " training set -> batch:279 loss:0.2689189314842224 and acc: 0.8931034207344055\n",
      " 279/2105 [==>...........................] - ETA: 2:47:49 - accuracy: 0.8931 - loss: 0.2689\n",
      " training set -> batch:280 loss:0.263688862323761 and acc: 0.8959731459617615\n",
      "\n",
      " validation set -> batch:280 val loss:0.24875569343566895 and val acc: 0.9002293348312378\n",
      " 280/2105 [==>...........................] - ETA: 2:50:38 - accuracy: 0.8960 - loss: 0.2637\n",
      " training set -> batch:281 loss:0.2478037178516388 and acc: 0.9004424810409546\n",
      " 281/2105 [===>..........................] - ETA: 2:50:13 - accuracy: 0.9004 - loss: 0.2478\n",
      " training set -> batch:282 loss:0.2566012144088745 and acc: 0.8974359035491943\n",
      " 282/2105 [===>..........................] - ETA: 2:49:48 - accuracy: 0.8974 - loss: 0.2566\n",
      " training set -> batch:283 loss:0.2585723102092743 and acc: 0.8977272510528564\n",
      " 283/2105 [===>..........................] - ETA: 2:49:23 - accuracy: 0.8977 - loss: 0.2586\n",
      " training set -> batch:284 loss:0.25402361154556274 and acc: 0.8989999890327454\n",
      " 284/2105 [===>..........................] - ETA: 2:48:58 - accuracy: 0.8990 - loss: 0.2540\n",
      " training set -> batch:285 loss:0.24959297478199005 and acc: 0.9001938104629517\n",
      " 285/2105 [===>..........................] - ETA: 2:48:33 - accuracy: 0.9002 - loss: 0.2496\n",
      " training set -> batch:286 loss:0.2505793571472168 and acc: 0.8994361162185669\n",
      " 286/2105 [===>..........................] - ETA: 2:48:08 - accuracy: 0.8994 - loss: 0.2506\n",
      " training set -> batch:287 loss:0.24796879291534424 and acc: 0.900547444820404\n",
      " 287/2105 [===>..........................] - ETA: 2:47:43 - accuracy: 0.9005 - loss: 0.2480\n",
      " training set -> batch:288 loss:0.24469156563282013 and acc: 0.902482271194458\n",
      " 288/2105 [===>..........................] - ETA: 2:47:19 - accuracy: 0.9025 - loss: 0.2447\n",
      " training set -> batch:289 loss:0.24736344814300537 and acc: 0.9008620977401733\n",
      " 289/2105 [===>..........................] - ETA: 2:46:54 - accuracy: 0.9009 - loss: 0.2474\n",
      " training set -> batch:290 loss:0.25208938121795654 and acc: 0.9001677632331848\n",
      "\n",
      " validation set -> batch:290 val loss:0.2620040774345398 and val acc: 0.8899082541465759\n",
      " 290/2105 [===>..........................] - ETA: 2:49:35 - accuracy: 0.9002 - loss: 0.2521\n",
      " training set -> batch:291 loss:0.2628017067909241 and acc: 0.8882743120193481\n",
      " 291/2105 [===>..........................] - ETA: 2:49:10 - accuracy: 0.8883 - loss: 0.2628\n",
      " training set -> batch:292 loss:0.27214163541793823 and acc: 0.8878205418586731\n",
      " 292/2105 [===>..........................] - ETA: 2:48:45 - accuracy: 0.8878 - loss: 0.2721\n",
      " training set -> batch:293 loss:0.27183815836906433 and acc: 0.8884297609329224\n",
      " 293/2105 [===>..........................] - ETA: 2:48:20 - accuracy: 0.8884 - loss: 0.2718\n",
      " training set -> batch:294 loss:0.2702035903930664 and acc: 0.8889999985694885\n",
      " 294/2105 [===>..........................] - ETA: 2:47:56 - accuracy: 0.8890 - loss: 0.2702\n",
      " training set -> batch:295 loss:0.2617937922477722 and acc: 0.8924418687820435\n",
      " 295/2105 [===>..........................] - ETA: 2:47:31 - accuracy: 0.8924 - loss: 0.2618\n",
      " training set -> batch:296 loss:0.2573269009590149 and acc: 0.893796980381012\n",
      " 296/2105 [===>..........................] - ETA: 2:47:07 - accuracy: 0.8938 - loss: 0.2573\n",
      " training set -> batch:297 loss:0.256973534822464 and acc: 0.8923357725143433\n",
      " 297/2105 [===>..........................] - ETA: 2:46:43 - accuracy: 0.8923 - loss: 0.2570\n",
      " training set -> batch:298 loss:0.24965395033359528 and acc: 0.8945035338401794\n",
      " 298/2105 [===>..........................] - ETA: 2:46:19 - accuracy: 0.8945 - loss: 0.2497\n",
      " training set -> batch:299 loss:0.24945443868637085 and acc: 0.8948276042938232\n",
      " 299/2105 [===>..........................] - ETA: 2:45:55 - accuracy: 0.8948 - loss: 0.2495\n",
      " training set -> batch:300 loss:0.24862505495548248 and acc: 0.8959731459617615\n",
      "\n",
      " validation set -> batch:300 val loss:0.2946001887321472 and val acc: 0.8899082541465759\n",
      " 300/2105 [===>..........................] - ETA: 2:48:34 - accuracy: 0.8960 - loss: 0.2486\n",
      " training set -> batch:301 loss:0.29324162006378174 and acc: 0.8893805146217346\n",
      " 301/2105 [===>..........................] - ETA: 2:48:11 - accuracy: 0.8894 - loss: 0.2932\n",
      " training set -> batch:302 loss:0.292619913816452 and acc: 0.8899572491645813\n",
      " 302/2105 [===>..........................] - ETA: 2:47:47 - accuracy: 0.8900 - loss: 0.2926\n",
      " training set -> batch:303 loss:0.293338418006897 and acc: 0.8904958963394165\n",
      " 303/2105 [===>..........................] - ETA: 2:47:23 - accuracy: 0.8905 - loss: 0.2933\n",
      " training set -> batch:304 loss:0.2960924506187439 and acc: 0.8899999856948853\n",
      " 304/2105 [===>..........................] - ETA: 2:47:00 - accuracy: 0.8900 - loss: 0.2961\n",
      " training set -> batch:305 loss:0.29006218910217285 and acc: 0.8905038833618164\n",
      " 305/2105 [===>..........................] - ETA: 2:46:37 - accuracy: 0.8905 - loss: 0.2901\n",
      " training set -> batch:306 loss:0.2855924963951111 and acc: 0.8919172883033752\n",
      " 306/2105 [===>..........................] - ETA: 2:46:14 - accuracy: 0.8919 - loss: 0.2856\n",
      " training set -> batch:307 loss:0.29029127955436707 and acc: 0.8905109763145447\n",
      " 307/2105 [===>..........................] - ETA: 2:45:51 - accuracy: 0.8905 - loss: 0.2903\n",
      " training set -> batch:308 loss:0.2889000177383423 and acc: 0.8918439745903015\n",
      " 308/2105 [===>..........................] - ETA: 2:45:29 - accuracy: 0.8918 - loss: 0.2889\n",
      " training set -> batch:309 loss:0.28609246015548706 and acc: 0.892241358757019\n",
      " 309/2105 [===>..........................] - ETA: 2:45:06 - accuracy: 0.8922 - loss: 0.2861\n",
      " training set -> batch:310 loss:0.27821943163871765 and acc: 0.8942952752113342\n",
      "\n",
      " validation set -> batch:310 val loss:0.3074776530265808 and val acc: 0.8784403800964355\n",
      " 310/2105 [===>..........................] - ETA: 2:47:36 - accuracy: 0.8943 - loss: 0.2782\n",
      " training set -> batch:311 loss:0.3025248646736145 and acc: 0.8794247508049011\n",
      " 311/2105 [===>..........................] - ETA: 2:47:13 - accuracy: 0.8794 - loss: 0.3025\n",
      " training set -> batch:312 loss:0.30415576696395874 and acc: 0.877136766910553\n",
      " 312/2105 [===>..........................] - ETA: 2:46:50 - accuracy: 0.8771 - loss: 0.3042\n",
      " training set -> batch:313 loss:0.3000163435935974 and acc: 0.8780992031097412\n",
      " 313/2105 [===>..........................] - ETA: 2:46:27 - accuracy: 0.8781 - loss: 0.3000\n",
      " training set -> batch:314 loss:0.3071102499961853 and acc: 0.8799999952316284\n",
      " 314/2105 [===>..........................] - ETA: 2:46:05 - accuracy: 0.8800 - loss: 0.3071\n",
      " training set -> batch:315 loss:0.3104042112827301 and acc: 0.8788759708404541\n",
      " 315/2105 [===>..........................] - ETA: 2:45:43 - accuracy: 0.8789 - loss: 0.3104\n",
      " training set -> batch:316 loss:0.3112781345844269 and acc: 0.8768796920776367\n",
      " 316/2105 [===>..........................] - ETA: 2:45:21 - accuracy: 0.8769 - loss: 0.3113\n",
      " training set -> batch:317 loss:0.30489298701286316 and acc: 0.8795620203018188\n",
      " 317/2105 [===>..........................] - ETA: 2:44:59 - accuracy: 0.8796 - loss: 0.3049\n",
      " training set -> batch:318 loss:0.3132978081703186 and acc: 0.8758864998817444\n",
      " 318/2105 [===>..........................] - ETA: 2:44:37 - accuracy: 0.8759 - loss: 0.3133\n",
      " training set -> batch:319 loss:0.3118685781955719 and acc: 0.875\n",
      " 319/2105 [===>..........................] - ETA: 2:44:15 - accuracy: 0.8750 - loss: 0.3119\n",
      " training set -> batch:320 loss:0.3068822920322418 and acc: 0.8766778707504272\n",
      "\n",
      " validation set -> batch:320 val loss:0.24246026575565338 and val acc: 0.911697268486023\n",
      " 320/2105 [===>..........................] - ETA: 2:46:46 - accuracy: 0.8767 - loss: 0.3069\n",
      " training set -> batch:321 loss:0.2443542182445526 and acc: 0.9115044474601746\n",
      " 321/2105 [===>..........................] - ETA: 2:46:25 - accuracy: 0.9115 - loss: 0.2444\n",
      " training set -> batch:322 loss:0.24112656712532043 and acc: 0.9123931527137756\n",
      " 322/2105 [===>..........................] - ETA: 2:46:02 - accuracy: 0.9124 - loss: 0.2411\n",
      " training set -> batch:323 loss:0.2377222627401352 and acc: 0.913223147392273\n",
      " 323/2105 [===>..........................] - ETA: 2:45:41 - accuracy: 0.9132 - loss: 0.2377\n",
      " training set -> batch:324 loss:0.24574024975299835 and acc: 0.9089999794960022\n",
      " 324/2105 [===>..........................] - ETA: 2:45:19 - accuracy: 0.9090 - loss: 0.2457\n",
      " training set -> batch:325 loss:0.24231848120689392 and acc: 0.9098837375640869\n",
      " 325/2105 [===>..........................] - ETA: 2:44:58 - accuracy: 0.9099 - loss: 0.2423\n",
      " training set -> batch:326 loss:0.2462024688720703 and acc: 0.9078947305679321\n",
      " 326/2105 [===>..........................] - ETA: 2:44:36 - accuracy: 0.9079 - loss: 0.2462\n",
      " training set -> batch:327 loss:0.25879091024398804 and acc: 0.9032846689224243\n",
      " 327/2105 [===>..........................] - ETA: 2:44:15 - accuracy: 0.9033 - loss: 0.2588\n",
      " training set -> batch:328 loss:0.25745514035224915 and acc: 0.9033687710762024\n",
      " 328/2105 [===>..........................] - ETA: 2:43:53 - accuracy: 0.9034 - loss: 0.2575\n",
      " training set -> batch:329 loss:0.2579863667488098 and acc: 0.9034482836723328\n",
      " 329/2105 [===>..........................] - ETA: 2:43:32 - accuracy: 0.9034 - loss: 0.2580\n",
      " training set -> batch:330 loss:0.2621289789676666 and acc: 0.9035235047340393\n",
      "\n",
      " validation set -> batch:330 val loss:0.25143980979919434 and val acc: 0.8990825414657593\n",
      " 330/2105 [===>..........................] - ETA: 2:45:54 - accuracy: 0.9035 - loss: 0.2621\n",
      " training set -> batch:331 loss:0.258184552192688 and acc: 0.8982300758361816\n",
      " 331/2105 [===>..........................] - ETA: 2:45:33 - accuracy: 0.8982 - loss: 0.2582\n",
      " training set -> batch:332 loss:0.2501341700553894 and acc: 0.9006410241127014\n",
      " 332/2105 [===>..........................] - ETA: 2:45:12 - accuracy: 0.9006 - loss: 0.2501\n",
      " training set -> batch:333 loss:0.2417629212141037 and acc: 0.9039255976676941\n",
      " 333/2105 [===>..........................] - ETA: 2:44:51 - accuracy: 0.9039 - loss: 0.2418\n",
      " training set -> batch:334 loss:0.23910437524318695 and acc: 0.9049999713897705\n",
      " 334/2105 [===>..........................] - ETA: 2:44:30 - accuracy: 0.9050 - loss: 0.2391\n",
      " training set -> batch:335 loss:0.23542341589927673 and acc: 0.9069767594337463\n",
      " 335/2105 [===>..........................] - ETA: 2:44:09 - accuracy: 0.9070 - loss: 0.2354\n",
      " training set -> batch:336 loss:0.2374313324689865 and acc: 0.9069548845291138\n",
      " 336/2105 [===>..........................] - ETA: 2:43:49 - accuracy: 0.9070 - loss: 0.2374\n",
      " training set -> batch:337 loss:0.240073561668396 and acc: 0.9069343209266663\n",
      " 337/2105 [===>..........................] - ETA: 2:43:27 - accuracy: 0.9069 - loss: 0.2401\n",
      " training set -> batch:338 loss:0.2445354014635086 and acc: 0.9060283899307251\n",
      " 338/2105 [===>..........................] - ETA: 2:43:06 - accuracy: 0.9060 - loss: 0.2445\n",
      " training set -> batch:339 loss:0.24684515595436096 and acc: 0.9068965315818787\n",
      " 339/2105 [===>..........................] - ETA: 2:42:46 - accuracy: 0.9069 - loss: 0.2468\n",
      " training set -> batch:340 loss:0.23975522816181183 and acc: 0.9093959927558899\n",
      "\n",
      " validation set -> batch:340 val loss:0.25571194291114807 and val acc: 0.89449542760849\n",
      " 340/2105 [===>..........................] - ETA: 2:45:06 - accuracy: 0.9094 - loss: 0.2398\n",
      " training set -> batch:341 loss:0.2492046058177948 and acc: 0.8960176706314087\n",
      " 341/2105 [===>..........................] - ETA: 2:44:45 - accuracy: 0.8960 - loss: 0.2492\n",
      " training set -> batch:342 loss:0.24742382764816284 and acc: 0.8985042572021484\n",
      " 342/2105 [===>..........................] - ETA: 2:44:24 - accuracy: 0.8985 - loss: 0.2474\n",
      " training set -> batch:343 loss:0.2518486976623535 and acc: 0.8997933864593506\n",
      " 343/2105 [===>..........................] - ETA: 2:44:03 - accuracy: 0.8998 - loss: 0.2518\n",
      " training set -> batch:344 loss:0.2564835250377655 and acc: 0.8989999890327454\n",
      " 344/2105 [===>..........................] - ETA: 2:43:42 - accuracy: 0.8990 - loss: 0.2565\n",
      " training set -> batch:345 loss:0.2507651746273041 and acc: 0.9001938104629517\n",
      " 345/2105 [===>..........................] - ETA: 2:43:21 - accuracy: 0.9002 - loss: 0.2508\n",
      " training set -> batch:346 loss:0.25407928228378296 and acc: 0.8984962701797485\n",
      " 346/2105 [===>..........................] - ETA: 2:43:01 - accuracy: 0.8985 - loss: 0.2541\n",
      " training set -> batch:347 loss:0.2566719949245453 and acc: 0.8978102207183838\n",
      " 347/2105 [===>..........................] - ETA: 2:42:40 - accuracy: 0.8978 - loss: 0.2567\n",
      " training set -> batch:348 loss:0.2642761468887329 and acc: 0.8953900933265686\n",
      " 348/2105 [===>..........................] - ETA: 2:42:20 - accuracy: 0.8954 - loss: 0.2643\n",
      " training set -> batch:349 loss:0.2650485634803772 and acc: 0.8939655423164368\n",
      " 349/2105 [===>..........................] - ETA: 2:41:59 - accuracy: 0.8940 - loss: 0.2650\n",
      " training set -> batch:350 loss:0.2692616581916809 and acc: 0.8926174640655518\n",
      "\n",
      " validation set -> batch:350 val loss:0.25092628598213196 and val acc: 0.8979358077049255\n",
      " 350/2105 [===>..........................] - ETA: 2:44:06 - accuracy: 0.8926 - loss: 0.2693\n",
      " training set -> batch:351 loss:0.2505280375480652 and acc: 0.8982300758361816\n",
      " 351/2105 [====>.........................] - ETA: 2:43:45 - accuracy: 0.8982 - loss: 0.2505\n",
      " training set -> batch:352 loss:0.25179266929626465 and acc: 0.8963675498962402\n",
      " 352/2105 [====>.........................] - ETA: 2:43:24 - accuracy: 0.8964 - loss: 0.2518\n",
      " training set -> batch:353 loss:0.24708150327205658 and acc: 0.8966942429542542\n",
      " 353/2105 [====>.........................] - ETA: 2:43:03 - accuracy: 0.8967 - loss: 0.2471\n",
      " training set -> batch:354 loss:0.24339920282363892 and acc: 0.8970000147819519\n",
      " 354/2105 [====>.........................] - ETA: 2:42:43 - accuracy: 0.8970 - loss: 0.2434\n",
      " training set -> batch:355 loss:0.2386370152235031 and acc: 0.8982558250427246\n",
      " 355/2105 [====>.........................] - ETA: 2:42:23 - accuracy: 0.8983 - loss: 0.2386\n",
      " training set -> batch:356 loss:0.24226155877113342 and acc: 0.8975563645362854\n",
      " 356/2105 [====>.........................] - ETA: 2:42:03 - accuracy: 0.8976 - loss: 0.2423\n",
      " training set -> batch:357 loss:0.24075235426425934 and acc: 0.8978102207183838\n",
      " 357/2105 [====>.........................] - ETA: 2:41:42 - accuracy: 0.8978 - loss: 0.2408\n",
      " training set -> batch:358 loss:0.24144743382930756 and acc: 0.8971630930900574\n",
      " 358/2105 [====>.........................] - ETA: 2:41:22 - accuracy: 0.8972 - loss: 0.2414\n",
      " training set -> batch:359 loss:0.24083663523197174 and acc: 0.8982758522033691\n",
      " 359/2105 [====>.........................] - ETA: 2:41:02 - accuracy: 0.8983 - loss: 0.2408\n",
      " training set -> batch:360 loss:0.2404693216085434 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:360 val loss:0.2436443716287613 and val acc: 0.9048165082931519\n",
      " 360/2105 [====>.........................] - ETA: 2:43:14 - accuracy: 0.8993 - loss: 0.2405\n",
      " training set -> batch:361 loss:0.23733234405517578 and acc: 0.9070796370506287\n",
      " 361/2105 [====>.........................] - ETA: 2:42:54 - accuracy: 0.9071 - loss: 0.2373\n",
      " training set -> batch:362 loss:0.2303135246038437 and acc: 0.9081196784973145\n",
      " 362/2105 [====>.........................] - ETA: 2:42:34 - accuracy: 0.9081 - loss: 0.2303\n",
      " training set -> batch:363 loss:0.22541995346546173 and acc: 0.9090909361839294\n",
      " 363/2105 [====>.........................] - ETA: 2:42:14 - accuracy: 0.9091 - loss: 0.2254\n",
      " training set -> batch:364 loss:0.2245161235332489 and acc: 0.9089999794960022\n",
      " 364/2105 [====>.........................] - ETA: 2:41:53 - accuracy: 0.9090 - loss: 0.2245\n",
      " training set -> batch:365 loss:0.232941672205925 and acc: 0.9089147448539734\n",
      " 365/2105 [====>.........................] - ETA: 2:41:33 - accuracy: 0.9089 - loss: 0.2329\n",
      " training set -> batch:366 loss:0.23187604546546936 and acc: 0.9078947305679321\n",
      " 366/2105 [====>.........................] - ETA: 2:41:13 - accuracy: 0.9079 - loss: 0.2319\n",
      " training set -> batch:367 loss:0.22911609709262848 and acc: 0.9078466892242432\n",
      " 367/2105 [====>.........................] - ETA: 2:40:54 - accuracy: 0.9078 - loss: 0.2291\n",
      " training set -> batch:368 loss:0.22444391250610352 and acc: 0.908687949180603\n",
      " 368/2105 [====>.........................] - ETA: 2:40:34 - accuracy: 0.9087 - loss: 0.2244\n",
      " training set -> batch:369 loss:0.228646919131279 and acc: 0.9060344696044922\n",
      " 369/2105 [====>.........................] - ETA: 2:40:14 - accuracy: 0.9060 - loss: 0.2286\n",
      " training set -> batch:370 loss:0.22197222709655762 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:370 val loss:0.2599577009677887 and val acc: 0.8956422209739685\n",
      " 370/2105 [====>.........................] - ETA: 2:42:13 - accuracy: 0.9077 - loss: 0.2220\n",
      " training set -> batch:371 loss:0.2555882930755615 and acc: 0.8960176706314087\n",
      " 371/2105 [====>.........................] - ETA: 2:41:54 - accuracy: 0.8960 - loss: 0.2556\n",
      " training set -> batch:372 loss:0.2542201578617096 and acc: 0.8974359035491943\n",
      " 372/2105 [====>.........................] - ETA: 2:41:34 - accuracy: 0.8974 - loss: 0.2542\n",
      " training set -> batch:373 loss:0.2669440805912018 and acc: 0.8956611752510071\n",
      " 373/2105 [====>.........................] - ETA: 2:41:14 - accuracy: 0.8957 - loss: 0.2669\n",
      " training set -> batch:374 loss:0.26256322860717773 and acc: 0.8949999809265137\n",
      " 374/2105 [====>.........................] - ETA: 2:40:54 - accuracy: 0.8950 - loss: 0.2626\n",
      " training set -> batch:375 loss:0.2595618665218353 and acc: 0.895348846912384\n",
      " 375/2105 [====>.........................] - ETA: 2:40:35 - accuracy: 0.8953 - loss: 0.2596\n",
      " training set -> batch:376 loss:0.25382915139198303 and acc: 0.8975563645362854\n",
      " 376/2105 [====>.........................] - ETA: 2:40:16 - accuracy: 0.8976 - loss: 0.2538\n",
      " training set -> batch:377 loss:0.2533037066459656 and acc: 0.8968977928161621\n",
      " 377/2105 [====>.........................] - ETA: 2:39:56 - accuracy: 0.8969 - loss: 0.2533\n",
      " training set -> batch:378 loss:0.25443485379219055 and acc: 0.8971630930900574\n",
      " 378/2105 [====>.........................] - ETA: 2:39:38 - accuracy: 0.8972 - loss: 0.2544\n",
      " training set -> batch:379 loss:0.2532216012477875 and acc: 0.8965517282485962\n",
      " 379/2105 [====>.........................] - ETA: 2:39:19 - accuracy: 0.8966 - loss: 0.2532\n",
      " training set -> batch:380 loss:0.247087761759758 and acc: 0.8984899520874023\n",
      "\n",
      " validation set -> batch:380 val loss:0.2504275441169739 and val acc: 0.8990825414657593\n",
      " 380/2105 [====>.........................] - ETA: 2:41:22 - accuracy: 0.8985 - loss: 0.2471\n",
      " training set -> batch:381 loss:0.24585677683353424 and acc: 0.8993362784385681\n",
      " 381/2105 [====>.........................] - ETA: 2:41:03 - accuracy: 0.8993 - loss: 0.2459\n",
      " training set -> batch:382 loss:0.24616257846355438 and acc: 0.8995726704597473\n",
      " 382/2105 [====>.........................] - ETA: 2:40:44 - accuracy: 0.8996 - loss: 0.2462\n",
      " training set -> batch:383 loss:0.24531204998493195 and acc: 0.9008264541625977\n",
      " 383/2105 [====>.........................] - ETA: 2:40:25 - accuracy: 0.9008 - loss: 0.2453\n",
      " training set -> batch:384 loss:0.25547465682029724 and acc: 0.8970000147819519\n",
      " 384/2105 [====>.........................] - ETA: 2:40:06 - accuracy: 0.8970 - loss: 0.2555\n",
      " training set -> batch:385 loss:0.25550132989883423 and acc: 0.8963178396224976\n",
      " 385/2105 [====>.........................] - ETA: 2:39:47 - accuracy: 0.8963 - loss: 0.2555\n",
      " training set -> batch:386 loss:0.24949586391448975 and acc: 0.896616518497467\n",
      " 386/2105 [====>.........................] - ETA: 2:39:28 - accuracy: 0.8966 - loss: 0.2495\n",
      " training set -> batch:387 loss:0.24890361726284027 and acc: 0.8959854245185852\n",
      " 387/2105 [====>.........................] - ETA: 2:39:10 - accuracy: 0.8960 - loss: 0.2489\n",
      " training set -> batch:388 loss:0.2504911720752716 and acc: 0.8953900933265686\n",
      " 388/2105 [====>.........................] - ETA: 2:38:51 - accuracy: 0.8954 - loss: 0.2505\n",
      " training set -> batch:389 loss:0.24702051281929016 and acc: 0.8974137902259827\n",
      " 389/2105 [====>.........................] - ETA: 2:38:33 - accuracy: 0.8974 - loss: 0.2470\n",
      " training set -> batch:390 loss:0.24008303880691528 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:390 val loss:0.25034675002098083 and val acc: 0.9002293348312378\n",
      " 390/2105 [====>.........................] - ETA: 2:40:30 - accuracy: 0.8993 - loss: 0.2401\n",
      " training set -> batch:391 loss:0.26852867007255554 and acc: 0.8960176706314087\n",
      " 391/2105 [====>.........................] - ETA: 2:40:12 - accuracy: 0.8960 - loss: 0.2685\n",
      " training set -> batch:392 loss:0.26646560430526733 and acc: 0.8963675498962402\n",
      " 392/2105 [====>.........................] - ETA: 2:39:53 - accuracy: 0.8964 - loss: 0.2665\n",
      " training set -> batch:393 loss:0.2649611234664917 and acc: 0.8956611752510071\n",
      " 393/2105 [====>.........................] - ETA: 2:39:34 - accuracy: 0.8957 - loss: 0.2650\n",
      " training set -> batch:394 loss:0.26540863513946533 and acc: 0.8960000276565552\n",
      " 394/2105 [====>.........................] - ETA: 2:39:16 - accuracy: 0.8960 - loss: 0.2654\n",
      " training set -> batch:395 loss:0.2602396309375763 and acc: 0.8982558250427246\n",
      " 395/2105 [====>.........................] - ETA: 2:38:58 - accuracy: 0.8983 - loss: 0.2602\n",
      " training set -> batch:396 loss:0.2663811147212982 and acc: 0.896616518497467\n",
      " 396/2105 [====>.........................] - ETA: 2:38:39 - accuracy: 0.8966 - loss: 0.2664\n",
      " training set -> batch:397 loss:0.26378196477890015 and acc: 0.8978102207183838\n",
      " 397/2105 [====>.........................] - ETA: 2:38:21 - accuracy: 0.8978 - loss: 0.2638\n",
      " training set -> batch:398 loss:0.26218661665916443 and acc: 0.8971630930900574\n",
      " 398/2105 [====>.........................] - ETA: 2:38:02 - accuracy: 0.8972 - loss: 0.2622\n",
      " training set -> batch:399 loss:0.255886048078537 and acc: 0.8982758522033691\n",
      " 399/2105 [====>.........................] - ETA: 2:37:44 - accuracy: 0.8983 - loss: 0.2559\n",
      " training set -> batch:400 loss:0.25268667936325073 and acc: 0.8984899520874023\n",
      "\n",
      " validation set -> batch:400 val loss:0.2513684928417206 and val acc: 0.8899082541465759\n",
      " 400/2105 [====>.........................] - ETA: 2:39:36 - accuracy: 0.8985 - loss: 0.2527\n",
      " training set -> batch:401 loss:0.2475135624408722 and acc: 0.8904867172241211\n",
      " 401/2105 [====>.........................] - ETA: 2:39:17 - accuracy: 0.8905 - loss: 0.2475\n",
      " training set -> batch:402 loss:0.24541383981704712 and acc: 0.8910256624221802\n",
      " 402/2105 [====>.........................] - ETA: 2:38:58 - accuracy: 0.8910 - loss: 0.2454\n",
      " training set -> batch:403 loss:0.24223458766937256 and acc: 0.8925619721412659\n",
      " 403/2105 [====>.........................] - ETA: 2:38:40 - accuracy: 0.8926 - loss: 0.2422\n",
      " training set -> batch:404 loss:0.2440468668937683 and acc: 0.8920000195503235\n",
      " 404/2105 [====>.........................] - ETA: 2:38:21 - accuracy: 0.8920 - loss: 0.2440\n",
      " training set -> batch:405 loss:0.2438601404428482 and acc: 0.893410861492157\n",
      " 405/2105 [====>.........................] - ETA: 2:38:03 - accuracy: 0.8934 - loss: 0.2439\n",
      " training set -> batch:406 loss:0.25569891929626465 and acc: 0.8928571343421936\n",
      " 406/2105 [====>.........................] - ETA: 2:37:45 - accuracy: 0.8929 - loss: 0.2557\n",
      " training set -> batch:407 loss:0.2660275101661682 and acc: 0.8905109763145447\n",
      " 407/2105 [====>.........................] - ETA: 2:37:27 - accuracy: 0.8905 - loss: 0.2660\n",
      " training set -> batch:408 loss:0.2730686068534851 and acc: 0.88741135597229\n",
      " 408/2105 [====>.........................] - ETA: 2:37:09 - accuracy: 0.8874 - loss: 0.2731\n",
      " training set -> batch:409 loss:0.26996010541915894 and acc: 0.8870689868927002\n",
      " 409/2105 [====>.........................] - ETA: 2:36:51 - accuracy: 0.8871 - loss: 0.2700\n",
      " training set -> batch:410 loss:0.26525813341140747 and acc: 0.8884228467941284\n",
      "\n",
      " validation set -> batch:410 val loss:0.25856542587280273 and val acc: 0.8933486342430115\n",
      " 410/2105 [====>.........................] - ETA: 2:38:47 - accuracy: 0.8884 - loss: 0.2653\n",
      " training set -> batch:411 loss:0.255635529756546 and acc: 0.8938053250312805\n",
      " 411/2105 [====>.........................] - ETA: 2:38:29 - accuracy: 0.8938 - loss: 0.2556\n",
      " training set -> batch:412 loss:0.2522180378437042 and acc: 0.8952991366386414\n",
      " 412/2105 [====>.........................] - ETA: 2:38:10 - accuracy: 0.8953 - loss: 0.2522\n",
      " training set -> batch:413 loss:0.2520291209220886 and acc: 0.8966942429542542\n",
      " 413/2105 [====>.........................] - ETA: 2:37:52 - accuracy: 0.8967 - loss: 0.2520\n",
      " training set -> batch:414 loss:0.2547917068004608 and acc: 0.8970000147819519\n",
      " 414/2105 [====>.........................] - ETA: 2:37:34 - accuracy: 0.8970 - loss: 0.2548\n",
      " training set -> batch:415 loss:0.2544163167476654 and acc: 0.8972868323326111\n",
      " 415/2105 [====>.........................] - ETA: 2:37:16 - accuracy: 0.8973 - loss: 0.2544\n",
      " training set -> batch:416 loss:0.24701166152954102 and acc: 0.9003759622573853\n",
      " 416/2105 [====>.........................] - ETA: 2:36:58 - accuracy: 0.9004 - loss: 0.2470\n",
      " training set -> batch:417 loss:0.2552575469017029 and acc: 0.8968977928161621\n",
      " 417/2105 [====>.........................] - ETA: 2:36:41 - accuracy: 0.8969 - loss: 0.2553\n",
      " training set -> batch:418 loss:0.2578940987586975 and acc: 0.8953900933265686\n",
      " 418/2105 [====>.........................] - ETA: 2:36:23 - accuracy: 0.8954 - loss: 0.2579\n",
      " training set -> batch:419 loss:0.2533918619155884 and acc: 0.8965517282485962\n",
      " 419/2105 [====>.........................] - ETA: 2:36:06 - accuracy: 0.8966 - loss: 0.2534\n",
      " training set -> batch:420 loss:0.24875521659851074 and acc: 0.8984899520874023\n",
      "\n",
      " validation set -> batch:420 val loss:0.24676819145679474 and val acc: 0.9048165082931519\n",
      " 420/2105 [====>.........................] - ETA: 2:37:51 - accuracy: 0.8985 - loss: 0.2488\n",
      " training set -> batch:421 loss:0.24890798330307007 and acc: 0.9048672318458557\n",
      " 421/2105 [=====>........................] - ETA: 2:37:33 - accuracy: 0.9049 - loss: 0.2489\n",
      " training set -> batch:422 loss:0.24840831756591797 and acc: 0.9049145579338074\n",
      " 422/2105 [=====>........................] - ETA: 2:37:16 - accuracy: 0.9049 - loss: 0.2484\n",
      " training set -> batch:423 loss:0.2554183304309845 and acc: 0.9028925895690918\n",
      " 423/2105 [=====>........................] - ETA: 2:36:58 - accuracy: 0.9029 - loss: 0.2554\n",
      " training set -> batch:424 loss:0.24998535215854645 and acc: 0.9039999842643738\n",
      " 424/2105 [=====>........................] - ETA: 2:36:40 - accuracy: 0.9040 - loss: 0.2500\n",
      " training set -> batch:425 loss:0.25777265429496765 and acc: 0.9031007885932922\n",
      " 425/2105 [=====>........................] - ETA: 2:36:23 - accuracy: 0.9031 - loss: 0.2578\n",
      " training set -> batch:426 loss:0.2487596720457077 and acc: 0.9060150384902954\n",
      " 426/2105 [=====>........................] - ETA: 2:36:05 - accuracy: 0.9060 - loss: 0.2488\n",
      " training set -> batch:427 loss:0.25085559487342834 and acc: 0.9060218930244446\n",
      " 427/2105 [=====>........................] - ETA: 2:35:48 - accuracy: 0.9060 - loss: 0.2509\n",
      " training set -> batch:428 loss:0.251282274723053 and acc: 0.9060283899307251\n",
      " 428/2105 [=====>........................] - ETA: 2:35:31 - accuracy: 0.9060 - loss: 0.2513\n",
      " training set -> batch:429 loss:0.2524735927581787 and acc: 0.9060344696044922\n",
      " 429/2105 [=====>........................] - ETA: 2:35:14 - accuracy: 0.9060 - loss: 0.2525\n",
      " training set -> batch:430 loss:0.25109732151031494 and acc: 0.9052013158798218\n",
      "\n",
      " validation set -> batch:430 val loss:0.2469908446073532 and val acc: 0.9002293348312378\n",
      " 430/2105 [=====>........................] - ETA: 2:36:53 - accuracy: 0.9052 - loss: 0.2511\n",
      " training set -> batch:431 loss:0.2509947121143341 and acc: 0.8971238732337952\n",
      " 431/2105 [=====>........................] - ETA: 2:36:37 - accuracy: 0.8971 - loss: 0.2510\n",
      " training set -> batch:432 loss:0.24466651678085327 and acc: 0.8985042572021484\n",
      " 432/2105 [=====>........................] - ETA: 2:36:19 - accuracy: 0.8985 - loss: 0.2447\n",
      " training set -> batch:433 loss:0.24526004493236542 and acc: 0.8987603187561035\n",
      " 433/2105 [=====>........................] - ETA: 2:36:02 - accuracy: 0.8988 - loss: 0.2453\n",
      " training set -> batch:434 loss:0.24904203414916992 and acc: 0.8970000147819519\n",
      " 434/2105 [=====>........................] - ETA: 2:35:45 - accuracy: 0.8970 - loss: 0.2490\n",
      " training set -> batch:435 loss:0.24121001362800598 and acc: 0.8992248177528381\n",
      " 435/2105 [=====>........................] - ETA: 2:35:28 - accuracy: 0.8992 - loss: 0.2412\n",
      " training set -> batch:436 loss:0.23473449051380157 and acc: 0.9013158082962036\n",
      " 436/2105 [=====>........................] - ETA: 2:35:11 - accuracy: 0.9013 - loss: 0.2347\n",
      " training set -> batch:437 loss:0.2322724163532257 and acc: 0.9014598727226257\n",
      " 437/2105 [=====>........................] - ETA: 2:34:55 - accuracy: 0.9015 - loss: 0.2323\n",
      " training set -> batch:438 loss:0.23078688979148865 and acc: 0.902482271194458\n",
      " 438/2105 [=====>........................] - ETA: 2:34:38 - accuracy: 0.9025 - loss: 0.2308\n",
      " training set -> batch:439 loss:0.2354944944381714 and acc: 0.9017241597175598\n",
      " 439/2105 [=====>........................] - ETA: 2:34:21 - accuracy: 0.9017 - loss: 0.2355\n",
      " training set -> batch:440 loss:0.2316204458475113 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:440 val loss:0.24642597138881683 and val acc: 0.8979358077049255\n",
      " 440/2105 [=====>........................] - ETA: 2:36:01 - accuracy: 0.9027 - loss: 0.2316\n",
      " training set -> batch:441 loss:0.24413496255874634 and acc: 0.8982300758361816\n",
      " 441/2105 [=====>........................] - ETA: 2:35:45 - accuracy: 0.8982 - loss: 0.2441\n",
      " training set -> batch:442 loss:0.2570109963417053 and acc: 0.8974359035491943\n",
      " 442/2105 [=====>........................] - ETA: 2:35:27 - accuracy: 0.8974 - loss: 0.2570\n",
      " training set -> batch:443 loss:0.25324031710624695 and acc: 0.8977272510528564\n",
      " 443/2105 [=====>........................] - ETA: 2:35:10 - accuracy: 0.8977 - loss: 0.2532\n",
      " training set -> batch:444 loss:0.24258248507976532 and acc: 0.9010000228881836\n",
      " 444/2105 [=====>........................] - ETA: 2:34:53 - accuracy: 0.9010 - loss: 0.2426\n",
      " training set -> batch:445 loss:0.2414146214723587 and acc: 0.9021317958831787\n",
      " 445/2105 [=====>........................] - ETA: 2:34:36 - accuracy: 0.9021 - loss: 0.2414\n",
      " training set -> batch:446 loss:0.23710305988788605 and acc: 0.902255654335022\n",
      " 446/2105 [=====>........................] - ETA: 2:34:20 - accuracy: 0.9023 - loss: 0.2371\n",
      " training set -> batch:447 loss:0.22973771393299103 and acc: 0.9051094651222229\n",
      " 447/2105 [=====>........................] - ETA: 2:34:03 - accuracy: 0.9051 - loss: 0.2297\n",
      " training set -> batch:448 loss:0.22309346497058868 and acc: 0.9069148898124695\n",
      " 448/2105 [=====>........................] - ETA: 2:33:46 - accuracy: 0.9069 - loss: 0.2231\n",
      " training set -> batch:449 loss:0.2225910723209381 and acc: 0.9077585935592651\n",
      " 449/2105 [=====>........................] - ETA: 2:33:29 - accuracy: 0.9078 - loss: 0.2226\n",
      " training set -> batch:450 loss:0.21695320308208466 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:450 val loss:0.2481238692998886 and val acc: 0.8990825414657593\n",
      " 450/2105 [=====>........................] - ETA: 2:35:03 - accuracy: 0.9102 - loss: 0.2170\n",
      " training set -> batch:451 loss:0.24387119710445404 and acc: 0.8982300758361816\n",
      " 451/2105 [=====>........................] - ETA: 2:34:46 - accuracy: 0.8982 - loss: 0.2439\n",
      " training set -> batch:452 loss:0.26546046137809753 and acc: 0.8963675498962402\n",
      " 452/2105 [=====>........................] - ETA: 2:34:29 - accuracy: 0.8964 - loss: 0.2655\n",
      " training set -> batch:453 loss:0.26225021481513977 and acc: 0.8956611752510071\n",
      " 453/2105 [=====>........................] - ETA: 2:34:13 - accuracy: 0.8957 - loss: 0.2623\n",
      " training set -> batch:454 loss:0.2594250440597534 and acc: 0.8949999809265137\n",
      " 454/2105 [=====>........................] - ETA: 2:33:56 - accuracy: 0.8950 - loss: 0.2594\n",
      " training set -> batch:455 loss:0.26567012071609497 and acc: 0.893410861492157\n",
      " 455/2105 [=====>........................] - ETA: 2:33:39 - accuracy: 0.8934 - loss: 0.2657\n",
      " training set -> batch:456 loss:0.2836204171180725 and acc: 0.8881579041481018\n",
      " 456/2105 [=====>........................] - ETA: 2:33:23 - accuracy: 0.8882 - loss: 0.2836\n",
      " training set -> batch:457 loss:0.2822844982147217 and acc: 0.8886861205101013\n",
      " 457/2105 [=====>........................] - ETA: 2:33:07 - accuracy: 0.8887 - loss: 0.2823\n",
      " training set -> batch:458 loss:0.27753308415412903 and acc: 0.8882978558540344\n",
      " 458/2105 [=====>........................] - ETA: 2:32:50 - accuracy: 0.8883 - loss: 0.2775\n",
      " training set -> batch:459 loss:0.27408283948898315 and acc: 0.8887931108474731\n",
      " 459/2105 [=====>........................] - ETA: 2:32:35 - accuracy: 0.8888 - loss: 0.2741\n",
      " training set -> batch:460 loss:0.27239224314689636 and acc: 0.8901006579399109\n",
      "\n",
      " validation set -> batch:460 val loss:0.2764255106449127 and val acc: 0.8910550475120544\n",
      " 460/2105 [=====>........................] - ETA: 2:34:04 - accuracy: 0.8901 - loss: 0.2724\n",
      " training set -> batch:461 loss:0.2621913254261017 and acc: 0.8938053250312805\n",
      " 461/2105 [=====>........................] - ETA: 2:33:48 - accuracy: 0.8938 - loss: 0.2622\n",
      " training set -> batch:462 loss:0.26956260204315186 and acc: 0.8910256624221802\n",
      " 462/2105 [=====>........................] - ETA: 2:33:31 - accuracy: 0.8910 - loss: 0.2696\n",
      " training set -> batch:463 loss:0.2700422704219818 and acc: 0.8915289044380188\n",
      " 463/2105 [=====>........................] - ETA: 2:33:15 - accuracy: 0.8915 - loss: 0.2700\n",
      " training set -> batch:464 loss:0.2604416012763977 and acc: 0.8939999938011169\n",
      " 464/2105 [=====>........................] - ETA: 2:32:58 - accuracy: 0.8940 - loss: 0.2604\n",
      " training set -> batch:465 loss:0.2526971995830536 and acc: 0.8972868323326111\n",
      " 465/2105 [=====>........................] - ETA: 2:32:42 - accuracy: 0.8973 - loss: 0.2527\n",
      " training set -> batch:466 loss:0.250027596950531 and acc: 0.896616518497467\n",
      " 466/2105 [=====>........................] - ETA: 2:32:26 - accuracy: 0.8966 - loss: 0.2500\n",
      " training set -> batch:467 loss:0.24353565275669098 and acc: 0.8987226486206055\n",
      " 467/2105 [=====>........................] - ETA: 2:32:09 - accuracy: 0.8987 - loss: 0.2435\n",
      " training set -> batch:468 loss:0.24248071014881134 and acc: 0.8989361524581909\n",
      " 468/2105 [=====>........................] - ETA: 2:31:53 - accuracy: 0.8989 - loss: 0.2425\n",
      " training set -> batch:469 loss:0.23850378394126892 and acc: 0.8999999761581421\n",
      " 469/2105 [=====>........................] - ETA: 2:31:38 - accuracy: 0.9000 - loss: 0.2385\n",
      " training set -> batch:470 loss:0.23624670505523682 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:470 val loss:0.24684853851795197 and val acc: 0.9002293348312378\n",
      " 470/2105 [=====>........................] - ETA: 2:33:06 - accuracy: 0.8993 - loss: 0.2362\n",
      " training set -> batch:471 loss:0.24192696809768677 and acc: 0.9015486836433411\n",
      " 471/2105 [=====>........................] - ETA: 2:32:50 - accuracy: 0.9015 - loss: 0.2419\n",
      " training set -> batch:472 loss:0.24284198880195618 and acc: 0.9006410241127014\n",
      " 472/2105 [=====>........................] - ETA: 2:32:33 - accuracy: 0.9006 - loss: 0.2428\n",
      " training set -> batch:473 loss:0.2393619269132614 and acc: 0.9018595218658447\n",
      " 473/2105 [=====>........................] - ETA: 2:32:17 - accuracy: 0.9019 - loss: 0.2394\n",
      " training set -> batch:474 loss:0.24519212543964386 and acc: 0.8989999890327454\n",
      " 474/2105 [=====>........................] - ETA: 2:32:01 - accuracy: 0.8990 - loss: 0.2452\n",
      " training set -> batch:475 loss:0.2484763115644455 and acc: 0.9001938104629517\n",
      " 475/2105 [=====>........................] - ETA: 2:31:45 - accuracy: 0.9002 - loss: 0.2485\n",
      " training set -> batch:476 loss:0.25017213821411133 and acc: 0.9003759622573853\n",
      " 476/2105 [=====>........................] - ETA: 2:31:29 - accuracy: 0.9004 - loss: 0.2502\n",
      " training set -> batch:477 loss:0.26125568151474 and acc: 0.8987226486206055\n",
      " 477/2105 [=====>........................] - ETA: 2:31:13 - accuracy: 0.8987 - loss: 0.2613\n",
      " training set -> batch:478 loss:0.2619315981864929 and acc: 0.8980496525764465\n",
      " 478/2105 [=====>........................] - ETA: 2:30:58 - accuracy: 0.8980 - loss: 0.2619\n",
      " training set -> batch:479 loss:0.261282354593277 and acc: 0.8982758522033691\n",
      " 479/2105 [=====>........................] - ETA: 2:30:43 - accuracy: 0.8983 - loss: 0.2613\n",
      " training set -> batch:480 loss:0.25844117999076843 and acc: 0.8984899520874023\n",
      "\n",
      " validation set -> batch:480 val loss:0.26172715425491333 and val acc: 0.8887614607810974\n",
      " 480/2105 [=====>........................] - ETA: 2:32:06 - accuracy: 0.8985 - loss: 0.2584\n",
      " training set -> batch:481 loss:0.2831387221813202 and acc: 0.88606196641922\n",
      " 481/2105 [=====>........................] - ETA: 2:31:51 - accuracy: 0.8861 - loss: 0.2831\n",
      " training set -> batch:482 loss:0.28396061062812805 and acc: 0.8856837749481201\n",
      " 482/2105 [=====>........................] - ETA: 2:31:35 - accuracy: 0.8857 - loss: 0.2840\n",
      " training set -> batch:483 loss:0.28992241621017456 and acc: 0.8832644820213318\n",
      " 483/2105 [=====>........................] - ETA: 2:31:19 - accuracy: 0.8833 - loss: 0.2899\n",
      " training set -> batch:484 loss:0.2820209264755249 and acc: 0.8859999775886536\n",
      " 484/2105 [=====>........................] - ETA: 2:31:03 - accuracy: 0.8860 - loss: 0.2820\n",
      " training set -> batch:485 loss:0.28571996092796326 and acc: 0.8846899271011353\n",
      " 485/2105 [=====>........................] - ETA: 2:30:48 - accuracy: 0.8847 - loss: 0.2857\n",
      " training set -> batch:486 loss:0.2755349576473236 and acc: 0.8881579041481018\n",
      " 486/2105 [=====>........................] - ETA: 2:30:32 - accuracy: 0.8882 - loss: 0.2755\n",
      " training set -> batch:487 loss:0.2738698422908783 and acc: 0.8886861205101013\n",
      " 487/2105 [=====>........................] - ETA: 2:30:17 - accuracy: 0.8887 - loss: 0.2739\n",
      " training set -> batch:488 loss:0.27400046586990356 and acc: 0.8882978558540344\n",
      " 488/2105 [=====>........................] - ETA: 2:30:02 - accuracy: 0.8883 - loss: 0.2740\n",
      " training set -> batch:489 loss:0.27327200770378113 and acc: 0.8879310488700867\n",
      " 489/2105 [=====>........................] - ETA: 2:29:46 - accuracy: 0.8879 - loss: 0.2733\n",
      " training set -> batch:490 loss:0.2709287405014038 and acc: 0.8901006579399109\n",
      "\n",
      " validation set -> batch:490 val loss:0.2410839945077896 and val acc: 0.9059633016586304\n",
      " 490/2105 [=====>........................] - ETA: 2:31:11 - accuracy: 0.8901 - loss: 0.2709\n",
      " training set -> batch:491 loss:0.2452487200498581 and acc: 0.9048672318458557\n",
      " 491/2105 [=====>........................] - ETA: 2:30:55 - accuracy: 0.9049 - loss: 0.2452\n",
      " training set -> batch:492 loss:0.24656395614147186 and acc: 0.9059829115867615\n",
      " 492/2105 [======>.......................] - ETA: 2:30:39 - accuracy: 0.9060 - loss: 0.2466\n",
      " training set -> batch:493 loss:0.2470349669456482 and acc: 0.9049586653709412\n",
      " 493/2105 [======>.......................] - ETA: 2:30:24 - accuracy: 0.9050 - loss: 0.2470\n",
      " training set -> batch:494 loss:0.2462453842163086 and acc: 0.9039999842643738\n",
      " 494/2105 [======>.......................] - ETA: 2:30:09 - accuracy: 0.9040 - loss: 0.2462\n",
      " training set -> batch:495 loss:0.23937518894672394 and acc: 0.9069767594337463\n",
      " 495/2105 [======>.......................] - ETA: 2:29:54 - accuracy: 0.9070 - loss: 0.2394\n",
      " training set -> batch:496 loss:0.23650717735290527 and acc: 0.9069548845291138\n",
      " 496/2105 [======>.......................] - ETA: 2:29:38 - accuracy: 0.9070 - loss: 0.2365\n",
      " training set -> batch:497 loss:0.24104513227939606 and acc: 0.9060218930244446\n",
      " 497/2105 [======>.......................] - ETA: 2:29:23 - accuracy: 0.9060 - loss: 0.2410\n",
      " training set -> batch:498 loss:0.23662187159061432 and acc: 0.9078013896942139\n",
      " 498/2105 [======>.......................] - ETA: 2:29:08 - accuracy: 0.9078 - loss: 0.2366\n",
      " training set -> batch:499 loss:0.23343384265899658 and acc: 0.9094827771186829\n",
      " 499/2105 [======>.......................] - ETA: 2:28:52 - accuracy: 0.9095 - loss: 0.2334\n",
      " training set -> batch:500 loss:0.2326079159975052 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:500 val loss:0.24674955010414124 and val acc: 0.8956422209739685\n",
      " 500/2105 [======>.......................] - ETA: 2:30:15 - accuracy: 0.9102 - loss: 0.2326\n",
      " training set -> batch:501 loss:0.25248831510543823 and acc: 0.894911527633667\n",
      " 501/2105 [======>.......................] - ETA: 2:30:00 - accuracy: 0.8949 - loss: 0.2525\n",
      " training set -> batch:502 loss:0.25716328620910645 and acc: 0.8942307829856873\n",
      " 502/2105 [======>.......................] - ETA: 2:29:44 - accuracy: 0.8942 - loss: 0.2572\n",
      " training set -> batch:503 loss:0.26450246572494507 and acc: 0.8935950398445129\n",
      " 503/2105 [======>.......................] - ETA: 2:29:29 - accuracy: 0.8936 - loss: 0.2645\n",
      " training set -> batch:504 loss:0.265913724899292 and acc: 0.8939999938011169\n",
      " 504/2105 [======>.......................] - ETA: 2:29:14 - accuracy: 0.8940 - loss: 0.2659\n",
      " training set -> batch:505 loss:0.2632504105567932 and acc: 0.895348846912384\n",
      " 505/2105 [======>.......................] - ETA: 2:28:59 - accuracy: 0.8953 - loss: 0.2633\n",
      " training set -> batch:506 loss:0.2725144028663635 and acc: 0.8919172883033752\n",
      " 506/2105 [======>.......................] - ETA: 2:28:44 - accuracy: 0.8919 - loss: 0.2725\n",
      " training set -> batch:507 loss:0.26595136523246765 and acc: 0.8941605687141418\n",
      " 507/2105 [======>.......................] - ETA: 2:28:29 - accuracy: 0.8942 - loss: 0.2660\n",
      " training set -> batch:508 loss:0.2621603012084961 and acc: 0.8953900933265686\n",
      " 508/2105 [======>.......................] - ETA: 2:28:14 - accuracy: 0.8954 - loss: 0.2622\n",
      " training set -> batch:509 loss:0.2574549615383148 and acc: 0.8974137902259827\n",
      " 509/2105 [======>.......................] - ETA: 2:27:59 - accuracy: 0.8974 - loss: 0.2575\n",
      " training set -> batch:510 loss:0.2591778337955475 and acc: 0.8968120813369751\n",
      "\n",
      " validation set -> batch:510 val loss:0.23533670604228973 and val acc: 0.9105504751205444\n",
      " 510/2105 [======>.......................] - ETA: 2:29:25 - accuracy: 0.8968 - loss: 0.2592\n",
      " training set -> batch:511 loss:0.23349341750144958 and acc: 0.9092920422554016\n",
      " 511/2105 [======>.......................] - ETA: 2:29:10 - accuracy: 0.9093 - loss: 0.2335\n",
      " training set -> batch:512 loss:0.23360344767570496 and acc: 0.9091880321502686\n",
      " 512/2105 [======>.......................] - ETA: 2:28:55 - accuracy: 0.9092 - loss: 0.2336\n",
      " training set -> batch:513 loss:0.23492851853370667 and acc: 0.9059917330741882\n",
      " 513/2105 [======>.......................] - ETA: 2:28:40 - accuracy: 0.9060 - loss: 0.2349\n",
      " training set -> batch:514 loss:0.23226775228977203 and acc: 0.906000018119812\n",
      " 514/2105 [======>.......................] - ETA: 2:28:25 - accuracy: 0.9060 - loss: 0.2323\n",
      " training set -> batch:515 loss:0.23604197800159454 and acc: 0.9050387740135193\n",
      " 515/2105 [======>.......................] - ETA: 2:28:11 - accuracy: 0.9050 - loss: 0.2360\n",
      " training set -> batch:516 loss:0.23169651627540588 and acc: 0.9060150384902954\n",
      " 516/2105 [======>.......................] - ETA: 2:27:56 - accuracy: 0.9060 - loss: 0.2317\n",
      " training set -> batch:517 loss:0.22592517733573914 and acc: 0.9078466892242432\n",
      " 517/2105 [======>.......................] - ETA: 2:27:41 - accuracy: 0.9078 - loss: 0.2259\n",
      " training set -> batch:518 loss:0.22293692827224731 and acc: 0.9078013896942139\n",
      " 518/2105 [======>.......................] - ETA: 2:27:27 - accuracy: 0.9078 - loss: 0.2229\n",
      " training set -> batch:519 loss:0.21588385105133057 and acc: 0.9103448390960693\n",
      " 519/2105 [======>.......................] - ETA: 2:27:12 - accuracy: 0.9103 - loss: 0.2159\n",
      " training set -> batch:520 loss:0.21163372695446014 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:520 val loss:0.25997093319892883 and val acc: 0.892201840877533\n",
      " 520/2105 [======>.......................] - ETA: 2:28:27 - accuracy: 0.9119 - loss: 0.2116\n",
      " training set -> batch:521 loss:0.26673460006713867 and acc: 0.8915929198265076\n",
      " 521/2105 [======>.......................] - ETA: 2:28:12 - accuracy: 0.8916 - loss: 0.2667\n",
      " training set -> batch:522 loss:0.27816706895828247 and acc: 0.8899572491645813\n",
      " 522/2105 [======>.......................] - ETA: 2:27:57 - accuracy: 0.8900 - loss: 0.2782\n",
      " training set -> batch:523 loss:0.27893558144569397 and acc: 0.8894628286361694\n",
      " 523/2105 [======>.......................] - ETA: 2:27:43 - accuracy: 0.8895 - loss: 0.2789\n",
      " training set -> batch:524 loss:0.27297866344451904 and acc: 0.890999972820282\n",
      " 524/2105 [======>.......................] - ETA: 2:27:28 - accuracy: 0.8910 - loss: 0.2730\n",
      " training set -> batch:525 loss:0.26781073212623596 and acc: 0.8914728760719299\n",
      " 525/2105 [======>.......................] - ETA: 2:27:13 - accuracy: 0.8915 - loss: 0.2678\n",
      " training set -> batch:526 loss:0.2640513777732849 and acc: 0.8928571343421936\n",
      " 526/2105 [======>.......................] - ETA: 2:26:58 - accuracy: 0.8929 - loss: 0.2641\n",
      " training set -> batch:527 loss:0.2630269527435303 and acc: 0.8923357725143433\n",
      " 527/2105 [======>.......................] - ETA: 2:26:43 - accuracy: 0.8923 - loss: 0.2630\n",
      " training set -> batch:528 loss:0.2659912705421448 and acc: 0.8918439745903015\n",
      " 528/2105 [======>.......................] - ETA: 2:26:29 - accuracy: 0.8918 - loss: 0.2660\n",
      " training set -> batch:529 loss:0.25899872183799744 and acc: 0.8939655423164368\n",
      " 529/2105 [======>.......................] - ETA: 2:26:14 - accuracy: 0.8940 - loss: 0.2590\n",
      " training set -> batch:530 loss:0.2655896842479706 and acc: 0.8917785286903381\n",
      "\n",
      " validation set -> batch:530 val loss:0.24192941188812256 and val acc: 0.9094036817550659\n",
      " 530/2105 [======>.......................] - ETA: 2:27:31 - accuracy: 0.8918 - loss: 0.2656\n",
      " training set -> batch:531 loss:0.2397545427083969 and acc: 0.9081858396530151\n",
      " 531/2105 [======>.......................] - ETA: 2:27:16 - accuracy: 0.9082 - loss: 0.2398\n",
      " training set -> batch:532 loss:0.23734520375728607 and acc: 0.9081196784973145\n",
      " 532/2105 [======>.......................] - ETA: 2:27:01 - accuracy: 0.9081 - loss: 0.2373\n",
      " training set -> batch:533 loss:0.23694419860839844 and acc: 0.9090909361839294\n",
      " 533/2105 [======>.......................] - ETA: 2:26:47 - accuracy: 0.9091 - loss: 0.2369\n",
      " training set -> batch:534 loss:0.23480793833732605 and acc: 0.9089999794960022\n",
      " 534/2105 [======>.......................] - ETA: 2:26:32 - accuracy: 0.9090 - loss: 0.2348\n",
      " training set -> batch:535 loss:0.24089427292346954 and acc: 0.9050387740135193\n",
      " 535/2105 [======>.......................] - ETA: 2:26:18 - accuracy: 0.9050 - loss: 0.2409\n",
      " training set -> batch:536 loss:0.2395927608013153 and acc: 0.9060150384902954\n",
      " 536/2105 [======>.......................] - ETA: 2:26:03 - accuracy: 0.9060 - loss: 0.2396\n",
      " training set -> batch:537 loss:0.23528575897216797 and acc: 0.9069343209266663\n",
      " 537/2105 [======>.......................] - ETA: 2:25:49 - accuracy: 0.9069 - loss: 0.2353\n",
      " training set -> batch:538 loss:0.23390835523605347 and acc: 0.9060283899307251\n",
      " 538/2105 [======>.......................] - ETA: 2:25:34 - accuracy: 0.9060 - loss: 0.2339\n",
      " training set -> batch:539 loss:0.2302589863538742 and acc: 0.9077585935592651\n",
      " 539/2105 [======>.......................] - ETA: 2:25:20 - accuracy: 0.9078 - loss: 0.2303\n",
      " training set -> batch:540 loss:0.2242754101753235 and acc: 0.9093959927558899\n",
      "\n",
      " validation set -> batch:540 val loss:0.2723349630832672 and val acc: 0.8853210806846619\n",
      " 540/2105 [======>.......................] - ETA: 2:26:32 - accuracy: 0.9094 - loss: 0.2243\n",
      " training set -> batch:541 loss:0.2814667820930481 and acc: 0.8827433586120605\n",
      " 541/2105 [======>.......................] - ETA: 2:26:18 - accuracy: 0.8827 - loss: 0.2815\n",
      " training set -> batch:542 loss:0.28898927569389343 and acc: 0.879273533821106\n",
      " 542/2105 [======>.......................] - ETA: 2:26:03 - accuracy: 0.8793 - loss: 0.2890\n",
      " training set -> batch:543 loss:0.2876902222633362 and acc: 0.8801652789115906\n",
      " 543/2105 [======>.......................] - ETA: 2:25:49 - accuracy: 0.8802 - loss: 0.2877\n",
      " training set -> batch:544 loss:0.27996066212654114 and acc: 0.8820000290870667\n",
      " 544/2105 [======>.......................] - ETA: 2:25:35 - accuracy: 0.8820 - loss: 0.2800\n",
      " training set -> batch:545 loss:0.28860512375831604 and acc: 0.8808139562606812\n",
      " 545/2105 [======>.......................] - ETA: 2:25:20 - accuracy: 0.8808 - loss: 0.2886\n",
      " training set -> batch:546 loss:0.279496967792511 and acc: 0.88345867395401\n",
      " 546/2105 [======>.......................] - ETA: 2:25:06 - accuracy: 0.8835 - loss: 0.2795\n",
      " training set -> batch:547 loss:0.28520041704177856 and acc: 0.8832116723060608\n",
      " 547/2105 [======>.......................] - ETA: 2:24:52 - accuracy: 0.8832 - loss: 0.2852\n",
      " training set -> batch:548 loss:0.28651702404022217 and acc: 0.883865237236023\n",
      " 548/2105 [======>.......................] - ETA: 2:24:38 - accuracy: 0.8839 - loss: 0.2865\n",
      " training set -> batch:549 loss:0.2800123691558838 and acc: 0.8862069249153137\n",
      " 549/2105 [======>.......................] - ETA: 2:24:24 - accuracy: 0.8862 - loss: 0.2800\n",
      " training set -> batch:550 loss:0.27724501490592957 and acc: 0.8884228467941284\n",
      "\n",
      " validation set -> batch:550 val loss:0.2333957850933075 and val acc: 0.8990825414657593\n",
      " 550/2105 [======>.......................] - ETA: 2:25:36 - accuracy: 0.8884 - loss: 0.2772\n",
      " training set -> batch:551 loss:0.22185476124286652 and acc: 0.9026548862457275\n",
      " 551/2105 [======>.......................] - ETA: 2:25:22 - accuracy: 0.9027 - loss: 0.2219\n",
      " training set -> batch:552 loss:0.21390223503112793 and acc: 0.9049145579338074\n",
      " 552/2105 [======>.......................] - ETA: 2:25:08 - accuracy: 0.9049 - loss: 0.2139\n",
      " training set -> batch:553 loss:0.21299847960472107 and acc: 0.9059917330741882\n",
      " 553/2105 [======>.......................] - ETA: 2:24:54 - accuracy: 0.9060 - loss: 0.2130\n",
      " training set -> batch:554 loss:0.2202940136194229 and acc: 0.9049999713897705\n",
      " 554/2105 [======>.......................] - ETA: 2:24:40 - accuracy: 0.9050 - loss: 0.2203\n",
      " training set -> batch:555 loss:0.213841512799263 and acc: 0.9079457521438599\n",
      " 555/2105 [======>.......................] - ETA: 2:24:26 - accuracy: 0.9079 - loss: 0.2138\n",
      " training set -> batch:556 loss:0.20990625023841858 and acc: 0.9097744226455688\n",
      " 556/2105 [======>.......................] - ETA: 2:24:12 - accuracy: 0.9098 - loss: 0.2099\n",
      " training set -> batch:557 loss:0.20700232684612274 and acc: 0.9114963412284851\n",
      " 557/2105 [======>.......................] - ETA: 2:23:58 - accuracy: 0.9115 - loss: 0.2070\n",
      " training set -> batch:558 loss:0.21229197084903717 and acc: 0.911347508430481\n",
      " 558/2105 [======>.......................] - ETA: 2:23:44 - accuracy: 0.9113 - loss: 0.2123\n",
      " training set -> batch:559 loss:0.21932290494441986 and acc: 0.9086207151412964\n",
      " 559/2105 [======>.......................] - ETA: 2:23:30 - accuracy: 0.9086 - loss: 0.2193\n",
      " training set -> batch:560 loss:0.21859849989414215 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:560 val loss:0.23225240409374237 and val acc: 0.9105504751205444\n",
      " 560/2105 [======>.......................] - ETA: 2:24:39 - accuracy: 0.9102 - loss: 0.2186\n",
      " training set -> batch:561 loss:0.236520916223526 and acc: 0.9081858396530151\n",
      " 561/2105 [======>.......................] - ETA: 2:24:25 - accuracy: 0.9082 - loss: 0.2365\n",
      " training set -> batch:562 loss:0.24314379692077637 and acc: 0.9081196784973145\n",
      " 562/2105 [=======>......................] - ETA: 2:24:11 - accuracy: 0.9081 - loss: 0.2431\n",
      " training set -> batch:563 loss:0.24076907336711884 and acc: 0.9080578684806824\n",
      " 563/2105 [=======>......................] - ETA: 2:23:57 - accuracy: 0.9081 - loss: 0.2408\n",
      " training set -> batch:564 loss:0.24076032638549805 and acc: 0.9089999794960022\n",
      " 564/2105 [=======>......................] - ETA: 2:23:43 - accuracy: 0.9090 - loss: 0.2408\n",
      " training set -> batch:565 loss:0.2330416589975357 and acc: 0.9108527302742004\n",
      " 565/2105 [=======>......................] - ETA: 2:23:30 - accuracy: 0.9109 - loss: 0.2330\n",
      " training set -> batch:566 loss:0.239159494638443 and acc: 0.9078947305679321\n",
      " 566/2105 [=======>......................] - ETA: 2:23:16 - accuracy: 0.9079 - loss: 0.2392\n",
      " training set -> batch:567 loss:0.23376646637916565 and acc: 0.9096715450286865\n",
      " 567/2105 [=======>......................] - ETA: 2:23:02 - accuracy: 0.9097 - loss: 0.2338\n",
      " training set -> batch:568 loss:0.24127262830734253 and acc: 0.9078013896942139\n",
      " 568/2105 [=======>......................] - ETA: 2:22:48 - accuracy: 0.9078 - loss: 0.2413\n",
      " training set -> batch:569 loss:0.2380814403295517 and acc: 0.9086207151412964\n",
      " 569/2105 [=======>......................] - ETA: 2:22:35 - accuracy: 0.9086 - loss: 0.2381\n",
      " training set -> batch:570 loss:0.23770473897457123 and acc: 0.906879186630249\n",
      "\n",
      " validation set -> batch:570 val loss:0.23656189441680908 and val acc: 0.9128440618515015\n",
      " 570/2105 [=======>......................] - ETA: 2:23:47 - accuracy: 0.9069 - loss: 0.2377\n",
      " training set -> batch:571 loss:0.22965465486049652 and acc: 0.9137167930603027\n",
      " 571/2105 [=======>......................] - ETA: 2:23:33 - accuracy: 0.9137 - loss: 0.2297\n",
      " training set -> batch:572 loss:0.2276162952184677 and acc: 0.9134615659713745\n",
      " 572/2105 [=======>......................] - ETA: 2:23:19 - accuracy: 0.9135 - loss: 0.2276\n",
      " training set -> batch:573 loss:0.22250092029571533 and acc: 0.9152892827987671\n",
      " 573/2105 [=======>......................] - ETA: 2:23:06 - accuracy: 0.9153 - loss: 0.2225\n",
      " training set -> batch:574 loss:0.22103960812091827 and acc: 0.9160000085830688\n",
      " 574/2105 [=======>......................] - ETA: 2:22:52 - accuracy: 0.9160 - loss: 0.2210\n",
      " training set -> batch:575 loss:0.2129698097705841 and acc: 0.9186046719551086\n",
      " 575/2105 [=======>......................] - ETA: 2:22:38 - accuracy: 0.9186 - loss: 0.2130\n",
      " training set -> batch:576 loss:0.21989774703979492 and acc: 0.9172932505607605\n",
      " 576/2105 [=======>......................] - ETA: 2:22:25 - accuracy: 0.9173 - loss: 0.2199\n",
      " training set -> batch:577 loss:0.21887563169002533 and acc: 0.9169707894325256\n",
      " 577/2105 [=======>......................] - ETA: 2:22:12 - accuracy: 0.9170 - loss: 0.2189\n",
      " training set -> batch:578 loss:0.22501534223556519 and acc: 0.9140070676803589\n",
      " 578/2105 [=======>......................] - ETA: 2:21:58 - accuracy: 0.9140 - loss: 0.2250\n",
      " training set -> batch:579 loss:0.2232712060213089 and acc: 0.915517270565033\n",
      " 579/2105 [=======>......................] - ETA: 2:21:44 - accuracy: 0.9155 - loss: 0.2233\n",
      " training set -> batch:580 loss:0.219543918967247 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:580 val loss:0.24758729338645935 and val acc: 0.9105504751205444\n",
      " 580/2105 [=======>......................] - ETA: 2:22:51 - accuracy: 0.9161 - loss: 0.2195\n",
      " training set -> batch:581 loss:0.24472051858901978 and acc: 0.9115044474601746\n",
      " 581/2105 [=======>......................] - ETA: 2:22:37 - accuracy: 0.9115 - loss: 0.2447\n",
      " training set -> batch:582 loss:0.23827053606510162 and acc: 0.9123931527137756\n",
      " 582/2105 [=======>......................] - ETA: 2:22:24 - accuracy: 0.9124 - loss: 0.2383\n",
      " training set -> batch:583 loss:0.2445993423461914 and acc: 0.9101239442825317\n",
      " 583/2105 [=======>......................] - ETA: 2:22:11 - accuracy: 0.9101 - loss: 0.2446\n",
      " training set -> batch:584 loss:0.24851934611797333 and acc: 0.9089999794960022\n",
      " 584/2105 [=======>......................] - ETA: 2:21:57 - accuracy: 0.9090 - loss: 0.2485\n",
      " training set -> batch:585 loss:0.2505815625190735 and acc: 0.9089147448539734\n",
      " 585/2105 [=======>......................] - ETA: 2:21:44 - accuracy: 0.9089 - loss: 0.2506\n",
      " training set -> batch:586 loss:0.24834546446800232 and acc: 0.9078947305679321\n",
      " 586/2105 [=======>......................] - ETA: 2:21:30 - accuracy: 0.9079 - loss: 0.2483\n",
      " training set -> batch:587 loss:0.24537155032157898 and acc: 0.9078466892242432\n",
      " 587/2105 [=======>......................] - ETA: 2:21:17 - accuracy: 0.9078 - loss: 0.2454\n",
      " training set -> batch:588 loss:0.24719977378845215 and acc: 0.9069148898124695\n",
      " 588/2105 [=======>......................] - ETA: 2:21:04 - accuracy: 0.9069 - loss: 0.2472\n",
      " training set -> batch:589 loss:0.24460279941558838 and acc: 0.9086207151412964\n",
      " 589/2105 [=======>......................] - ETA: 2:20:51 - accuracy: 0.9086 - loss: 0.2446\n",
      " training set -> batch:590 loss:0.2487878054380417 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:590 val loss:0.24821631610393524 and val acc: 0.9059633016586304\n",
      " 590/2105 [=======>......................] - ETA: 2:21:57 - accuracy: 0.9060 - loss: 0.2488\n",
      " training set -> batch:591 loss:0.244741752743721 and acc: 0.9059734344482422\n",
      " 591/2105 [=======>......................] - ETA: 2:21:44 - accuracy: 0.9060 - loss: 0.2447\n",
      " training set -> batch:592 loss:0.24468065798282623 and acc: 0.9049145579338074\n",
      " 592/2105 [=======>......................] - ETA: 2:21:30 - accuracy: 0.9049 - loss: 0.2447\n",
      " training set -> batch:593 loss:0.24052785336971283 and acc: 0.9059917330741882\n",
      " 593/2105 [=======>......................] - ETA: 2:21:17 - accuracy: 0.9060 - loss: 0.2405\n",
      " training set -> batch:594 loss:0.23973120748996735 and acc: 0.906000018119812\n",
      " 594/2105 [=======>......................] - ETA: 2:21:04 - accuracy: 0.9060 - loss: 0.2397\n",
      " training set -> batch:595 loss:0.2420002967119217 and acc: 0.9060077667236328\n",
      " 595/2105 [=======>......................] - ETA: 2:20:51 - accuracy: 0.9060 - loss: 0.2420\n",
      " training set -> batch:596 loss:0.24828402698040009 and acc: 0.9031955003738403\n",
      " 596/2105 [=======>......................] - ETA: 2:20:38 - accuracy: 0.9032 - loss: 0.2483\n",
      " training set -> batch:597 loss:0.24133098125457764 and acc: 0.9060218930244446\n",
      " 597/2105 [=======>......................] - ETA: 2:20:25 - accuracy: 0.9060 - loss: 0.2413\n",
      " training set -> batch:598 loss:0.23756976425647736 and acc: 0.9069148898124695\n",
      " 598/2105 [=======>......................] - ETA: 2:20:11 - accuracy: 0.9069 - loss: 0.2376\n",
      " training set -> batch:599 loss:0.23795810341835022 and acc: 0.9060344696044922\n",
      " 599/2105 [=======>......................] - ETA: 2:19:58 - accuracy: 0.9060 - loss: 0.2380\n",
      " training set -> batch:600 loss:0.2358652949333191 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:600 val loss:0.23185372352600098 and val acc: 0.9094036817550659\n",
      " 600/2105 [=======>......................] - ETA: 2:21:05 - accuracy: 0.9060 - loss: 0.2359\n",
      " training set -> batch:601 loss:0.23313994705677032 and acc: 0.9081858396530151\n",
      " 601/2105 [=======>......................] - ETA: 2:20:51 - accuracy: 0.9082 - loss: 0.2331\n",
      " training set -> batch:602 loss:0.23479586839675903 and acc: 0.9081196784973145\n",
      " 602/2105 [=======>......................] - ETA: 2:20:38 - accuracy: 0.9081 - loss: 0.2348\n",
      " training set -> batch:603 loss:0.2406895011663437 and acc: 0.9059917330741882\n",
      " 603/2105 [=======>......................] - ETA: 2:20:25 - accuracy: 0.9060 - loss: 0.2407\n",
      " training set -> batch:604 loss:0.24008604884147644 and acc: 0.9039999842643738\n",
      " 604/2105 [=======>......................] - ETA: 2:20:12 - accuracy: 0.9040 - loss: 0.2401\n",
      " training set -> batch:605 loss:0.2365957796573639 and acc: 0.9040697813034058\n",
      " 605/2105 [=======>......................] - ETA: 2:19:59 - accuracy: 0.9041 - loss: 0.2366\n",
      " training set -> batch:606 loss:0.23821291327476501 and acc: 0.9031955003738403\n",
      " 606/2105 [=======>......................] - ETA: 2:19:46 - accuracy: 0.9032 - loss: 0.2382\n",
      " training set -> batch:607 loss:0.23423774540424347 and acc: 0.9051094651222229\n",
      " 607/2105 [=======>......................] - ETA: 2:19:33 - accuracy: 0.9051 - loss: 0.2342\n",
      " training set -> batch:608 loss:0.22961974143981934 and acc: 0.9069148898124695\n",
      " 608/2105 [=======>......................] - ETA: 2:19:20 - accuracy: 0.9069 - loss: 0.2296\n",
      " training set -> batch:609 loss:0.22712697088718414 and acc: 0.9068965315818787\n",
      " 609/2105 [=======>......................] - ETA: 2:19:07 - accuracy: 0.9069 - loss: 0.2271\n",
      " training set -> batch:610 loss:0.22460202872753143 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:610 val loss:0.22947759926319122 and val acc: 0.9162843823432922\n",
      " 610/2105 [=======>......................] - ETA: 2:20:08 - accuracy: 0.9077 - loss: 0.2246\n",
      " training set -> batch:611 loss:0.22054964303970337 and acc: 0.9181416034698486\n",
      " 611/2105 [=======>......................] - ETA: 2:19:55 - accuracy: 0.9181 - loss: 0.2205\n",
      " training set -> batch:612 loss:0.22916188836097717 and acc: 0.9145299196243286\n",
      " 612/2105 [=======>......................] - ETA: 2:19:42 - accuracy: 0.9145 - loss: 0.2292\n",
      " training set -> batch:613 loss:0.21975421905517578 and acc: 0.9163222908973694\n",
      " 613/2105 [=======>......................] - ETA: 2:19:29 - accuracy: 0.9163 - loss: 0.2198\n",
      " training set -> batch:614 loss:0.21755047142505646 and acc: 0.9169999957084656\n",
      " 614/2105 [=======>......................] - ETA: 2:19:16 - accuracy: 0.9170 - loss: 0.2176\n",
      " training set -> batch:615 loss:0.21050401031970978 and acc: 0.9195736646652222\n",
      " 615/2105 [=======>......................] - ETA: 2:19:03 - accuracy: 0.9196 - loss: 0.2105\n",
      " training set -> batch:616 loss:0.2186695635318756 and acc: 0.9182330965995789\n",
      " 616/2105 [=======>......................] - ETA: 2:18:50 - accuracy: 0.9182 - loss: 0.2187\n",
      " training set -> batch:617 loss:0.21456393599510193 and acc: 0.9197080135345459\n",
      " 617/2105 [=======>......................] - ETA: 2:18:37 - accuracy: 0.9197 - loss: 0.2146\n",
      " training set -> batch:618 loss:0.21626240015029907 and acc: 0.9202127456665039\n",
      " 618/2105 [=======>......................] - ETA: 2:18:24 - accuracy: 0.9202 - loss: 0.2163\n",
      " training set -> batch:619 loss:0.21157780289649963 and acc: 0.9224137663841248\n",
      " 619/2105 [=======>......................] - ETA: 2:18:11 - accuracy: 0.9224 - loss: 0.2116\n",
      " training set -> batch:620 loss:0.2117733210325241 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:620 val loss:0.27186688780784607 and val acc: 0.9013761281967163\n",
      " 620/2105 [=======>......................] - ETA: 2:19:11 - accuracy: 0.9220 - loss: 0.2118\n",
      " training set -> batch:621 loss:0.2625771760940552 and acc: 0.9026548862457275\n",
      " 621/2105 [=======>......................] - ETA: 2:18:58 - accuracy: 0.9027 - loss: 0.2626\n",
      " training set -> batch:622 loss:0.25964877009391785 and acc: 0.9017093777656555\n",
      " 622/2105 [=======>......................] - ETA: 2:18:46 - accuracy: 0.9017 - loss: 0.2596\n",
      " training set -> batch:623 loss:0.2542327046394348 and acc: 0.9018595218658447\n",
      " 623/2105 [=======>......................] - ETA: 2:18:33 - accuracy: 0.9019 - loss: 0.2542\n",
      " training set -> batch:624 loss:0.2463161200284958 and acc: 0.9039999842643738\n",
      " 624/2105 [=======>......................] - ETA: 2:18:20 - accuracy: 0.9040 - loss: 0.2463\n",
      " training set -> batch:625 loss:0.24837137758731842 and acc: 0.9031007885932922\n",
      " 625/2105 [=======>......................] - ETA: 2:18:07 - accuracy: 0.9031 - loss: 0.2484\n",
      " training set -> batch:626 loss:0.2505178451538086 and acc: 0.9031955003738403\n",
      " 626/2105 [=======>......................] - ETA: 2:17:55 - accuracy: 0.9032 - loss: 0.2505\n",
      " training set -> batch:627 loss:0.2427234649658203 and acc: 0.9051094651222229\n",
      " 627/2105 [=======>......................] - ETA: 2:17:42 - accuracy: 0.9051 - loss: 0.2427\n",
      " training set -> batch:628 loss:0.2530265152454376 and acc: 0.9033687710762024\n",
      " 628/2105 [=======>......................] - ETA: 2:17:29 - accuracy: 0.9034 - loss: 0.2530\n",
      " training set -> batch:629 loss:0.2525917887687683 and acc: 0.9025862216949463\n",
      " 629/2105 [=======>......................] - ETA: 2:17:16 - accuracy: 0.9026 - loss: 0.2526\n",
      " training set -> batch:630 loss:0.2487524300813675 and acc: 0.9035235047340393\n",
      "\n",
      " validation set -> batch:630 val loss:0.2583661377429962 and val acc: 0.9128440618515015\n",
      " 630/2105 [=======>......................] - ETA: 2:18:14 - accuracy: 0.9035 - loss: 0.2488\n",
      " training set -> batch:631 loss:0.24637575447559357 and acc: 0.9137167930603027\n",
      " 631/2105 [=======>......................] - ETA: 2:18:01 - accuracy: 0.9137 - loss: 0.2464\n",
      " training set -> batch:632 loss:0.24192781746387482 and acc: 0.9155982732772827\n",
      " 632/2105 [========>.....................] - ETA: 2:17:48 - accuracy: 0.9156 - loss: 0.2419\n",
      " training set -> batch:633 loss:0.24363870918750763 and acc: 0.91425621509552\n",
      " 633/2105 [========>.....................] - ETA: 2:17:35 - accuracy: 0.9143 - loss: 0.2436\n",
      " training set -> batch:634 loss:0.23619908094406128 and acc: 0.9150000214576721\n",
      " 634/2105 [========>.....................] - ETA: 2:17:22 - accuracy: 0.9150 - loss: 0.2362\n",
      " training set -> batch:635 loss:0.24073238670825958 and acc: 0.913759708404541\n",
      " 635/2105 [========>.....................] - ETA: 2:17:09 - accuracy: 0.9138 - loss: 0.2407\n",
      " training set -> batch:636 loss:0.24130240082740784 and acc: 0.9144737124443054\n",
      " 636/2105 [========>.....................] - ETA: 2:16:57 - accuracy: 0.9145 - loss: 0.2413\n",
      " training set -> batch:637 loss:0.23438899219036102 and acc: 0.9160584211349487\n",
      " 637/2105 [========>.....................] - ETA: 2:16:44 - accuracy: 0.9161 - loss: 0.2344\n",
      " training set -> batch:638 loss:0.22839142382144928 and acc: 0.917553186416626\n",
      " 638/2105 [========>.....................] - ETA: 2:16:31 - accuracy: 0.9176 - loss: 0.2284\n",
      " training set -> batch:639 loss:0.2273826152086258 and acc: 0.9181034564971924\n",
      " 639/2105 [========>.....................] - ETA: 2:16:18 - accuracy: 0.9181 - loss: 0.2274\n",
      " training set -> batch:640 loss:0.23919178545475006 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:640 val loss:0.2586122155189514 and val acc: 0.892201840877533\n",
      " 640/2105 [========>.....................] - ETA: 2:17:14 - accuracy: 0.9136 - loss: 0.2392\n",
      " training set -> batch:641 loss:0.2603476345539093 and acc: 0.8915929198265076\n",
      " 641/2105 [========>.....................] - ETA: 2:17:02 - accuracy: 0.8916 - loss: 0.2603\n",
      " training set -> batch:642 loss:0.2563049793243408 and acc: 0.8899572491645813\n",
      " 642/2105 [========>.....................] - ETA: 2:16:49 - accuracy: 0.8900 - loss: 0.2563\n",
      " training set -> batch:643 loss:0.25715315341949463 and acc: 0.8894628286361694\n",
      " 643/2105 [========>.....................] - ETA: 2:16:36 - accuracy: 0.8895 - loss: 0.2572\n",
      " training set -> batch:644 loss:0.2549067437648773 and acc: 0.890999972820282\n",
      " 644/2105 [========>.....................] - ETA: 2:16:24 - accuracy: 0.8910 - loss: 0.2549\n",
      " training set -> batch:645 loss:0.251665323972702 and acc: 0.8914728760719299\n",
      " 645/2105 [========>.....................] - ETA: 2:16:11 - accuracy: 0.8915 - loss: 0.2517\n",
      " training set -> batch:646 loss:0.2563205361366272 and acc: 0.8909774422645569\n",
      " 646/2105 [========>.....................] - ETA: 2:15:58 - accuracy: 0.8910 - loss: 0.2563\n",
      " training set -> batch:647 loss:0.25651392340660095 and acc: 0.8905109763145447\n",
      " 647/2105 [========>.....................] - ETA: 2:15:46 - accuracy: 0.8905 - loss: 0.2565\n",
      " training set -> batch:648 loss:0.25172093510627747 and acc: 0.8936170339584351\n",
      " 648/2105 [========>.....................] - ETA: 2:15:34 - accuracy: 0.8936 - loss: 0.2517\n",
      " training set -> batch:649 loss:0.24632659554481506 and acc: 0.8956896662712097\n",
      " 649/2105 [========>.....................] - ETA: 2:15:21 - accuracy: 0.8957 - loss: 0.2463\n",
      " training set -> batch:650 loss:0.24011220037937164 and acc: 0.8984899520874023\n",
      "\n",
      " validation set -> batch:650 val loss:0.2501465380191803 and val acc: 0.9082568883895874\n",
      " 650/2105 [========>.....................] - ETA: 2:16:16 - accuracy: 0.8985 - loss: 0.2401\n",
      " training set -> batch:651 loss:0.25729936361312866 and acc: 0.9070796370506287\n",
      " 651/2105 [========>.....................] - ETA: 2:16:04 - accuracy: 0.9071 - loss: 0.2573\n",
      " training set -> batch:652 loss:0.2549535930156708 and acc: 0.9059829115867615\n",
      " 652/2105 [========>.....................] - ETA: 2:15:51 - accuracy: 0.9060 - loss: 0.2550\n",
      " training set -> batch:653 loss:0.26184511184692383 and acc: 0.9049586653709412\n",
      " 653/2105 [========>.....................] - ETA: 2:15:39 - accuracy: 0.9050 - loss: 0.2618\n",
      " training set -> batch:654 loss:0.26056843996047974 and acc: 0.9039999842643738\n",
      " 654/2105 [========>.....................] - ETA: 2:15:26 - accuracy: 0.9040 - loss: 0.2606\n",
      " training set -> batch:655 loss:0.25285616517066956 and acc: 0.9060077667236328\n",
      " 655/2105 [========>.....................] - ETA: 2:15:14 - accuracy: 0.9060 - loss: 0.2529\n",
      " training set -> batch:656 loss:0.24768075346946716 and acc: 0.9069548845291138\n",
      " 656/2105 [========>.....................] - ETA: 2:15:02 - accuracy: 0.9070 - loss: 0.2477\n",
      " training set -> batch:657 loss:0.2419336885213852 and acc: 0.9078466892242432\n",
      " 657/2105 [========>.....................] - ETA: 2:14:49 - accuracy: 0.9078 - loss: 0.2419\n",
      " training set -> batch:658 loss:0.23694495856761932 and acc: 0.908687949180603\n",
      " 658/2105 [========>.....................] - ETA: 2:14:37 - accuracy: 0.9087 - loss: 0.2369\n",
      " training set -> batch:659 loss:0.23218370974063873 and acc: 0.9094827771186829\n",
      " 659/2105 [========>.....................] - ETA: 2:14:25 - accuracy: 0.9095 - loss: 0.2322\n",
      " training set -> batch:660 loss:0.237107053399086 and acc: 0.906879186630249\n",
      "\n",
      " validation set -> batch:660 val loss:0.2634812891483307 and val acc: 0.8864678740501404\n",
      " 660/2105 [========>.....................] - ETA: 2:15:17 - accuracy: 0.9069 - loss: 0.2371\n",
      " training set -> batch:661 loss:0.2514535188674927 and acc: 0.8904867172241211\n",
      " 661/2105 [========>.....................] - ETA: 2:15:05 - accuracy: 0.8905 - loss: 0.2515\n",
      " training set -> batch:662 loss:0.24035993218421936 and acc: 0.8931623697280884\n",
      " 662/2105 [========>.....................] - ETA: 2:14:52 - accuracy: 0.8932 - loss: 0.2404\n",
      " training set -> batch:663 loss:0.2352442890405655 and acc: 0.89462810754776\n",
      " 663/2105 [========>.....................] - ETA: 2:14:40 - accuracy: 0.8946 - loss: 0.2352\n",
      " training set -> batch:664 loss:0.23826315999031067 and acc: 0.8949999809265137\n",
      " 664/2105 [========>.....................] - ETA: 2:14:28 - accuracy: 0.8950 - loss: 0.2383\n",
      " training set -> batch:665 loss:0.23284299671649933 and acc: 0.895348846912384\n",
      " 665/2105 [========>.....................] - ETA: 2:14:16 - accuracy: 0.8953 - loss: 0.2328\n",
      " training set -> batch:666 loss:0.228078693151474 and acc: 0.896616518497467\n",
      " 666/2105 [========>.....................] - ETA: 2:14:03 - accuracy: 0.8966 - loss: 0.2281\n",
      " training set -> batch:667 loss:0.21946141123771667 and acc: 0.8987226486206055\n",
      " 667/2105 [========>.....................] - ETA: 2:13:51 - accuracy: 0.8987 - loss: 0.2195\n",
      " training set -> batch:668 loss:0.21470999717712402 and acc: 0.8998227119445801\n",
      " 668/2105 [========>.....................] - ETA: 2:13:39 - accuracy: 0.8998 - loss: 0.2147\n",
      " training set -> batch:669 loss:0.22075852751731873 and acc: 0.8991379141807556\n",
      " 669/2105 [========>.....................] - ETA: 2:13:27 - accuracy: 0.8991 - loss: 0.2208\n",
      " training set -> batch:670 loss:0.22293929755687714 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:670 val loss:0.2560986280441284 and val acc: 0.9151375889778137\n",
      " 670/2105 [========>.....................] - ETA: 2:14:16 - accuracy: 0.8993 - loss: 0.2229\n",
      " training set -> batch:671 loss:0.24476926028728485 and acc: 0.9159291982650757\n",
      " 671/2105 [========>.....................] - ETA: 2:14:04 - accuracy: 0.9159 - loss: 0.2448\n",
      " training set -> batch:672 loss:0.24034035205841064 and acc: 0.9177350401878357\n",
      " 672/2105 [========>.....................] - ETA: 2:13:52 - accuracy: 0.9177 - loss: 0.2403\n",
      " training set -> batch:673 loss:0.24842548370361328 and acc: 0.9173553586006165\n",
      " 673/2105 [========>.....................] - ETA: 2:13:39 - accuracy: 0.9174 - loss: 0.2484\n",
      " training set -> batch:674 loss:0.24091853201389313 and acc: 0.9179999828338623\n",
      " 674/2105 [========>.....................] - ETA: 2:13:27 - accuracy: 0.9180 - loss: 0.2409\n",
      " training set -> batch:675 loss:0.24089233577251434 and acc: 0.9186046719551086\n",
      " 675/2105 [========>.....................] - ETA: 2:13:15 - accuracy: 0.9186 - loss: 0.2409\n",
      " training set -> batch:676 loss:0.23396310210227966 and acc: 0.9191729426383972\n",
      " 676/2105 [========>.....................] - ETA: 2:13:03 - accuracy: 0.9192 - loss: 0.2340\n",
      " training set -> batch:677 loss:0.23411957919597626 and acc: 0.9197080135345459\n",
      " 677/2105 [========>.....................] - ETA: 2:12:51 - accuracy: 0.9197 - loss: 0.2341\n",
      " training set -> batch:678 loss:0.22995585203170776 and acc: 0.9202127456665039\n",
      " 678/2105 [========>.....................] - ETA: 2:12:39 - accuracy: 0.9202 - loss: 0.2300\n",
      " training set -> batch:679 loss:0.2347552329301834 and acc: 0.9189655184745789\n",
      " 679/2105 [========>.....................] - ETA: 2:12:27 - accuracy: 0.9190 - loss: 0.2348\n",
      " training set -> batch:680 loss:0.23425279557704926 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:680 val loss:0.24595442414283752 and val acc: 0.9013761281967163\n",
      " 680/2105 [========>.....................] - ETA: 2:13:15 - accuracy: 0.9178 - loss: 0.2343\n",
      " training set -> batch:681 loss:0.2391337752342224 and acc: 0.9026548862457275\n",
      " 681/2105 [========>.....................] - ETA: 2:13:03 - accuracy: 0.9027 - loss: 0.2391\n",
      " training set -> batch:682 loss:0.22967009246349335 and acc: 0.9049145579338074\n",
      " 682/2105 [========>.....................] - ETA: 2:12:51 - accuracy: 0.9049 - loss: 0.2297\n",
      " training set -> batch:683 loss:0.22832311689853668 and acc: 0.9039255976676941\n",
      " 683/2105 [========>.....................] - ETA: 2:12:39 - accuracy: 0.9039 - loss: 0.2283\n",
      " training set -> batch:684 loss:0.2278335839509964 and acc: 0.9049999713897705\n",
      " 684/2105 [========>.....................] - ETA: 2:12:27 - accuracy: 0.9050 - loss: 0.2278\n",
      " training set -> batch:685 loss:0.23815184831619263 and acc: 0.9031007885932922\n",
      " 685/2105 [========>.....................] - ETA: 2:12:15 - accuracy: 0.9031 - loss: 0.2382\n",
      " training set -> batch:686 loss:0.24620874226093292 and acc: 0.9013158082962036\n",
      " 686/2105 [========>.....................] - ETA: 2:12:03 - accuracy: 0.9013 - loss: 0.2462\n",
      " training set -> batch:687 loss:0.2544359862804413 and acc: 0.8987226486206055\n",
      " 687/2105 [========>.....................] - ETA: 2:11:51 - accuracy: 0.8987 - loss: 0.2544\n",
      " training set -> batch:688 loss:0.24940532445907593 and acc: 0.8989361524581909\n",
      " 688/2105 [========>.....................] - ETA: 2:11:40 - accuracy: 0.8989 - loss: 0.2494\n",
      " training set -> batch:689 loss:0.24916329979896545 and acc: 0.8982758522033691\n",
      " 689/2105 [========>.....................] - ETA: 2:11:28 - accuracy: 0.8983 - loss: 0.2492\n",
      " training set -> batch:690 loss:0.24182011187076569 and acc: 0.9010066986083984\n",
      "\n",
      " validation set -> batch:690 val loss:0.2383417934179306 and val acc: 0.9025229215621948\n",
      " 690/2105 [========>.....................] - ETA: 2:12:17 - accuracy: 0.9010 - loss: 0.2418\n",
      " training set -> batch:691 loss:0.23191455006599426 and acc: 0.903761088848114\n",
      " 691/2105 [========>.....................] - ETA: 2:12:05 - accuracy: 0.9038 - loss: 0.2319\n",
      " training set -> batch:692 loss:0.22923201322555542 and acc: 0.9038461446762085\n",
      " 692/2105 [========>.....................] - ETA: 2:11:53 - accuracy: 0.9038 - loss: 0.2292\n",
      " training set -> batch:693 loss:0.22669494152069092 and acc: 0.9049586653709412\n",
      " 693/2105 [========>.....................] - ETA: 2:11:41 - accuracy: 0.9050 - loss: 0.2267\n",
      " training set -> batch:694 loss:0.22608582675457 and acc: 0.906000018119812\n",
      " 694/2105 [========>.....................] - ETA: 2:11:29 - accuracy: 0.9060 - loss: 0.2261\n",
      " training set -> batch:695 loss:0.23085154592990875 and acc: 0.9040697813034058\n",
      " 695/2105 [========>.....................] - ETA: 2:11:18 - accuracy: 0.9041 - loss: 0.2309\n",
      " training set -> batch:696 loss:0.23568865656852722 and acc: 0.902255654335022\n",
      " 696/2105 [========>.....................] - ETA: 2:11:06 - accuracy: 0.9023 - loss: 0.2357\n",
      " training set -> batch:697 loss:0.2343902885913849 and acc: 0.904197096824646\n",
      " 697/2105 [========>.....................] - ETA: 2:10:54 - accuracy: 0.9042 - loss: 0.2344\n",
      " training set -> batch:698 loss:0.23239289224147797 and acc: 0.9051418304443359\n",
      " 698/2105 [========>.....................] - ETA: 2:10:42 - accuracy: 0.9051 - loss: 0.2324\n",
      " training set -> batch:699 loss:0.22897718846797943 and acc: 0.9068965315818787\n",
      " 699/2105 [========>.....................] - ETA: 2:10:31 - accuracy: 0.9069 - loss: 0.2290\n",
      " training set -> batch:700 loss:0.22469133138656616 and acc: 0.9085570573806763\n",
      "\n",
      " validation set -> batch:700 val loss:0.24810729920864105 and val acc: 0.896789014339447\n",
      " 700/2105 [========>.....................] - ETA: 2:11:18 - accuracy: 0.9086 - loss: 0.2247\n",
      " training set -> batch:701 loss:0.2471635937690735 and acc: 0.8971238732337952\n",
      " 701/2105 [========>.....................] - ETA: 2:11:06 - accuracy: 0.8971 - loss: 0.2472\n",
      " training set -> batch:702 loss:0.24286390841007233 and acc: 0.8974359035491943\n",
      " 702/2105 [=========>....................] - ETA: 2:10:54 - accuracy: 0.8974 - loss: 0.2429\n",
      " training set -> batch:703 loss:0.2366497963666916 and acc: 0.8997933864593506\n",
      " 703/2105 [=========>....................] - ETA: 2:10:43 - accuracy: 0.8998 - loss: 0.2366\n",
      " training set -> batch:704 loss:0.24223089218139648 and acc: 0.8960000276565552\n",
      " 704/2105 [=========>....................] - ETA: 2:10:31 - accuracy: 0.8960 - loss: 0.2422\n",
      " training set -> batch:705 loss:0.24765817821025848 and acc: 0.8943798542022705\n",
      " 705/2105 [=========>....................] - ETA: 2:10:19 - accuracy: 0.8944 - loss: 0.2477\n",
      " training set -> batch:706 loss:0.24013173580169678 and acc: 0.8975563645362854\n",
      " 706/2105 [=========>....................] - ETA: 2:10:07 - accuracy: 0.8976 - loss: 0.2401\n",
      " training set -> batch:707 loss:0.24408452212810516 and acc: 0.8968977928161621\n",
      " 707/2105 [=========>....................] - ETA: 2:09:56 - accuracy: 0.8969 - loss: 0.2441\n",
      " training set -> batch:708 loss:0.23641090095043182 and acc: 0.8989361524581909\n",
      " 708/2105 [=========>....................] - ETA: 2:09:44 - accuracy: 0.8989 - loss: 0.2364\n",
      " training set -> batch:709 loss:0.2366952896118164 and acc: 0.8991379141807556\n",
      " 709/2105 [=========>....................] - ETA: 2:09:32 - accuracy: 0.8991 - loss: 0.2367\n",
      " training set -> batch:710 loss:0.23244576156139374 and acc: 0.9010066986083984\n",
      "\n",
      " validation set -> batch:710 val loss:0.23469261825084686 and val acc: 0.9094036817550659\n",
      " 710/2105 [=========>....................] - ETA: 2:10:17 - accuracy: 0.9010 - loss: 0.2324\n",
      " training set -> batch:711 loss:0.2357717901468277 and acc: 0.9081858396530151\n",
      " 711/2105 [=========>....................] - ETA: 2:10:06 - accuracy: 0.9082 - loss: 0.2358\n",
      " training set -> batch:712 loss:0.23730415105819702 and acc: 0.9081196784973145\n",
      " 712/2105 [=========>....................] - ETA: 2:09:54 - accuracy: 0.9081 - loss: 0.2373\n",
      " training set -> batch:713 loss:0.23585891723632812 and acc: 0.9090909361839294\n",
      " 713/2105 [=========>....................] - ETA: 2:09:43 - accuracy: 0.9091 - loss: 0.2359\n",
      " training set -> batch:714 loss:0.24000920355319977 and acc: 0.906000018119812\n",
      " 714/2105 [=========>....................] - ETA: 2:09:31 - accuracy: 0.9060 - loss: 0.2400\n",
      " training set -> batch:715 loss:0.2468181848526001 and acc: 0.9050387740135193\n",
      " 715/2105 [=========>....................] - ETA: 2:09:20 - accuracy: 0.9050 - loss: 0.2468\n",
      " training set -> batch:716 loss:0.23812592029571533 and acc: 0.9069548845291138\n",
      " 716/2105 [=========>....................] - ETA: 2:09:08 - accuracy: 0.9070 - loss: 0.2381\n",
      " training set -> batch:717 loss:0.23181234300136566 and acc: 0.9096715450286865\n",
      " 717/2105 [=========>....................] - ETA: 2:08:56 - accuracy: 0.9097 - loss: 0.2318\n",
      " training set -> batch:718 loss:0.2274213433265686 and acc: 0.9104610085487366\n",
      " 718/2105 [=========>....................] - ETA: 2:08:45 - accuracy: 0.9105 - loss: 0.2274\n",
      " training set -> batch:719 loss:0.22718937695026398 and acc: 0.9112069010734558\n",
      " 719/2105 [=========>....................] - ETA: 2:08:33 - accuracy: 0.9112 - loss: 0.2272\n",
      " training set -> batch:720 loss:0.22692210972309113 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:720 val loss:0.2613058090209961 and val acc: 0.896789014339447\n",
      " 720/2105 [=========>....................] - ETA: 2:09:21 - accuracy: 0.9119 - loss: 0.2269\n",
      " training set -> batch:721 loss:0.2552852928638458 and acc: 0.8993362784385681\n",
      " 721/2105 [=========>....................] - ETA: 2:09:09 - accuracy: 0.8993 - loss: 0.2553\n",
      " training set -> batch:722 loss:0.2549704313278198 and acc: 0.8995726704597473\n",
      " 722/2105 [=========>....................] - ETA: 2:08:57 - accuracy: 0.8996 - loss: 0.2550\n",
      " training set -> batch:723 loss:0.24752400815486908 and acc: 0.9008264541625977\n",
      " 723/2105 [=========>....................] - ETA: 2:08:46 - accuracy: 0.9008 - loss: 0.2475\n",
      " training set -> batch:724 loss:0.24410377442836761 and acc: 0.8999999761581421\n",
      " 724/2105 [=========>....................] - ETA: 2:08:35 - accuracy: 0.9000 - loss: 0.2441\n",
      " training set -> batch:725 loss:0.23660539090633392 and acc: 0.9021317958831787\n",
      " 725/2105 [=========>....................] - ETA: 2:08:23 - accuracy: 0.9021 - loss: 0.2366\n",
      " training set -> batch:726 loss:0.23477187752723694 and acc: 0.902255654335022\n",
      " 726/2105 [=========>....................] - ETA: 2:08:12 - accuracy: 0.9023 - loss: 0.2348\n",
      " training set -> batch:727 loss:0.244460791349411 and acc: 0.9014598727226257\n",
      " 727/2105 [=========>....................] - ETA: 2:08:00 - accuracy: 0.9015 - loss: 0.2445\n",
      " training set -> batch:728 loss:0.2413349747657776 and acc: 0.902482271194458\n",
      " 728/2105 [=========>....................] - ETA: 2:07:49 - accuracy: 0.9025 - loss: 0.2413\n",
      " training set -> batch:729 loss:0.23926611244678497 and acc: 0.9034482836723328\n",
      " 729/2105 [=========>....................] - ETA: 2:07:38 - accuracy: 0.9034 - loss: 0.2393\n",
      " training set -> batch:730 loss:0.24063746631145477 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:730 val loss:0.22883234918117523 and val acc: 0.9082568883895874\n",
      " 730/2105 [=========>....................] - ETA: 2:08:21 - accuracy: 0.9027 - loss: 0.2406\n",
      " training set -> batch:731 loss:0.22458644211292267 and acc: 0.9092920422554016\n",
      " 731/2105 [=========>....................] - ETA: 2:08:10 - accuracy: 0.9093 - loss: 0.2246\n",
      " training set -> batch:732 loss:0.21915525197982788 and acc: 0.9091880321502686\n",
      " 732/2105 [=========>....................] - ETA: 2:07:59 - accuracy: 0.9092 - loss: 0.2192\n",
      " training set -> batch:733 loss:0.2205616980791092 and acc: 0.9101239442825317\n",
      " 733/2105 [=========>....................] - ETA: 2:07:47 - accuracy: 0.9101 - loss: 0.2206\n",
      " training set -> batch:734 loss:0.2182539403438568 and acc: 0.9100000262260437\n",
      " 734/2105 [=========>....................] - ETA: 2:07:36 - accuracy: 0.9100 - loss: 0.2183\n",
      " training set -> batch:735 loss:0.2118893712759018 and acc: 0.911821722984314\n",
      " 735/2105 [=========>....................] - ETA: 2:07:25 - accuracy: 0.9118 - loss: 0.2119\n",
      " training set -> batch:736 loss:0.22091642022132874 and acc: 0.9107142686843872\n",
      " 736/2105 [=========>....................] - ETA: 2:07:14 - accuracy: 0.9107 - loss: 0.2209\n",
      " training set -> batch:737 loss:0.21743354201316833 and acc: 0.9114963412284851\n",
      " 737/2105 [=========>....................] - ETA: 2:07:02 - accuracy: 0.9115 - loss: 0.2174\n",
      " training set -> batch:738 loss:0.22984075546264648 and acc: 0.9095744490623474\n",
      " 738/2105 [=========>....................] - ETA: 2:06:51 - accuracy: 0.9096 - loss: 0.2298\n",
      " training set -> batch:739 loss:0.2224101573228836 and acc: 0.9120689630508423\n",
      " 739/2105 [=========>....................] - ETA: 2:06:40 - accuracy: 0.9121 - loss: 0.2224\n",
      " training set -> batch:740 loss:0.2209578901529312 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:740 val loss:0.2305465042591095 and val acc: 0.9105504751205444\n",
      " 740/2105 [=========>....................] - ETA: 2:07:22 - accuracy: 0.9128 - loss: 0.2210\n",
      " training set -> batch:741 loss:0.22266119718551636 and acc: 0.9126105904579163\n",
      " 741/2105 [=========>....................] - ETA: 2:07:10 - accuracy: 0.9126 - loss: 0.2227\n",
      " training set -> batch:742 loss:0.2220826894044876 and acc: 0.9145299196243286\n",
      " 742/2105 [=========>....................] - ETA: 2:06:59 - accuracy: 0.9145 - loss: 0.2221\n",
      " training set -> batch:743 loss:0.2201477438211441 and acc: 0.9152892827987671\n",
      " 743/2105 [=========>....................] - ETA: 2:06:48 - accuracy: 0.9153 - loss: 0.2201\n",
      " training set -> batch:744 loss:0.2214781641960144 and acc: 0.9129999876022339\n",
      " 744/2105 [=========>....................] - ETA: 2:06:37 - accuracy: 0.9130 - loss: 0.2215\n",
      " training set -> batch:745 loss:0.21401026844978333 and acc: 0.9156976938247681\n",
      " 745/2105 [=========>....................] - ETA: 2:06:26 - accuracy: 0.9157 - loss: 0.2140\n",
      " training set -> batch:746 loss:0.22069934010505676 and acc: 0.9125939607620239\n",
      " 746/2105 [=========>....................] - ETA: 2:06:14 - accuracy: 0.9126 - loss: 0.2207\n",
      " training set -> batch:747 loss:0.21655575931072235 and acc: 0.9133211970329285\n",
      " 747/2105 [=========>....................] - ETA: 2:06:03 - accuracy: 0.9133 - loss: 0.2166\n",
      " training set -> batch:748 loss:0.2253810465335846 and acc: 0.9104610085487366\n",
      " 748/2105 [=========>....................] - ETA: 2:05:52 - accuracy: 0.9105 - loss: 0.2254\n",
      " training set -> batch:749 loss:0.22305266559123993 and acc: 0.9094827771186829\n",
      " 749/2105 [=========>....................] - ETA: 2:05:41 - accuracy: 0.9095 - loss: 0.2231\n",
      " training set -> batch:750 loss:0.21954436600208282 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:750 val loss:0.23474419116973877 and val acc: 0.91399085521698\n",
      " 750/2105 [=========>....................] - ETA: 2:06:22 - accuracy: 0.9111 - loss: 0.2195\n",
      " training set -> batch:751 loss:0.23641560971736908 and acc: 0.9137167930603027\n",
      " 751/2105 [=========>....................] - ETA: 2:06:11 - accuracy: 0.9137 - loss: 0.2364\n",
      " training set -> batch:752 loss:0.23040635883808136 and acc: 0.9145299196243286\n",
      " 752/2105 [=========>....................] - ETA: 2:06:00 - accuracy: 0.9145 - loss: 0.2304\n",
      " training set -> batch:753 loss:0.2270798236131668 and acc: 0.91425621509552\n",
      " 753/2105 [=========>....................] - ETA: 2:05:48 - accuracy: 0.9143 - loss: 0.2271\n",
      " training set -> batch:754 loss:0.22713854908943176 and acc: 0.9139999747276306\n",
      " 754/2105 [=========>....................] - ETA: 2:05:37 - accuracy: 0.9140 - loss: 0.2271\n",
      " training set -> batch:755 loss:0.22178080677986145 and acc: 0.9156976938247681\n",
      " 755/2105 [=========>....................] - ETA: 2:05:26 - accuracy: 0.9157 - loss: 0.2218\n",
      " training set -> batch:756 loss:0.21550974249839783 and acc: 0.9172932505607605\n",
      " 756/2105 [=========>....................] - ETA: 2:05:15 - accuracy: 0.9173 - loss: 0.2155\n",
      " training set -> batch:757 loss:0.22102238237857819 and acc: 0.9169707894325256\n",
      " 757/2105 [=========>....................] - ETA: 2:05:04 - accuracy: 0.9170 - loss: 0.2210\n",
      " training set -> batch:758 loss:0.22287410497665405 and acc: 0.9157801270484924\n",
      " 758/2105 [=========>....................] - ETA: 2:04:53 - accuracy: 0.9158 - loss: 0.2229\n",
      " training set -> batch:759 loss:0.23268048465251923 and acc: 0.9146551489830017\n",
      " 759/2105 [=========>....................] - ETA: 2:04:42 - accuracy: 0.9147 - loss: 0.2327\n",
      " training set -> batch:760 loss:0.23424001038074493 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:760 val loss:0.24393436312675476 and val acc: 0.9151375889778137\n",
      " 760/2105 [=========>....................] - ETA: 2:05:28 - accuracy: 0.9136 - loss: 0.2342\n",
      " training set -> batch:761 loss:0.24121244251728058 and acc: 0.9148229956626892\n",
      " 761/2105 [=========>....................] - ETA: 2:05:17 - accuracy: 0.9148 - loss: 0.2412\n",
      " training set -> batch:762 loss:0.2356920689344406 and acc: 0.9145299196243286\n",
      " 762/2105 [=========>....................] - ETA: 2:05:06 - accuracy: 0.9145 - loss: 0.2357\n",
      " training set -> batch:763 loss:0.23086416721343994 and acc: 0.91425621509552\n",
      " 763/2105 [=========>....................] - ETA: 2:04:55 - accuracy: 0.9143 - loss: 0.2309\n",
      " training set -> batch:764 loss:0.23170042037963867 and acc: 0.9139999747276306\n",
      " 764/2105 [=========>....................] - ETA: 2:04:44 - accuracy: 0.9140 - loss: 0.2317\n",
      " training set -> batch:765 loss:0.22943396866321564 and acc: 0.9127907156944275\n",
      " 765/2105 [=========>....................] - ETA: 2:04:33 - accuracy: 0.9128 - loss: 0.2294\n",
      " training set -> batch:766 loss:0.22364434599876404 and acc: 0.9135338068008423\n",
      " 766/2105 [=========>....................] - ETA: 2:04:22 - accuracy: 0.9135 - loss: 0.2236\n",
      " training set -> batch:767 loss:0.23810391128063202 and acc: 0.9105839133262634\n",
      " 767/2105 [=========>....................] - ETA: 2:04:11 - accuracy: 0.9106 - loss: 0.2381\n",
      " training set -> batch:768 loss:0.2338937669992447 and acc: 0.9122340679168701\n",
      " 768/2105 [=========>....................] - ETA: 2:04:00 - accuracy: 0.9122 - loss: 0.2339\n",
      " training set -> batch:769 loss:0.23119880259037018 and acc: 0.9120689630508423\n",
      " 769/2105 [=========>....................] - ETA: 2:03:49 - accuracy: 0.9121 - loss: 0.2312\n",
      " training set -> batch:770 loss:0.22702358663082123 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:770 val loss:0.24179407954216003 and val acc: 0.9151375889778137\n",
      " 770/2105 [=========>....................] - ETA: 2:04:29 - accuracy: 0.9136 - loss: 0.2270\n",
      " training set -> batch:771 loss:0.23991361260414124 and acc: 0.9159291982650757\n",
      " 771/2105 [=========>....................] - ETA: 2:04:18 - accuracy: 0.9159 - loss: 0.2399\n",
      " training set -> batch:772 loss:0.2411787062883377 and acc: 0.9155982732772827\n",
      " 772/2105 [==========>...................] - ETA: 2:04:07 - accuracy: 0.9156 - loss: 0.2412\n",
      " training set -> batch:773 loss:0.23655277490615845 and acc: 0.9163222908973694\n",
      " 773/2105 [==========>...................] - ETA: 2:03:56 - accuracy: 0.9163 - loss: 0.2366\n",
      " training set -> batch:774 loss:0.23464232683181763 and acc: 0.9169999957084656\n",
      " 774/2105 [==========>...................] - ETA: 2:03:45 - accuracy: 0.9170 - loss: 0.2346\n",
      " training set -> batch:775 loss:0.23603278398513794 and acc: 0.9156976938247681\n",
      " 775/2105 [==========>...................] - ETA: 2:03:34 - accuracy: 0.9157 - loss: 0.2360\n",
      " training set -> batch:776 loss:0.23381571471691132 and acc: 0.9163534045219421\n",
      " 776/2105 [==========>...................] - ETA: 2:03:24 - accuracy: 0.9164 - loss: 0.2338\n",
      " training set -> batch:777 loss:0.23773068189620972 and acc: 0.9160584211349487\n",
      " 777/2105 [==========>...................] - ETA: 2:03:13 - accuracy: 0.9161 - loss: 0.2377\n",
      " training set -> batch:778 loss:0.23724938929080963 and acc: 0.9166666865348816\n",
      " 778/2105 [==========>...................] - ETA: 2:03:02 - accuracy: 0.9167 - loss: 0.2372\n",
      " training set -> batch:779 loss:0.23026737570762634 and acc: 0.9181034564971924\n",
      " 779/2105 [==========>...................] - ETA: 2:02:51 - accuracy: 0.9181 - loss: 0.2303\n",
      " training set -> batch:780 loss:0.23095525801181793 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:780 val loss:0.2333594709634781 and val acc: 0.9128440618515015\n",
      " 780/2105 [==========>...................] - ETA: 2:03:30 - accuracy: 0.9186 - loss: 0.2310\n",
      " training set -> batch:781 loss:0.23028214275836945 and acc: 0.9126105904579163\n",
      " 781/2105 [==========>...................] - ETA: 2:03:19 - accuracy: 0.9126 - loss: 0.2303\n",
      " training set -> batch:782 loss:0.2214554101228714 and acc: 0.9145299196243286\n",
      " 782/2105 [==========>...................] - ETA: 2:03:08 - accuracy: 0.9145 - loss: 0.2215\n",
      " training set -> batch:783 loss:0.22328650951385498 and acc: 0.9121900796890259\n",
      " 783/2105 [==========>...................] - ETA: 2:02:57 - accuracy: 0.9122 - loss: 0.2233\n",
      " training set -> batch:784 loss:0.21662534773349762 and acc: 0.9139999747276306\n",
      " 784/2105 [==========>...................] - ETA: 2:02:47 - accuracy: 0.9140 - loss: 0.2166\n",
      " training set -> batch:785 loss:0.21655654907226562 and acc: 0.9147287011146545\n",
      " 785/2105 [==========>...................] - ETA: 2:02:36 - accuracy: 0.9147 - loss: 0.2166\n",
      " training set -> batch:786 loss:0.21396736800670624 and acc: 0.9163534045219421\n",
      " 786/2105 [==========>...................] - ETA: 2:02:25 - accuracy: 0.9164 - loss: 0.2140\n",
      " training set -> batch:787 loss:0.2116203010082245 and acc: 0.9160584211349487\n",
      " 787/2105 [==========>...................] - ETA: 2:02:14 - accuracy: 0.9161 - loss: 0.2116\n",
      " training set -> batch:788 loss:0.21121622622013092 and acc: 0.9157801270484924\n",
      " 788/2105 [==========>...................] - ETA: 2:02:04 - accuracy: 0.9158 - loss: 0.2112\n",
      " training set -> batch:789 loss:0.21264797449111938 and acc: 0.9137930870056152\n",
      " 789/2105 [==========>...................] - ETA: 2:01:53 - accuracy: 0.9138 - loss: 0.2126\n",
      " training set -> batch:790 loss:0.21400195360183716 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:790 val loss:0.23583510518074036 and val acc: 0.9059633016586304\n",
      " 790/2105 [==========>...................] - ETA: 2:02:31 - accuracy: 0.9136 - loss: 0.2140\n",
      " training set -> batch:791 loss:0.2323596328496933 and acc: 0.9070796370506287\n",
      " 791/2105 [==========>...................] - ETA: 2:02:20 - accuracy: 0.9071 - loss: 0.2324\n",
      " training set -> batch:792 loss:0.22571226954460144 and acc: 0.9091880321502686\n",
      " 792/2105 [==========>...................] - ETA: 2:02:09 - accuracy: 0.9092 - loss: 0.2257\n",
      " training set -> batch:793 loss:0.2206282615661621 and acc: 0.9111570119857788\n",
      " 793/2105 [==========>...................] - ETA: 2:01:59 - accuracy: 0.9112 - loss: 0.2206\n",
      " training set -> batch:794 loss:0.24375754594802856 and acc: 0.9079999923706055\n",
      " 794/2105 [==========>...................] - ETA: 2:01:48 - accuracy: 0.9080 - loss: 0.2438\n",
      " training set -> batch:795 loss:0.24826188385486603 and acc: 0.9069767594337463\n",
      " 795/2105 [==========>...................] - ETA: 2:01:37 - accuracy: 0.9070 - loss: 0.2483\n",
      " training set -> batch:796 loss:0.24910514056682587 and acc: 0.9060150384902954\n",
      " 796/2105 [==========>...................] - ETA: 2:01:27 - accuracy: 0.9060 - loss: 0.2491\n",
      " training set -> batch:797 loss:0.2491094321012497 and acc: 0.9051094651222229\n",
      " 797/2105 [==========>...................] - ETA: 2:01:16 - accuracy: 0.9051 - loss: 0.2491\n",
      " training set -> batch:798 loss:0.2442891001701355 and acc: 0.9060283899307251\n",
      " 798/2105 [==========>...................] - ETA: 2:01:05 - accuracy: 0.9060 - loss: 0.2443\n",
      " training set -> batch:799 loss:0.23661687970161438 and acc: 0.9086207151412964\n",
      " 799/2105 [==========>...................] - ETA: 2:00:55 - accuracy: 0.9086 - loss: 0.2366\n",
      " training set -> batch:800 loss:0.2325441986322403 and acc: 0.9093959927558899\n",
      "\n",
      " validation set -> batch:800 val loss:0.21490056812763214 and val acc: 0.9151375889778137\n",
      " 800/2105 [==========>...................] - ETA: 2:01:32 - accuracy: 0.9094 - loss: 0.2325\n",
      " training set -> batch:801 loss:0.2166379988193512 and acc: 0.9148229956626892\n",
      " 801/2105 [==========>...................] - ETA: 2:01:21 - accuracy: 0.9148 - loss: 0.2166\n",
      " training set -> batch:802 loss:0.22707119584083557 and acc: 0.9123931527137756\n",
      " 802/2105 [==========>...................] - ETA: 2:01:11 - accuracy: 0.9124 - loss: 0.2271\n",
      " training set -> batch:803 loss:0.2239741086959839 and acc: 0.913223147392273\n",
      " 803/2105 [==========>...................] - ETA: 2:01:00 - accuracy: 0.9132 - loss: 0.2240\n",
      " training set -> batch:804 loss:0.2223297506570816 and acc: 0.9139999747276306\n",
      " 804/2105 [==========>...................] - ETA: 2:00:50 - accuracy: 0.9140 - loss: 0.2223\n",
      " training set -> batch:805 loss:0.21741926670074463 and acc: 0.9147287011146545\n",
      " 805/2105 [==========>...................] - ETA: 2:00:39 - accuracy: 0.9147 - loss: 0.2174\n",
      " training set -> batch:806 loss:0.21359491348266602 and acc: 0.9154135584831238\n",
      " 806/2105 [==========>...................] - ETA: 2:00:29 - accuracy: 0.9154 - loss: 0.2136\n",
      " training set -> batch:807 loss:0.2120964676141739 and acc: 0.9169707894325256\n",
      " 807/2105 [==========>...................] - ETA: 2:00:18 - accuracy: 0.9170 - loss: 0.2121\n",
      " training set -> batch:808 loss:0.21584086120128632 and acc: 0.9157801270484924\n",
      " 808/2105 [==========>...................] - ETA: 2:00:08 - accuracy: 0.9158 - loss: 0.2158\n",
      " training set -> batch:809 loss:0.2185792475938797 and acc: 0.9137930870056152\n",
      " 809/2105 [==========>...................] - ETA: 1:59:57 - accuracy: 0.9138 - loss: 0.2186\n",
      " training set -> batch:810 loss:0.2159481793642044 and acc: 0.9144295454025269\n",
      "\n",
      " validation set -> batch:810 val loss:0.21043609082698822 and val acc: 0.9220183491706848\n",
      " 810/2105 [==========>...................] - ETA: 2:00:34 - accuracy: 0.9144 - loss: 0.2159\n",
      " training set -> batch:811 loss:0.2122083157300949 and acc: 0.9214601516723633\n",
      " 811/2105 [==========>...................] - ETA: 2:00:24 - accuracy: 0.9215 - loss: 0.2122\n",
      " training set -> batch:812 loss:0.21748018264770508 and acc: 0.9209401607513428\n",
      " 812/2105 [==========>...................] - ETA: 2:00:13 - accuracy: 0.9209 - loss: 0.2175\n",
      " training set -> batch:813 loss:0.2152644246816635 and acc: 0.9204545617103577\n",
      " 813/2105 [==========>...................] - ETA: 2:00:03 - accuracy: 0.9205 - loss: 0.2153\n",
      " training set -> batch:814 loss:0.21880336105823517 and acc: 0.9190000295639038\n",
      " 814/2105 [==========>...................] - ETA: 1:59:52 - accuracy: 0.9190 - loss: 0.2188\n",
      " training set -> batch:815 loss:0.22119511663913727 and acc: 0.9176356792449951\n",
      " 815/2105 [==========>...................] - ETA: 1:59:42 - accuracy: 0.9176 - loss: 0.2212\n",
      " training set -> batch:816 loss:0.22822527587413788 and acc: 0.9163534045219421\n",
      " 816/2105 [==========>...................] - ETA: 1:59:31 - accuracy: 0.9164 - loss: 0.2282\n",
      " training set -> batch:817 loss:0.22480766475200653 and acc: 0.918795645236969\n",
      " 817/2105 [==========>...................] - ETA: 1:59:21 - accuracy: 0.9188 - loss: 0.2248\n",
      " training set -> batch:818 loss:0.22219590842723846 and acc: 0.9193262457847595\n",
      " 818/2105 [==========>...................] - ETA: 1:59:11 - accuracy: 0.9193 - loss: 0.2222\n",
      " training set -> batch:819 loss:0.2202187180519104 and acc: 0.9198275804519653\n",
      " 819/2105 [==========>...................] - ETA: 1:59:00 - accuracy: 0.9198 - loss: 0.2202\n",
      " training set -> batch:820 loss:0.22018547356128693 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:820 val loss:0.24825246632099152 and val acc: 0.9059633016586304\n",
      " 820/2105 [==========>...................] - ETA: 1:59:35 - accuracy: 0.9195 - loss: 0.2202\n",
      " training set -> batch:821 loss:0.24816960096359253 and acc: 0.9059734344482422\n",
      " 821/2105 [==========>...................] - ETA: 1:59:25 - accuracy: 0.9060 - loss: 0.2482\n",
      " training set -> batch:822 loss:0.2428831309080124 and acc: 0.9081196784973145\n",
      " 822/2105 [==========>...................] - ETA: 1:59:14 - accuracy: 0.9081 - loss: 0.2429\n",
      " training set -> batch:823 loss:0.24318209290504456 and acc: 0.9090909361839294\n",
      " 823/2105 [==========>...................] - ETA: 1:59:04 - accuracy: 0.9091 - loss: 0.2432\n",
      " training set -> batch:824 loss:0.23987320065498352 and acc: 0.9089999794960022\n",
      " 824/2105 [==========>...................] - ETA: 1:58:53 - accuracy: 0.9090 - loss: 0.2399\n",
      " training set -> batch:825 loss:0.23582985997200012 and acc: 0.9098837375640869\n",
      " 825/2105 [==========>...................] - ETA: 1:58:43 - accuracy: 0.9099 - loss: 0.2358\n",
      " training set -> batch:826 loss:0.23124703764915466 and acc: 0.9107142686843872\n",
      " 826/2105 [==========>...................] - ETA: 1:58:33 - accuracy: 0.9107 - loss: 0.2312\n",
      " training set -> batch:827 loss:0.2273700088262558 and acc: 0.9124087691307068\n",
      " 827/2105 [==========>...................] - ETA: 1:58:23 - accuracy: 0.9124 - loss: 0.2274\n",
      " training set -> batch:828 loss:0.22330980002880096 and acc: 0.9140070676803589\n",
      " 828/2105 [==========>...................] - ETA: 1:58:12 - accuracy: 0.9140 - loss: 0.2233\n",
      " training set -> batch:829 loss:0.22541046142578125 and acc: 0.9129310250282288\n",
      " 829/2105 [==========>...................] - ETA: 1:58:02 - accuracy: 0.9129 - loss: 0.2254\n",
      " training set -> batch:830 loss:0.221615269780159 and acc: 0.9144295454025269\n",
      "\n",
      " validation set -> batch:830 val loss:0.22442327439785004 and val acc: 0.9208715558052063\n",
      " 830/2105 [==========>...................] - ETA: 1:58:36 - accuracy: 0.9144 - loss: 0.2216\n",
      " training set -> batch:831 loss:0.22275857627391815 and acc: 0.9203540086746216\n",
      " 831/2105 [==========>...................] - ETA: 1:58:26 - accuracy: 0.9204 - loss: 0.2228\n",
      " training set -> batch:832 loss:0.21619468927383423 and acc: 0.9220085740089417\n",
      " 832/2105 [==========>...................] - ETA: 1:58:15 - accuracy: 0.9220 - loss: 0.2162\n",
      " training set -> batch:833 loss:0.21197569370269775 and acc: 0.922520637512207\n",
      " 833/2105 [==========>...................] - ETA: 1:58:05 - accuracy: 0.9225 - loss: 0.2120\n",
      " training set -> batch:834 loss:0.21267162263393402 and acc: 0.9229999780654907\n",
      " 834/2105 [==========>...................] - ETA: 1:57:55 - accuracy: 0.9230 - loss: 0.2127\n",
      " training set -> batch:835 loss:0.2211715281009674 and acc: 0.9205426573753357\n",
      " 835/2105 [==========>...................] - ETA: 1:57:45 - accuracy: 0.9205 - loss: 0.2212\n",
      " training set -> batch:836 loss:0.21568529307842255 and acc: 0.9210526347160339\n",
      " 836/2105 [==========>...................] - ETA: 1:57:35 - accuracy: 0.9211 - loss: 0.2157\n",
      " training set -> batch:837 loss:0.213038370013237 and acc: 0.9215328693389893\n",
      " 837/2105 [==========>...................] - ETA: 1:57:25 - accuracy: 0.9215 - loss: 0.2130\n",
      " training set -> batch:838 loss:0.20848117768764496 and acc: 0.9228723645210266\n",
      " 838/2105 [==========>...................] - ETA: 1:57:15 - accuracy: 0.9229 - loss: 0.2085\n",
      " training set -> batch:839 loss:0.20758582651615143 and acc: 0.9215517044067383\n",
      " 839/2105 [==========>...................] - ETA: 1:57:04 - accuracy: 0.9216 - loss: 0.2076\n",
      " training set -> batch:840 loss:0.2099524587392807 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:840 val loss:0.23475170135498047 and val acc: 0.91399085521698\n",
      " 840/2105 [==========>...................] - ETA: 1:57:39 - accuracy: 0.9211 - loss: 0.2100\n",
      " training set -> batch:841 loss:0.23018378019332886 and acc: 0.9126105904579163\n",
      " 841/2105 [==========>...................] - ETA: 1:57:29 - accuracy: 0.9126 - loss: 0.2302\n",
      " training set -> batch:842 loss:0.2337050884962082 and acc: 0.9113247990608215\n",
      " 842/2105 [===========>..................] - ETA: 1:57:18 - accuracy: 0.9113 - loss: 0.2337\n",
      " training set -> batch:843 loss:0.2329426407814026 and acc: 0.9111570119857788\n",
      " 843/2105 [===========>..................] - ETA: 1:57:08 - accuracy: 0.9112 - loss: 0.2329\n",
      " training set -> batch:844 loss:0.22492003440856934 and acc: 0.9129999876022339\n",
      " 844/2105 [===========>..................] - ETA: 1:56:58 - accuracy: 0.9130 - loss: 0.2249\n",
      " training set -> batch:845 loss:0.23064914345741272 and acc: 0.911821722984314\n",
      " 845/2105 [===========>..................] - ETA: 1:56:48 - accuracy: 0.9118 - loss: 0.2306\n",
      " training set -> batch:846 loss:0.23770542442798615 and acc: 0.9097744226455688\n",
      " 846/2105 [===========>..................] - ETA: 1:56:38 - accuracy: 0.9098 - loss: 0.2377\n",
      " training set -> batch:847 loss:0.2300562858581543 and acc: 0.9124087691307068\n",
      " 847/2105 [===========>..................] - ETA: 1:56:28 - accuracy: 0.9124 - loss: 0.2301\n",
      " training set -> batch:848 loss:0.22420261800289154 and acc: 0.9140070676803589\n",
      " 848/2105 [===========>..................] - ETA: 1:56:18 - accuracy: 0.9140 - loss: 0.2242\n",
      " training set -> batch:849 loss:0.22060327231884003 and acc: 0.9146551489830017\n",
      " 849/2105 [===========>..................] - ETA: 1:56:08 - accuracy: 0.9147 - loss: 0.2206\n",
      " training set -> batch:850 loss:0.21385587751865387 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:850 val loss:0.21187177300453186 and val acc: 0.9128440618515015\n",
      " 850/2105 [===========>..................] - ETA: 1:56:42 - accuracy: 0.9169 - loss: 0.2139\n",
      " training set -> batch:851 loss:0.20356346666812897 and acc: 0.9137167930603027\n",
      " 851/2105 [===========>..................] - ETA: 1:56:32 - accuracy: 0.9137 - loss: 0.2036\n",
      " training set -> batch:852 loss:0.2097681611776352 and acc: 0.9123931527137756\n",
      " 852/2105 [===========>..................] - ETA: 1:56:22 - accuracy: 0.9124 - loss: 0.2098\n",
      " training set -> batch:853 loss:0.21371328830718994 and acc: 0.9101239442825317\n",
      " 853/2105 [===========>..................] - ETA: 1:56:12 - accuracy: 0.9101 - loss: 0.2137\n",
      " training set -> batch:854 loss:0.20843248069286346 and acc: 0.9120000004768372\n",
      " 854/2105 [===========>..................] - ETA: 1:56:02 - accuracy: 0.9120 - loss: 0.2084\n",
      " training set -> batch:855 loss:0.20558851957321167 and acc: 0.9127907156944275\n",
      " 855/2105 [===========>..................] - ETA: 1:55:52 - accuracy: 0.9128 - loss: 0.2056\n",
      " training set -> batch:856 loss:0.20476877689361572 and acc: 0.9135338068008423\n",
      " 856/2105 [===========>..................] - ETA: 1:55:42 - accuracy: 0.9135 - loss: 0.2048\n",
      " training set -> batch:857 loss:0.2000439465045929 and acc: 0.9142335653305054\n",
      " 857/2105 [===========>..................] - ETA: 1:55:32 - accuracy: 0.9142 - loss: 0.2000\n",
      " training set -> batch:858 loss:0.20315933227539062 and acc: 0.9140070676803589\n",
      " 858/2105 [===========>..................] - ETA: 1:55:22 - accuracy: 0.9140 - loss: 0.2032\n",
      " training set -> batch:859 loss:0.19932875037193298 and acc: 0.915517270565033\n",
      " 859/2105 [===========>..................] - ETA: 1:55:12 - accuracy: 0.9155 - loss: 0.1993\n",
      " training set -> batch:860 loss:0.19913597404956818 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:860 val loss:0.19811251759529114 and val acc: 0.9231651425361633\n",
      " 860/2105 [===========>..................] - ETA: 1:55:46 - accuracy: 0.9153 - loss: 0.1991\n",
      " training set -> batch:861 loss:0.2057194858789444 and acc: 0.9203540086746216\n",
      " 861/2105 [===========>..................] - ETA: 1:55:36 - accuracy: 0.9204 - loss: 0.2057\n",
      " training set -> batch:862 loss:0.21091926097869873 and acc: 0.9188033938407898\n",
      " 862/2105 [===========>..................] - ETA: 1:55:26 - accuracy: 0.9188 - loss: 0.2109\n",
      " training set -> batch:863 loss:0.21107326447963715 and acc: 0.9194214940071106\n",
      " 863/2105 [===========>..................] - ETA: 1:55:16 - accuracy: 0.9194 - loss: 0.2111\n",
      " training set -> batch:864 loss:0.2104925513267517 and acc: 0.9200000166893005\n",
      " 864/2105 [===========>..................] - ETA: 1:55:06 - accuracy: 0.9200 - loss: 0.2105\n",
      " training set -> batch:865 loss:0.20944559574127197 and acc: 0.9205426573753357\n",
      " 865/2105 [===========>..................] - ETA: 1:54:56 - accuracy: 0.9205 - loss: 0.2094\n",
      " training set -> batch:866 loss:0.20295147597789764 and acc: 0.9229323267936707\n",
      " 866/2105 [===========>..................] - ETA: 1:54:46 - accuracy: 0.9229 - loss: 0.2030\n",
      " training set -> batch:867 loss:0.20277175307273865 and acc: 0.9233576655387878\n",
      " 867/2105 [===========>..................] - ETA: 1:54:36 - accuracy: 0.9234 - loss: 0.2028\n",
      " training set -> batch:868 loss:0.20850692689418793 and acc: 0.9219858050346375\n",
      " 868/2105 [===========>..................] - ETA: 1:54:26 - accuracy: 0.9220 - loss: 0.2085\n",
      " training set -> batch:869 loss:0.2045924812555313 and acc: 0.9215517044067383\n",
      " 869/2105 [===========>..................] - ETA: 1:54:17 - accuracy: 0.9216 - loss: 0.2046\n",
      " training set -> batch:870 loss:0.20616070926189423 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:870 val loss:0.28462451696395874 and val acc: 0.8933486342430115\n",
      " 870/2105 [===========>..................] - ETA: 1:54:49 - accuracy: 0.9211 - loss: 0.2062\n",
      " training set -> batch:871 loss:0.28394630551338196 and acc: 0.8938053250312805\n",
      " 871/2105 [===========>..................] - ETA: 1:54:40 - accuracy: 0.8938 - loss: 0.2839\n",
      " training set -> batch:872 loss:0.2742159366607666 and acc: 0.8952991366386414\n",
      " 872/2105 [===========>..................] - ETA: 1:54:30 - accuracy: 0.8953 - loss: 0.2742\n",
      " training set -> batch:873 loss:0.28974899649620056 and acc: 0.8935950398445129\n",
      " 873/2105 [===========>..................] - ETA: 1:54:20 - accuracy: 0.8936 - loss: 0.2897\n",
      " training set -> batch:874 loss:0.2831425368785858 and acc: 0.8949999809265137\n",
      " 874/2105 [===========>..................] - ETA: 1:54:10 - accuracy: 0.8950 - loss: 0.2831\n",
      " training set -> batch:875 loss:0.27528440952301025 and acc: 0.8963178396224976\n",
      " 875/2105 [===========>..................] - ETA: 1:54:00 - accuracy: 0.8963 - loss: 0.2753\n",
      " training set -> batch:876 loss:0.2662701904773712 and acc: 0.8984962701797485\n",
      " 876/2105 [===========>..................] - ETA: 1:53:51 - accuracy: 0.8985 - loss: 0.2663\n",
      " training set -> batch:877 loss:0.2604941725730896 and acc: 0.8996350169181824\n",
      " 877/2105 [===========>..................] - ETA: 1:53:41 - accuracy: 0.8996 - loss: 0.2605\n",
      " training set -> batch:878 loss:0.2578262984752655 and acc: 0.8998227119445801\n",
      " 878/2105 [===========>..................] - ETA: 1:53:31 - accuracy: 0.8998 - loss: 0.2578\n",
      " training set -> batch:879 loss:0.252351850271225 and acc: 0.9017241597175598\n",
      " 879/2105 [===========>..................] - ETA: 1:53:21 - accuracy: 0.9017 - loss: 0.2524\n",
      " training set -> batch:880 loss:0.25293248891830444 and acc: 0.9010066986083984\n",
      "\n",
      " validation set -> batch:880 val loss:0.19783148169517517 and val acc: 0.9208715558052063\n",
      " 880/2105 [===========>..................] - ETA: 1:53:54 - accuracy: 0.9010 - loss: 0.2529\n",
      " training set -> batch:881 loss:0.19403646886348724 and acc: 0.9225663542747498\n",
      " 881/2105 [===========>..................] - ETA: 1:53:44 - accuracy: 0.9226 - loss: 0.1940\n",
      " training set -> batch:882 loss:0.1956474781036377 and acc: 0.9230769276618958\n",
      " 882/2105 [===========>..................] - ETA: 1:53:35 - accuracy: 0.9231 - loss: 0.1956\n",
      " training set -> batch:883 loss:0.20502594113349915 and acc: 0.9214876294136047\n",
      " 883/2105 [===========>..................] - ETA: 1:53:25 - accuracy: 0.9215 - loss: 0.2050\n",
      " training set -> batch:884 loss:0.20520810782909393 and acc: 0.9200000166893005\n",
      " 884/2105 [===========>..................] - ETA: 1:53:15 - accuracy: 0.9200 - loss: 0.2052\n",
      " training set -> batch:885 loss:0.20223456621170044 and acc: 0.9205426573753357\n",
      " 885/2105 [===========>..................] - ETA: 1:53:05 - accuracy: 0.9205 - loss: 0.2022\n",
      " training set -> batch:886 loss:0.2138112485408783 and acc: 0.9172932505607605\n",
      " 886/2105 [===========>..................] - ETA: 1:52:56 - accuracy: 0.9173 - loss: 0.2138\n",
      " training set -> batch:887 loss:0.23423528671264648 and acc: 0.9133211970329285\n",
      " 887/2105 [===========>..................] - ETA: 1:52:46 - accuracy: 0.9133 - loss: 0.2342\n",
      " training set -> batch:888 loss:0.2280098795890808 and acc: 0.914893627166748\n",
      " 888/2105 [===========>..................] - ETA: 1:52:36 - accuracy: 0.9149 - loss: 0.2280\n",
      " training set -> batch:889 loss:0.22944435477256775 and acc: 0.915517270565033\n",
      " 889/2105 [===========>..................] - ETA: 1:52:27 - accuracy: 0.9155 - loss: 0.2294\n",
      " training set -> batch:890 loss:0.22949691116809845 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:890 val loss:0.20486243069171906 and val acc: 0.9220183491706848\n",
      " 890/2105 [===========>..................] - ETA: 1:52:58 - accuracy: 0.9161 - loss: 0.2295\n",
      " training set -> batch:891 loss:0.203495591878891 and acc: 0.9236725568771362\n",
      " 891/2105 [===========>..................] - ETA: 1:52:49 - accuracy: 0.9237 - loss: 0.2035\n",
      " training set -> batch:892 loss:0.2024005502462387 and acc: 0.9241452813148499\n",
      " 892/2105 [===========>..................] - ETA: 1:52:39 - accuracy: 0.9241 - loss: 0.2024\n",
      " training set -> batch:893 loss:0.20184415578842163 and acc: 0.9245867729187012\n",
      " 893/2105 [===========>..................] - ETA: 1:52:29 - accuracy: 0.9246 - loss: 0.2018\n",
      " training set -> batch:894 loss:0.20834356546401978 and acc: 0.9210000038146973\n",
      " 894/2105 [===========>..................] - ETA: 1:52:20 - accuracy: 0.9210 - loss: 0.2083\n",
      " training set -> batch:895 loss:0.20720940828323364 and acc: 0.9215116500854492\n",
      " 895/2105 [===========>..................] - ETA: 1:52:10 - accuracy: 0.9215 - loss: 0.2072\n",
      " training set -> batch:896 loss:0.2061411440372467 and acc: 0.9219924807548523\n",
      " 896/2105 [===========>..................] - ETA: 1:52:00 - accuracy: 0.9220 - loss: 0.2061\n",
      " training set -> batch:897 loss:0.20947659015655518 and acc: 0.9215328693389893\n",
      " 897/2105 [===========>..................] - ETA: 1:51:51 - accuracy: 0.9215 - loss: 0.2095\n",
      " training set -> batch:898 loss:0.20736408233642578 and acc: 0.9228723645210266\n",
      " 898/2105 [===========>..................] - ETA: 1:51:41 - accuracy: 0.9229 - loss: 0.2074\n",
      " training set -> batch:899 loss:0.20407460629940033 and acc: 0.923275887966156\n",
      " 899/2105 [===========>..................] - ETA: 1:51:31 - accuracy: 0.9233 - loss: 0.2041\n",
      " training set -> batch:900 loss:0.19859474897384644 and acc: 0.9244966506958008\n",
      "\n",
      " validation set -> batch:900 val loss:0.21160127222537994 and val acc: 0.9208715558052063\n",
      " 900/2105 [===========>..................] - ETA: 1:52:03 - accuracy: 0.9245 - loss: 0.1986\n",
      " training set -> batch:901 loss:0.20293472707271576 and acc: 0.9236725568771362\n",
      " 901/2105 [===========>..................] - ETA: 1:51:53 - accuracy: 0.9237 - loss: 0.2029\n",
      " training set -> batch:902 loss:0.19859054684638977 and acc: 0.9241452813148499\n",
      " 902/2105 [===========>..................] - ETA: 1:51:44 - accuracy: 0.9241 - loss: 0.1986\n",
      " training set -> batch:903 loss:0.20322182774543762 and acc: 0.922520637512207\n",
      " 903/2105 [===========>..................] - ETA: 1:51:34 - accuracy: 0.9225 - loss: 0.2032\n",
      " training set -> batch:904 loss:0.20157504081726074 and acc: 0.9229999780654907\n",
      " 904/2105 [===========>..................] - ETA: 1:51:24 - accuracy: 0.9230 - loss: 0.2016\n",
      " training set -> batch:905 loss:0.2045580893754959 and acc: 0.9224806427955627\n",
      " 905/2105 [===========>..................] - ETA: 1:51:15 - accuracy: 0.9225 - loss: 0.2046\n",
      " training set -> batch:906 loss:0.2009158432483673 and acc: 0.9229323267936707\n",
      " 906/2105 [===========>..................] - ETA: 1:51:05 - accuracy: 0.9229 - loss: 0.2009\n",
      " training set -> batch:907 loss:0.19518983364105225 and acc: 0.9251824617385864\n",
      " 907/2105 [===========>..................] - ETA: 1:50:56 - accuracy: 0.9252 - loss: 0.1952\n",
      " training set -> batch:908 loss:0.19587679207324982 and acc: 0.923758864402771\n",
      " 908/2105 [===========>..................] - ETA: 1:50:46 - accuracy: 0.9238 - loss: 0.1959\n",
      " training set -> batch:909 loss:0.19321101903915405 and acc: 0.9241379499435425\n",
      " 909/2105 [===========>..................] - ETA: 1:50:37 - accuracy: 0.9241 - loss: 0.1932\n",
      " training set -> batch:910 loss:0.19064819812774658 and acc: 0.9244966506958008\n",
      "\n",
      " validation set -> batch:910 val loss:0.2308201938867569 and val acc: 0.9197247624397278\n",
      " 910/2105 [===========>..................] - ETA: 1:51:05 - accuracy: 0.9245 - loss: 0.1906\n",
      " training set -> batch:911 loss:0.23744602501392365 and acc: 0.9203540086746216\n",
      " 911/2105 [===========>..................] - ETA: 1:50:55 - accuracy: 0.9204 - loss: 0.2374\n",
      " training set -> batch:912 loss:0.22875700891017914 and acc: 0.9209401607513428\n",
      " 912/2105 [===========>..................] - ETA: 1:50:45 - accuracy: 0.9209 - loss: 0.2288\n",
      " training set -> batch:913 loss:0.21859952807426453 and acc: 0.9235537052154541\n",
      " 913/2105 [============>.................] - ETA: 1:50:36 - accuracy: 0.9236 - loss: 0.2186\n",
      " training set -> batch:914 loss:0.21481077373027802 and acc: 0.9240000247955322\n",
      " 914/2105 [============>.................] - ETA: 1:50:26 - accuracy: 0.9240 - loss: 0.2148\n",
      " training set -> batch:915 loss:0.2077399492263794 and acc: 0.9253876209259033\n",
      " 915/2105 [============>.................] - ETA: 1:50:16 - accuracy: 0.9254 - loss: 0.2077\n",
      " training set -> batch:916 loss:0.20436029136180878 and acc: 0.9257518649101257\n",
      " 916/2105 [============>.................] - ETA: 1:50:07 - accuracy: 0.9258 - loss: 0.2044\n",
      " training set -> batch:917 loss:0.21605323255062103 and acc: 0.9251824617385864\n",
      " 917/2105 [============>.................] - ETA: 1:49:57 - accuracy: 0.9252 - loss: 0.2161\n",
      " training set -> batch:918 loss:0.21016807854175568 and acc: 0.9264184236526489\n",
      " 918/2105 [============>.................] - ETA: 1:49:48 - accuracy: 0.9264 - loss: 0.2102\n",
      " training set -> batch:919 loss:0.212250754237175 and acc: 0.9258620738983154\n",
      " 919/2105 [============>.................] - ETA: 1:49:39 - accuracy: 0.9259 - loss: 0.2123\n",
      " training set -> batch:920 loss:0.2058539241552353 and acc: 0.9278523325920105\n",
      "\n",
      " validation set -> batch:920 val loss:0.2488817274570465 and val acc: 0.9151375889778137\n",
      " 920/2105 [============>.................] - ETA: 1:50:08 - accuracy: 0.9279 - loss: 0.2059\n",
      " training set -> batch:921 loss:0.24110126495361328 and acc: 0.9159291982650757\n",
      " 921/2105 [============>.................] - ETA: 1:49:58 - accuracy: 0.9159 - loss: 0.2411\n",
      " training set -> batch:922 loss:0.24768801033496857 and acc: 0.9134615659713745\n",
      " 922/2105 [============>.................] - ETA: 1:49:49 - accuracy: 0.9135 - loss: 0.2477\n",
      " training set -> batch:923 loss:0.2423461228609085 and acc: 0.9152892827987671\n",
      " 923/2105 [============>.................] - ETA: 1:49:39 - accuracy: 0.9153 - loss: 0.2423\n",
      " training set -> batch:924 loss:0.23430770635604858 and acc: 0.9160000085830688\n",
      " 924/2105 [============>.................] - ETA: 1:49:30 - accuracy: 0.9160 - loss: 0.2343\n",
      " training set -> batch:925 loss:0.22478143870830536 and acc: 0.9186046719551086\n",
      " 925/2105 [============>.................] - ETA: 1:49:20 - accuracy: 0.9186 - loss: 0.2248\n",
      " training set -> batch:926 loss:0.21835188567638397 and acc: 0.9201127886772156\n",
      " 926/2105 [============>.................] - ETA: 1:49:11 - accuracy: 0.9201 - loss: 0.2184\n",
      " training set -> batch:927 loss:0.21266812086105347 and acc: 0.9224452376365662\n",
      " 927/2105 [============>.................] - ETA: 1:49:02 - accuracy: 0.9224 - loss: 0.2127\n",
      " training set -> batch:928 loss:0.21350637078285217 and acc: 0.9219858050346375\n",
      " 928/2105 [============>.................] - ETA: 1:48:52 - accuracy: 0.9220 - loss: 0.2135\n",
      " training set -> batch:929 loss:0.20683184266090393 and acc: 0.9241379499435425\n",
      " 929/2105 [============>.................] - ETA: 1:48:43 - accuracy: 0.9241 - loss: 0.2068\n",
      " training set -> batch:930 loss:0.2090570479631424 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:930 val loss:0.25983792543411255 and val acc: 0.9151375889778137\n",
      " 930/2105 [============>.................] - ETA: 1:49:11 - accuracy: 0.9237 - loss: 0.2091\n",
      " training set -> batch:931 loss:0.24918721616268158 and acc: 0.9159291982650757\n",
      " 931/2105 [============>.................] - ETA: 1:49:02 - accuracy: 0.9159 - loss: 0.2492\n",
      " training set -> batch:932 loss:0.2543542683124542 and acc: 0.9134615659713745\n",
      " 932/2105 [============>.................] - ETA: 1:48:52 - accuracy: 0.9135 - loss: 0.2544\n",
      " training set -> batch:933 loss:0.2553732395172119 and acc: 0.91425621509552\n",
      " 933/2105 [============>.................] - ETA: 1:48:43 - accuracy: 0.9143 - loss: 0.2554\n",
      " training set -> batch:934 loss:0.2561703622341156 and acc: 0.9139999747276306\n",
      " 934/2105 [============>.................] - ETA: 1:48:33 - accuracy: 0.9140 - loss: 0.2562\n",
      " training set -> batch:935 loss:0.25611886382102966 and acc: 0.911821722984314\n",
      " 935/2105 [============>.................] - ETA: 1:48:24 - accuracy: 0.9118 - loss: 0.2561\n",
      " training set -> batch:936 loss:0.2522468566894531 and acc: 0.9125939607620239\n",
      " 936/2105 [============>.................] - ETA: 1:48:14 - accuracy: 0.9126 - loss: 0.2522\n",
      " training set -> batch:937 loss:0.25193649530410767 and acc: 0.9114963412284851\n",
      " 937/2105 [============>.................] - ETA: 1:48:05 - accuracy: 0.9115 - loss: 0.2519\n",
      " training set -> batch:938 loss:0.24801653623580933 and acc: 0.9122340679168701\n",
      " 938/2105 [============>.................] - ETA: 1:47:56 - accuracy: 0.9122 - loss: 0.2480\n",
      " training set -> batch:939 loss:0.25032350420951843 and acc: 0.9129310250282288\n",
      " 939/2105 [============>.................] - ETA: 1:47:47 - accuracy: 0.9129 - loss: 0.2503\n",
      " training set -> batch:940 loss:0.2454816699028015 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:940 val loss:0.2143893539905548 and val acc: 0.9162843823432922\n",
      " 940/2105 [============>.................] - ETA: 1:48:15 - accuracy: 0.9136 - loss: 0.2455\n",
      " training set -> batch:941 loss:0.21398861706256866 and acc: 0.9159291982650757\n",
      " 941/2105 [============>.................] - ETA: 1:48:06 - accuracy: 0.9159 - loss: 0.2140\n",
      " training set -> batch:942 loss:0.2127564698457718 and acc: 0.9166666865348816\n",
      " 942/2105 [============>.................] - ETA: 1:47:56 - accuracy: 0.9167 - loss: 0.2128\n",
      " training set -> batch:943 loss:0.20886921882629395 and acc: 0.9173553586006165\n",
      " 943/2105 [============>.................] - ETA: 1:47:47 - accuracy: 0.9174 - loss: 0.2089\n",
      " training set -> batch:944 loss:0.21485573053359985 and acc: 0.9160000085830688\n",
      " 944/2105 [============>.................] - ETA: 1:47:38 - accuracy: 0.9160 - loss: 0.2149\n",
      " training set -> batch:945 loss:0.21452654898166656 and acc: 0.9166666865348816\n",
      " 945/2105 [============>.................] - ETA: 1:47:28 - accuracy: 0.9167 - loss: 0.2145\n",
      " training set -> batch:946 loss:0.20949475467205048 and acc: 0.9191729426383972\n",
      " 946/2105 [============>.................] - ETA: 1:47:19 - accuracy: 0.9192 - loss: 0.2095\n",
      " training set -> batch:947 loss:0.20981550216674805 and acc: 0.918795645236969\n",
      " 947/2105 [============>.................] - ETA: 1:47:10 - accuracy: 0.9188 - loss: 0.2098\n",
      " training set -> batch:948 loss:0.21162039041519165 and acc: 0.9184397459030151\n",
      " 948/2105 [============>.................] - ETA: 1:47:01 - accuracy: 0.9184 - loss: 0.2116\n",
      " training set -> batch:949 loss:0.2130165547132492 and acc: 0.9172413945198059\n",
      " 949/2105 [============>.................] - ETA: 1:46:51 - accuracy: 0.9172 - loss: 0.2130\n",
      " training set -> batch:950 loss:0.2121303528547287 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:950 val loss:0.22135557234287262 and val acc: 0.9151375889778137\n",
      " 950/2105 [============>.................] - ETA: 1:47:18 - accuracy: 0.9169 - loss: 0.2121\n",
      " training set -> batch:951 loss:0.21551121771335602 and acc: 0.9159291982650757\n",
      " 951/2105 [============>.................] - ETA: 1:47:09 - accuracy: 0.9159 - loss: 0.2155\n",
      " training set -> batch:952 loss:0.21550936996936798 and acc: 0.9145299196243286\n",
      " 952/2105 [============>.................] - ETA: 1:47:00 - accuracy: 0.9145 - loss: 0.2155\n",
      " training set -> batch:953 loss:0.2080717831850052 and acc: 0.9163222908973694\n",
      " 953/2105 [============>.................] - ETA: 1:46:50 - accuracy: 0.9163 - loss: 0.2081\n",
      " training set -> batch:954 loss:0.2041826695203781 and acc: 0.9160000085830688\n",
      " 954/2105 [============>.................] - ETA: 1:46:41 - accuracy: 0.9160 - loss: 0.2042\n",
      " training set -> batch:955 loss:0.2087249606847763 and acc: 0.9147287011146545\n",
      " 955/2105 [============>.................] - ETA: 1:46:32 - accuracy: 0.9147 - loss: 0.2087\n",
      " training set -> batch:956 loss:0.20817574858665466 and acc: 0.9163534045219421\n",
      " 956/2105 [============>.................] - ETA: 1:46:23 - accuracy: 0.9164 - loss: 0.2082\n",
      " training set -> batch:957 loss:0.2009693831205368 and acc: 0.918795645236969\n",
      " 957/2105 [============>.................] - ETA: 1:46:13 - accuracy: 0.9188 - loss: 0.2010\n",
      " training set -> batch:958 loss:0.20280900597572327 and acc: 0.9193262457847595\n",
      " 958/2105 [============>.................] - ETA: 1:46:04 - accuracy: 0.9193 - loss: 0.2028\n",
      " training set -> batch:959 loss:0.20612049102783203 and acc: 0.9198275804519653\n",
      " 959/2105 [============>.................] - ETA: 1:45:55 - accuracy: 0.9198 - loss: 0.2061\n",
      " training set -> batch:960 loss:0.20620079338550568 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:960 val loss:0.229462131857872 and val acc: 0.911697268486023\n",
      " 960/2105 [============>.................] - ETA: 1:46:21 - accuracy: 0.9186 - loss: 0.2062\n",
      " training set -> batch:961 loss:0.22764916718006134 and acc: 0.9126105904579163\n",
      " 961/2105 [============>.................] - ETA: 1:46:12 - accuracy: 0.9126 - loss: 0.2276\n",
      " training set -> batch:962 loss:0.2381213754415512 and acc: 0.9113247990608215\n",
      " 962/2105 [============>.................] - ETA: 1:46:03 - accuracy: 0.9113 - loss: 0.2381\n",
      " training set -> batch:963 loss:0.23383477330207825 and acc: 0.9121900796890259\n",
      " 963/2105 [============>.................] - ETA: 1:45:53 - accuracy: 0.9122 - loss: 0.2338\n",
      " training set -> batch:964 loss:0.2407604306936264 and acc: 0.9100000262260437\n",
      " 964/2105 [============>.................] - ETA: 1:45:44 - accuracy: 0.9100 - loss: 0.2408\n",
      " training set -> batch:965 loss:0.23170697689056396 and acc: 0.9127907156944275\n",
      " 965/2105 [============>.................] - ETA: 1:45:35 - accuracy: 0.9128 - loss: 0.2317\n",
      " training set -> batch:966 loss:0.23580105602741241 and acc: 0.9116541147232056\n",
      " 966/2105 [============>.................] - ETA: 1:45:26 - accuracy: 0.9117 - loss: 0.2358\n",
      " training set -> batch:967 loss:0.2344297468662262 and acc: 0.9133211970329285\n",
      " 967/2105 [============>.................] - ETA: 1:45:17 - accuracy: 0.9133 - loss: 0.2344\n",
      " training set -> batch:968 loss:0.22817111015319824 and acc: 0.9140070676803589\n",
      " 968/2105 [============>.................] - ETA: 1:45:08 - accuracy: 0.9140 - loss: 0.2282\n",
      " training set -> batch:969 loss:0.2279033660888672 and acc: 0.9120689630508423\n",
      " 969/2105 [============>.................] - ETA: 1:44:58 - accuracy: 0.9121 - loss: 0.2279\n",
      " training set -> batch:970 loss:0.22433556616306305 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:970 val loss:0.2538362443447113 and val acc: 0.9105504751205444\n",
      " 970/2105 [============>.................] - ETA: 1:45:25 - accuracy: 0.9128 - loss: 0.2243\n",
      " training set -> batch:971 loss:0.24955739080905914 and acc: 0.9115044474601746\n",
      " 971/2105 [============>.................] - ETA: 1:45:16 - accuracy: 0.9115 - loss: 0.2496\n",
      " training set -> batch:972 loss:0.2610132396221161 and acc: 0.9091880321502686\n",
      " 972/2105 [============>.................] - ETA: 1:45:06 - accuracy: 0.9092 - loss: 0.2610\n",
      " training set -> batch:973 loss:0.2524530589580536 and acc: 0.9101239442825317\n",
      " 973/2105 [============>.................] - ETA: 1:44:57 - accuracy: 0.9101 - loss: 0.2525\n",
      " training set -> batch:974 loss:0.24582014977931976 and acc: 0.9110000133514404\n",
      " 974/2105 [============>.................] - ETA: 1:44:48 - accuracy: 0.9110 - loss: 0.2458\n",
      " training set -> batch:975 loss:0.23832698166370392 and acc: 0.9127907156944275\n",
      " 975/2105 [============>.................] - ETA: 1:44:39 - accuracy: 0.9128 - loss: 0.2383\n",
      " training set -> batch:976 loss:0.23108525574207306 and acc: 0.9154135584831238\n",
      " 976/2105 [============>.................] - ETA: 1:44:30 - accuracy: 0.9154 - loss: 0.2311\n",
      " training set -> batch:977 loss:0.2263660430908203 and acc: 0.9160584211349487\n",
      " 977/2105 [============>.................] - ETA: 1:44:21 - accuracy: 0.9161 - loss: 0.2264\n",
      " training set -> batch:978 loss:0.22615398466587067 and acc: 0.9157801270484924\n",
      " 978/2105 [============>.................] - ETA: 1:44:11 - accuracy: 0.9158 - loss: 0.2262\n",
      " training set -> batch:979 loss:0.22178129851818085 and acc: 0.9163793325424194\n",
      " 979/2105 [============>.................] - ETA: 1:44:02 - accuracy: 0.9164 - loss: 0.2218\n",
      " training set -> batch:980 loss:0.21790824830532074 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:980 val loss:0.23066580295562744 and val acc: 0.9151375889778137\n",
      " 980/2105 [============>.................] - ETA: 1:44:28 - accuracy: 0.9178 - loss: 0.2179\n",
      " training set -> batch:981 loss:0.23132047057151794 and acc: 0.9148229956626892\n",
      " 981/2105 [============>.................] - ETA: 1:44:19 - accuracy: 0.9148 - loss: 0.2313\n",
      " training set -> batch:982 loss:0.22366641461849213 and acc: 0.9166666865348816\n",
      " 982/2105 [============>.................] - ETA: 1:44:10 - accuracy: 0.9167 - loss: 0.2237\n",
      " training set -> batch:983 loss:0.2146400809288025 and acc: 0.9194214940071106\n",
      " 983/2105 [=============>................] - ETA: 1:44:01 - accuracy: 0.9194 - loss: 0.2146\n",
      " training set -> batch:984 loss:0.20982778072357178 and acc: 0.9200000166893005\n",
      " 984/2105 [=============>................] - ETA: 1:43:52 - accuracy: 0.9200 - loss: 0.2098\n",
      " training set -> batch:985 loss:0.2100939303636551 and acc: 0.9195736646652222\n",
      " 985/2105 [=============>................] - ETA: 1:43:43 - accuracy: 0.9196 - loss: 0.2101\n",
      " training set -> batch:986 loss:0.20891928672790527 and acc: 0.9191729426383972\n",
      " 986/2105 [=============>................] - ETA: 1:43:34 - accuracy: 0.9192 - loss: 0.2089\n",
      " training set -> batch:987 loss:0.21259555220603943 and acc: 0.9206204414367676\n",
      " 987/2105 [=============>................] - ETA: 1:43:25 - accuracy: 0.9206 - loss: 0.2126\n",
      " training set -> batch:988 loss:0.20722974836826324 and acc: 0.9228723645210266\n",
      " 988/2105 [=============>................] - ETA: 1:43:16 - accuracy: 0.9229 - loss: 0.2072\n",
      " training set -> batch:989 loss:0.21520234644412994 and acc: 0.9215517044067383\n",
      " 989/2105 [=============>................] - ETA: 1:43:07 - accuracy: 0.9216 - loss: 0.2152\n",
      " training set -> batch:990 loss:0.21136610209941864 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:990 val loss:0.24762411415576935 and val acc: 0.9128440618515015\n",
      " 990/2105 [=============>................] - ETA: 1:43:35 - accuracy: 0.9228 - loss: 0.2114\n",
      " training set -> batch:991 loss:0.25290748476982117 and acc: 0.9126105904579163\n",
      " 991/2105 [=============>................] - ETA: 1:43:26 - accuracy: 0.9126 - loss: 0.2529\n",
      " training set -> batch:992 loss:0.25352805852890015 and acc: 0.9113247990608215\n",
      " 992/2105 [=============>................] - ETA: 1:43:17 - accuracy: 0.9113 - loss: 0.2535\n",
      " training set -> batch:993 loss:0.2507602274417877 and acc: 0.9111570119857788\n",
      " 993/2105 [=============>................] - ETA: 1:43:08 - accuracy: 0.9112 - loss: 0.2508\n",
      " training set -> batch:994 loss:0.25014567375183105 and acc: 0.9129999876022339\n",
      " 994/2105 [=============>................] - ETA: 1:42:59 - accuracy: 0.9130 - loss: 0.2501\n",
      " training set -> batch:995 loss:0.24852938950061798 and acc: 0.911821722984314\n",
      " 995/2105 [=============>................] - ETA: 1:42:50 - accuracy: 0.9118 - loss: 0.2485\n",
      " training set -> batch:996 loss:0.25378912687301636 and acc: 0.9116541147232056\n",
      " 996/2105 [=============>................] - ETA: 1:42:41 - accuracy: 0.9117 - loss: 0.2538\n",
      " training set -> batch:997 loss:0.25126415491104126 and acc: 0.9124087691307068\n",
      " 997/2105 [=============>................] - ETA: 1:42:32 - accuracy: 0.9124 - loss: 0.2513\n",
      " training set -> batch:998 loss:0.25527241826057434 and acc: 0.911347508430481\n",
      " 998/2105 [=============>................] - ETA: 1:42:23 - accuracy: 0.9113 - loss: 0.2553\n",
      " training set -> batch:999 loss:0.2532386779785156 and acc: 0.9120689630508423\n",
      " 999/2105 [=============>................] - ETA: 1:42:14 - accuracy: 0.9121 - loss: 0.2532\n",
      " training set -> batch:1000 loss:0.2511453330516815 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:1000 val loss:0.22431302070617676 and val acc: 0.9231651425361633\n",
      "1000/2105 [=============>................] - ETA: 1:42:40 - accuracy: 0.9119 - loss: 0.2511\n",
      " training set -> batch:1001 loss:0.2209215760231018 and acc: 0.9225663542747498\n",
      "1001/2105 [=============>................] - ETA: 1:42:31 - accuracy: 0.9226 - loss: 0.2209\n",
      " training set -> batch:1002 loss:0.222831130027771 and acc: 0.9220085740089417\n",
      "1002/2105 [=============>................] - ETA: 1:42:22 - accuracy: 0.9220 - loss: 0.2228\n",
      " training set -> batch:1003 loss:0.21843601763248444 and acc: 0.922520637512207\n",
      "1003/2105 [=============>................] - ETA: 1:42:13 - accuracy: 0.9225 - loss: 0.2184\n",
      " training set -> batch:1004 loss:0.2126135379076004 and acc: 0.9229999780654907\n",
      "1004/2105 [=============>................] - ETA: 1:42:04 - accuracy: 0.9230 - loss: 0.2126\n",
      " training set -> batch:1005 loss:0.21582402288913727 and acc: 0.9234496355056763\n",
      "1005/2105 [=============>................] - ETA: 1:41:55 - accuracy: 0.9234 - loss: 0.2158\n",
      " training set -> batch:1006 loss:0.22243349254131317 and acc: 0.9229323267936707\n",
      "1006/2105 [=============>................] - ETA: 1:41:46 - accuracy: 0.9229 - loss: 0.2224\n",
      " training set -> batch:1007 loss:0.21690021455287933 and acc: 0.9251824617385864\n",
      "1007/2105 [=============>................] - ETA: 1:41:37 - accuracy: 0.9252 - loss: 0.2169\n",
      " training set -> batch:1008 loss:0.22413431107997894 and acc: 0.923758864402771\n",
      "1008/2105 [=============>................] - ETA: 1:41:29 - accuracy: 0.9238 - loss: 0.2241\n",
      " training set -> batch:1009 loss:0.2216285765171051 and acc: 0.9241379499435425\n",
      "1009/2105 [=============>................] - ETA: 1:41:20 - accuracy: 0.9241 - loss: 0.2216\n",
      " training set -> batch:1010 loss:0.21704910695552826 and acc: 0.9244966506958008\n",
      "\n",
      " validation set -> batch:1010 val loss:0.2739490568637848 and val acc: 0.8910550475120544\n",
      "1010/2105 [=============>................] - ETA: 1:41:44 - accuracy: 0.9245 - loss: 0.2170\n",
      " training set -> batch:1011 loss:0.273410439491272 and acc: 0.892699122428894\n",
      "1011/2105 [=============>................] - ETA: 1:41:35 - accuracy: 0.8927 - loss: 0.2734\n",
      " training set -> batch:1012 loss:0.2628646492958069 and acc: 0.8952991366386414\n",
      "1012/2105 [=============>................] - ETA: 1:41:27 - accuracy: 0.8953 - loss: 0.2629\n",
      " training set -> batch:1013 loss:0.2597113251686096 and acc: 0.8956611752510071\n",
      "1013/2105 [=============>................] - ETA: 1:41:18 - accuracy: 0.8957 - loss: 0.2597\n",
      " training set -> batch:1014 loss:0.2524222135543823 and acc: 0.8980000019073486\n",
      "1014/2105 [=============>................] - ETA: 1:41:09 - accuracy: 0.8980 - loss: 0.2524\n",
      " training set -> batch:1015 loss:0.24802722036838531 and acc: 0.8982558250427246\n",
      "1015/2105 [=============>................] - ETA: 1:41:00 - accuracy: 0.8983 - loss: 0.2480\n",
      " training set -> batch:1016 loss:0.24622781574726105 and acc: 0.8994361162185669\n",
      "1016/2105 [=============>................] - ETA: 1:40:51 - accuracy: 0.8994 - loss: 0.2462\n",
      " training set -> batch:1017 loss:0.2443864643573761 and acc: 0.8996350169181824\n",
      "1017/2105 [=============>................] - ETA: 1:40:42 - accuracy: 0.8996 - loss: 0.2444\n",
      " training set -> batch:1018 loss:0.24321724474430084 and acc: 0.8998227119445801\n",
      "1018/2105 [=============>................] - ETA: 1:40:34 - accuracy: 0.8998 - loss: 0.2432\n",
      " training set -> batch:1019 loss:0.2411777675151825 and acc: 0.8999999761581421\n",
      "1019/2105 [=============>................] - ETA: 1:40:25 - accuracy: 0.9000 - loss: 0.2412\n",
      " training set -> batch:1020 loss:0.23529767990112305 and acc: 0.9018456339836121\n",
      "\n",
      " validation set -> batch:1020 val loss:0.2247982770204544 and val acc: 0.9162843823432922\n",
      "1020/2105 [=============>................] - ETA: 1:40:48 - accuracy: 0.9018 - loss: 0.2353\n",
      " training set -> batch:1021 loss:0.21903786063194275 and acc: 0.9181416034698486\n",
      "1021/2105 [=============>................] - ETA: 1:40:39 - accuracy: 0.9181 - loss: 0.2190\n",
      " training set -> batch:1022 loss:0.2157135009765625 and acc: 0.9177350401878357\n",
      "1022/2105 [=============>................] - ETA: 1:40:30 - accuracy: 0.9177 - loss: 0.2157\n",
      " training set -> batch:1023 loss:0.21966184675693512 and acc: 0.9163222908973694\n",
      "1023/2105 [=============>................] - ETA: 1:40:21 - accuracy: 0.9163 - loss: 0.2197\n",
      " training set -> batch:1024 loss:0.221709206700325 and acc: 0.9160000085830688\n",
      "1024/2105 [=============>................] - ETA: 1:40:13 - accuracy: 0.9160 - loss: 0.2217\n",
      " training set -> batch:1025 loss:0.22070150077342987 and acc: 0.9176356792449951\n",
      "1025/2105 [=============>................] - ETA: 1:40:04 - accuracy: 0.9176 - loss: 0.2207\n",
      " training set -> batch:1026 loss:0.21784281730651855 and acc: 0.9172932505607605\n",
      "1026/2105 [=============>................] - ETA: 1:39:55 - accuracy: 0.9173 - loss: 0.2178\n",
      " training set -> batch:1027 loss:0.21394817531108856 and acc: 0.9169707894325256\n",
      "1027/2105 [=============>................] - ETA: 1:39:46 - accuracy: 0.9170 - loss: 0.2139\n",
      " training set -> batch:1028 loss:0.21780432760715485 and acc: 0.914893627166748\n",
      "1028/2105 [=============>................] - ETA: 1:39:37 - accuracy: 0.9149 - loss: 0.2178\n",
      " training set -> batch:1029 loss:0.21044117212295532 and acc: 0.9172413945198059\n",
      "1029/2105 [=============>................] - ETA: 1:39:29 - accuracy: 0.9172 - loss: 0.2104\n",
      " training set -> batch:1030 loss:0.20544980466365814 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1030 val loss:0.24506251513957977 and val acc: 0.9013761281967163\n",
      "1030/2105 [=============>................] - ETA: 1:39:52 - accuracy: 0.9186 - loss: 0.2054\n",
      " training set -> batch:1031 loss:0.2509528398513794 and acc: 0.9015486836433411\n",
      "1031/2105 [=============>................] - ETA: 1:39:43 - accuracy: 0.9015 - loss: 0.2510\n",
      " training set -> batch:1032 loss:0.2583434283733368 and acc: 0.9006410241127014\n",
      "1032/2105 [=============>................] - ETA: 1:39:34 - accuracy: 0.9006 - loss: 0.2583\n",
      " training set -> batch:1033 loss:0.2535107433795929 and acc: 0.9018595218658447\n",
      "1033/2105 [=============>................] - ETA: 1:39:26 - accuracy: 0.9019 - loss: 0.2535\n",
      " training set -> batch:1034 loss:0.2534423768520355 and acc: 0.9010000228881836\n",
      "1034/2105 [=============>................] - ETA: 1:39:17 - accuracy: 0.9010 - loss: 0.2534\n",
      " training set -> batch:1035 loss:0.25231701135635376 and acc: 0.9031007885932922\n",
      "1035/2105 [=============>................] - ETA: 1:39:08 - accuracy: 0.9031 - loss: 0.2523\n",
      " training set -> batch:1036 loss:0.24872057139873505 and acc: 0.9031955003738403\n",
      "1036/2105 [=============>................] - ETA: 1:39:00 - accuracy: 0.9032 - loss: 0.2487\n",
      " training set -> batch:1037 loss:0.23976576328277588 and acc: 0.9060218930244446\n",
      "1037/2105 [=============>................] - ETA: 1:38:51 - accuracy: 0.9060 - loss: 0.2398\n",
      " training set -> batch:1038 loss:0.23260092735290527 and acc: 0.908687949180603\n",
      "1038/2105 [=============>................] - ETA: 1:38:42 - accuracy: 0.9087 - loss: 0.2326\n",
      " training set -> batch:1039 loss:0.23758479952812195 and acc: 0.9068965315818787\n",
      "1039/2105 [=============>................] - ETA: 1:38:33 - accuracy: 0.9069 - loss: 0.2376\n",
      " training set -> batch:1040 loss:0.23633790016174316 and acc: 0.906879186630249\n",
      "\n",
      " validation set -> batch:1040 val loss:0.24257716536521912 and val acc: 0.9025229215621948\n",
      "1040/2105 [=============>................] - ETA: 1:38:55 - accuracy: 0.9069 - loss: 0.2363\n",
      " training set -> batch:1041 loss:0.23283115029335022 and acc: 0.9048672318458557\n",
      "1041/2105 [=============>................] - ETA: 1:38:47 - accuracy: 0.9049 - loss: 0.2328\n",
      " training set -> batch:1042 loss:0.23310406506061554 and acc: 0.9049145579338074\n",
      "1042/2105 [=============>................] - ETA: 1:38:38 - accuracy: 0.9049 - loss: 0.2331\n",
      " training set -> batch:1043 loss:0.2277635633945465 and acc: 0.9059917330741882\n",
      "1043/2105 [=============>................] - ETA: 1:38:29 - accuracy: 0.9060 - loss: 0.2278\n",
      " training set -> batch:1044 loss:0.22196520864963531 and acc: 0.9070000052452087\n",
      "1044/2105 [=============>................] - ETA: 1:38:20 - accuracy: 0.9070 - loss: 0.2220\n",
      " training set -> batch:1045 loss:0.22218556702136993 and acc: 0.9069767594337463\n",
      "1045/2105 [=============>................] - ETA: 1:38:12 - accuracy: 0.9070 - loss: 0.2222\n",
      " training set -> batch:1046 loss:0.21740297973155975 and acc: 0.9078947305679321\n",
      "1046/2105 [=============>................] - ETA: 1:38:03 - accuracy: 0.9079 - loss: 0.2174\n",
      " training set -> batch:1047 loss:0.21010519564151764 and acc: 0.9096715450286865\n",
      "1047/2105 [=============>................] - ETA: 1:37:54 - accuracy: 0.9097 - loss: 0.2101\n",
      " training set -> batch:1048 loss:0.20425911247730255 and acc: 0.9122340679168701\n",
      "1048/2105 [=============>................] - ETA: 1:37:46 - accuracy: 0.9122 - loss: 0.2043\n",
      " training set -> batch:1049 loss:0.20934151113033295 and acc: 0.9112069010734558\n",
      "1049/2105 [=============>................] - ETA: 1:37:37 - accuracy: 0.9112 - loss: 0.2093\n",
      " training set -> batch:1050 loss:0.2027413249015808 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:1050 val loss:0.24909161031246185 and val acc: 0.9094036817550659\n",
      "1050/2105 [=============>................] - ETA: 1:37:59 - accuracy: 0.9136 - loss: 0.2027\n",
      " training set -> batch:1051 loss:0.2341824173927307 and acc: 0.9126105904579163\n",
      "1051/2105 [=============>................] - ETA: 1:37:50 - accuracy: 0.9126 - loss: 0.2342\n",
      " training set -> batch:1052 loss:0.23758482933044434 and acc: 0.9113247990608215\n",
      "1052/2105 [=============>................] - ETA: 1:37:41 - accuracy: 0.9113 - loss: 0.2376\n",
      " training set -> batch:1053 loss:0.24216632544994354 and acc: 0.9101239442825317\n",
      "1053/2105 [==============>...............] - ETA: 1:37:33 - accuracy: 0.9101 - loss: 0.2422\n",
      " training set -> batch:1054 loss:0.2370564192533493 and acc: 0.9100000262260437\n",
      "1054/2105 [==============>...............] - ETA: 1:37:24 - accuracy: 0.9100 - loss: 0.2371\n",
      " training set -> batch:1055 loss:0.2367735207080841 and acc: 0.9089147448539734\n",
      "1055/2105 [==============>...............] - ETA: 1:37:15 - accuracy: 0.9089 - loss: 0.2368\n",
      " training set -> batch:1056 loss:0.23675696551799774 and acc: 0.9097744226455688\n",
      "1056/2105 [==============>...............] - ETA: 1:37:07 - accuracy: 0.9098 - loss: 0.2368\n",
      " training set -> batch:1057 loss:0.2411380112171173 and acc: 0.9087591171264648\n",
      "1057/2105 [==============>...............] - ETA: 1:36:58 - accuracy: 0.9088 - loss: 0.2411\n",
      " training set -> batch:1058 loss:0.23424853384494781 and acc: 0.911347508430481\n",
      "1058/2105 [==============>...............] - ETA: 1:36:50 - accuracy: 0.9113 - loss: 0.2342\n",
      " training set -> batch:1059 loss:0.23211638629436493 and acc: 0.9129310250282288\n",
      "1059/2105 [==============>...............] - ETA: 1:36:41 - accuracy: 0.9129 - loss: 0.2321\n",
      " training set -> batch:1060 loss:0.2320028692483902 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:1060 val loss:0.24336613714694977 and val acc: 0.9025229215621948\n",
      "1060/2105 [==============>...............] - ETA: 1:37:03 - accuracy: 0.9119 - loss: 0.2320\n",
      " training set -> batch:1061 loss:0.23220902681350708 and acc: 0.9048672318458557\n",
      "1061/2105 [==============>...............] - ETA: 1:36:54 - accuracy: 0.9049 - loss: 0.2322\n",
      " training set -> batch:1062 loss:0.24293851852416992 and acc: 0.9017093777656555\n",
      "1062/2105 [==============>...............] - ETA: 1:36:46 - accuracy: 0.9017 - loss: 0.2429\n",
      " training set -> batch:1063 loss:0.24456535279750824 and acc: 0.9018595218658447\n",
      "1063/2105 [==============>...............] - ETA: 1:36:37 - accuracy: 0.9019 - loss: 0.2446\n",
      " training set -> batch:1064 loss:0.2354510873556137 and acc: 0.9049999713897705\n",
      "1064/2105 [==============>...............] - ETA: 1:36:29 - accuracy: 0.9050 - loss: 0.2355\n",
      " training set -> batch:1065 loss:0.23007124662399292 and acc: 0.9069767594337463\n",
      "1065/2105 [==============>...............] - ETA: 1:36:20 - accuracy: 0.9070 - loss: 0.2301\n",
      " training set -> batch:1066 loss:0.22475352883338928 and acc: 0.9078947305679321\n",
      "1066/2105 [==============>...............] - ETA: 1:36:12 - accuracy: 0.9079 - loss: 0.2248\n",
      " training set -> batch:1067 loss:0.22213922441005707 and acc: 0.9096715450286865\n",
      "1067/2105 [==============>...............] - ETA: 1:36:03 - accuracy: 0.9097 - loss: 0.2221\n",
      " training set -> batch:1068 loss:0.2216895967721939 and acc: 0.9104610085487366\n",
      "1068/2105 [==============>...............] - ETA: 1:35:54 - accuracy: 0.9105 - loss: 0.2217\n",
      " training set -> batch:1069 loss:0.22159835696220398 and acc: 0.9112069010734558\n",
      "1069/2105 [==============>...............] - ETA: 1:35:46 - accuracy: 0.9112 - loss: 0.2216\n",
      " training set -> batch:1070 loss:0.22557313740253448 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1070 val loss:0.24658890068531036 and val acc: 0.9059633016586304\n",
      "1070/2105 [==============>...............] - ETA: 1:36:07 - accuracy: 0.9111 - loss: 0.2256\n",
      " training set -> batch:1071 loss:0.26205411553382874 and acc: 0.9048672318458557\n",
      "1071/2105 [==============>...............] - ETA: 1:35:58 - accuracy: 0.9049 - loss: 0.2621\n",
      " training set -> batch:1072 loss:0.2635292708873749 and acc: 0.9038461446762085\n",
      "1072/2105 [==============>...............] - ETA: 1:35:50 - accuracy: 0.9038 - loss: 0.2635\n",
      " training set -> batch:1073 loss:0.27090582251548767 and acc: 0.9028925895690918\n",
      "1073/2105 [==============>...............] - ETA: 1:35:41 - accuracy: 0.9029 - loss: 0.2709\n",
      " training set -> batch:1074 loss:0.26562073826789856 and acc: 0.9039999842643738\n",
      "1074/2105 [==============>...............] - ETA: 1:35:33 - accuracy: 0.9040 - loss: 0.2656\n",
      " training set -> batch:1075 loss:0.25926151871681213 and acc: 0.9050387740135193\n",
      "1075/2105 [==============>...............] - ETA: 1:35:24 - accuracy: 0.9050 - loss: 0.2593\n",
      " training set -> batch:1076 loss:0.2559594511985779 and acc: 0.905075192451477\n",
      "1076/2105 [==============>...............] - ETA: 1:35:16 - accuracy: 0.9051 - loss: 0.2560\n",
      " training set -> batch:1077 loss:0.24826113879680634 and acc: 0.9069343209266663\n",
      "1077/2105 [==============>...............] - ETA: 1:35:08 - accuracy: 0.9069 - loss: 0.2483\n",
      " training set -> batch:1078 loss:0.2458992898464203 and acc: 0.9078013896942139\n",
      "1078/2105 [==============>...............] - ETA: 1:34:59 - accuracy: 0.9078 - loss: 0.2459\n",
      " training set -> batch:1079 loss:0.24416935443878174 and acc: 0.9068965315818787\n",
      "1079/2105 [==============>...............] - ETA: 1:34:51 - accuracy: 0.9069 - loss: 0.2442\n",
      " training set -> batch:1080 loss:0.24308733642101288 and acc: 0.906879186630249\n",
      "\n",
      " validation set -> batch:1080 val loss:0.2525806427001953 and val acc: 0.8990825414657593\n",
      "1080/2105 [==============>...............] - ETA: 1:35:11 - accuracy: 0.9069 - loss: 0.2431\n",
      " training set -> batch:1081 loss:0.25849178433418274 and acc: 0.8971238732337952\n",
      "1081/2105 [==============>...............] - ETA: 1:35:02 - accuracy: 0.8971 - loss: 0.2585\n",
      " training set -> batch:1082 loss:0.2547548711299896 and acc: 0.8974359035491943\n",
      "1082/2105 [==============>...............] - ETA: 1:34:54 - accuracy: 0.8974 - loss: 0.2548\n",
      " training set -> batch:1083 loss:0.25076794624328613 and acc: 0.8987603187561035\n",
      "1083/2105 [==============>...............] - ETA: 1:34:45 - accuracy: 0.8988 - loss: 0.2508\n",
      " training set -> batch:1084 loss:0.24538595974445343 and acc: 0.9010000228881836\n",
      "1084/2105 [==============>...............] - ETA: 1:34:37 - accuracy: 0.9010 - loss: 0.2454\n",
      " training set -> batch:1085 loss:0.24395334720611572 and acc: 0.9011628031730652\n",
      "1085/2105 [==============>...............] - ETA: 1:34:29 - accuracy: 0.9012 - loss: 0.2440\n",
      " training set -> batch:1086 loss:0.23713967204093933 and acc: 0.9031955003738403\n",
      "1086/2105 [==============>...............] - ETA: 1:34:20 - accuracy: 0.9032 - loss: 0.2371\n",
      " training set -> batch:1087 loss:0.23110100626945496 and acc: 0.9051094651222229\n",
      "1087/2105 [==============>...............] - ETA: 1:34:12 - accuracy: 0.9051 - loss: 0.2311\n",
      " training set -> batch:1088 loss:0.22995634377002716 and acc: 0.9060283899307251\n",
      "1088/2105 [==============>...............] - ETA: 1:34:03 - accuracy: 0.9060 - loss: 0.2300\n",
      " training set -> batch:1089 loss:0.22770069539546967 and acc: 0.9060344696044922\n",
      "1089/2105 [==============>...............] - ETA: 1:33:55 - accuracy: 0.9060 - loss: 0.2277\n",
      " training set -> batch:1090 loss:0.2279842495918274 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:1090 val loss:0.22935621440410614 and val acc: 0.9162843823432922\n",
      "1090/2105 [==============>...............] - ETA: 1:34:14 - accuracy: 0.9060 - loss: 0.2280\n",
      " training set -> batch:1091 loss:0.22576700150966644 and acc: 0.9181416034698486\n",
      "1091/2105 [==============>...............] - ETA: 1:34:06 - accuracy: 0.9181 - loss: 0.2258\n",
      " training set -> batch:1092 loss:0.22596655786037445 and acc: 0.9177350401878357\n",
      "1092/2105 [==============>...............] - ETA: 1:33:57 - accuracy: 0.9177 - loss: 0.2260\n",
      " training set -> batch:1093 loss:0.22111758589744568 and acc: 0.9183884263038635\n",
      "1093/2105 [==============>...............] - ETA: 1:33:49 - accuracy: 0.9184 - loss: 0.2211\n",
      " training set -> batch:1094 loss:0.2175300568342209 and acc: 0.9190000295639038\n",
      "1094/2105 [==============>...............] - ETA: 1:33:40 - accuracy: 0.9190 - loss: 0.2175\n",
      " training set -> batch:1095 loss:0.22117555141448975 and acc: 0.9166666865348816\n",
      "1095/2105 [==============>...............] - ETA: 1:33:32 - accuracy: 0.9167 - loss: 0.2212\n",
      " training set -> batch:1096 loss:0.22849687933921814 and acc: 0.9154135584831238\n",
      "1096/2105 [==============>...............] - ETA: 1:33:24 - accuracy: 0.9154 - loss: 0.2285\n",
      " training set -> batch:1097 loss:0.22196826338768005 and acc: 0.9178832173347473\n",
      "1097/2105 [==============>...............] - ETA: 1:33:15 - accuracy: 0.9179 - loss: 0.2220\n",
      " training set -> batch:1098 loss:0.2206798940896988 and acc: 0.917553186416626\n",
      "1098/2105 [==============>...............] - ETA: 1:33:07 - accuracy: 0.9176 - loss: 0.2207\n",
      " training set -> batch:1099 loss:0.21385671198368073 and acc: 0.9198275804519653\n",
      "1099/2105 [==============>...............] - ETA: 1:32:59 - accuracy: 0.9198 - loss: 0.2139\n",
      " training set -> batch:1100 loss:0.21267874538898468 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1100 val loss:0.23436342179775238 and val acc: 0.91399085521698\n",
      "1100/2105 [==============>...............] - ETA: 1:33:18 - accuracy: 0.9203 - loss: 0.2127\n",
      " training set -> batch:1101 loss:0.23228642344474792 and acc: 0.9148229956626892\n",
      "1101/2105 [==============>...............] - ETA: 1:33:10 - accuracy: 0.9148 - loss: 0.2323\n",
      " training set -> batch:1102 loss:0.22058095037937164 and acc: 0.9177350401878357\n",
      "1102/2105 [==============>...............] - ETA: 1:33:01 - accuracy: 0.9177 - loss: 0.2206\n",
      " training set -> batch:1103 loss:0.21068069338798523 and acc: 0.9194214940071106\n",
      "1103/2105 [==============>...............] - ETA: 1:32:53 - accuracy: 0.9194 - loss: 0.2107\n",
      " training set -> batch:1104 loss:0.20732766389846802 and acc: 0.9210000038146973\n",
      "1104/2105 [==============>...............] - ETA: 1:32:45 - accuracy: 0.9210 - loss: 0.2073\n",
      " training set -> batch:1105 loss:0.2047872692346573 and acc: 0.9224806427955627\n",
      "1105/2105 [==============>...............] - ETA: 1:32:36 - accuracy: 0.9225 - loss: 0.2048\n",
      " training set -> batch:1106 loss:0.20307239890098572 and acc: 0.9229323267936707\n",
      "1106/2105 [==============>...............] - ETA: 1:32:28 - accuracy: 0.9229 - loss: 0.2031\n",
      " training set -> batch:1107 loss:0.1983826756477356 and acc: 0.9242700934410095\n",
      "1107/2105 [==============>...............] - ETA: 1:32:20 - accuracy: 0.9243 - loss: 0.1984\n",
      " training set -> batch:1108 loss:0.19067972898483276 and acc: 0.9264184236526489\n",
      "1108/2105 [==============>...............] - ETA: 1:32:11 - accuracy: 0.9264 - loss: 0.1907\n",
      " training set -> batch:1109 loss:0.18496069312095642 and acc: 0.9284482598304749\n",
      "1109/2105 [==============>...............] - ETA: 1:32:03 - accuracy: 0.9284 - loss: 0.1850\n",
      " training set -> batch:1110 loss:0.17933909595012665 and acc: 0.9303691387176514\n",
      "\n",
      " validation set -> batch:1110 val loss:0.23564516007900238 and val acc: 0.9151375889778137\n",
      "1110/2105 [==============>...............] - ETA: 1:32:23 - accuracy: 0.9304 - loss: 0.1793\n",
      " training set -> batch:1111 loss:0.23272255063056946 and acc: 0.9159291982650757\n",
      "1111/2105 [==============>...............] - ETA: 1:32:14 - accuracy: 0.9159 - loss: 0.2327\n",
      " training set -> batch:1112 loss:0.22825448215007782 and acc: 0.9177350401878357\n",
      "1112/2105 [==============>...............] - ETA: 1:32:06 - accuracy: 0.9177 - loss: 0.2283\n",
      " training set -> batch:1113 loss:0.23154594004154205 and acc: 0.9173553586006165\n",
      "1113/2105 [==============>...............] - ETA: 1:31:58 - accuracy: 0.9174 - loss: 0.2315\n",
      " training set -> batch:1114 loss:0.23384149372577667 and acc: 0.9169999957084656\n",
      "1114/2105 [==============>...............] - ETA: 1:31:49 - accuracy: 0.9170 - loss: 0.2338\n",
      " training set -> batch:1115 loss:0.22420285642147064 and acc: 0.9186046719551086\n",
      "1115/2105 [==============>...............] - ETA: 1:31:41 - accuracy: 0.9186 - loss: 0.2242\n",
      " training set -> batch:1116 loss:0.22997239232063293 and acc: 0.9172932505607605\n",
      "1116/2105 [==============>...............] - ETA: 1:31:33 - accuracy: 0.9173 - loss: 0.2300\n",
      " training set -> batch:1117 loss:0.22328004240989685 and acc: 0.918795645236969\n",
      "1117/2105 [==============>...............] - ETA: 1:31:25 - accuracy: 0.9188 - loss: 0.2233\n",
      " training set -> batch:1118 loss:0.21802163124084473 and acc: 0.9202127456665039\n",
      "1118/2105 [==============>...............] - ETA: 1:31:16 - accuracy: 0.9202 - loss: 0.2180\n",
      " training set -> batch:1119 loss:0.21937571465969086 and acc: 0.9189655184745789\n",
      "1119/2105 [==============>...............] - ETA: 1:31:08 - accuracy: 0.9190 - loss: 0.2194\n",
      " training set -> batch:1120 loss:0.21766026318073273 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1120 val loss:0.23037756979465485 and val acc: 0.9174311757087708\n",
      "1120/2105 [==============>...............] - ETA: 1:31:26 - accuracy: 0.9203 - loss: 0.2177\n",
      " training set -> batch:1121 loss:0.2251911461353302 and acc: 0.9181416034698486\n",
      "1121/2105 [==============>...............] - ETA: 1:31:18 - accuracy: 0.9181 - loss: 0.2252\n",
      " training set -> batch:1122 loss:0.2193111777305603 and acc: 0.9198718070983887\n",
      "1122/2105 [==============>...............] - ETA: 1:31:10 - accuracy: 0.9199 - loss: 0.2193\n",
      " training set -> batch:1123 loss:0.21789215505123138 and acc: 0.9173553586006165\n",
      "1123/2105 [===============>..............] - ETA: 1:31:02 - accuracy: 0.9174 - loss: 0.2179\n",
      " training set -> batch:1124 loss:0.22720223665237427 and acc: 0.9129999876022339\n",
      "1124/2105 [===============>..............] - ETA: 1:30:53 - accuracy: 0.9130 - loss: 0.2272\n",
      " training set -> batch:1125 loss:0.22295565903186798 and acc: 0.913759708404541\n",
      "1125/2105 [===============>..............] - ETA: 1:30:45 - accuracy: 0.9138 - loss: 0.2230\n",
      " training set -> batch:1126 loss:0.2215433120727539 and acc: 0.9154135584831238\n",
      "1126/2105 [===============>..............] - ETA: 1:30:37 - accuracy: 0.9154 - loss: 0.2215\n",
      " training set -> batch:1127 loss:0.21235717833042145 and acc: 0.9178832173347473\n",
      "1127/2105 [===============>..............] - ETA: 1:30:29 - accuracy: 0.9179 - loss: 0.2124\n",
      " training set -> batch:1128 loss:0.2140335589647293 and acc: 0.9184397459030151\n",
      "1128/2105 [===============>..............] - ETA: 1:30:21 - accuracy: 0.9184 - loss: 0.2140\n",
      " training set -> batch:1129 loss:0.2120635062456131 and acc: 0.9181034564971924\n",
      "1129/2105 [===============>..............] - ETA: 1:30:12 - accuracy: 0.9181 - loss: 0.2121\n",
      " training set -> batch:1130 loss:0.2049524337053299 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1130 val loss:0.221999853849411 and val acc: 0.91399085521698\n",
      "1130/2105 [===============>..............] - ETA: 1:30:31 - accuracy: 0.9195 - loss: 0.2050\n",
      " training set -> batch:1131 loss:0.21764157712459564 and acc: 0.9148229956626892\n",
      "1131/2105 [===============>..............] - ETA: 1:30:23 - accuracy: 0.9148 - loss: 0.2176\n",
      " training set -> batch:1132 loss:0.2103816419839859 and acc: 0.9166666865348816\n",
      "1132/2105 [===============>..............] - ETA: 1:30:14 - accuracy: 0.9167 - loss: 0.2104\n",
      " training set -> batch:1133 loss:0.21016868948936462 and acc: 0.9173553586006165\n",
      "1133/2105 [===============>..............] - ETA: 1:30:06 - accuracy: 0.9174 - loss: 0.2102\n",
      " training set -> batch:1134 loss:0.21256032586097717 and acc: 0.9169999957084656\n",
      "1134/2105 [===============>..............] - ETA: 1:29:58 - accuracy: 0.9170 - loss: 0.2126\n",
      " training set -> batch:1135 loss:0.2217824012041092 and acc: 0.9156976938247681\n",
      "1135/2105 [===============>..............] - ETA: 1:29:50 - accuracy: 0.9157 - loss: 0.2218\n",
      " training set -> batch:1136 loss:0.21764564514160156 and acc: 0.9163534045219421\n",
      "1136/2105 [===============>..............] - ETA: 1:29:42 - accuracy: 0.9164 - loss: 0.2176\n",
      " training set -> batch:1137 loss:0.21628758311271667 and acc: 0.9160584211349487\n",
      "1137/2105 [===============>..............] - ETA: 1:29:34 - accuracy: 0.9161 - loss: 0.2163\n",
      " training set -> batch:1138 loss:0.2112690508365631 and acc: 0.9166666865348816\n",
      "1138/2105 [===============>..............] - ETA: 1:29:25 - accuracy: 0.9167 - loss: 0.2113\n",
      " training set -> batch:1139 loss:0.2059645801782608 and acc: 0.9181034564971924\n",
      "1139/2105 [===============>..............] - ETA: 1:29:17 - accuracy: 0.9181 - loss: 0.2060\n",
      " training set -> batch:1140 loss:0.20284824073314667 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1140 val loss:0.2191622257232666 and val acc: 0.9094036817550659\n",
      "1140/2105 [===============>..............] - ETA: 1:29:34 - accuracy: 0.9186 - loss: 0.2028\n",
      " training set -> batch:1141 loss:0.21079859137535095 and acc: 0.9115044474601746\n",
      "1141/2105 [===============>..............] - ETA: 1:29:26 - accuracy: 0.9115 - loss: 0.2108\n",
      " training set -> batch:1142 loss:0.21486452221870422 and acc: 0.9113247990608215\n",
      "1142/2105 [===============>..............] - ETA: 1:29:18 - accuracy: 0.9113 - loss: 0.2149\n",
      " training set -> batch:1143 loss:0.21049046516418457 and acc: 0.913223147392273\n",
      "1143/2105 [===============>..............] - ETA: 1:29:10 - accuracy: 0.9132 - loss: 0.2105\n",
      " training set -> batch:1144 loss:0.212141752243042 and acc: 0.9120000004768372\n",
      "1144/2105 [===============>..............] - ETA: 1:29:02 - accuracy: 0.9120 - loss: 0.2121\n",
      " training set -> batch:1145 loss:0.21521106362342834 and acc: 0.9098837375640869\n",
      "1145/2105 [===============>..............] - ETA: 1:28:54 - accuracy: 0.9099 - loss: 0.2152\n",
      " training set -> batch:1146 loss:0.21633894741535187 and acc: 0.9107142686843872\n",
      "1146/2105 [===============>..............] - ETA: 1:28:46 - accuracy: 0.9107 - loss: 0.2163\n",
      " training set -> batch:1147 loss:0.21862207353115082 and acc: 0.9105839133262634\n",
      "1147/2105 [===============>..............] - ETA: 1:28:38 - accuracy: 0.9106 - loss: 0.2186\n",
      " training set -> batch:1148 loss:0.21282817423343658 and acc: 0.9131205677986145\n",
      "1148/2105 [===============>..............] - ETA: 1:28:30 - accuracy: 0.9131 - loss: 0.2128\n",
      " training set -> batch:1149 loss:0.2092726081609726 and acc: 0.915517270565033\n",
      "1149/2105 [===============>..............] - ETA: 1:28:21 - accuracy: 0.9155 - loss: 0.2093\n",
      " training set -> batch:1150 loss:0.20656341314315796 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1150 val loss:0.2194455862045288 and val acc: 0.91399085521698\n",
      "1150/2105 [===============>..............] - ETA: 1:28:39 - accuracy: 0.9169 - loss: 0.2066\n",
      " training set -> batch:1151 loss:0.22494414448738098 and acc: 0.9126105904579163\n",
      "1151/2105 [===============>..............] - ETA: 1:28:31 - accuracy: 0.9126 - loss: 0.2249\n",
      " training set -> batch:1152 loss:0.21517984569072723 and acc: 0.9145299196243286\n",
      "1152/2105 [===============>..............] - ETA: 1:28:23 - accuracy: 0.9145 - loss: 0.2152\n",
      " training set -> batch:1153 loss:0.22163689136505127 and acc: 0.913223147392273\n",
      "1153/2105 [===============>..............] - ETA: 1:28:14 - accuracy: 0.9132 - loss: 0.2216\n",
      " training set -> batch:1154 loss:0.21990670263767242 and acc: 0.9150000214576721\n",
      "1154/2105 [===============>..............] - ETA: 1:28:06 - accuracy: 0.9150 - loss: 0.2199\n",
      " training set -> batch:1155 loss:0.22305375337600708 and acc: 0.913759708404541\n",
      "1155/2105 [===============>..............] - ETA: 1:27:58 - accuracy: 0.9138 - loss: 0.2231\n",
      " training set -> batch:1156 loss:0.21713419258594513 and acc: 0.9154135584831238\n",
      "1156/2105 [===============>..............] - ETA: 1:27:50 - accuracy: 0.9154 - loss: 0.2171\n",
      " training set -> batch:1157 loss:0.20921339094638824 and acc: 0.9178832173347473\n",
      "1157/2105 [===============>..............] - ETA: 1:27:42 - accuracy: 0.9179 - loss: 0.2092\n",
      " training set -> batch:1158 loss:0.21161073446273804 and acc: 0.9157801270484924\n",
      "1158/2105 [===============>..............] - ETA: 1:27:34 - accuracy: 0.9158 - loss: 0.2116\n",
      " training set -> batch:1159 loss:0.21225345134735107 and acc: 0.9163793325424194\n",
      "1159/2105 [===============>..............] - ETA: 1:27:26 - accuracy: 0.9164 - loss: 0.2123\n",
      " training set -> batch:1160 loss:0.21128006279468536 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1160 val loss:0.22796115279197693 and val acc: 0.9105504751205444\n",
      "1160/2105 [===============>..............] - ETA: 1:27:42 - accuracy: 0.9169 - loss: 0.2113\n",
      " training set -> batch:1161 loss:0.23068752884864807 and acc: 0.9092920422554016\n",
      "1161/2105 [===============>..............] - ETA: 1:27:34 - accuracy: 0.9093 - loss: 0.2307\n",
      " training set -> batch:1162 loss:0.22689007222652435 and acc: 0.9102563858032227\n",
      "1162/2105 [===============>..............] - ETA: 1:27:26 - accuracy: 0.9103 - loss: 0.2269\n",
      " training set -> batch:1163 loss:0.23070763051509857 and acc: 0.9090909361839294\n",
      "1163/2105 [===============>..............] - ETA: 1:27:18 - accuracy: 0.9091 - loss: 0.2307\n",
      " training set -> batch:1164 loss:0.223980113863945 and acc: 0.9110000133514404\n",
      "1164/2105 [===============>..............] - ETA: 1:27:10 - accuracy: 0.9110 - loss: 0.2240\n",
      " training set -> batch:1165 loss:0.22646452486515045 and acc: 0.9108527302742004\n",
      "1165/2105 [===============>..............] - ETA: 1:27:02 - accuracy: 0.9109 - loss: 0.2265\n",
      " training set -> batch:1166 loss:0.2294895201921463 and acc: 0.9107142686843872\n",
      "1166/2105 [===============>..............] - ETA: 1:26:54 - accuracy: 0.9107 - loss: 0.2295\n",
      " training set -> batch:1167 loss:0.22122158110141754 and acc: 0.9133211970329285\n",
      "1167/2105 [===============>..............] - ETA: 1:26:46 - accuracy: 0.9133 - loss: 0.2212\n",
      " training set -> batch:1168 loss:0.2253490388393402 and acc: 0.9122340679168701\n",
      "1168/2105 [===============>..............] - ETA: 1:26:38 - accuracy: 0.9122 - loss: 0.2253\n",
      " training set -> batch:1169 loss:0.22967952489852905 and acc: 0.9103448390960693\n",
      "1169/2105 [===============>..............] - ETA: 1:26:30 - accuracy: 0.9103 - loss: 0.2297\n",
      " training set -> batch:1170 loss:0.2308986335992813 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1170 val loss:0.23667407035827637 and val acc: 0.9082568883895874\n",
      "1170/2105 [===============>..............] - ETA: 1:26:46 - accuracy: 0.9111 - loss: 0.2309\n",
      " training set -> batch:1171 loss:0.23357436060905457 and acc: 0.9092920422554016\n",
      "1171/2105 [===============>..............] - ETA: 1:26:38 - accuracy: 0.9093 - loss: 0.2336\n",
      " training set -> batch:1172 loss:0.23200584948062897 and acc: 0.9102563858032227\n",
      "1172/2105 [===============>..............] - ETA: 1:26:30 - accuracy: 0.9103 - loss: 0.2320\n",
      " training set -> batch:1173 loss:0.22508279979228973 and acc: 0.9111570119857788\n",
      "1173/2105 [===============>..............] - ETA: 1:26:22 - accuracy: 0.9112 - loss: 0.2251\n",
      " training set -> batch:1174 loss:0.22566437721252441 and acc: 0.9110000133514404\n",
      "1174/2105 [===============>..............] - ETA: 1:26:14 - accuracy: 0.9110 - loss: 0.2257\n",
      " training set -> batch:1175 loss:0.2231835573911667 and acc: 0.9127907156944275\n",
      "1175/2105 [===============>..............] - ETA: 1:26:06 - accuracy: 0.9128 - loss: 0.2232\n",
      " training set -> batch:1176 loss:0.21809954941272736 and acc: 0.9144737124443054\n",
      "1176/2105 [===============>..............] - ETA: 1:25:58 - accuracy: 0.9145 - loss: 0.2181\n",
      " training set -> batch:1177 loss:0.21277058124542236 and acc: 0.9169707894325256\n",
      "1177/2105 [===============>..............] - ETA: 1:25:50 - accuracy: 0.9170 - loss: 0.2128\n",
      " training set -> batch:1178 loss:0.2126718908548355 and acc: 0.9166666865348816\n",
      "1178/2105 [===============>..............] - ETA: 1:25:42 - accuracy: 0.9167 - loss: 0.2127\n",
      " training set -> batch:1179 loss:0.21985933184623718 and acc: 0.9146551489830017\n",
      "1179/2105 [===============>..............] - ETA: 1:25:34 - accuracy: 0.9147 - loss: 0.2199\n",
      " training set -> batch:1180 loss:0.22325986623764038 and acc: 0.9144295454025269\n",
      "\n",
      " validation set -> batch:1180 val loss:0.2067704200744629 and val acc: 0.9151375889778137\n",
      "1180/2105 [===============>..............] - ETA: 1:25:49 - accuracy: 0.9144 - loss: 0.2233\n",
      " training set -> batch:1181 loss:0.19946490228176117 and acc: 0.9170354008674622\n",
      "1181/2105 [===============>..............] - ETA: 1:25:41 - accuracy: 0.9170 - loss: 0.1995\n",
      " training set -> batch:1182 loss:0.19871655106544495 and acc: 0.9177350401878357\n",
      "1182/2105 [===============>..............] - ETA: 1:25:33 - accuracy: 0.9177 - loss: 0.1987\n",
      " training set -> batch:1183 loss:0.19322018325328827 and acc: 0.9204545617103577\n",
      "1183/2105 [===============>..............] - ETA: 1:25:25 - accuracy: 0.9205 - loss: 0.1932\n",
      " training set -> batch:1184 loss:0.19445358216762543 and acc: 0.9200000166893005\n",
      "1184/2105 [===============>..............] - ETA: 1:25:17 - accuracy: 0.9200 - loss: 0.1945\n",
      " training set -> batch:1185 loss:0.1885531097650528 and acc: 0.9215116500854492\n",
      "1185/2105 [===============>..............] - ETA: 1:25:10 - accuracy: 0.9215 - loss: 0.1886\n",
      " training set -> batch:1186 loss:0.185424342751503 and acc: 0.9229323267936707\n",
      "1186/2105 [===============>..............] - ETA: 1:25:02 - accuracy: 0.9229 - loss: 0.1854\n",
      " training set -> batch:1187 loss:0.1826651692390442 and acc: 0.9224452376365662\n",
      "1187/2105 [===============>..............] - ETA: 1:24:54 - accuracy: 0.9224 - loss: 0.1827\n",
      " training set -> batch:1188 loss:0.18081285059452057 and acc: 0.9228723645210266\n",
      "1188/2105 [===============>..............] - ETA: 1:24:46 - accuracy: 0.9229 - loss: 0.1808\n",
      " training set -> batch:1189 loss:0.17519749701023102 and acc: 0.925000011920929\n",
      "1189/2105 [===============>..............] - ETA: 1:24:38 - accuracy: 0.9250 - loss: 0.1752\n",
      " training set -> batch:1190 loss:0.17303182184696198 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1190 val loss:0.2429288625717163 and val acc: 0.9071100950241089\n",
      "1190/2105 [===============>..............] - ETA: 1:24:53 - accuracy: 0.9253 - loss: 0.1730\n",
      " training set -> batch:1191 loss:0.23186947405338287 and acc: 0.9092920422554016\n",
      "1191/2105 [===============>..............] - ETA: 1:24:45 - accuracy: 0.9093 - loss: 0.2319\n",
      " training set -> batch:1192 loss:0.22554461658000946 and acc: 0.9102563858032227\n",
      "1192/2105 [===============>..............] - ETA: 1:24:37 - accuracy: 0.9103 - loss: 0.2255\n",
      " training set -> batch:1193 loss:0.22243236005306244 and acc: 0.9111570119857788\n",
      "1193/2105 [================>.............] - ETA: 1:24:29 - accuracy: 0.9112 - loss: 0.2224\n",
      " training set -> batch:1194 loss:0.21885746717453003 and acc: 0.9129999876022339\n",
      "1194/2105 [================>.............] - ETA: 1:24:21 - accuracy: 0.9130 - loss: 0.2189\n",
      " training set -> batch:1195 loss:0.2185884565114975 and acc: 0.9127907156944275\n",
      "1195/2105 [================>.............] - ETA: 1:24:14 - accuracy: 0.9128 - loss: 0.2186\n",
      " training set -> batch:1196 loss:0.2327917367219925 and acc: 0.9097744226455688\n",
      "1196/2105 [================>.............] - ETA: 1:24:06 - accuracy: 0.9098 - loss: 0.2328\n",
      " training set -> batch:1197 loss:0.2271314412355423 and acc: 0.9105839133262634\n",
      "1197/2105 [================>.............] - ETA: 1:23:58 - accuracy: 0.9106 - loss: 0.2271\n",
      " training set -> batch:1198 loss:0.22347711026668549 and acc: 0.9104610085487366\n",
      "1198/2105 [================>.............] - ETA: 1:23:50 - accuracy: 0.9105 - loss: 0.2235\n",
      " training set -> batch:1199 loss:0.22093217074871063 and acc: 0.9112069010734558\n",
      "1199/2105 [================>.............] - ETA: 1:23:42 - accuracy: 0.9112 - loss: 0.2209\n",
      " training set -> batch:1200 loss:0.2178112119436264 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:1200 val loss:0.22162576019763947 and val acc: 0.9162843823432922\n",
      "1200/2105 [================>.............] - ETA: 1:23:56 - accuracy: 0.9119 - loss: 0.2178\n",
      " training set -> batch:1201 loss:0.22925999760627747 and acc: 0.9159291982650757\n",
      "1201/2105 [================>.............] - ETA: 1:23:48 - accuracy: 0.9159 - loss: 0.2293\n",
      " training set -> batch:1202 loss:0.22200711071491241 and acc: 0.9166666865348816\n",
      "1202/2105 [================>.............] - ETA: 1:23:40 - accuracy: 0.9167 - loss: 0.2220\n",
      " training set -> batch:1203 loss:0.21854689717292786 and acc: 0.9173553586006165\n",
      "1203/2105 [================>.............] - ETA: 1:23:32 - accuracy: 0.9174 - loss: 0.2185\n",
      " training set -> batch:1204 loss:0.20892469584941864 and acc: 0.9200000166893005\n",
      "1204/2105 [================>.............] - ETA: 1:23:25 - accuracy: 0.9200 - loss: 0.2089\n",
      " training set -> batch:1205 loss:0.20312847197055817 and acc: 0.9215116500854492\n",
      "1205/2105 [================>.............] - ETA: 1:23:17 - accuracy: 0.9215 - loss: 0.2031\n",
      " training set -> batch:1206 loss:0.20234215259552002 and acc: 0.9219924807548523\n",
      "1206/2105 [================>.............] - ETA: 1:23:09 - accuracy: 0.9220 - loss: 0.2023\n",
      " training set -> batch:1207 loss:0.20240004360675812 and acc: 0.9224452376365662\n",
      "1207/2105 [================>.............] - ETA: 1:23:01 - accuracy: 0.9224 - loss: 0.2024\n",
      " training set -> batch:1208 loss:0.20132483541965485 and acc: 0.9219858050346375\n",
      "1208/2105 [================>.............] - ETA: 1:22:53 - accuracy: 0.9220 - loss: 0.2013\n",
      " training set -> batch:1209 loss:0.19922398030757904 and acc: 0.923275887966156\n",
      "1209/2105 [================>.............] - ETA: 1:22:45 - accuracy: 0.9233 - loss: 0.1992\n",
      " training set -> batch:1210 loss:0.20156341791152954 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1210 val loss:0.2124890238046646 and val acc: 0.9151375889778137\n",
      "1210/2105 [================>.............] - ETA: 1:23:00 - accuracy: 0.9220 - loss: 0.2016\n",
      " training set -> batch:1211 loss:0.20960964262485504 and acc: 0.9170354008674622\n",
      "1211/2105 [================>.............] - ETA: 1:22:52 - accuracy: 0.9170 - loss: 0.2096\n",
      " training set -> batch:1212 loss:0.20274618268013 and acc: 0.9188033938407898\n",
      "1212/2105 [================>.............] - ETA: 1:22:44 - accuracy: 0.9188 - loss: 0.2027\n",
      " training set -> batch:1213 loss:0.20652808248996735 and acc: 0.9163222908973694\n",
      "1213/2105 [================>.............] - ETA: 1:22:37 - accuracy: 0.9163 - loss: 0.2065\n",
      " training set -> batch:1214 loss:0.21123676002025604 and acc: 0.9150000214576721\n",
      "1214/2105 [================>.............] - ETA: 1:22:29 - accuracy: 0.9150 - loss: 0.2112\n",
      " training set -> batch:1215 loss:0.20638379454612732 and acc: 0.9166666865348816\n",
      "1215/2105 [================>.............] - ETA: 1:22:21 - accuracy: 0.9167 - loss: 0.2064\n",
      " training set -> batch:1216 loss:0.20628535747528076 and acc: 0.9182330965995789\n",
      "1216/2105 [================>.............] - ETA: 1:22:13 - accuracy: 0.9182 - loss: 0.2063\n",
      " training set -> batch:1217 loss:0.19947819411754608 and acc: 0.9206204414367676\n",
      "1217/2105 [================>.............] - ETA: 1:22:06 - accuracy: 0.9206 - loss: 0.1995\n",
      " training set -> batch:1218 loss:0.1997239738702774 and acc: 0.9193262457847595\n",
      "1218/2105 [================>.............] - ETA: 1:21:58 - accuracy: 0.9193 - loss: 0.1997\n",
      " training set -> batch:1219 loss:0.20006762444972992 and acc: 0.9198275804519653\n",
      "1219/2105 [================>.............] - ETA: 1:21:50 - accuracy: 0.9198 - loss: 0.2001\n",
      " training set -> batch:1220 loss:0.20131392776966095 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1220 val loss:0.21267423033714294 and val acc: 0.9128440618515015\n",
      "1220/2105 [================>.............] - ETA: 1:22:04 - accuracy: 0.9186 - loss: 0.2013\n",
      " training set -> batch:1221 loss:0.2110932171344757 and acc: 0.9137167930603027\n",
      "1221/2105 [================>.............] - ETA: 1:21:56 - accuracy: 0.9137 - loss: 0.2111\n",
      " training set -> batch:1222 loss:0.21668627858161926 and acc: 0.9134615659713745\n",
      "1222/2105 [================>.............] - ETA: 1:21:48 - accuracy: 0.9135 - loss: 0.2167\n",
      " training set -> batch:1223 loss:0.22177262604236603 and acc: 0.9111570119857788\n",
      "1223/2105 [================>.............] - ETA: 1:21:40 - accuracy: 0.9112 - loss: 0.2218\n",
      " training set -> batch:1224 loss:0.22638770937919617 and acc: 0.9100000262260437\n",
      "1224/2105 [================>.............] - ETA: 1:21:32 - accuracy: 0.9100 - loss: 0.2264\n",
      " training set -> batch:1225 loss:0.23142914474010468 and acc: 0.9089147448539734\n",
      "1225/2105 [================>.............] - ETA: 1:21:25 - accuracy: 0.9089 - loss: 0.2314\n",
      " training set -> batch:1226 loss:0.22837981581687927 and acc: 0.9097744226455688\n",
      "1226/2105 [================>.............] - ETA: 1:21:17 - accuracy: 0.9098 - loss: 0.2284\n",
      " training set -> batch:1227 loss:0.22868351638317108 and acc: 0.9096715450286865\n",
      "1227/2105 [================>.............] - ETA: 1:21:09 - accuracy: 0.9097 - loss: 0.2287\n",
      " training set -> batch:1228 loss:0.2267029881477356 and acc: 0.908687949180603\n",
      "1228/2105 [================>.............] - ETA: 1:21:01 - accuracy: 0.9087 - loss: 0.2267\n",
      " training set -> batch:1229 loss:0.22424744069576263 and acc: 0.9086207151412964\n",
      "1229/2105 [================>.............] - ETA: 1:20:54 - accuracy: 0.9086 - loss: 0.2242\n",
      " training set -> batch:1230 loss:0.2247115969657898 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:1230 val loss:0.21327601373195648 and val acc: 0.9162843823432922\n",
      "1230/2105 [================>.............] - ETA: 1:21:07 - accuracy: 0.9077 - loss: 0.2247\n",
      " training set -> batch:1231 loss:0.21571049094200134 and acc: 0.9159291982650757\n",
      "1231/2105 [================>.............] - ETA: 1:20:59 - accuracy: 0.9159 - loss: 0.2157\n",
      " training set -> batch:1232 loss:0.20885074138641357 and acc: 0.9188033938407898\n",
      "1232/2105 [================>.............] - ETA: 1:20:51 - accuracy: 0.9188 - loss: 0.2089\n",
      " training set -> batch:1233 loss:0.2008667290210724 and acc: 0.9214876294136047\n",
      "1233/2105 [================>.............] - ETA: 1:20:44 - accuracy: 0.9215 - loss: 0.2009\n",
      " training set -> batch:1234 loss:0.19945211708545685 and acc: 0.9229999780654907\n",
      "1234/2105 [================>.............] - ETA: 1:20:36 - accuracy: 0.9230 - loss: 0.1995\n",
      " training set -> batch:1235 loss:0.20884697139263153 and acc: 0.9205426573753357\n",
      "1235/2105 [================>.............] - ETA: 1:20:28 - accuracy: 0.9205 - loss: 0.2088\n",
      " training set -> batch:1236 loss:0.20611059665679932 and acc: 0.9219924807548523\n",
      "1236/2105 [================>.............] - ETA: 1:20:21 - accuracy: 0.9220 - loss: 0.2061\n",
      " training set -> batch:1237 loss:0.20329970121383667 and acc: 0.9215328693389893\n",
      "1237/2105 [================>.............] - ETA: 1:20:13 - accuracy: 0.9215 - loss: 0.2033\n",
      " training set -> batch:1238 loss:0.21245549619197845 and acc: 0.917553186416626\n",
      "1238/2105 [================>.............] - ETA: 1:20:05 - accuracy: 0.9176 - loss: 0.2125\n",
      " training set -> batch:1239 loss:0.2195991426706314 and acc: 0.9146551489830017\n",
      "1239/2105 [================>.............] - ETA: 1:19:57 - accuracy: 0.9147 - loss: 0.2196\n",
      " training set -> batch:1240 loss:0.21653224527835846 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1240 val loss:0.21969614923000336 and val acc: 0.9082568883895874\n",
      "1240/2105 [================>.............] - ETA: 1:20:11 - accuracy: 0.9161 - loss: 0.2165\n",
      " training set -> batch:1241 loss:0.20948061347007751 and acc: 0.9115044474601746\n",
      "1241/2105 [================>.............] - ETA: 1:20:03 - accuracy: 0.9115 - loss: 0.2095\n",
      " training set -> batch:1242 loss:0.20407450199127197 and acc: 0.9123931527137756\n",
      "1242/2105 [================>.............] - ETA: 1:19:55 - accuracy: 0.9124 - loss: 0.2041\n",
      " training set -> batch:1243 loss:0.20021677017211914 and acc: 0.913223147392273\n",
      "1243/2105 [================>.............] - ETA: 1:19:48 - accuracy: 0.9132 - loss: 0.2002\n",
      " training set -> batch:1244 loss:0.20020438730716705 and acc: 0.9150000214576721\n",
      "1244/2105 [================>.............] - ETA: 1:19:40 - accuracy: 0.9150 - loss: 0.2002\n",
      " training set -> batch:1245 loss:0.20164945721626282 and acc: 0.9147287011146545\n",
      "1245/2105 [================>.............] - ETA: 1:19:32 - accuracy: 0.9147 - loss: 0.2016\n",
      " training set -> batch:1246 loss:0.20126596093177795 and acc: 0.9154135584831238\n",
      "1246/2105 [================>.............] - ETA: 1:19:24 - accuracy: 0.9154 - loss: 0.2013\n",
      " training set -> batch:1247 loss:0.19467154145240784 and acc: 0.9178832173347473\n",
      "1247/2105 [================>.............] - ETA: 1:19:17 - accuracy: 0.9179 - loss: 0.1947\n",
      " training set -> batch:1248 loss:0.19848443567752838 and acc: 0.9157801270484924\n",
      "1248/2105 [================>.............] - ETA: 1:19:09 - accuracy: 0.9158 - loss: 0.1985\n",
      " training set -> batch:1249 loss:0.20495669543743134 and acc: 0.9137930870056152\n",
      "1249/2105 [================>.............] - ETA: 1:19:02 - accuracy: 0.9138 - loss: 0.2050\n",
      " training set -> batch:1250 loss:0.21063649654388428 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1250 val loss:0.234318807721138 and val acc: 0.9082568883895874\n",
      "1250/2105 [================>.............] - ETA: 1:19:15 - accuracy: 0.9111 - loss: 0.2106\n",
      " training set -> batch:1251 loss:0.2211342751979828 and acc: 0.9115044474601746\n",
      "1251/2105 [================>.............] - ETA: 1:19:07 - accuracy: 0.9115 - loss: 0.2211\n",
      " training set -> batch:1252 loss:0.21290449798107147 and acc: 0.9134615659713745\n",
      "1252/2105 [================>.............] - ETA: 1:19:00 - accuracy: 0.9135 - loss: 0.2129\n",
      " training set -> batch:1253 loss:0.20796075463294983 and acc: 0.91425621509552\n",
      "1253/2105 [================>.............] - ETA: 1:18:52 - accuracy: 0.9143 - loss: 0.2080\n",
      " training set -> batch:1254 loss:0.21026736497879028 and acc: 0.9150000214576721\n",
      "1254/2105 [================>.............] - ETA: 1:18:44 - accuracy: 0.9150 - loss: 0.2103\n",
      " training set -> batch:1255 loss:0.21325229108333588 and acc: 0.9147287011146545\n",
      "1255/2105 [================>.............] - ETA: 1:18:37 - accuracy: 0.9147 - loss: 0.2133\n",
      " training set -> batch:1256 loss:0.2123698890209198 and acc: 0.9154135584831238\n",
      "1256/2105 [================>.............] - ETA: 1:18:29 - accuracy: 0.9154 - loss: 0.2124\n",
      " training set -> batch:1257 loss:0.20802390575408936 and acc: 0.9160584211349487\n",
      "1257/2105 [================>.............] - ETA: 1:18:22 - accuracy: 0.9161 - loss: 0.2080\n",
      " training set -> batch:1258 loss:0.20457254350185394 and acc: 0.917553186416626\n",
      "1258/2105 [================>.............] - ETA: 1:18:14 - accuracy: 0.9176 - loss: 0.2046\n",
      " training set -> batch:1259 loss:0.20835289359092712 and acc: 0.9181034564971924\n",
      "1259/2105 [================>.............] - ETA: 1:18:06 - accuracy: 0.9181 - loss: 0.2084\n",
      " training set -> batch:1260 loss:0.21090121567249298 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1260 val loss:0.21113221347332 and val acc: 0.9174311757087708\n",
      "1260/2105 [================>.............] - ETA: 1:18:19 - accuracy: 0.9178 - loss: 0.2109\n",
      " training set -> batch:1261 loss:0.20684702694416046 and acc: 0.9170354008674622\n",
      "1261/2105 [================>.............] - ETA: 1:18:12 - accuracy: 0.9170 - loss: 0.2068\n",
      " training set -> batch:1262 loss:0.2044924944639206 and acc: 0.9166666865348816\n",
      "1262/2105 [================>.............] - ETA: 1:18:04 - accuracy: 0.9167 - loss: 0.2045\n",
      " training set -> batch:1263 loss:0.20253057777881622 and acc: 0.9163222908973694\n",
      "1263/2105 [=================>............] - ETA: 1:17:56 - accuracy: 0.9163 - loss: 0.2025\n",
      " training set -> batch:1264 loss:0.21439625322818756 and acc: 0.9150000214576721\n",
      "1264/2105 [=================>............] - ETA: 1:17:49 - accuracy: 0.9150 - loss: 0.2144\n",
      " training set -> batch:1265 loss:0.2088056355714798 and acc: 0.9166666865348816\n",
      "1265/2105 [=================>............] - ETA: 1:17:41 - accuracy: 0.9167 - loss: 0.2088\n",
      " training set -> batch:1266 loss:0.20050661265850067 and acc: 0.9191729426383972\n",
      "1266/2105 [=================>............] - ETA: 1:17:34 - accuracy: 0.9192 - loss: 0.2005\n",
      " training set -> batch:1267 loss:0.19968926906585693 and acc: 0.9197080135345459\n",
      "1267/2105 [=================>............] - ETA: 1:17:26 - accuracy: 0.9197 - loss: 0.1997\n",
      " training set -> batch:1268 loss:0.20763292908668518 and acc: 0.9184397459030151\n",
      "1268/2105 [=================>............] - ETA: 1:17:19 - accuracy: 0.9184 - loss: 0.2076\n",
      " training set -> batch:1269 loss:0.204280287027359 and acc: 0.9189655184745789\n",
      "1269/2105 [=================>............] - ETA: 1:17:11 - accuracy: 0.9190 - loss: 0.2043\n",
      " training set -> batch:1270 loss:0.2076229304075241 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1270 val loss:0.2119593471288681 and val acc: 0.9185779690742493\n",
      "1270/2105 [=================>............] - ETA: 1:17:23 - accuracy: 0.9178 - loss: 0.2076\n",
      " training set -> batch:1271 loss:0.20818915963172913 and acc: 0.9181416034698486\n",
      "1271/2105 [=================>............] - ETA: 1:17:16 - accuracy: 0.9181 - loss: 0.2082\n",
      " training set -> batch:1272 loss:0.19981825351715088 and acc: 0.9209401607513428\n",
      "1272/2105 [=================>............] - ETA: 1:17:08 - accuracy: 0.9209 - loss: 0.1998\n",
      " training set -> batch:1273 loss:0.19342650473117828 and acc: 0.9235537052154541\n",
      "1273/2105 [=================>............] - ETA: 1:17:01 - accuracy: 0.9236 - loss: 0.1934\n",
      " training set -> batch:1274 loss:0.19213928282260895 and acc: 0.9229999780654907\n",
      "1274/2105 [=================>............] - ETA: 1:16:54 - accuracy: 0.9230 - loss: 0.1921\n",
      " training set -> batch:1275 loss:0.19044208526611328 and acc: 0.9224806427955627\n",
      "1275/2105 [=================>............] - ETA: 1:16:46 - accuracy: 0.9225 - loss: 0.1904\n",
      " training set -> batch:1276 loss:0.1927928775548935 and acc: 0.9219924807548523\n",
      "1276/2105 [=================>............] - ETA: 1:16:38 - accuracy: 0.9220 - loss: 0.1928\n",
      " training set -> batch:1277 loss:0.1937621682882309 and acc: 0.9233576655387878\n",
      "1277/2105 [=================>............] - ETA: 1:16:31 - accuracy: 0.9234 - loss: 0.1938\n",
      " training set -> batch:1278 loss:0.19952183961868286 and acc: 0.9228723645210266\n",
      "1278/2105 [=================>............] - ETA: 1:16:24 - accuracy: 0.9229 - loss: 0.1995\n",
      " training set -> batch:1279 loss:0.19822140038013458 and acc: 0.9224137663841248\n",
      "1279/2105 [=================>............] - ETA: 1:16:16 - accuracy: 0.9224 - loss: 0.1982\n",
      " training set -> batch:1280 loss:0.19800211489200592 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1280 val loss:0.2237459421157837 and val acc: 0.9151375889778137\n",
      "1280/2105 [=================>............] - ETA: 1:16:29 - accuracy: 0.9211 - loss: 0.1980\n",
      " training set -> batch:1281 loss:0.22574561834335327 and acc: 0.9137167930603027\n",
      "1281/2105 [=================>............] - ETA: 1:16:21 - accuracy: 0.9137 - loss: 0.2257\n",
      " training set -> batch:1282 loss:0.23118887841701508 and acc: 0.9134615659713745\n",
      "1282/2105 [=================>............] - ETA: 1:16:14 - accuracy: 0.9135 - loss: 0.2312\n",
      " training set -> batch:1283 loss:0.22642965614795685 and acc: 0.9152892827987671\n",
      "1283/2105 [=================>............] - ETA: 1:16:06 - accuracy: 0.9153 - loss: 0.2264\n",
      " training set -> batch:1284 loss:0.2379978448152542 and acc: 0.9139999747276306\n",
      "1284/2105 [=================>............] - ETA: 1:15:59 - accuracy: 0.9140 - loss: 0.2380\n",
      " training set -> batch:1285 loss:0.232341468334198 and acc: 0.9147287011146545\n",
      "1285/2105 [=================>............] - ETA: 1:15:51 - accuracy: 0.9147 - loss: 0.2323\n",
      " training set -> batch:1286 loss:0.22400827705860138 and acc: 0.9172932505607605\n",
      "1286/2105 [=================>............] - ETA: 1:15:44 - accuracy: 0.9173 - loss: 0.2240\n",
      " training set -> batch:1287 loss:0.21790985763072968 and acc: 0.918795645236969\n",
      "1287/2105 [=================>............] - ETA: 1:15:36 - accuracy: 0.9188 - loss: 0.2179\n",
      " training set -> batch:1288 loss:0.21061955392360687 and acc: 0.9210993051528931\n",
      "1288/2105 [=================>............] - ETA: 1:15:29 - accuracy: 0.9211 - loss: 0.2106\n",
      " training set -> batch:1289 loss:0.20936588943004608 and acc: 0.9215517044067383\n",
      "1289/2105 [=================>............] - ETA: 1:15:22 - accuracy: 0.9216 - loss: 0.2094\n",
      " training set -> batch:1290 loss:0.20476984977722168 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1290 val loss:0.2302464246749878 and val acc: 0.9094036817550659\n",
      "1290/2105 [=================>............] - ETA: 1:15:33 - accuracy: 0.9228 - loss: 0.2048\n",
      " training set -> batch:1291 loss:0.22803407907485962 and acc: 0.9103982448577881\n",
      "1291/2105 [=================>............] - ETA: 1:15:25 - accuracy: 0.9104 - loss: 0.2280\n",
      " training set -> batch:1292 loss:0.2283487319946289 and acc: 0.9113247990608215\n",
      "1292/2105 [=================>............] - ETA: 1:15:18 - accuracy: 0.9113 - loss: 0.2283\n",
      " training set -> batch:1293 loss:0.2195313572883606 and acc: 0.91425621509552\n",
      "1293/2105 [=================>............] - ETA: 1:15:10 - accuracy: 0.9143 - loss: 0.2195\n",
      " training set -> batch:1294 loss:0.21479813754558563 and acc: 0.9139999747276306\n",
      "1294/2105 [=================>............] - ETA: 1:15:03 - accuracy: 0.9140 - loss: 0.2148\n",
      " training set -> batch:1295 loss:0.21280896663665771 and acc: 0.9147287011146545\n",
      "1295/2105 [=================>............] - ETA: 1:14:55 - accuracy: 0.9147 - loss: 0.2128\n",
      " training set -> batch:1296 loss:0.21243998408317566 and acc: 0.9144737124443054\n",
      "1296/2105 [=================>............] - ETA: 1:14:48 - accuracy: 0.9145 - loss: 0.2124\n",
      " training set -> batch:1297 loss:0.21458213031291962 and acc: 0.9142335653305054\n",
      "1297/2105 [=================>............] - ETA: 1:14:40 - accuracy: 0.9142 - loss: 0.2146\n",
      " training set -> batch:1298 loss:0.21551981568336487 and acc: 0.914893627166748\n",
      "1298/2105 [=================>............] - ETA: 1:14:33 - accuracy: 0.9149 - loss: 0.2155\n",
      " training set -> batch:1299 loss:0.21710458397865295 and acc: 0.9137930870056152\n",
      "1299/2105 [=================>............] - ETA: 1:14:26 - accuracy: 0.9138 - loss: 0.2171\n",
      " training set -> batch:1300 loss:0.21418964862823486 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:1300 val loss:0.22571967542171478 and val acc: 0.91399085521698\n",
      "1300/2105 [=================>............] - ETA: 1:14:37 - accuracy: 0.9153 - loss: 0.2142\n",
      " training set -> batch:1301 loss:0.21958905458450317 and acc: 0.9159291982650757\n",
      "1301/2105 [=================>............] - ETA: 1:14:30 - accuracy: 0.9159 - loss: 0.2196\n",
      " training set -> batch:1302 loss:0.2098461389541626 and acc: 0.9188033938407898\n",
      "1302/2105 [=================>............] - ETA: 1:14:22 - accuracy: 0.9188 - loss: 0.2098\n",
      " training set -> batch:1303 loss:0.20670881867408752 and acc: 0.9204545617103577\n",
      "1303/2105 [=================>............] - ETA: 1:14:15 - accuracy: 0.9205 - loss: 0.2067\n",
      " training set -> batch:1304 loss:0.2133057713508606 and acc: 0.9179999828338623\n",
      "1304/2105 [=================>............] - ETA: 1:14:08 - accuracy: 0.9180 - loss: 0.2133\n",
      " training set -> batch:1305 loss:0.21380142867565155 and acc: 0.9186046719551086\n",
      "1305/2105 [=================>............] - ETA: 1:14:00 - accuracy: 0.9186 - loss: 0.2138\n",
      " training set -> batch:1306 loss:0.20961275696754456 and acc: 0.9201127886772156\n",
      "1306/2105 [=================>............] - ETA: 1:13:53 - accuracy: 0.9201 - loss: 0.2096\n",
      " training set -> batch:1307 loss:0.21402479708194733 and acc: 0.918795645236969\n",
      "1307/2105 [=================>............] - ETA: 1:13:45 - accuracy: 0.9188 - loss: 0.2140\n",
      " training set -> batch:1308 loss:0.21456629037857056 and acc: 0.9184397459030151\n",
      "1308/2105 [=================>............] - ETA: 1:13:38 - accuracy: 0.9184 - loss: 0.2146\n",
      " training set -> batch:1309 loss:0.2122751772403717 and acc: 0.9189655184745789\n",
      "1309/2105 [=================>............] - ETA: 1:13:31 - accuracy: 0.9190 - loss: 0.2123\n",
      " training set -> batch:1310 loss:0.2059417963027954 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1310 val loss:0.23512350022792816 and val acc: 0.9082568883895874\n",
      "1310/2105 [=================>............] - ETA: 1:13:42 - accuracy: 0.9203 - loss: 0.2059\n",
      " training set -> batch:1311 loss:0.22661472856998444 and acc: 0.9092920422554016\n",
      "1311/2105 [=================>............] - ETA: 1:13:34 - accuracy: 0.9093 - loss: 0.2266\n",
      " training set -> batch:1312 loss:0.22851257026195526 and acc: 0.9091880321502686\n",
      "1312/2105 [=================>............] - ETA: 1:13:27 - accuracy: 0.9092 - loss: 0.2285\n",
      " training set -> batch:1313 loss:0.23432330787181854 and acc: 0.9080578684806824\n",
      "1313/2105 [=================>............] - ETA: 1:13:19 - accuracy: 0.9081 - loss: 0.2343\n",
      " training set -> batch:1314 loss:0.2268064320087433 and acc: 0.9110000133514404\n",
      "1314/2105 [=================>............] - ETA: 1:13:12 - accuracy: 0.9110 - loss: 0.2268\n",
      " training set -> batch:1315 loss:0.21963588893413544 and acc: 0.9127907156944275\n",
      "1315/2105 [=================>............] - ETA: 1:13:05 - accuracy: 0.9128 - loss: 0.2196\n",
      " training set -> batch:1316 loss:0.2121487408876419 and acc: 0.9154135584831238\n",
      "1316/2105 [=================>............] - ETA: 1:12:57 - accuracy: 0.9154 - loss: 0.2121\n",
      " training set -> batch:1317 loss:0.20649896562099457 and acc: 0.9160584211349487\n",
      "1317/2105 [=================>............] - ETA: 1:12:50 - accuracy: 0.9161 - loss: 0.2065\n",
      " training set -> batch:1318 loss:0.20313720405101776 and acc: 0.9166666865348816\n",
      "1318/2105 [=================>............] - ETA: 1:12:43 - accuracy: 0.9167 - loss: 0.2031\n",
      " training set -> batch:1319 loss:0.20750273764133453 and acc: 0.915517270565033\n",
      "1319/2105 [=================>............] - ETA: 1:12:35 - accuracy: 0.9155 - loss: 0.2075\n",
      " training set -> batch:1320 loss:0.20330925285816193 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1320 val loss:0.22693116962909698 and val acc: 0.9094036817550659\n",
      "1320/2105 [=================>............] - ETA: 1:12:46 - accuracy: 0.9169 - loss: 0.2033\n",
      " training set -> batch:1321 loss:0.22246165573596954 and acc: 0.9115044474601746\n",
      "1321/2105 [=================>............] - ETA: 1:12:39 - accuracy: 0.9115 - loss: 0.2225\n",
      " training set -> batch:1322 loss:0.22148658335208893 and acc: 0.9123931527137756\n",
      "1322/2105 [=================>............] - ETA: 1:12:31 - accuracy: 0.9124 - loss: 0.2215\n",
      " training set -> batch:1323 loss:0.2146008163690567 and acc: 0.91425621509552\n",
      "1323/2105 [=================>............] - ETA: 1:12:24 - accuracy: 0.9143 - loss: 0.2146\n",
      " training set -> batch:1324 loss:0.2137186974287033 and acc: 0.9160000085830688\n",
      "1324/2105 [=================>............] - ETA: 1:12:17 - accuracy: 0.9160 - loss: 0.2137\n",
      " training set -> batch:1325 loss:0.20970125496387482 and acc: 0.9156976938247681\n",
      "1325/2105 [=================>............] - ETA: 1:12:09 - accuracy: 0.9157 - loss: 0.2097\n",
      " training set -> batch:1326 loss:0.20880034565925598 and acc: 0.9163534045219421\n",
      "1326/2105 [=================>............] - ETA: 1:12:02 - accuracy: 0.9164 - loss: 0.2088\n",
      " training set -> batch:1327 loss:0.20906700193881989 and acc: 0.9169707894325256\n",
      "1327/2105 [=================>............] - ETA: 1:11:54 - accuracy: 0.9170 - loss: 0.2091\n",
      " training set -> batch:1328 loss:0.2139931619167328 and acc: 0.914893627166748\n",
      "1328/2105 [=================>............] - ETA: 1:11:47 - accuracy: 0.9149 - loss: 0.2140\n",
      " training set -> batch:1329 loss:0.2132454812526703 and acc: 0.9146551489830017\n",
      "1329/2105 [=================>............] - ETA: 1:11:40 - accuracy: 0.9147 - loss: 0.2132\n",
      " training set -> batch:1330 loss:0.22180217504501343 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:1330 val loss:0.24005912244319916 and val acc: 0.9048165082931519\n",
      "1330/2105 [=================>............] - ETA: 1:11:50 - accuracy: 0.9128 - loss: 0.2218\n",
      " training set -> batch:1331 loss:0.2498740255832672 and acc: 0.903761088848114\n",
      "1331/2105 [=================>............] - ETA: 1:11:43 - accuracy: 0.9038 - loss: 0.2499\n",
      " training set -> batch:1332 loss:0.24603936076164246 and acc: 0.9038461446762085\n",
      "1332/2105 [=================>............] - ETA: 1:11:35 - accuracy: 0.9038 - loss: 0.2460\n",
      " training set -> batch:1333 loss:0.24220438301563263 and acc: 0.9049586653709412\n",
      "1333/2105 [=================>............] - ETA: 1:11:28 - accuracy: 0.9050 - loss: 0.2422\n",
      " training set -> batch:1334 loss:0.23298604786396027 and acc: 0.9070000052452087\n",
      "1334/2105 [==================>...........] - ETA: 1:11:21 - accuracy: 0.9070 - loss: 0.2330\n",
      " training set -> batch:1335 loss:0.2257903516292572 and acc: 0.9089147448539734\n",
      "1335/2105 [==================>...........] - ETA: 1:11:13 - accuracy: 0.9089 - loss: 0.2258\n",
      " training set -> batch:1336 loss:0.2198195904493332 and acc: 0.9107142686843872\n",
      "1336/2105 [==================>...........] - ETA: 1:11:06 - accuracy: 0.9107 - loss: 0.2198\n",
      " training set -> batch:1337 loss:0.21534855663776398 and acc: 0.9114963412284851\n",
      "1337/2105 [==================>...........] - ETA: 1:10:59 - accuracy: 0.9115 - loss: 0.2153\n",
      " training set -> batch:1338 loss:0.21021471917629242 and acc: 0.9131205677986145\n",
      "1338/2105 [==================>...........] - ETA: 1:10:51 - accuracy: 0.9131 - loss: 0.2102\n",
      " training set -> batch:1339 loss:0.2025943249464035 and acc: 0.915517270565033\n",
      "1339/2105 [==================>...........] - ETA: 1:10:44 - accuracy: 0.9155 - loss: 0.2026\n",
      " training set -> batch:1340 loss:0.19772370159626007 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1340 val loss:0.23470035195350647 and val acc: 0.9048165082931519\n",
      "1340/2105 [==================>...........] - ETA: 1:10:54 - accuracy: 0.9169 - loss: 0.1977\n",
      " training set -> batch:1341 loss:0.22172068059444427 and acc: 0.9081858396530151\n",
      "1341/2105 [==================>...........] - ETA: 1:10:46 - accuracy: 0.9082 - loss: 0.2217\n",
      " training set -> batch:1342 loss:0.21308104693889618 and acc: 0.9102563858032227\n",
      "1342/2105 [==================>...........] - ETA: 1:10:39 - accuracy: 0.9103 - loss: 0.2131\n",
      " training set -> batch:1343 loss:0.2054383009672165 and acc: 0.9121900796890259\n",
      "1343/2105 [==================>...........] - ETA: 1:10:32 - accuracy: 0.9122 - loss: 0.2054\n",
      " training set -> batch:1344 loss:0.2145708203315735 and acc: 0.9110000133514404\n",
      "1344/2105 [==================>...........] - ETA: 1:10:24 - accuracy: 0.9110 - loss: 0.2146\n",
      " training set -> batch:1345 loss:0.2123430222272873 and acc: 0.911821722984314\n",
      "1345/2105 [==================>...........] - ETA: 1:10:17 - accuracy: 0.9118 - loss: 0.2123\n",
      " training set -> batch:1346 loss:0.20667394995689392 and acc: 0.9135338068008423\n",
      "1346/2105 [==================>...........] - ETA: 1:10:10 - accuracy: 0.9135 - loss: 0.2067\n",
      " training set -> batch:1347 loss:0.20452778041362762 and acc: 0.9142335653305054\n",
      "1347/2105 [==================>...........] - ETA: 1:10:03 - accuracy: 0.9142 - loss: 0.2045\n",
      " training set -> batch:1348 loss:0.2086287885904312 and acc: 0.911347508430481\n",
      "1348/2105 [==================>...........] - ETA: 1:09:55 - accuracy: 0.9113 - loss: 0.2086\n",
      " training set -> batch:1349 loss:0.209125354886055 and acc: 0.9103448390960693\n",
      "1349/2105 [==================>...........] - ETA: 1:09:48 - accuracy: 0.9103 - loss: 0.2091\n",
      " training set -> batch:1350 loss:0.20654989778995514 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1350 val loss:0.21870751678943634 and val acc: 0.9094036817550659\n",
      "1350/2105 [==================>...........] - ETA: 1:09:58 - accuracy: 0.9111 - loss: 0.2065\n",
      " training set -> batch:1351 loss:0.21336650848388672 and acc: 0.9115044474601746\n",
      "1351/2105 [==================>...........] - ETA: 1:09:51 - accuracy: 0.9115 - loss: 0.2134\n",
      " training set -> batch:1352 loss:0.22044134140014648 and acc: 0.9113247990608215\n",
      "1352/2105 [==================>...........] - ETA: 1:09:44 - accuracy: 0.9113 - loss: 0.2204\n",
      " training set -> batch:1353 loss:0.21089117228984833 and acc: 0.91425621509552\n",
      "1353/2105 [==================>...........] - ETA: 1:09:36 - accuracy: 0.9143 - loss: 0.2109\n",
      " training set -> batch:1354 loss:0.20679527521133423 and acc: 0.9150000214576721\n",
      "1354/2105 [==================>...........] - ETA: 1:09:29 - accuracy: 0.9150 - loss: 0.2068\n",
      " training set -> batch:1355 loss:0.2059798240661621 and acc: 0.9166666865348816\n",
      "1355/2105 [==================>...........] - ETA: 1:09:22 - accuracy: 0.9167 - loss: 0.2060\n",
      " training set -> batch:1356 loss:0.2112911492586136 and acc: 0.9144737124443054\n",
      "1356/2105 [==================>...........] - ETA: 1:09:15 - accuracy: 0.9145 - loss: 0.2113\n",
      " training set -> batch:1357 loss:0.2121519148349762 and acc: 0.9142335653305054\n",
      "1357/2105 [==================>...........] - ETA: 1:09:07 - accuracy: 0.9142 - loss: 0.2122\n",
      " training set -> batch:1358 loss:0.20791968703269958 and acc: 0.9157801270484924\n",
      "1358/2105 [==================>...........] - ETA: 1:09:00 - accuracy: 0.9158 - loss: 0.2079\n",
      " training set -> batch:1359 loss:0.21401309967041016 and acc: 0.9146551489830017\n",
      "1359/2105 [==================>...........] - ETA: 1:08:53 - accuracy: 0.9147 - loss: 0.2140\n",
      " training set -> batch:1360 loss:0.21249355375766754 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:1360 val loss:0.22209154069423676 and val acc: 0.9059633016586304\n",
      "1360/2105 [==================>...........] - ETA: 1:09:02 - accuracy: 0.9153 - loss: 0.2125\n",
      " training set -> batch:1361 loss:0.21296215057373047 and acc: 0.9070796370506287\n",
      "1361/2105 [==================>...........] - ETA: 1:08:55 - accuracy: 0.9071 - loss: 0.2130\n",
      " training set -> batch:1362 loss:0.2067532241344452 and acc: 0.9091880321502686\n",
      "1362/2105 [==================>...........] - ETA: 1:08:47 - accuracy: 0.9092 - loss: 0.2068\n",
      " training set -> batch:1363 loss:0.1978202760219574 and acc: 0.9111570119857788\n",
      "1363/2105 [==================>...........] - ETA: 1:08:40 - accuracy: 0.9112 - loss: 0.1978\n",
      " training set -> batch:1364 loss:0.1952233463525772 and acc: 0.9129999876022339\n",
      "1364/2105 [==================>...........] - ETA: 1:08:33 - accuracy: 0.9130 - loss: 0.1952\n",
      " training set -> batch:1365 loss:0.2006286084651947 and acc: 0.913759708404541\n",
      "1365/2105 [==================>...........] - ETA: 1:08:26 - accuracy: 0.9138 - loss: 0.2006\n",
      " training set -> batch:1366 loss:0.1992284059524536 and acc: 0.9144737124443054\n",
      "1366/2105 [==================>...........] - ETA: 1:08:19 - accuracy: 0.9145 - loss: 0.1992\n",
      " training set -> batch:1367 loss:0.19725759327411652 and acc: 0.9160584211349487\n",
      "1367/2105 [==================>...........] - ETA: 1:08:12 - accuracy: 0.9161 - loss: 0.1973\n",
      " training set -> batch:1368 loss:0.19538792967796326 and acc: 0.9166666865348816\n",
      "1368/2105 [==================>...........] - ETA: 1:08:04 - accuracy: 0.9167 - loss: 0.1954\n",
      " training set -> batch:1369 loss:0.192308709025383 and acc: 0.9181034564971924\n",
      "1369/2105 [==================>...........] - ETA: 1:07:57 - accuracy: 0.9181 - loss: 0.1923\n",
      " training set -> batch:1370 loss:0.19378383457660675 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1370 val loss:0.21083541214466095 and val acc: 0.9128440618515015\n",
      "1370/2105 [==================>...........] - ETA: 1:08:06 - accuracy: 0.9186 - loss: 0.1938\n",
      " training set -> batch:1371 loss:0.2215733677148819 and acc: 0.9103982448577881\n",
      "1371/2105 [==================>...........] - ETA: 1:07:59 - accuracy: 0.9104 - loss: 0.2216\n",
      " training set -> batch:1372 loss:0.21745330095291138 and acc: 0.9123931527137756\n",
      "1372/2105 [==================>...........] - ETA: 1:07:52 - accuracy: 0.9124 - loss: 0.2175\n",
      " training set -> batch:1373 loss:0.20987935364246368 and acc: 0.91425621509552\n",
      "1373/2105 [==================>...........] - ETA: 1:07:45 - accuracy: 0.9143 - loss: 0.2099\n",
      " training set -> batch:1374 loss:0.20133234560489655 and acc: 0.9169999957084656\n",
      "1374/2105 [==================>...........] - ETA: 1:07:37 - accuracy: 0.9170 - loss: 0.2013\n",
      " training set -> batch:1375 loss:0.19500264525413513 and acc: 0.9186046719551086\n",
      "1375/2105 [==================>...........] - ETA: 1:07:30 - accuracy: 0.9186 - loss: 0.1950\n",
      " training set -> batch:1376 loss:0.19609896838665009 and acc: 0.9191729426383972\n",
      "1376/2105 [==================>...........] - ETA: 1:07:23 - accuracy: 0.9192 - loss: 0.1961\n",
      " training set -> batch:1377 loss:0.19908387959003448 and acc: 0.9178832173347473\n",
      "1377/2105 [==================>...........] - ETA: 1:07:16 - accuracy: 0.9179 - loss: 0.1991\n",
      " training set -> batch:1378 loss:0.1930759996175766 and acc: 0.9202127456665039\n",
      "1378/2105 [==================>...........] - ETA: 1:07:09 - accuracy: 0.9202 - loss: 0.1931\n",
      " training set -> batch:1379 loss:0.1885460764169693 and acc: 0.9215517044067383\n",
      "1379/2105 [==================>...........] - ETA: 1:07:02 - accuracy: 0.9216 - loss: 0.1885\n",
      " training set -> batch:1380 loss:0.18978597223758698 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1380 val loss:0.21013657748699188 and val acc: 0.9105504751205444\n",
      "1380/2105 [==================>...........] - ETA: 1:07:10 - accuracy: 0.9220 - loss: 0.1898\n",
      " training set -> batch:1381 loss:0.20258890092372894 and acc: 0.9126105904579163\n",
      "1381/2105 [==================>...........] - ETA: 1:07:03 - accuracy: 0.9126 - loss: 0.2026\n",
      " training set -> batch:1382 loss:0.19397370517253876 and acc: 0.9155982732772827\n",
      "1382/2105 [==================>...........] - ETA: 1:06:56 - accuracy: 0.9156 - loss: 0.1940\n",
      " training set -> batch:1383 loss:0.1906895637512207 and acc: 0.9173553586006165\n",
      "1383/2105 [==================>...........] - ETA: 1:06:49 - accuracy: 0.9174 - loss: 0.1907\n",
      " training set -> batch:1384 loss:0.19019180536270142 and acc: 0.9179999828338623\n",
      "1384/2105 [==================>...........] - ETA: 1:06:42 - accuracy: 0.9180 - loss: 0.1902\n",
      " training set -> batch:1385 loss:0.18661947548389435 and acc: 0.9195736646652222\n",
      "1385/2105 [==================>...........] - ETA: 1:06:34 - accuracy: 0.9196 - loss: 0.1866\n",
      " training set -> batch:1386 loss:0.18563584983348846 and acc: 0.9210526347160339\n",
      "1386/2105 [==================>...........] - ETA: 1:06:27 - accuracy: 0.9211 - loss: 0.1856\n",
      " training set -> batch:1387 loss:0.178767129778862 and acc: 0.9224452376365662\n",
      "1387/2105 [==================>...........] - ETA: 1:06:20 - accuracy: 0.9224 - loss: 0.1788\n",
      " training set -> batch:1388 loss:0.1864776760339737 and acc: 0.9219858050346375\n",
      "1388/2105 [==================>...........] - ETA: 1:06:13 - accuracy: 0.9220 - loss: 0.1865\n",
      " training set -> batch:1389 loss:0.18464484810829163 and acc: 0.923275887966156\n",
      "1389/2105 [==================>...........] - ETA: 1:06:06 - accuracy: 0.9233 - loss: 0.1846\n",
      " training set -> batch:1390 loss:0.19390298426151276 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1390 val loss:0.2196756899356842 and val acc: 0.9151375889778137\n",
      "1390/2105 [==================>...........] - ETA: 1:06:15 - accuracy: 0.9211 - loss: 0.1939\n",
      " training set -> batch:1391 loss:0.20876926183700562 and acc: 0.9170354008674622\n",
      "1391/2105 [==================>...........] - ETA: 1:06:08 - accuracy: 0.9170 - loss: 0.2088\n",
      " training set -> batch:1392 loss:0.1995338648557663 and acc: 0.9188033938407898\n",
      "1392/2105 [==================>...........] - ETA: 1:06:00 - accuracy: 0.9188 - loss: 0.1995\n",
      " training set -> batch:1393 loss:0.19255752861499786 and acc: 0.9194214940071106\n",
      "1393/2105 [==================>...........] - ETA: 1:05:53 - accuracy: 0.9194 - loss: 0.1926\n",
      " training set -> batch:1394 loss:0.19625318050384521 and acc: 0.9200000166893005\n",
      "1394/2105 [==================>...........] - ETA: 1:05:46 - accuracy: 0.9200 - loss: 0.1963\n",
      " training set -> batch:1395 loss:0.1895405501127243 and acc: 0.9224806427955627\n",
      "1395/2105 [==================>...........] - ETA: 1:05:39 - accuracy: 0.9225 - loss: 0.1895\n",
      " training set -> batch:1396 loss:0.19939640164375305 and acc: 0.9219924807548523\n",
      "1396/2105 [==================>...........] - ETA: 1:05:32 - accuracy: 0.9220 - loss: 0.1994\n",
      " training set -> batch:1397 loss:0.1972626894712448 and acc: 0.9215328693389893\n",
      "1397/2105 [==================>...........] - ETA: 1:05:25 - accuracy: 0.9215 - loss: 0.1973\n",
      " training set -> batch:1398 loss:0.19878754019737244 and acc: 0.9210993051528931\n",
      "1398/2105 [==================>...........] - ETA: 1:05:18 - accuracy: 0.9211 - loss: 0.1988\n",
      " training set -> batch:1399 loss:0.19977033138275146 and acc: 0.9215517044067383\n",
      "1399/2105 [==================>...........] - ETA: 1:05:11 - accuracy: 0.9216 - loss: 0.1998\n",
      " training set -> batch:1400 loss:0.1954246163368225 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1400 val loss:0.2245204597711563 and val acc: 0.9151375889778137\n",
      "1400/2105 [==================>...........] - ETA: 1:05:19 - accuracy: 0.9228 - loss: 0.1954\n",
      " training set -> batch:1401 loss:0.2168359011411667 and acc: 0.9159291982650757\n",
      "1401/2105 [==================>...........] - ETA: 1:05:12 - accuracy: 0.9159 - loss: 0.2168\n",
      " training set -> batch:1402 loss:0.2228635549545288 and acc: 0.9145299196243286\n",
      "1402/2105 [==================>...........] - ETA: 1:05:05 - accuracy: 0.9145 - loss: 0.2229\n",
      " training set -> batch:1403 loss:0.22667329013347626 and acc: 0.913223147392273\n",
      "1403/2105 [==================>...........] - ETA: 1:04:58 - accuracy: 0.9132 - loss: 0.2267\n",
      " training set -> batch:1404 loss:0.22231623530387878 and acc: 0.9139999747276306\n",
      "1404/2105 [===================>..........] - ETA: 1:04:51 - accuracy: 0.9140 - loss: 0.2223\n",
      " training set -> batch:1405 loss:0.22620506584644318 and acc: 0.913759708404541\n",
      "1405/2105 [===================>..........] - ETA: 1:04:44 - accuracy: 0.9138 - loss: 0.2262\n",
      " training set -> batch:1406 loss:0.2203846424818039 and acc: 0.9154135584831238\n",
      "1406/2105 [===================>..........] - ETA: 1:04:37 - accuracy: 0.9154 - loss: 0.2204\n",
      " training set -> batch:1407 loss:0.22345806658267975 and acc: 0.915145993232727\n",
      "1407/2105 [===================>..........] - ETA: 1:04:30 - accuracy: 0.9151 - loss: 0.2235\n",
      " training set -> batch:1408 loss:0.2293218970298767 and acc: 0.9131205677986145\n",
      "1408/2105 [===================>..........] - ETA: 1:04:23 - accuracy: 0.9131 - loss: 0.2293\n",
      " training set -> batch:1409 loss:0.23475921154022217 and acc: 0.9103448390960693\n",
      "1409/2105 [===================>..........] - ETA: 1:04:16 - accuracy: 0.9103 - loss: 0.2348\n",
      " training set -> batch:1410 loss:0.235531285405159 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:1410 val loss:0.22012756764888763 and val acc: 0.9105504751205444\n",
      "1410/2105 [===================>..........] - ETA: 1:04:24 - accuracy: 0.9102 - loss: 0.2355\n",
      " training set -> batch:1411 loss:0.21488013863563538 and acc: 0.9115044474601746\n",
      "1411/2105 [===================>..........] - ETA: 1:04:17 - accuracy: 0.9115 - loss: 0.2149\n",
      " training set -> batch:1412 loss:0.21647046506404877 and acc: 0.9113247990608215\n",
      "1412/2105 [===================>..........] - ETA: 1:04:10 - accuracy: 0.9113 - loss: 0.2165\n",
      " training set -> batch:1413 loss:0.2145794928073883 and acc: 0.913223147392273\n",
      "1413/2105 [===================>..........] - ETA: 1:04:03 - accuracy: 0.9132 - loss: 0.2146\n",
      " training set -> batch:1414 loss:0.20970706641674042 and acc: 0.9160000085830688\n",
      "1414/2105 [===================>..........] - ETA: 1:03:56 - accuracy: 0.9160 - loss: 0.2097\n",
      " training set -> batch:1415 loss:0.20848514139652252 and acc: 0.9166666865348816\n",
      "1415/2105 [===================>..........] - ETA: 1:03:49 - accuracy: 0.9167 - loss: 0.2085\n",
      " training set -> batch:1416 loss:0.21201476454734802 and acc: 0.9154135584831238\n",
      "1416/2105 [===================>..........] - ETA: 1:03:41 - accuracy: 0.9154 - loss: 0.2120\n",
      " training set -> batch:1417 loss:0.20994926989078522 and acc: 0.9160584211349487\n",
      "1417/2105 [===================>..........] - ETA: 1:03:34 - accuracy: 0.9161 - loss: 0.2099\n",
      " training set -> batch:1418 loss:0.21193461120128632 and acc: 0.9166666865348816\n",
      "1418/2105 [===================>..........] - ETA: 1:03:27 - accuracy: 0.9167 - loss: 0.2119\n",
      " training set -> batch:1419 loss:0.20649868249893188 and acc: 0.9181034564971924\n",
      "1419/2105 [===================>..........] - ETA: 1:03:20 - accuracy: 0.9181 - loss: 0.2065\n",
      " training set -> batch:1420 loss:0.2046617865562439 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1420 val loss:0.21339866518974304 and val acc: 0.911697268486023\n",
      "1420/2105 [===================>..........] - ETA: 1:03:28 - accuracy: 0.9195 - loss: 0.2047\n",
      " training set -> batch:1421 loss:0.21223534643650055 and acc: 0.9115044474601746\n",
      "1421/2105 [===================>..........] - ETA: 1:03:21 - accuracy: 0.9115 - loss: 0.2122\n",
      " training set -> batch:1422 loss:0.21506132185459137 and acc: 0.9113247990608215\n",
      "1422/2105 [===================>..........] - ETA: 1:03:14 - accuracy: 0.9113 - loss: 0.2151\n",
      " training set -> batch:1423 loss:0.2071029394865036 and acc: 0.913223147392273\n",
      "1423/2105 [===================>..........] - ETA: 1:03:07 - accuracy: 0.9132 - loss: 0.2071\n",
      " training set -> batch:1424 loss:0.20666038990020752 and acc: 0.9139999747276306\n",
      "1424/2105 [===================>..........] - ETA: 1:03:00 - accuracy: 0.9140 - loss: 0.2067\n",
      " training set -> batch:1425 loss:0.21698635816574097 and acc: 0.9108527302742004\n",
      "1425/2105 [===================>..........] - ETA: 1:02:53 - accuracy: 0.9109 - loss: 0.2170\n",
      " training set -> batch:1426 loss:0.20866867899894714 and acc: 0.9135338068008423\n",
      "1426/2105 [===================>..........] - ETA: 1:02:46 - accuracy: 0.9135 - loss: 0.2087\n",
      " training set -> batch:1427 loss:0.2072480469942093 and acc: 0.9142335653305054\n",
      "1427/2105 [===================>..........] - ETA: 1:02:39 - accuracy: 0.9142 - loss: 0.2072\n",
      " training set -> batch:1428 loss:0.2051468938589096 and acc: 0.9157801270484924\n",
      "1428/2105 [===================>..........] - ETA: 1:02:32 - accuracy: 0.9158 - loss: 0.2051\n",
      " training set -> batch:1429 loss:0.2048184722661972 and acc: 0.9172413945198059\n",
      "1429/2105 [===================>..........] - ETA: 1:02:25 - accuracy: 0.9172 - loss: 0.2048\n",
      " training set -> batch:1430 loss:0.2020004540681839 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1430 val loss:0.20967210829257965 and val acc: 0.9185779690742493\n",
      "1430/2105 [===================>..........] - ETA: 1:02:33 - accuracy: 0.9186 - loss: 0.2020\n",
      " training set -> batch:1431 loss:0.20717693865299225 and acc: 0.9181416034698486\n",
      "1431/2105 [===================>..........] - ETA: 1:02:26 - accuracy: 0.9181 - loss: 0.2072\n",
      " training set -> batch:1432 loss:0.20461907982826233 and acc: 0.9177350401878357\n",
      "1432/2105 [===================>..........] - ETA: 1:02:19 - accuracy: 0.9177 - loss: 0.2046\n",
      " training set -> batch:1433 loss:0.20780819654464722 and acc: 0.9163222908973694\n",
      "1433/2105 [===================>..........] - ETA: 1:02:12 - accuracy: 0.9163 - loss: 0.2078\n",
      " training set -> batch:1434 loss:0.21967241168022156 and acc: 0.9150000214576721\n",
      "1434/2105 [===================>..........] - ETA: 1:02:05 - accuracy: 0.9150 - loss: 0.2197\n",
      " training set -> batch:1435 loss:0.2233760803937912 and acc: 0.9156976938247681\n",
      "1435/2105 [===================>..........] - ETA: 1:01:58 - accuracy: 0.9157 - loss: 0.2234\n",
      " training set -> batch:1436 loss:0.22218330204486847 and acc: 0.9163534045219421\n",
      "1436/2105 [===================>..........] - ETA: 1:01:51 - accuracy: 0.9164 - loss: 0.2222\n",
      " training set -> batch:1437 loss:0.21597051620483398 and acc: 0.9178832173347473\n",
      "1437/2105 [===================>..........] - ETA: 1:01:44 - accuracy: 0.9179 - loss: 0.2160\n",
      " training set -> batch:1438 loss:0.21958117187023163 and acc: 0.9166666865348816\n",
      "1438/2105 [===================>..........] - ETA: 1:01:37 - accuracy: 0.9167 - loss: 0.2196\n",
      " training set -> batch:1439 loss:0.21507777273654938 and acc: 0.9181034564971924\n",
      "1439/2105 [===================>..........] - ETA: 1:01:30 - accuracy: 0.9181 - loss: 0.2151\n",
      " training set -> batch:1440 loss:0.21410216391086578 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1440 val loss:0.20429137349128723 and val acc: 0.9151375889778137\n",
      "1440/2105 [===================>..........] - ETA: 1:01:37 - accuracy: 0.9195 - loss: 0.2141\n",
      " training set -> batch:1441 loss:0.2027699202299118 and acc: 0.9148229956626892\n",
      "1441/2105 [===================>..........] - ETA: 1:01:30 - accuracy: 0.9148 - loss: 0.2028\n",
      " training set -> batch:1442 loss:0.19227661192417145 and acc: 0.9177350401878357\n",
      "1442/2105 [===================>..........] - ETA: 1:01:23 - accuracy: 0.9177 - loss: 0.1923\n",
      " training set -> batch:1443 loss:0.1877603977918625 and acc: 0.9194214940071106\n",
      "1443/2105 [===================>..........] - ETA: 1:01:16 - accuracy: 0.9194 - loss: 0.1878\n",
      " training set -> batch:1444 loss:0.19489379227161407 and acc: 0.9179999828338623\n",
      "1444/2105 [===================>..........] - ETA: 1:01:09 - accuracy: 0.9180 - loss: 0.1949\n",
      " training set -> batch:1445 loss:0.19960105419158936 and acc: 0.9156976938247681\n",
      "1445/2105 [===================>..........] - ETA: 1:01:02 - accuracy: 0.9157 - loss: 0.1996\n",
      " training set -> batch:1446 loss:0.19297917187213898 and acc: 0.9172932505607605\n",
      "1446/2105 [===================>..........] - ETA: 1:00:55 - accuracy: 0.9173 - loss: 0.1930\n",
      " training set -> batch:1447 loss:0.19531719386577606 and acc: 0.9160584211349487\n",
      "1447/2105 [===================>..........] - ETA: 1:00:48 - accuracy: 0.9161 - loss: 0.1953\n",
      " training set -> batch:1448 loss:0.19293245673179626 and acc: 0.9166666865348816\n",
      "1448/2105 [===================>..........] - ETA: 1:00:41 - accuracy: 0.9167 - loss: 0.1929\n",
      " training set -> batch:1449 loss:0.19878174364566803 and acc: 0.9163793325424194\n",
      "1449/2105 [===================>..........] - ETA: 1:00:34 - accuracy: 0.9164 - loss: 0.1988\n",
      " training set -> batch:1450 loss:0.19952793419361115 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1450 val loss:0.19922922551631927 and val acc: 0.9162843823432922\n",
      "1450/2105 [===================>..........] - ETA: 1:00:41 - accuracy: 0.9161 - loss: 0.1995\n",
      " training set -> batch:1451 loss:0.19645196199417114 and acc: 0.9170354008674622\n",
      "1451/2105 [===================>..........] - ETA: 1:00:34 - accuracy: 0.9170 - loss: 0.1965\n",
      " training set -> batch:1452 loss:0.19265539944171906 and acc: 0.9177350401878357\n",
      "1452/2105 [===================>..........] - ETA: 1:00:27 - accuracy: 0.9177 - loss: 0.1927\n",
      " training set -> batch:1453 loss:0.1866455376148224 and acc: 0.9194214940071106\n",
      "1453/2105 [===================>..........] - ETA: 1:00:20 - accuracy: 0.9194 - loss: 0.1866\n",
      " training set -> batch:1454 loss:0.18122588098049164 and acc: 0.9200000166893005\n",
      "1454/2105 [===================>..........] - ETA: 1:00:13 - accuracy: 0.9200 - loss: 0.1812\n",
      " training set -> batch:1455 loss:0.17848387360572815 and acc: 0.9215116500854492\n",
      "1455/2105 [===================>..........] - ETA: 1:00:06 - accuracy: 0.9215 - loss: 0.1785\n",
      " training set -> batch:1456 loss:0.1766376793384552 and acc: 0.9229323267936707\n",
      "1456/2105 [===================>..........] - ETA: 59:59 - accuracy: 0.9229 - loss: 0.1766  \n",
      " training set -> batch:1457 loss:0.1737954318523407 and acc: 0.9242700934410095\n",
      "1457/2105 [===================>..........] - ETA: 59:52 - accuracy: 0.9243 - loss: 0.1738\n",
      " training set -> batch:1458 loss:0.1709679812192917 and acc: 0.9246453642845154\n",
      "1458/2105 [===================>..........] - ETA: 59:45 - accuracy: 0.9246 - loss: 0.1710\n",
      " training set -> batch:1459 loss:0.17020320892333984 and acc: 0.925000011920929\n",
      "1459/2105 [===================>..........] - ETA: 59:39 - accuracy: 0.9250 - loss: 0.1702\n",
      " training set -> batch:1460 loss:0.17306357622146606 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1460 val loss:0.20593464374542236 and val acc: 0.9174311757087708\n",
      "1460/2105 [===================>..........] - ETA: 59:45 - accuracy: 0.9253 - loss: 0.1731\n",
      " training set -> batch:1461 loss:0.21703466773033142 and acc: 0.9170354008674622\n",
      "1461/2105 [===================>..........] - ETA: 59:38 - accuracy: 0.9170 - loss: 0.2170\n",
      " training set -> batch:1462 loss:0.2124079018831253 and acc: 0.9166666865348816\n",
      "1462/2105 [===================>..........] - ETA: 59:31 - accuracy: 0.9167 - loss: 0.2124\n",
      " training set -> batch:1463 loss:0.2070160061120987 and acc: 0.9183884263038635\n",
      "1463/2105 [===================>..........] - ETA: 59:24 - accuracy: 0.9184 - loss: 0.2070\n",
      " training set -> batch:1464 loss:0.20641618967056274 and acc: 0.9169999957084656\n",
      "1464/2105 [===================>..........] - ETA: 59:17 - accuracy: 0.9170 - loss: 0.2064\n",
      " training set -> batch:1465 loss:0.20220798254013062 and acc: 0.9186046719551086\n",
      "1465/2105 [===================>..........] - ETA: 59:10 - accuracy: 0.9186 - loss: 0.2022\n",
      " training set -> batch:1466 loss:0.20518982410430908 and acc: 0.9172932505607605\n",
      "1466/2105 [===================>..........] - ETA: 59:03 - accuracy: 0.9173 - loss: 0.2052\n",
      " training set -> batch:1467 loss:0.20143239200115204 and acc: 0.918795645236969\n",
      "1467/2105 [===================>..........] - ETA: 58:57 - accuracy: 0.9188 - loss: 0.2014\n",
      " training set -> batch:1468 loss:0.20996350049972534 and acc: 0.9184397459030151\n",
      "1468/2105 [===================>..........] - ETA: 58:50 - accuracy: 0.9184 - loss: 0.2100\n",
      " training set -> batch:1469 loss:0.2101193368434906 and acc: 0.9181034564971924\n",
      "1469/2105 [===================>..........] - ETA: 58:43 - accuracy: 0.9181 - loss: 0.2101\n",
      " training set -> batch:1470 loss:0.21180276572704315 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1470 val loss:0.19828681647777557 and val acc: 0.9185779690742493\n",
      "1470/2105 [===================>..........] - ETA: 58:49 - accuracy: 0.9178 - loss: 0.2118\n",
      " training set -> batch:1471 loss:0.20001275837421417 and acc: 0.9181416034698486\n",
      "1471/2105 [===================>..........] - ETA: 58:42 - accuracy: 0.9181 - loss: 0.2000\n",
      " training set -> batch:1472 loss:0.19532771408557892 and acc: 0.9198718070983887\n",
      "1472/2105 [===================>..........] - ETA: 58:35 - accuracy: 0.9199 - loss: 0.1953\n",
      " training set -> batch:1473 loss:0.19417434930801392 and acc: 0.9204545617103577\n",
      "1473/2105 [===================>..........] - ETA: 58:28 - accuracy: 0.9205 - loss: 0.1942\n",
      " training set -> batch:1474 loss:0.19671492278575897 and acc: 0.9200000166893005\n",
      "1474/2105 [====================>.........] - ETA: 58:21 - accuracy: 0.9200 - loss: 0.1967\n",
      " training set -> batch:1475 loss:0.19388404488563538 and acc: 0.9215116500854492\n",
      "1475/2105 [====================>.........] - ETA: 58:14 - accuracy: 0.9215 - loss: 0.1939\n",
      " training set -> batch:1476 loss:0.18834227323532104 and acc: 0.923872172832489\n",
      "1476/2105 [====================>.........] - ETA: 58:08 - accuracy: 0.9239 - loss: 0.1883\n",
      " training set -> batch:1477 loss:0.18822729587554932 and acc: 0.9242700934410095\n",
      "1477/2105 [====================>.........] - ETA: 58:01 - accuracy: 0.9243 - loss: 0.1882\n",
      " training set -> batch:1478 loss:0.1832227110862732 and acc: 0.9264184236526489\n",
      "1478/2105 [====================>.........] - ETA: 57:54 - accuracy: 0.9264 - loss: 0.1832\n",
      " training set -> batch:1479 loss:0.1782156080007553 and acc: 0.9275861978530884\n",
      "1479/2105 [====================>.........] - ETA: 57:47 - accuracy: 0.9276 - loss: 0.1782\n",
      " training set -> batch:1480 loss:0.17931373417377472 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1480 val loss:0.20024116337299347 and val acc: 0.91399085521698\n",
      "1480/2105 [====================>.........] - ETA: 57:53 - accuracy: 0.9270 - loss: 0.1793\n",
      " training set -> batch:1481 loss:0.19666649401187897 and acc: 0.9148229956626892\n",
      "1481/2105 [====================>.........] - ETA: 57:46 - accuracy: 0.9148 - loss: 0.1967\n",
      " training set -> batch:1482 loss:0.19176869094371796 and acc: 0.9166666865348816\n",
      "1482/2105 [====================>.........] - ETA: 57:39 - accuracy: 0.9167 - loss: 0.1918\n",
      " training set -> batch:1483 loss:0.18540862202644348 and acc: 0.9194214940071106\n",
      "1483/2105 [====================>.........] - ETA: 57:32 - accuracy: 0.9194 - loss: 0.1854\n",
      " training set -> batch:1484 loss:0.19023223221302032 and acc: 0.9190000295639038\n",
      "1484/2105 [====================>.........] - ETA: 57:25 - accuracy: 0.9190 - loss: 0.1902\n",
      " training set -> batch:1485 loss:0.19848525524139404 and acc: 0.9176356792449951\n",
      "1485/2105 [====================>.........] - ETA: 57:19 - accuracy: 0.9176 - loss: 0.1985\n",
      " training set -> batch:1486 loss:0.19475194811820984 and acc: 0.9191729426383972\n",
      "1486/2105 [====================>.........] - ETA: 57:12 - accuracy: 0.9192 - loss: 0.1948\n",
      " training set -> batch:1487 loss:0.1930273324251175 and acc: 0.918795645236969\n",
      "1487/2105 [====================>.........] - ETA: 57:05 - accuracy: 0.9188 - loss: 0.1930\n",
      " training set -> batch:1488 loss:0.19266758859157562 and acc: 0.9202127456665039\n",
      "1488/2105 [====================>.........] - ETA: 56:58 - accuracy: 0.9202 - loss: 0.1927\n",
      " training set -> batch:1489 loss:0.19182012975215912 and acc: 0.9198275804519653\n",
      "1489/2105 [====================>.........] - ETA: 56:51 - accuracy: 0.9198 - loss: 0.1918\n",
      " training set -> batch:1490 loss:0.18714654445648193 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1490 val loss:0.21647287905216217 and val acc: 0.9151375889778137\n",
      "1490/2105 [====================>.........] - ETA: 56:57 - accuracy: 0.9211 - loss: 0.1871\n",
      " training set -> batch:1491 loss:0.20960769057273865 and acc: 0.9170354008674622\n",
      "1491/2105 [====================>.........] - ETA: 56:50 - accuracy: 0.9170 - loss: 0.2096\n",
      " training set -> batch:1492 loss:0.20365728437900543 and acc: 0.9177350401878357\n",
      "1492/2105 [====================>.........] - ETA: 56:43 - accuracy: 0.9177 - loss: 0.2037\n",
      " training set -> batch:1493 loss:0.2068791389465332 and acc: 0.9173553586006165\n",
      "1493/2105 [====================>.........] - ETA: 56:37 - accuracy: 0.9174 - loss: 0.2069\n",
      " training set -> batch:1494 loss:0.2063082903623581 and acc: 0.9179999828338623\n",
      "1494/2105 [====================>.........] - ETA: 56:30 - accuracy: 0.9180 - loss: 0.2063\n",
      " training set -> batch:1495 loss:0.2125723510980606 and acc: 0.9156976938247681\n",
      "1495/2105 [====================>.........] - ETA: 56:23 - accuracy: 0.9157 - loss: 0.2126\n",
      " training set -> batch:1496 loss:0.21326637268066406 and acc: 0.9144737124443054\n",
      "1496/2105 [====================>.........] - ETA: 56:16 - accuracy: 0.9145 - loss: 0.2133\n",
      " training set -> batch:1497 loss:0.20724031329154968 and acc: 0.9160584211349487\n",
      "1497/2105 [====================>.........] - ETA: 56:09 - accuracy: 0.9161 - loss: 0.2072\n",
      " training set -> batch:1498 loss:0.21049484610557556 and acc: 0.9166666865348816\n",
      "1498/2105 [====================>.........] - ETA: 56:03 - accuracy: 0.9167 - loss: 0.2105\n",
      " training set -> batch:1499 loss:0.2031763792037964 and acc: 0.9189655184745789\n",
      "1499/2105 [====================>.........] - ETA: 55:56 - accuracy: 0.9190 - loss: 0.2032\n",
      " training set -> batch:1500 loss:0.2041437029838562 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1500 val loss:0.22303783893585205 and val acc: 0.9082568883895874\n",
      "1500/2105 [====================>.........] - ETA: 56:02 - accuracy: 0.9195 - loss: 0.2041\n",
      " training set -> batch:1501 loss:0.21550001204013824 and acc: 0.9092920422554016\n",
      "1501/2105 [====================>.........] - ETA: 55:55 - accuracy: 0.9093 - loss: 0.2155\n",
      " training set -> batch:1502 loss:0.21195785701274872 and acc: 0.9102563858032227\n",
      "1502/2105 [====================>.........] - ETA: 55:48 - accuracy: 0.9103 - loss: 0.2120\n",
      " training set -> batch:1503 loss:0.20454834401607513 and acc: 0.9111570119857788\n",
      "1503/2105 [====================>.........] - ETA: 55:41 - accuracy: 0.9112 - loss: 0.2045\n",
      " training set -> batch:1504 loss:0.20367908477783203 and acc: 0.9120000004768372\n",
      "1504/2105 [====================>.........] - ETA: 55:35 - accuracy: 0.9120 - loss: 0.2037\n",
      " training set -> batch:1505 loss:0.20326896011829376 and acc: 0.9127907156944275\n",
      "1505/2105 [====================>.........] - ETA: 55:28 - accuracy: 0.9128 - loss: 0.2033\n",
      " training set -> batch:1506 loss:0.22106948494911194 and acc: 0.9097744226455688\n",
      "1506/2105 [====================>.........] - ETA: 55:21 - accuracy: 0.9098 - loss: 0.2211\n",
      " training set -> batch:1507 loss:0.21803222596645355 and acc: 0.9105839133262634\n",
      "1507/2105 [====================>.........] - ETA: 55:14 - accuracy: 0.9106 - loss: 0.2180\n",
      " training set -> batch:1508 loss:0.21290583908557892 and acc: 0.9122340679168701\n",
      "1508/2105 [====================>.........] - ETA: 55:08 - accuracy: 0.9122 - loss: 0.2129\n",
      " training set -> batch:1509 loss:0.20652607083320618 and acc: 0.9146551489830017\n",
      "1509/2105 [====================>.........] - ETA: 55:01 - accuracy: 0.9147 - loss: 0.2065\n",
      " training set -> batch:1510 loss:0.20777088403701782 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:1510 val loss:0.2133089154958725 and val acc: 0.9151375889778137\n",
      "1510/2105 [====================>.........] - ETA: 55:06 - accuracy: 0.9136 - loss: 0.2078\n",
      " training set -> batch:1511 loss:0.20958448946475983 and acc: 0.9170354008674622\n",
      "1511/2105 [====================>.........] - ETA: 55:00 - accuracy: 0.9170 - loss: 0.2096\n",
      " training set -> batch:1512 loss:0.20430806279182434 and acc: 0.9188033938407898\n",
      "1512/2105 [====================>.........] - ETA: 54:53 - accuracy: 0.9188 - loss: 0.2043\n",
      " training set -> batch:1513 loss:0.19810840487480164 and acc: 0.9194214940071106\n",
      "1513/2105 [====================>.........] - ETA: 54:46 - accuracy: 0.9194 - loss: 0.1981\n",
      " training set -> batch:1514 loss:0.19774991273880005 and acc: 0.9179999828338623\n",
      "1514/2105 [====================>.........] - ETA: 54:39 - accuracy: 0.9180 - loss: 0.1977\n",
      " training set -> batch:1515 loss:0.20077621936798096 and acc: 0.9176356792449951\n",
      "1515/2105 [====================>.........] - ETA: 54:33 - accuracy: 0.9176 - loss: 0.2008\n",
      " training set -> batch:1516 loss:0.21056798100471497 and acc: 0.9163534045219421\n",
      "1516/2105 [====================>.........] - ETA: 54:26 - accuracy: 0.9164 - loss: 0.2106\n",
      " training set -> batch:1517 loss:0.20468980073928833 and acc: 0.9178832173347473\n",
      "1517/2105 [====================>.........] - ETA: 54:19 - accuracy: 0.9179 - loss: 0.2047\n",
      " training set -> batch:1518 loss:0.20606671273708344 and acc: 0.9184397459030151\n",
      "1518/2105 [====================>.........] - ETA: 54:12 - accuracy: 0.9184 - loss: 0.2061\n",
      " training set -> batch:1519 loss:0.20297543704509735 and acc: 0.9198275804519653\n",
      "1519/2105 [====================>.........] - ETA: 54:06 - accuracy: 0.9198 - loss: 0.2030\n",
      " training set -> batch:1520 loss:0.20174677670001984 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1520 val loss:0.2036193460226059 and val acc: 0.91399085521698\n",
      "1520/2105 [====================>.........] - ETA: 54:10 - accuracy: 0.9195 - loss: 0.2017\n",
      " training set -> batch:1521 loss:0.19561022520065308 and acc: 0.9159291982650757\n",
      "1521/2105 [====================>.........] - ETA: 54:04 - accuracy: 0.9159 - loss: 0.1956\n",
      " training set -> batch:1522 loss:0.1886191964149475 and acc: 0.9177350401878357\n",
      "1522/2105 [====================>.........] - ETA: 53:57 - accuracy: 0.9177 - loss: 0.1886\n",
      " training set -> batch:1523 loss:0.1874362975358963 and acc: 0.9183884263038635\n",
      "1523/2105 [====================>.........] - ETA: 53:50 - accuracy: 0.9184 - loss: 0.1874\n",
      " training set -> batch:1524 loss:0.1902974545955658 and acc: 0.9169999957084656\n",
      "1524/2105 [====================>.........] - ETA: 53:44 - accuracy: 0.9170 - loss: 0.1903\n",
      " training set -> batch:1525 loss:0.1867036670446396 and acc: 0.9176356792449951\n",
      "1525/2105 [====================>.........] - ETA: 53:37 - accuracy: 0.9176 - loss: 0.1867\n",
      " training set -> batch:1526 loss:0.18241623044013977 and acc: 0.9191729426383972\n",
      "1526/2105 [====================>.........] - ETA: 53:30 - accuracy: 0.9192 - loss: 0.1824\n",
      " training set -> batch:1527 loss:0.18582378327846527 and acc: 0.9178832173347473\n",
      "1527/2105 [====================>.........] - ETA: 53:23 - accuracy: 0.9179 - loss: 0.1858\n",
      " training set -> batch:1528 loss:0.18625396490097046 and acc: 0.9166666865348816\n",
      "1528/2105 [====================>.........] - ETA: 53:17 - accuracy: 0.9167 - loss: 0.1863\n",
      " training set -> batch:1529 loss:0.18411703407764435 and acc: 0.9172413945198059\n",
      "1529/2105 [====================>.........] - ETA: 53:10 - accuracy: 0.9172 - loss: 0.1841\n",
      " training set -> batch:1530 loss:0.18106286227703094 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1530 val loss:0.21882085502147675 and val acc: 0.9151375889778137\n",
      "1530/2105 [====================>.........] - ETA: 53:14 - accuracy: 0.9178 - loss: 0.1811\n",
      " training set -> batch:1531 loss:0.22044655680656433 and acc: 0.9148229956626892\n",
      "1531/2105 [====================>.........] - ETA: 53:08 - accuracy: 0.9148 - loss: 0.2204\n",
      " training set -> batch:1532 loss:0.21695387363433838 and acc: 0.9155982732772827\n",
      "1532/2105 [====================>.........] - ETA: 53:01 - accuracy: 0.9156 - loss: 0.2170\n",
      " training set -> batch:1533 loss:0.2201748788356781 and acc: 0.9152892827987671\n",
      "1533/2105 [====================>.........] - ETA: 52:54 - accuracy: 0.9153 - loss: 0.2202\n",
      " training set -> batch:1534 loss:0.22367429733276367 and acc: 0.9160000085830688\n",
      "1534/2105 [====================>.........] - ETA: 52:48 - accuracy: 0.9160 - loss: 0.2237\n",
      " training set -> batch:1535 loss:0.2207414209842682 and acc: 0.9156976938247681\n",
      "1535/2105 [====================>.........] - ETA: 52:41 - accuracy: 0.9157 - loss: 0.2207\n",
      " training set -> batch:1536 loss:0.2228543758392334 and acc: 0.9135338068008423\n",
      "1536/2105 [====================>.........] - ETA: 52:34 - accuracy: 0.9135 - loss: 0.2229\n",
      " training set -> batch:1537 loss:0.22469064593315125 and acc: 0.9142335653305054\n",
      "1537/2105 [====================>.........] - ETA: 52:28 - accuracy: 0.9142 - loss: 0.2247\n",
      " training set -> batch:1538 loss:0.22723785042762756 and acc: 0.9140070676803589\n",
      "1538/2105 [====================>.........] - ETA: 52:21 - accuracy: 0.9140 - loss: 0.2272\n",
      " training set -> batch:1539 loss:0.2253933548927307 and acc: 0.9146551489830017\n",
      "1539/2105 [====================>.........] - ETA: 52:14 - accuracy: 0.9147 - loss: 0.2254\n",
      " training set -> batch:1540 loss:0.224521204829216 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:1540 val loss:0.20628441870212555 and val acc: 0.9197247624397278\n",
      "1540/2105 [====================>.........] - ETA: 52:19 - accuracy: 0.9153 - loss: 0.2245\n",
      " training set -> batch:1541 loss:0.19957110285758972 and acc: 0.9203540086746216\n",
      "1541/2105 [====================>.........] - ETA: 52:12 - accuracy: 0.9204 - loss: 0.1996\n",
      " training set -> batch:1542 loss:0.20087069272994995 and acc: 0.9209401607513428\n",
      "1542/2105 [====================>.........] - ETA: 52:05 - accuracy: 0.9209 - loss: 0.2009\n",
      " training set -> batch:1543 loss:0.20276980102062225 and acc: 0.9194214940071106\n",
      "1543/2105 [====================>.........] - ETA: 51:59 - accuracy: 0.9194 - loss: 0.2028\n",
      " training set -> batch:1544 loss:0.20431460440158844 and acc: 0.9200000166893005\n",
      "1544/2105 [=====================>........] - ETA: 51:52 - accuracy: 0.9200 - loss: 0.2043\n",
      " training set -> batch:1545 loss:0.2054622918367386 and acc: 0.9205426573753357\n",
      "1545/2105 [=====================>........] - ETA: 51:45 - accuracy: 0.9205 - loss: 0.2055\n",
      " training set -> batch:1546 loss:0.20683078467845917 and acc: 0.9210526347160339\n",
      "1546/2105 [=====================>........] - ETA: 51:39 - accuracy: 0.9211 - loss: 0.2068\n",
      " training set -> batch:1547 loss:0.20604602992534637 and acc: 0.9215328693389893\n",
      "1547/2105 [=====================>........] - ETA: 51:32 - accuracy: 0.9215 - loss: 0.2060\n",
      " training set -> batch:1548 loss:0.21182292699813843 and acc: 0.9193262457847595\n",
      "1548/2105 [=====================>........] - ETA: 51:26 - accuracy: 0.9193 - loss: 0.2118\n",
      " training set -> batch:1549 loss:0.20897676050662994 and acc: 0.9206896424293518\n",
      "1549/2105 [=====================>........] - ETA: 51:19 - accuracy: 0.9207 - loss: 0.2090\n",
      " training set -> batch:1550 loss:0.20725207030773163 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1550 val loss:0.22005341947078705 and val acc: 0.9082568883895874\n",
      "1550/2105 [=====================>........] - ETA: 51:23 - accuracy: 0.9211 - loss: 0.2073\n",
      " training set -> batch:1551 loss:0.22432440519332886 and acc: 0.9059734344482422\n",
      "1551/2105 [=====================>........] - ETA: 51:16 - accuracy: 0.9060 - loss: 0.2243\n",
      " training set -> batch:1552 loss:0.22631345689296722 and acc: 0.9070512652397156\n",
      "1552/2105 [=====================>........] - ETA: 51:10 - accuracy: 0.9071 - loss: 0.2263\n",
      " training set -> batch:1553 loss:0.23056086897850037 and acc: 0.9059917330741882\n",
      "1553/2105 [=====================>........] - ETA: 51:03 - accuracy: 0.9060 - loss: 0.2306\n",
      " training set -> batch:1554 loss:0.22837522625923157 and acc: 0.9070000052452087\n",
      "1554/2105 [=====================>........] - ETA: 50:57 - accuracy: 0.9070 - loss: 0.2284\n",
      " training set -> batch:1555 loss:0.22837822139263153 and acc: 0.9069767594337463\n",
      "1555/2105 [=====================>........] - ETA: 50:50 - accuracy: 0.9070 - loss: 0.2284\n",
      " training set -> batch:1556 loss:0.2240341156721115 and acc: 0.9088345766067505\n",
      "1556/2105 [=====================>........] - ETA: 50:43 - accuracy: 0.9088 - loss: 0.2240\n",
      " training set -> batch:1557 loss:0.21861150860786438 and acc: 0.9114963412284851\n",
      "1557/2105 [=====================>........] - ETA: 50:37 - accuracy: 0.9115 - loss: 0.2186\n",
      " training set -> batch:1558 loss:0.21744376420974731 and acc: 0.9095744490623474\n",
      "1558/2105 [=====================>........] - ETA: 50:30 - accuracy: 0.9096 - loss: 0.2174\n",
      " training set -> batch:1559 loss:0.21257439255714417 and acc: 0.9103448390960693\n",
      "1559/2105 [=====================>........] - ETA: 50:23 - accuracy: 0.9103 - loss: 0.2126\n",
      " training set -> batch:1560 loss:0.20824898779392242 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:1560 val loss:0.2073540836572647 and val acc: 0.9185779690742493\n",
      "1560/2105 [=====================>........] - ETA: 50:27 - accuracy: 0.9119 - loss: 0.2082\n",
      " training set -> batch:1561 loss:0.20025955140590668 and acc: 0.9203540086746216\n",
      "1561/2105 [=====================>........] - ETA: 50:21 - accuracy: 0.9204 - loss: 0.2003\n",
      " training set -> batch:1562 loss:0.1980065554380417 and acc: 0.9209401607513428\n",
      "1562/2105 [=====================>........] - ETA: 50:14 - accuracy: 0.9209 - loss: 0.1980\n",
      " training set -> batch:1563 loss:0.19752740859985352 and acc: 0.9214876294136047\n",
      "1563/2105 [=====================>........] - ETA: 50:08 - accuracy: 0.9215 - loss: 0.1975\n",
      " training set -> batch:1564 loss:0.19132347404956818 and acc: 0.9229999780654907\n",
      "1564/2105 [=====================>........] - ETA: 50:01 - accuracy: 0.9230 - loss: 0.1913\n",
      " training set -> batch:1565 loss:0.19560988247394562 and acc: 0.9234496355056763\n",
      "1565/2105 [=====================>........] - ETA: 49:54 - accuracy: 0.9234 - loss: 0.1956\n",
      " training set -> batch:1566 loss:0.19448785483837128 and acc: 0.923872172832489\n",
      "1566/2105 [=====================>........] - ETA: 49:48 - accuracy: 0.9239 - loss: 0.1945\n",
      " training set -> batch:1567 loss:0.18665707111358643 and acc: 0.9260948896408081\n",
      "1567/2105 [=====================>........] - ETA: 49:41 - accuracy: 0.9261 - loss: 0.1867\n",
      " training set -> batch:1568 loss:0.17996561527252197 and acc: 0.9281914830207825\n",
      "1568/2105 [=====================>........] - ETA: 49:35 - accuracy: 0.9282 - loss: 0.1800\n",
      " training set -> batch:1569 loss:0.17801040410995483 and acc: 0.9293103218078613\n",
      "1569/2105 [=====================>........] - ETA: 49:28 - accuracy: 0.9293 - loss: 0.1780\n",
      " training set -> batch:1570 loss:0.17524337768554688 and acc: 0.9303691387176514\n",
      "\n",
      " validation set -> batch:1570 val loss:0.22548475861549377 and val acc: 0.911697268486023\n",
      "1570/2105 [=====================>........] - ETA: 49:32 - accuracy: 0.9304 - loss: 0.1752\n",
      " training set -> batch:1571 loss:0.22695420682430267 and acc: 0.9115044474601746\n",
      "1571/2105 [=====================>........] - ETA: 49:25 - accuracy: 0.9115 - loss: 0.2270\n",
      " training set -> batch:1572 loss:0.2200988382101059 and acc: 0.9134615659713745\n",
      "1572/2105 [=====================>........] - ETA: 49:19 - accuracy: 0.9135 - loss: 0.2201\n",
      " training set -> batch:1573 loss:0.22006413340568542 and acc: 0.91425621509552\n",
      "1573/2105 [=====================>........] - ETA: 49:12 - accuracy: 0.9143 - loss: 0.2201\n",
      " training set -> batch:1574 loss:0.21859580278396606 and acc: 0.9150000214576721\n",
      "1574/2105 [=====================>........] - ETA: 49:05 - accuracy: 0.9150 - loss: 0.2186\n",
      " training set -> batch:1575 loss:0.22339673340320587 and acc: 0.9156976938247681\n",
      "1575/2105 [=====================>........] - ETA: 48:59 - accuracy: 0.9157 - loss: 0.2234\n",
      " training set -> batch:1576 loss:0.21833284199237823 and acc: 0.9163534045219421\n",
      "1576/2105 [=====================>........] - ETA: 48:52 - accuracy: 0.9164 - loss: 0.2183\n",
      " training set -> batch:1577 loss:0.21402645111083984 and acc: 0.9169707894325256\n",
      "1577/2105 [=====================>........] - ETA: 48:46 - accuracy: 0.9170 - loss: 0.2140\n",
      " training set -> batch:1578 loss:0.21121571958065033 and acc: 0.9184397459030151\n",
      "1578/2105 [=====================>........] - ETA: 48:39 - accuracy: 0.9184 - loss: 0.2112\n",
      " training set -> batch:1579 loss:0.21074466407299042 and acc: 0.9181034564971924\n",
      "1579/2105 [=====================>........] - ETA: 48:32 - accuracy: 0.9181 - loss: 0.2107\n",
      " training set -> batch:1580 loss:0.207816943526268 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1580 val loss:0.22139586508274078 and val acc: 0.9151375889778137\n",
      "1580/2105 [=====================>........] - ETA: 48:36 - accuracy: 0.9186 - loss: 0.2078\n",
      " training set -> batch:1581 loss:0.2102198451757431 and acc: 0.9181416034698486\n",
      "1581/2105 [=====================>........] - ETA: 48:30 - accuracy: 0.9181 - loss: 0.2102\n",
      " training set -> batch:1582 loss:0.2103363424539566 and acc: 0.9155982732772827\n",
      "1582/2105 [=====================>........] - ETA: 48:23 - accuracy: 0.9156 - loss: 0.2103\n",
      " training set -> batch:1583 loss:0.21703840792179108 and acc: 0.9152892827987671\n",
      "1583/2105 [=====================>........] - ETA: 48:16 - accuracy: 0.9153 - loss: 0.2170\n",
      " training set -> batch:1584 loss:0.2117445319890976 and acc: 0.9160000085830688\n",
      "1584/2105 [=====================>........] - ETA: 48:10 - accuracy: 0.9160 - loss: 0.2117\n",
      " training set -> batch:1585 loss:0.20539164543151855 and acc: 0.9176356792449951\n",
      "1585/2105 [=====================>........] - ETA: 48:03 - accuracy: 0.9176 - loss: 0.2054\n",
      " training set -> batch:1586 loss:0.20225997269153595 and acc: 0.9182330965995789\n",
      "1586/2105 [=====================>........] - ETA: 47:57 - accuracy: 0.9182 - loss: 0.2023\n",
      " training set -> batch:1587 loss:0.20691274106502533 and acc: 0.9178832173347473\n",
      "1587/2105 [=====================>........] - ETA: 47:50 - accuracy: 0.9179 - loss: 0.2069\n",
      " training set -> batch:1588 loss:0.21351346373558044 and acc: 0.9157801270484924\n",
      "1588/2105 [=====================>........] - ETA: 47:44 - accuracy: 0.9158 - loss: 0.2135\n",
      " training set -> batch:1589 loss:0.21225307881832123 and acc: 0.9163793325424194\n",
      "1589/2105 [=====================>........] - ETA: 47:37 - accuracy: 0.9164 - loss: 0.2123\n",
      " training set -> batch:1590 loss:0.21682876348495483 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:1590 val loss:0.2190331667661667 and val acc: 0.9128440618515015\n",
      "1590/2105 [=====================>........] - ETA: 47:41 - accuracy: 0.9153 - loss: 0.2168\n",
      " training set -> batch:1591 loss:0.21261456608772278 and acc: 0.9126105904579163\n",
      "1591/2105 [=====================>........] - ETA: 47:34 - accuracy: 0.9126 - loss: 0.2126\n",
      " training set -> batch:1592 loss:0.20668815076351166 and acc: 0.9145299196243286\n",
      "1592/2105 [=====================>........] - ETA: 47:27 - accuracy: 0.9145 - loss: 0.2067\n",
      " training set -> batch:1593 loss:0.19746382534503937 and acc: 0.9173553586006165\n",
      "1593/2105 [=====================>........] - ETA: 47:21 - accuracy: 0.9174 - loss: 0.1975\n",
      " training set -> batch:1594 loss:0.19279876351356506 and acc: 0.9190000295639038\n",
      "1594/2105 [=====================>........] - ETA: 47:14 - accuracy: 0.9190 - loss: 0.1928\n",
      " training set -> batch:1595 loss:0.21305660903453827 and acc: 0.9147287011146545\n",
      "1595/2105 [=====================>........] - ETA: 47:08 - accuracy: 0.9147 - loss: 0.2131\n",
      " training set -> batch:1596 loss:0.20828938484191895 and acc: 0.9154135584831238\n",
      "1596/2105 [=====================>........] - ETA: 47:01 - accuracy: 0.9154 - loss: 0.2083\n",
      " training set -> batch:1597 loss:0.20491521060466766 and acc: 0.9169707894325256\n",
      "1597/2105 [=====================>........] - ETA: 46:55 - accuracy: 0.9170 - loss: 0.2049\n",
      " training set -> batch:1598 loss:0.20940960943698883 and acc: 0.9157801270484924\n",
      "1598/2105 [=====================>........] - ETA: 46:48 - accuracy: 0.9158 - loss: 0.2094\n",
      " training set -> batch:1599 loss:0.20998524129390717 and acc: 0.9146551489830017\n",
      "1599/2105 [=====================>........] - ETA: 46:42 - accuracy: 0.9147 - loss: 0.2100\n",
      " training set -> batch:1600 loss:0.20308977365493774 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1600 val loss:0.21020999550819397 and val acc: 0.9174311757087708\n",
      "1600/2105 [=====================>........] - ETA: 46:45 - accuracy: 0.9169 - loss: 0.2031\n",
      " training set -> batch:1601 loss:0.2107117921113968 and acc: 0.9159291982650757\n",
      "1601/2105 [=====================>........] - ETA: 46:38 - accuracy: 0.9159 - loss: 0.2107\n",
      " training set -> batch:1602 loss:0.20607136189937592 and acc: 0.9177350401878357\n",
      "1602/2105 [=====================>........] - ETA: 46:32 - accuracy: 0.9177 - loss: 0.2061\n",
      " training set -> batch:1603 loss:0.200822114944458 and acc: 0.9194214940071106\n",
      "1603/2105 [=====================>........] - ETA: 46:25 - accuracy: 0.9194 - loss: 0.2008\n",
      " training set -> batch:1604 loss:0.19898253679275513 and acc: 0.9210000038146973\n",
      "1604/2105 [=====================>........] - ETA: 46:19 - accuracy: 0.9210 - loss: 0.1990\n",
      " training set -> batch:1605 loss:0.20226071774959564 and acc: 0.9215116500854492\n",
      "1605/2105 [=====================>........] - ETA: 46:12 - accuracy: 0.9215 - loss: 0.2023\n",
      " training set -> batch:1606 loss:0.19802895188331604 and acc: 0.9229323267936707\n",
      "1606/2105 [=====================>........] - ETA: 46:06 - accuracy: 0.9229 - loss: 0.1980\n",
      " training set -> batch:1607 loss:0.2027478963136673 and acc: 0.9224452376365662\n",
      "1607/2105 [=====================>........] - ETA: 45:59 - accuracy: 0.9224 - loss: 0.2027\n",
      " training set -> batch:1608 loss:0.20292286574840546 and acc: 0.9228723645210266\n",
      "1608/2105 [=====================>........] - ETA: 45:53 - accuracy: 0.9229 - loss: 0.2029\n",
      " training set -> batch:1609 loss:0.20081426203250885 and acc: 0.9224137663841248\n",
      "1609/2105 [=====================>........] - ETA: 45:46 - accuracy: 0.9224 - loss: 0.2008\n",
      " training set -> batch:1610 loss:0.20007960498332977 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1610 val loss:0.21215668320655823 and val acc: 0.9185779690742493\n",
      "1610/2105 [=====================>........] - ETA: 45:49 - accuracy: 0.9237 - loss: 0.2001\n",
      " training set -> batch:1611 loss:0.2038603574037552 and acc: 0.9203540086746216\n",
      "1611/2105 [=====================>........] - ETA: 45:42 - accuracy: 0.9204 - loss: 0.2039\n",
      " training set -> batch:1612 loss:0.19755923748016357 and acc: 0.9220085740089417\n",
      "1612/2105 [=====================>........] - ETA: 45:36 - accuracy: 0.9220 - loss: 0.1976\n",
      " training set -> batch:1613 loss:0.1987980306148529 and acc: 0.9214876294136047\n",
      "1613/2105 [=====================>........] - ETA: 45:29 - accuracy: 0.9215 - loss: 0.1988\n",
      " training set -> batch:1614 loss:0.1903047114610672 and acc: 0.9240000247955322\n",
      "1614/2105 [======================>.......] - ETA: 45:23 - accuracy: 0.9240 - loss: 0.1903\n",
      " training set -> batch:1615 loss:0.1990499049425125 and acc: 0.9224806427955627\n",
      "1615/2105 [======================>.......] - ETA: 45:17 - accuracy: 0.9225 - loss: 0.1990\n",
      " training set -> batch:1616 loss:0.19178445637226105 and acc: 0.9248120188713074\n",
      "1616/2105 [======================>.......] - ETA: 45:10 - accuracy: 0.9248 - loss: 0.1918\n",
      " training set -> batch:1617 loss:0.2003781646490097 and acc: 0.9242700934410095\n",
      "1617/2105 [======================>.......] - ETA: 45:04 - accuracy: 0.9243 - loss: 0.2004\n",
      " training set -> batch:1618 loss:0.19490285217761993 and acc: 0.9255319237709045\n",
      "1618/2105 [======================>.......] - ETA: 44:57 - accuracy: 0.9255 - loss: 0.1949\n",
      " training set -> batch:1619 loss:0.19471020996570587 and acc: 0.9267241358757019\n",
      "1619/2105 [======================>.......] - ETA: 44:51 - accuracy: 0.9267 - loss: 0.1947\n",
      " training set -> batch:1620 loss:0.19145655632019043 and acc: 0.9278523325920105\n",
      "\n",
      " validation set -> batch:1620 val loss:0.22494710981845856 and val acc: 0.9185779690742493\n",
      "1620/2105 [======================>.......] - ETA: 44:53 - accuracy: 0.9279 - loss: 0.1915\n",
      " training set -> batch:1621 loss:0.23491734266281128 and acc: 0.9181416034698486\n",
      "1621/2105 [======================>.......] - ETA: 44:47 - accuracy: 0.9181 - loss: 0.2349\n",
      " training set -> batch:1622 loss:0.23384279012680054 and acc: 0.9177350401878357\n",
      "1622/2105 [======================>.......] - ETA: 44:41 - accuracy: 0.9177 - loss: 0.2338\n",
      " training set -> batch:1623 loss:0.23919951915740967 and acc: 0.9183884263038635\n",
      "1623/2105 [======================>.......] - ETA: 44:34 - accuracy: 0.9184 - loss: 0.2392\n",
      " training set -> batch:1624 loss:0.2336227148771286 and acc: 0.9179999828338623\n",
      "1624/2105 [======================>.......] - ETA: 44:28 - accuracy: 0.9180 - loss: 0.2336\n",
      " training set -> batch:1625 loss:0.2318929135799408 and acc: 0.9176356792449951\n",
      "1625/2105 [======================>.......] - ETA: 44:21 - accuracy: 0.9176 - loss: 0.2319\n",
      " training set -> batch:1626 loss:0.22956201434135437 and acc: 0.9182330965995789\n",
      "1626/2105 [======================>.......] - ETA: 44:15 - accuracy: 0.9182 - loss: 0.2296\n",
      " training set -> batch:1627 loss:0.2282835841178894 and acc: 0.9178832173347473\n",
      "1627/2105 [======================>.......] - ETA: 44:08 - accuracy: 0.9179 - loss: 0.2283\n",
      " training set -> batch:1628 loss:0.22241273522377014 and acc: 0.9184397459030151\n",
      "1628/2105 [======================>.......] - ETA: 44:02 - accuracy: 0.9184 - loss: 0.2224\n",
      " training set -> batch:1629 loss:0.22537533938884735 and acc: 0.9189655184745789\n",
      "1629/2105 [======================>.......] - ETA: 43:55 - accuracy: 0.9190 - loss: 0.2254\n",
      " training set -> batch:1630 loss:0.2176794409751892 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1630 val loss:0.21373127400875092 and val acc: 0.9151375889778137\n",
      "1630/2105 [======================>.......] - ETA: 43:58 - accuracy: 0.9211 - loss: 0.2177\n",
      " training set -> batch:1631 loss:0.21865560114383698 and acc: 0.9148229956626892\n",
      "1631/2105 [======================>.......] - ETA: 43:51 - accuracy: 0.9148 - loss: 0.2187\n",
      " training set -> batch:1632 loss:0.21822939813137054 and acc: 0.9155982732772827\n",
      "1632/2105 [======================>.......] - ETA: 43:45 - accuracy: 0.9156 - loss: 0.2182\n",
      " training set -> batch:1633 loss:0.22114160656929016 and acc: 0.913223147392273\n",
      "1633/2105 [======================>.......] - ETA: 43:38 - accuracy: 0.9132 - loss: 0.2211\n",
      " training set -> batch:1634 loss:0.22567322850227356 and acc: 0.9129999876022339\n",
      "1634/2105 [======================>.......] - ETA: 43:32 - accuracy: 0.9130 - loss: 0.2257\n",
      " training set -> batch:1635 loss:0.22117120027542114 and acc: 0.913759708404541\n",
      "1635/2105 [======================>.......] - ETA: 43:26 - accuracy: 0.9138 - loss: 0.2212\n",
      " training set -> batch:1636 loss:0.2256658524274826 and acc: 0.9116541147232056\n",
      "1636/2105 [======================>.......] - ETA: 43:19 - accuracy: 0.9117 - loss: 0.2257\n",
      " training set -> batch:1637 loss:0.22127710282802582 and acc: 0.9124087691307068\n",
      "1637/2105 [======================>.......] - ETA: 43:13 - accuracy: 0.9124 - loss: 0.2213\n",
      " training set -> batch:1638 loss:0.2184704840183258 and acc: 0.9131205677986145\n",
      "1638/2105 [======================>.......] - ETA: 43:06 - accuracy: 0.9131 - loss: 0.2185\n",
      " training set -> batch:1639 loss:0.21423979103565216 and acc: 0.9137930870056152\n",
      "1639/2105 [======================>.......] - ETA: 43:00 - accuracy: 0.9138 - loss: 0.2142\n",
      " training set -> batch:1640 loss:0.21254676580429077 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:1640 val loss:0.22342729568481445 and val acc: 0.9071100950241089\n",
      "1640/2105 [======================>.......] - ETA: 43:03 - accuracy: 0.9153 - loss: 0.2125\n",
      " training set -> batch:1641 loss:0.22302700579166412 and acc: 0.9059734344482422\n",
      "1641/2105 [======================>.......] - ETA: 42:57 - accuracy: 0.9060 - loss: 0.2230\n",
      " training set -> batch:1642 loss:0.21949701011180878 and acc: 0.9081196784973145\n",
      "1642/2105 [======================>.......] - ETA: 42:51 - accuracy: 0.9081 - loss: 0.2195\n",
      " training set -> batch:1643 loss:0.2137025147676468 and acc: 0.9101239442825317\n",
      "1643/2105 [======================>.......] - ETA: 42:44 - accuracy: 0.9101 - loss: 0.2137\n",
      " training set -> batch:1644 loss:0.2167334258556366 and acc: 0.9089999794960022\n",
      "1644/2105 [======================>.......] - ETA: 42:38 - accuracy: 0.9090 - loss: 0.2167\n",
      " training set -> batch:1645 loss:0.21198610961437225 and acc: 0.9108527302742004\n",
      "1645/2105 [======================>.......] - ETA: 42:31 - accuracy: 0.9109 - loss: 0.2120\n",
      " training set -> batch:1646 loss:0.2132669985294342 and acc: 0.9116541147232056\n",
      "1646/2105 [======================>.......] - ETA: 42:25 - accuracy: 0.9117 - loss: 0.2133\n",
      " training set -> batch:1647 loss:0.2083262950181961 and acc: 0.9133211970329285\n",
      "1647/2105 [======================>.......] - ETA: 42:19 - accuracy: 0.9133 - loss: 0.2083\n",
      " training set -> batch:1648 loss:0.20632535219192505 and acc: 0.9140070676803589\n",
      "1648/2105 [======================>.......] - ETA: 42:12 - accuracy: 0.9140 - loss: 0.2063\n",
      " training set -> batch:1649 loss:0.20425383746623993 and acc: 0.915517270565033\n",
      "1649/2105 [======================>.......] - ETA: 42:06 - accuracy: 0.9155 - loss: 0.2043\n",
      " training set -> batch:1650 loss:0.19794327020645142 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1650 val loss:0.2202097326517105 and val acc: 0.911697268486023\n",
      "1650/2105 [======================>.......] - ETA: 42:08 - accuracy: 0.9178 - loss: 0.1979\n",
      " training set -> batch:1651 loss:0.21288137137889862 and acc: 0.9137167930603027\n",
      "1651/2105 [======================>.......] - ETA: 42:02 - accuracy: 0.9137 - loss: 0.2129\n",
      " training set -> batch:1652 loss:0.21362550556659698 and acc: 0.9134615659713745\n",
      "1652/2105 [======================>.......] - ETA: 41:55 - accuracy: 0.9135 - loss: 0.2136\n",
      " training set -> batch:1653 loss:0.2060459703207016 and acc: 0.9152892827987671\n",
      "1653/2105 [======================>.......] - ETA: 41:49 - accuracy: 0.9153 - loss: 0.2060\n",
      " training set -> batch:1654 loss:0.20265886187553406 and acc: 0.9160000085830688\n",
      "1654/2105 [======================>.......] - ETA: 41:42 - accuracy: 0.9160 - loss: 0.2027\n",
      " training set -> batch:1655 loss:0.19780166447162628 and acc: 0.9176356792449951\n",
      "1655/2105 [======================>.......] - ETA: 41:36 - accuracy: 0.9176 - loss: 0.1978\n",
      " training set -> batch:1656 loss:0.19422686100006104 and acc: 0.9191729426383972\n",
      "1656/2105 [======================>.......] - ETA: 41:30 - accuracy: 0.9192 - loss: 0.1942\n",
      " training set -> batch:1657 loss:0.19370710849761963 and acc: 0.9197080135345459\n",
      "1657/2105 [======================>.......] - ETA: 41:23 - accuracy: 0.9197 - loss: 0.1937\n",
      " training set -> batch:1658 loss:0.19217373430728912 and acc: 0.9193262457847595\n",
      "1658/2105 [======================>.......] - ETA: 41:17 - accuracy: 0.9193 - loss: 0.1922\n",
      " training set -> batch:1659 loss:0.19323953986167908 and acc: 0.9189655184745789\n",
      "1659/2105 [======================>.......] - ETA: 41:11 - accuracy: 0.9190 - loss: 0.1932\n",
      " training set -> batch:1660 loss:0.19209665060043335 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1660 val loss:0.20924212038516998 and val acc: 0.9174311757087708\n",
      "1660/2105 [======================>.......] - ETA: 41:12 - accuracy: 0.9203 - loss: 0.1921\n",
      " training set -> batch:1661 loss:0.19811660051345825 and acc: 0.9203540086746216\n",
      "1661/2105 [======================>.......] - ETA: 41:06 - accuracy: 0.9204 - loss: 0.1981\n",
      " training set -> batch:1662 loss:0.1979731172323227 and acc: 0.9198718070983887\n",
      "1662/2105 [======================>.......] - ETA: 41:00 - accuracy: 0.9199 - loss: 0.1980\n",
      " training set -> batch:1663 loss:0.19616849720478058 and acc: 0.9204545617103577\n",
      "1663/2105 [======================>.......] - ETA: 40:53 - accuracy: 0.9205 - loss: 0.1962\n",
      " training set -> batch:1664 loss:0.19325970113277435 and acc: 0.9210000038146973\n",
      "1664/2105 [======================>.......] - ETA: 40:47 - accuracy: 0.9210 - loss: 0.1933\n",
      " training set -> batch:1665 loss:0.19682298600673676 and acc: 0.9205426573753357\n",
      "1665/2105 [======================>.......] - ETA: 40:40 - accuracy: 0.9205 - loss: 0.1968\n",
      " training set -> batch:1666 loss:0.19164299964904785 and acc: 0.9219924807548523\n",
      "1666/2105 [======================>.......] - ETA: 40:34 - accuracy: 0.9220 - loss: 0.1916\n",
      " training set -> batch:1667 loss:0.19127832353115082 and acc: 0.9224452376365662\n",
      "1667/2105 [======================>.......] - ETA: 40:28 - accuracy: 0.9224 - loss: 0.1913\n",
      " training set -> batch:1668 loss:0.18620596826076508 and acc: 0.923758864402771\n",
      "1668/2105 [======================>.......] - ETA: 40:21 - accuracy: 0.9238 - loss: 0.1862\n",
      " training set -> batch:1669 loss:0.18729352951049805 and acc: 0.9224137663841248\n",
      "1669/2105 [======================>.......] - ETA: 40:15 - accuracy: 0.9224 - loss: 0.1873\n",
      " training set -> batch:1670 loss:0.18577581644058228 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1670 val loss:0.21680818498134613 and val acc: 0.9174311757087708\n",
      "1670/2105 [======================>.......] - ETA: 40:17 - accuracy: 0.9228 - loss: 0.1858\n",
      " training set -> batch:1671 loss:0.21299679577350616 and acc: 0.9192478060722351\n",
      "1671/2105 [======================>.......] - ETA: 40:10 - accuracy: 0.9192 - loss: 0.2130\n",
      " training set -> batch:1672 loss:0.20337507128715515 and acc: 0.9209401607513428\n",
      "1672/2105 [======================>.......] - ETA: 40:04 - accuracy: 0.9209 - loss: 0.2034\n",
      " training set -> batch:1673 loss:0.19916436076164246 and acc: 0.922520637512207\n",
      "1673/2105 [======================>.......] - ETA: 39:57 - accuracy: 0.9225 - loss: 0.1992\n",
      " training set -> batch:1674 loss:0.19307300448417664 and acc: 0.9240000247955322\n",
      "1674/2105 [======================>.......] - ETA: 39:51 - accuracy: 0.9240 - loss: 0.1931\n",
      " training set -> batch:1675 loss:0.19133329391479492 and acc: 0.9244186282157898\n",
      "1675/2105 [======================>.......] - ETA: 39:45 - accuracy: 0.9244 - loss: 0.1913\n",
      " training set -> batch:1676 loss:0.19431857764720917 and acc: 0.923872172832489\n",
      "1676/2105 [======================>.......] - ETA: 39:39 - accuracy: 0.9239 - loss: 0.1943\n",
      " training set -> batch:1677 loss:0.18755662441253662 and acc: 0.9260948896408081\n",
      "1677/2105 [======================>.......] - ETA: 39:32 - accuracy: 0.9261 - loss: 0.1876\n",
      " training set -> batch:1678 loss:0.1963368058204651 and acc: 0.9246453642845154\n",
      "1678/2105 [======================>.......] - ETA: 39:26 - accuracy: 0.9246 - loss: 0.1963\n",
      " training set -> batch:1679 loss:0.20343352854251862 and acc: 0.9241379499435425\n",
      "1679/2105 [======================>.......] - ETA: 39:20 - accuracy: 0.9241 - loss: 0.2034\n",
      " training set -> batch:1680 loss:0.19906063377857208 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1680 val loss:0.22188040614128113 and val acc: 0.911697268486023\n",
      "1680/2105 [======================>.......] - ETA: 39:21 - accuracy: 0.9253 - loss: 0.1991\n",
      " training set -> batch:1681 loss:0.21422487497329712 and acc: 0.9137167930603027\n",
      "1681/2105 [======================>.......] - ETA: 39:15 - accuracy: 0.9137 - loss: 0.2142\n",
      " training set -> batch:1682 loss:0.20756353437900543 and acc: 0.9155982732772827\n",
      "1682/2105 [======================>.......] - ETA: 39:08 - accuracy: 0.9156 - loss: 0.2076\n",
      " training set -> batch:1683 loss:0.21583643555641174 and acc: 0.91425621509552\n",
      "1683/2105 [======================>.......] - ETA: 39:02 - accuracy: 0.9143 - loss: 0.2158\n",
      " training set -> batch:1684 loss:0.2132897824048996 and acc: 0.9150000214576721\n",
      "1684/2105 [=======================>......] - ETA: 38:56 - accuracy: 0.9150 - loss: 0.2133\n",
      " training set -> batch:1685 loss:0.20879054069519043 and acc: 0.9156976938247681\n",
      "1685/2105 [=======================>......] - ETA: 38:49 - accuracy: 0.9157 - loss: 0.2088\n",
      " training set -> batch:1686 loss:0.20204845070838928 and acc: 0.9182330965995789\n",
      "1686/2105 [=======================>......] - ETA: 38:43 - accuracy: 0.9182 - loss: 0.2020\n",
      " training set -> batch:1687 loss:0.19572584331035614 and acc: 0.9206204414367676\n",
      "1687/2105 [=======================>......] - ETA: 38:37 - accuracy: 0.9206 - loss: 0.1957\n",
      " training set -> batch:1688 loss:0.1979837566614151 and acc: 0.9193262457847595\n",
      "1688/2105 [=======================>......] - ETA: 38:30 - accuracy: 0.9193 - loss: 0.1980\n",
      " training set -> batch:1689 loss:0.1971910148859024 and acc: 0.9189655184745789\n",
      "1689/2105 [=======================>......] - ETA: 38:24 - accuracy: 0.9190 - loss: 0.1972\n",
      " training set -> batch:1690 loss:0.19309544563293457 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1690 val loss:0.23485109210014343 and val acc: 0.9059633016586304\n",
      "1690/2105 [=======================>......] - ETA: 38:25 - accuracy: 0.9195 - loss: 0.1931\n",
      " training set -> batch:1691 loss:0.22238174080848694 and acc: 0.9092920422554016\n",
      "1691/2105 [=======================>......] - ETA: 38:19 - accuracy: 0.9093 - loss: 0.2224\n",
      " training set -> batch:1692 loss:0.21946299076080322 and acc: 0.9081196784973145\n",
      "1692/2105 [=======================>......] - ETA: 38:13 - accuracy: 0.9081 - loss: 0.2195\n",
      " training set -> batch:1693 loss:0.22508752346038818 and acc: 0.9080578684806824\n",
      "1693/2105 [=======================>......] - ETA: 38:06 - accuracy: 0.9081 - loss: 0.2251\n",
      " training set -> batch:1694 loss:0.2254442423582077 and acc: 0.9079999923706055\n",
      "1694/2105 [=======================>......] - ETA: 38:00 - accuracy: 0.9080 - loss: 0.2254\n",
      " training set -> batch:1695 loss:0.22714383900165558 and acc: 0.9069767594337463\n",
      "1695/2105 [=======================>......] - ETA: 37:54 - accuracy: 0.9070 - loss: 0.2271\n",
      " training set -> batch:1696 loss:0.2243880033493042 and acc: 0.9078947305679321\n",
      "1696/2105 [=======================>......] - ETA: 37:48 - accuracy: 0.9079 - loss: 0.2244\n",
      " training set -> batch:1697 loss:0.218096524477005 and acc: 0.9096715450286865\n",
      "1697/2105 [=======================>......] - ETA: 37:41 - accuracy: 0.9097 - loss: 0.2181\n",
      " training set -> batch:1698 loss:0.21814316511154175 and acc: 0.9104610085487366\n",
      "1698/2105 [=======================>......] - ETA: 37:35 - accuracy: 0.9105 - loss: 0.2181\n",
      " training set -> batch:1699 loss:0.21754787862300873 and acc: 0.9103448390960693\n",
      "1699/2105 [=======================>......] - ETA: 37:29 - accuracy: 0.9103 - loss: 0.2175\n",
      " training set -> batch:1700 loss:0.2173977643251419 and acc: 0.9093959927558899\n",
      "\n",
      " validation set -> batch:1700 val loss:0.2724737226963043 and val acc: 0.9036697149276733\n",
      "1700/2105 [=======================>......] - ETA: 37:30 - accuracy: 0.9094 - loss: 0.2174\n",
      " training set -> batch:1701 loss:0.2583579123020172 and acc: 0.9059734344482422\n",
      "1701/2105 [=======================>......] - ETA: 37:24 - accuracy: 0.9060 - loss: 0.2584\n",
      " training set -> batch:1702 loss:0.25193050503730774 and acc: 0.9070512652397156\n",
      "1702/2105 [=======================>......] - ETA: 37:17 - accuracy: 0.9071 - loss: 0.2519\n",
      " training set -> batch:1703 loss:0.2460370659828186 and acc: 0.9080578684806824\n",
      "1703/2105 [=======================>......] - ETA: 37:11 - accuracy: 0.9081 - loss: 0.2460\n",
      " training set -> batch:1704 loss:0.23749515414237976 and acc: 0.9110000133514404\n",
      "1704/2105 [=======================>......] - ETA: 37:05 - accuracy: 0.9110 - loss: 0.2375\n",
      " training set -> batch:1705 loss:0.23606249690055847 and acc: 0.9108527302742004\n",
      "1705/2105 [=======================>......] - ETA: 36:59 - accuracy: 0.9109 - loss: 0.2361\n",
      " training set -> batch:1706 loss:0.22974368929862976 and acc: 0.9116541147232056\n",
      "1706/2105 [=======================>......] - ETA: 36:52 - accuracy: 0.9117 - loss: 0.2297\n",
      " training set -> batch:1707 loss:0.2241288423538208 and acc: 0.9124087691307068\n",
      "1707/2105 [=======================>......] - ETA: 36:46 - accuracy: 0.9124 - loss: 0.2241\n",
      " training set -> batch:1708 loss:0.2202688306570053 and acc: 0.911347508430481\n",
      "1708/2105 [=======================>......] - ETA: 36:40 - accuracy: 0.9113 - loss: 0.2203\n",
      " training set -> batch:1709 loss:0.21606041491031647 and acc: 0.9129310250282288\n",
      "1709/2105 [=======================>......] - ETA: 36:34 - accuracy: 0.9129 - loss: 0.2161\n",
      " training set -> batch:1710 loss:0.21535749733448029 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:1710 val loss:0.21704211831092834 and val acc: 0.9174311757087708\n",
      "1710/2105 [=======================>......] - ETA: 36:34 - accuracy: 0.9136 - loss: 0.2154\n",
      " training set -> batch:1711 loss:0.22627860307693481 and acc: 0.9159291982650757\n",
      "1711/2105 [=======================>......] - ETA: 36:28 - accuracy: 0.9159 - loss: 0.2263\n",
      " training set -> batch:1712 loss:0.22447672486305237 and acc: 0.9166666865348816\n",
      "1712/2105 [=======================>......] - ETA: 36:22 - accuracy: 0.9167 - loss: 0.2245\n",
      " training set -> batch:1713 loss:0.22154690325260162 and acc: 0.9163222908973694\n",
      "1713/2105 [=======================>......] - ETA: 36:16 - accuracy: 0.9163 - loss: 0.2215\n",
      " training set -> batch:1714 loss:0.21732600033283234 and acc: 0.9169999957084656\n",
      "1714/2105 [=======================>......] - ETA: 36:09 - accuracy: 0.9170 - loss: 0.2173\n",
      " training set -> batch:1715 loss:0.21585391461849213 and acc: 0.9186046719551086\n",
      "1715/2105 [=======================>......] - ETA: 36:03 - accuracy: 0.9186 - loss: 0.2159\n",
      " training set -> batch:1716 loss:0.20990502834320068 and acc: 0.9201127886772156\n",
      "1716/2105 [=======================>......] - ETA: 35:57 - accuracy: 0.9201 - loss: 0.2099\n",
      " training set -> batch:1717 loss:0.2112373560667038 and acc: 0.9206204414367676\n",
      "1717/2105 [=======================>......] - ETA: 35:51 - accuracy: 0.9206 - loss: 0.2112\n",
      " training set -> batch:1718 loss:0.2166614532470703 and acc: 0.9193262457847595\n",
      "1718/2105 [=======================>......] - ETA: 35:44 - accuracy: 0.9193 - loss: 0.2167\n",
      " training set -> batch:1719 loss:0.2145431786775589 and acc: 0.9189655184745789\n",
      "1719/2105 [=======================>......] - ETA: 35:38 - accuracy: 0.9190 - loss: 0.2145\n",
      " training set -> batch:1720 loss:0.2197321057319641 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1720 val loss:0.2144092470407486 and val acc: 0.9162843823432922\n",
      "1720/2105 [=======================>......] - ETA: 35:39 - accuracy: 0.9186 - loss: 0.2197\n",
      " training set -> batch:1721 loss:0.21454830467700958 and acc: 0.9159291982650757\n",
      "1721/2105 [=======================>......] - ETA: 35:32 - accuracy: 0.9159 - loss: 0.2145\n",
      " training set -> batch:1722 loss:0.2066325545310974 and acc: 0.9177350401878357\n",
      "1722/2105 [=======================>......] - ETA: 35:26 - accuracy: 0.9177 - loss: 0.2066\n",
      " training set -> batch:1723 loss:0.20022979378700256 and acc: 0.9204545617103577\n",
      "1723/2105 [=======================>......] - ETA: 35:20 - accuracy: 0.9205 - loss: 0.2002\n",
      " training set -> batch:1724 loss:0.2005499005317688 and acc: 0.921999990940094\n",
      "1724/2105 [=======================>......] - ETA: 35:14 - accuracy: 0.9220 - loss: 0.2005\n",
      " training set -> batch:1725 loss:0.1967359036207199 and acc: 0.9234496355056763\n",
      "1725/2105 [=======================>......] - ETA: 35:07 - accuracy: 0.9234 - loss: 0.1967\n",
      " training set -> batch:1726 loss:0.1917821615934372 and acc: 0.9248120188713074\n",
      "1726/2105 [=======================>......] - ETA: 35:01 - accuracy: 0.9248 - loss: 0.1918\n",
      " training set -> batch:1727 loss:0.2056785523891449 and acc: 0.9215328693389893\n",
      "1727/2105 [=======================>......] - ETA: 34:55 - accuracy: 0.9215 - loss: 0.2057\n",
      " training set -> batch:1728 loss:0.20649176836013794 and acc: 0.9219858050346375\n",
      "1728/2105 [=======================>......] - ETA: 34:49 - accuracy: 0.9220 - loss: 0.2065\n",
      " training set -> batch:1729 loss:0.20858389139175415 and acc: 0.9215517044067383\n",
      "1729/2105 [=======================>......] - ETA: 34:43 - accuracy: 0.9216 - loss: 0.2086\n",
      " training set -> batch:1730 loss:0.20321376621723175 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1730 val loss:0.21397140622138977 and val acc: 0.9151375889778137\n",
      "1730/2105 [=======================>......] - ETA: 34:43 - accuracy: 0.9237 - loss: 0.2032\n",
      " training set -> batch:1731 loss:0.2286914736032486 and acc: 0.9126105904579163\n",
      "1731/2105 [=======================>......] - ETA: 34:37 - accuracy: 0.9126 - loss: 0.2287\n",
      " training set -> batch:1732 loss:0.22395837306976318 and acc: 0.9134615659713745\n",
      "1732/2105 [=======================>......] - ETA: 34:30 - accuracy: 0.9135 - loss: 0.2240\n",
      " training set -> batch:1733 loss:0.2218349725008011 and acc: 0.913223147392273\n",
      "1733/2105 [=======================>......] - ETA: 34:24 - accuracy: 0.9132 - loss: 0.2218\n",
      " training set -> batch:1734 loss:0.21740588545799255 and acc: 0.9150000214576721\n",
      "1734/2105 [=======================>......] - ETA: 34:18 - accuracy: 0.9150 - loss: 0.2174\n",
      " training set -> batch:1735 loss:0.21650677919387817 and acc: 0.9127907156944275\n",
      "1735/2105 [=======================>......] - ETA: 34:12 - accuracy: 0.9128 - loss: 0.2165\n",
      " training set -> batch:1736 loss:0.21075725555419922 and acc: 0.9144737124443054\n",
      "1736/2105 [=======================>......] - ETA: 34:06 - accuracy: 0.9145 - loss: 0.2108\n",
      " training set -> batch:1737 loss:0.210650235414505 and acc: 0.915145993232727\n",
      "1737/2105 [=======================>......] - ETA: 33:59 - accuracy: 0.9151 - loss: 0.2107\n",
      " training set -> batch:1738 loss:0.20826616883277893 and acc: 0.9157801270484924\n",
      "1738/2105 [=======================>......] - ETA: 33:53 - accuracy: 0.9158 - loss: 0.2083\n",
      " training set -> batch:1739 loss:0.2022847682237625 and acc: 0.9181034564971924\n",
      "1739/2105 [=======================>......] - ETA: 33:47 - accuracy: 0.9181 - loss: 0.2023\n",
      " training set -> batch:1740 loss:0.2043197602033615 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1740 val loss:0.2112630158662796 and val acc: 0.9197247624397278\n",
      "1740/2105 [=======================>......] - ETA: 33:47 - accuracy: 0.9178 - loss: 0.2043\n",
      " training set -> batch:1741 loss:0.20649981498718262 and acc: 0.9192478060722351\n",
      "1741/2105 [=======================>......] - ETA: 33:41 - accuracy: 0.9192 - loss: 0.2065\n",
      " training set -> batch:1742 loss:0.1964525431394577 and acc: 0.9220085740089417\n",
      "1742/2105 [=======================>......] - ETA: 33:35 - accuracy: 0.9220 - loss: 0.1965\n",
      " training set -> batch:1743 loss:0.19378624856472015 and acc: 0.922520637512207\n",
      "1743/2105 [=======================>......] - ETA: 33:29 - accuracy: 0.9225 - loss: 0.1938\n",
      " training set -> batch:1744 loss:0.19170504808425903 and acc: 0.9229999780654907\n",
      "1744/2105 [=======================>......] - ETA: 33:23 - accuracy: 0.9230 - loss: 0.1917\n",
      " training set -> batch:1745 loss:0.1842837631702423 and acc: 0.9253876209259033\n",
      "1745/2105 [=======================>......] - ETA: 33:16 - accuracy: 0.9254 - loss: 0.1843\n",
      " training set -> batch:1746 loss:0.19150231778621674 and acc: 0.9248120188713074\n",
      "1746/2105 [=======================>......] - ETA: 33:10 - accuracy: 0.9248 - loss: 0.1915\n",
      " training set -> batch:1747 loss:0.20067818462848663 and acc: 0.9251824617385864\n",
      "1747/2105 [=======================>......] - ETA: 33:04 - accuracy: 0.9252 - loss: 0.2007\n",
      " training set -> batch:1748 loss:0.19735917448997498 and acc: 0.9255319237709045\n",
      "1748/2105 [=======================>......] - ETA: 32:58 - accuracy: 0.9255 - loss: 0.1974\n",
      " training set -> batch:1749 loss:0.19428129494190216 and acc: 0.9267241358757019\n",
      "1749/2105 [=======================>......] - ETA: 32:52 - accuracy: 0.9267 - loss: 0.1943\n",
      " training set -> batch:1750 loss:0.19413672387599945 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1750 val loss:0.2142094224691391 and val acc: 0.9162843823432922\n",
      "1750/2105 [=======================>......] - ETA: 32:52 - accuracy: 0.9270 - loss: 0.1941\n",
      " training set -> batch:1751 loss:0.21222110092639923 and acc: 0.9170354008674622\n",
      "1751/2105 [=======================>......] - ETA: 32:45 - accuracy: 0.9170 - loss: 0.2122\n",
      " training set -> batch:1752 loss:0.2109466791152954 and acc: 0.9166666865348816\n",
      "1752/2105 [=======================>......] - ETA: 32:39 - accuracy: 0.9167 - loss: 0.2109\n",
      " training set -> batch:1753 loss:0.21178071200847626 and acc: 0.9163222908973694\n",
      "1753/2105 [=======================>......] - ETA: 32:33 - accuracy: 0.9163 - loss: 0.2118\n",
      " training set -> batch:1754 loss:0.2099481225013733 and acc: 0.9169999957084656\n",
      "1754/2105 [=======================>......] - ETA: 32:27 - accuracy: 0.9170 - loss: 0.2099\n",
      " training set -> batch:1755 loss:0.21257390081882477 and acc: 0.9176356792449951\n",
      "1755/2105 [========================>.....] - ETA: 32:21 - accuracy: 0.9176 - loss: 0.2126\n",
      " training set -> batch:1756 loss:0.21557290852069855 and acc: 0.9172932505607605\n",
      "1756/2105 [========================>.....] - ETA: 32:15 - accuracy: 0.9173 - loss: 0.2156\n",
      " training set -> batch:1757 loss:0.22263409197330475 and acc: 0.9169707894325256\n",
      "1757/2105 [========================>.....] - ETA: 32:08 - accuracy: 0.9170 - loss: 0.2226\n",
      " training set -> batch:1758 loss:0.2147446572780609 and acc: 0.9193262457847595\n",
      "1758/2105 [========================>.....] - ETA: 32:02 - accuracy: 0.9193 - loss: 0.2147\n",
      " training set -> batch:1759 loss:0.21633853018283844 and acc: 0.9189655184745789\n",
      "1759/2105 [========================>.....] - ETA: 31:56 - accuracy: 0.9190 - loss: 0.2163\n",
      " training set -> batch:1760 loss:0.21203194558620453 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1760 val loss:0.21549741923809052 and val acc: 0.9151375889778137\n",
      "1760/2105 [========================>.....] - ETA: 31:56 - accuracy: 0.9195 - loss: 0.2120\n",
      " training set -> batch:1761 loss:0.21063455939292908 and acc: 0.9159291982650757\n",
      "1761/2105 [========================>.....] - ETA: 31:50 - accuracy: 0.9159 - loss: 0.2106\n",
      " training set -> batch:1762 loss:0.2083045393228531 and acc: 0.9155982732772827\n",
      "1762/2105 [========================>.....] - ETA: 31:44 - accuracy: 0.9156 - loss: 0.2083\n",
      " training set -> batch:1763 loss:0.207334965467453 and acc: 0.9163222908973694\n",
      "1763/2105 [========================>.....] - ETA: 31:37 - accuracy: 0.9163 - loss: 0.2073\n",
      " training set -> batch:1764 loss:0.20569579303264618 and acc: 0.9169999957084656\n",
      "1764/2105 [========================>.....] - ETA: 31:31 - accuracy: 0.9170 - loss: 0.2057\n",
      " training set -> batch:1765 loss:0.20499826967716217 and acc: 0.9166666865348816\n",
      "1765/2105 [========================>.....] - ETA: 31:25 - accuracy: 0.9167 - loss: 0.2050\n",
      " training set -> batch:1766 loss:0.20251503586769104 and acc: 0.9172932505607605\n",
      "1766/2105 [========================>.....] - ETA: 31:19 - accuracy: 0.9173 - loss: 0.2025\n",
      " training set -> batch:1767 loss:0.20507317781448364 and acc: 0.9178832173347473\n",
      "1767/2105 [========================>.....] - ETA: 31:13 - accuracy: 0.9179 - loss: 0.2051\n",
      " training set -> batch:1768 loss:0.20171195268630981 and acc: 0.9184397459030151\n",
      "1768/2105 [========================>.....] - ETA: 31:07 - accuracy: 0.9184 - loss: 0.2017\n",
      " training set -> batch:1769 loss:0.1999974250793457 and acc: 0.9189655184745789\n",
      "1769/2105 [========================>.....] - ETA: 31:01 - accuracy: 0.9190 - loss: 0.2000\n",
      " training set -> batch:1770 loss:0.20211364328861237 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1770 val loss:0.22273223102092743 and val acc: 0.9071100950241089\n",
      "1770/2105 [========================>.....] - ETA: 31:00 - accuracy: 0.9195 - loss: 0.2021\n",
      " training set -> batch:1771 loss:0.21542295813560486 and acc: 0.9092920422554016\n",
      "1771/2105 [========================>.....] - ETA: 30:54 - accuracy: 0.9093 - loss: 0.2154\n",
      " training set -> batch:1772 loss:0.21171624958515167 and acc: 0.9102563858032227\n",
      "1772/2105 [========================>.....] - ETA: 30:48 - accuracy: 0.9103 - loss: 0.2117\n",
      " training set -> batch:1773 loss:0.20919393002986908 and acc: 0.9111570119857788\n",
      "1773/2105 [========================>.....] - ETA: 30:42 - accuracy: 0.9112 - loss: 0.2092\n",
      " training set -> batch:1774 loss:0.20090995728969574 and acc: 0.9139999747276306\n",
      "1774/2105 [========================>.....] - ETA: 30:36 - accuracy: 0.9140 - loss: 0.2009\n",
      " training set -> batch:1775 loss:0.19290883839130402 and acc: 0.9166666865348816\n",
      "1775/2105 [========================>.....] - ETA: 30:30 - accuracy: 0.9167 - loss: 0.1929\n",
      " training set -> batch:1776 loss:0.19508615136146545 and acc: 0.9154135584831238\n",
      "1776/2105 [========================>.....] - ETA: 30:24 - accuracy: 0.9154 - loss: 0.1951\n",
      " training set -> batch:1777 loss:0.1902584731578827 and acc: 0.9178832173347473\n",
      "1777/2105 [========================>.....] - ETA: 30:17 - accuracy: 0.9179 - loss: 0.1903\n",
      " training set -> batch:1778 loss:0.19389086961746216 and acc: 0.917553186416626\n",
      "1778/2105 [========================>.....] - ETA: 30:11 - accuracy: 0.9176 - loss: 0.1939\n",
      " training set -> batch:1779 loss:0.1946110725402832 and acc: 0.9172413945198059\n",
      "1779/2105 [========================>.....] - ETA: 30:05 - accuracy: 0.9172 - loss: 0.1946\n",
      " training set -> batch:1780 loss:0.18897877633571625 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1780 val loss:0.20392939448356628 and val acc: 0.911697268486023\n",
      "1780/2105 [========================>.....] - ETA: 30:05 - accuracy: 0.9195 - loss: 0.1890\n",
      " training set -> batch:1781 loss:0.20264486968517303 and acc: 0.9103982448577881\n",
      "1781/2105 [========================>.....] - ETA: 29:59 - accuracy: 0.9104 - loss: 0.2026\n",
      " training set -> batch:1782 loss:0.19709442555904388 and acc: 0.9113247990608215\n",
      "1782/2105 [========================>.....] - ETA: 29:52 - accuracy: 0.9113 - loss: 0.1971\n",
      " training set -> batch:1783 loss:0.19730211794376373 and acc: 0.9111570119857788\n",
      "1783/2105 [========================>.....] - ETA: 29:46 - accuracy: 0.9112 - loss: 0.1973\n",
      " training set -> batch:1784 loss:0.20166164636611938 and acc: 0.9100000262260437\n",
      "1784/2105 [========================>.....] - ETA: 29:40 - accuracy: 0.9100 - loss: 0.2017\n",
      " training set -> batch:1785 loss:0.20371553301811218 and acc: 0.9089147448539734\n",
      "1785/2105 [========================>.....] - ETA: 29:34 - accuracy: 0.9089 - loss: 0.2037\n",
      " training set -> batch:1786 loss:0.20530498027801514 and acc: 0.9107142686843872\n",
      "1786/2105 [========================>.....] - ETA: 29:28 - accuracy: 0.9107 - loss: 0.2053\n",
      " training set -> batch:1787 loss:0.22249753773212433 and acc: 0.9069343209266663\n",
      "1787/2105 [========================>.....] - ETA: 29:22 - accuracy: 0.9069 - loss: 0.2225\n",
      " training set -> batch:1788 loss:0.21537967026233673 and acc: 0.9095744490623474\n",
      "1788/2105 [========================>.....] - ETA: 29:16 - accuracy: 0.9096 - loss: 0.2154\n",
      " training set -> batch:1789 loss:0.21349860727787018 and acc: 0.9094827771186829\n",
      "1789/2105 [========================>.....] - ETA: 29:10 - accuracy: 0.9095 - loss: 0.2135\n",
      " training set -> batch:1790 loss:0.210272416472435 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1790 val loss:0.19767650961875916 and val acc: 0.91399085521698\n",
      "1790/2105 [========================>.....] - ETA: 29:09 - accuracy: 0.9111 - loss: 0.2103\n",
      " training set -> batch:1791 loss:0.19136497378349304 and acc: 0.9159291982650757\n",
      "1791/2105 [========================>.....] - ETA: 29:03 - accuracy: 0.9159 - loss: 0.1914\n",
      " training set -> batch:1792 loss:0.18926121294498444 and acc: 0.9155982732772827\n",
      "1792/2105 [========================>.....] - ETA: 28:57 - accuracy: 0.9156 - loss: 0.1893\n",
      " training set -> batch:1793 loss:0.18748217821121216 and acc: 0.9173553586006165\n",
      "1793/2105 [========================>.....] - ETA: 28:51 - accuracy: 0.9174 - loss: 0.1875\n",
      " training set -> batch:1794 loss:0.18200400471687317 and acc: 0.9200000166893005\n",
      "1794/2105 [========================>.....] - ETA: 28:45 - accuracy: 0.9200 - loss: 0.1820\n",
      " training set -> batch:1795 loss:0.17784471809864044 and acc: 0.9215116500854492\n",
      "1795/2105 [========================>.....] - ETA: 28:39 - accuracy: 0.9215 - loss: 0.1778\n",
      " training set -> batch:1796 loss:0.1737840175628662 and acc: 0.9229323267936707\n",
      "1796/2105 [========================>.....] - ETA: 28:32 - accuracy: 0.9229 - loss: 0.1738\n",
      " training set -> batch:1797 loss:0.17099441587924957 and acc: 0.9242700934410095\n",
      "1797/2105 [========================>.....] - ETA: 28:26 - accuracy: 0.9243 - loss: 0.1710\n",
      " training set -> batch:1798 loss:0.16867554187774658 and acc: 0.9255319237709045\n",
      "1798/2105 [========================>.....] - ETA: 28:20 - accuracy: 0.9255 - loss: 0.1687\n",
      " training set -> batch:1799 loss:0.16837498545646667 and acc: 0.9258620738983154\n",
      "1799/2105 [========================>.....] - ETA: 28:14 - accuracy: 0.9259 - loss: 0.1684\n",
      " training set -> batch:1800 loss:0.17396914958953857 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1800 val loss:0.20247791707515717 and val acc: 0.91399085521698\n",
      "1800/2105 [========================>.....] - ETA: 28:13 - accuracy: 0.9253 - loss: 0.1740\n",
      " training set -> batch:1801 loss:0.2030816525220871 and acc: 0.9148229956626892\n",
      "1801/2105 [========================>.....] - ETA: 28:07 - accuracy: 0.9148 - loss: 0.2031\n",
      " training set -> batch:1802 loss:0.20380225777626038 and acc: 0.9155982732772827\n",
      "1802/2105 [========================>.....] - ETA: 28:01 - accuracy: 0.9156 - loss: 0.2038\n",
      " training set -> batch:1803 loss:0.2014891505241394 and acc: 0.9152892827987671\n",
      "1803/2105 [========================>.....] - ETA: 27:55 - accuracy: 0.9153 - loss: 0.2015\n",
      " training set -> batch:1804 loss:0.204342320561409 and acc: 0.9150000214576721\n",
      "1804/2105 [========================>.....] - ETA: 27:49 - accuracy: 0.9150 - loss: 0.2043\n",
      " training set -> batch:1805 loss:0.19540943205356598 and acc: 0.9176356792449951\n",
      "1805/2105 [========================>.....] - ETA: 27:43 - accuracy: 0.9176 - loss: 0.1954\n",
      " training set -> batch:1806 loss:0.1970529854297638 and acc: 0.9163534045219421\n",
      "1806/2105 [========================>.....] - ETA: 27:37 - accuracy: 0.9164 - loss: 0.1971\n",
      " training set -> batch:1807 loss:0.19205452501773834 and acc: 0.9178832173347473\n",
      "1807/2105 [========================>.....] - ETA: 27:31 - accuracy: 0.9179 - loss: 0.1921\n",
      " training set -> batch:1808 loss:0.1878822296857834 and acc: 0.9184397459030151\n",
      "1808/2105 [========================>.....] - ETA: 27:25 - accuracy: 0.9184 - loss: 0.1879\n",
      " training set -> batch:1809 loss:0.1917099505662918 and acc: 0.9163793325424194\n",
      "1809/2105 [========================>.....] - ETA: 27:19 - accuracy: 0.9164 - loss: 0.1917\n",
      " training set -> batch:1810 loss:0.1973777413368225 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1810 val loss:0.2099781483411789 and val acc: 0.9151375889778137\n",
      "1810/2105 [========================>.....] - ETA: 27:18 - accuracy: 0.9161 - loss: 0.1974\n",
      " training set -> batch:1811 loss:0.20209284126758575 and acc: 0.9170354008674622\n",
      "1811/2105 [========================>.....] - ETA: 27:12 - accuracy: 0.9170 - loss: 0.2021\n",
      " training set -> batch:1812 loss:0.19863027334213257 and acc: 0.9177350401878357\n",
      "1812/2105 [========================>.....] - ETA: 27:06 - accuracy: 0.9177 - loss: 0.1986\n",
      " training set -> batch:1813 loss:0.20159578323364258 and acc: 0.9173553586006165\n",
      "1813/2105 [========================>.....] - ETA: 27:00 - accuracy: 0.9174 - loss: 0.2016\n",
      " training set -> batch:1814 loss:0.1953590363264084 and acc: 0.9200000166893005\n",
      "1814/2105 [========================>.....] - ETA: 26:54 - accuracy: 0.9200 - loss: 0.1954\n",
      " training set -> batch:1815 loss:0.18885894119739532 and acc: 0.9215116500854492\n",
      "1815/2105 [========================>.....] - ETA: 26:48 - accuracy: 0.9215 - loss: 0.1889\n",
      " training set -> batch:1816 loss:0.1871996521949768 and acc: 0.9210526347160339\n",
      "1816/2105 [========================>.....] - ETA: 26:42 - accuracy: 0.9211 - loss: 0.1872\n",
      " training set -> batch:1817 loss:0.191998690366745 and acc: 0.9206204414367676\n",
      "1817/2105 [========================>.....] - ETA: 26:36 - accuracy: 0.9206 - loss: 0.1920\n",
      " training set -> batch:1818 loss:0.18677020072937012 and acc: 0.9219858050346375\n",
      "1818/2105 [========================>.....] - ETA: 26:30 - accuracy: 0.9220 - loss: 0.1868\n",
      " training set -> batch:1819 loss:0.1863015592098236 and acc: 0.9224137663841248\n",
      "1819/2105 [========================>.....] - ETA: 26:24 - accuracy: 0.9224 - loss: 0.1863\n",
      " training set -> batch:1820 loss:0.1829259991645813 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1820 val loss:0.2086935043334961 and val acc: 0.9071100950241089\n",
      "1820/2105 [========================>.....] - ETA: 26:22 - accuracy: 0.9237 - loss: 0.1829\n",
      " training set -> batch:1821 loss:0.21090134978294373 and acc: 0.9081858396530151\n",
      "1821/2105 [========================>.....] - ETA: 26:16 - accuracy: 0.9082 - loss: 0.2109\n",
      " training set -> batch:1822 loss:0.20523478090763092 and acc: 0.9102563858032227\n",
      "1822/2105 [========================>.....] - ETA: 26:10 - accuracy: 0.9103 - loss: 0.2052\n",
      " training set -> batch:1823 loss:0.20280790328979492 and acc: 0.9101239442825317\n",
      "1823/2105 [========================>.....] - ETA: 26:04 - accuracy: 0.9101 - loss: 0.2028\n",
      " training set -> batch:1824 loss:0.19869212806224823 and acc: 0.9120000004768372\n",
      "1824/2105 [========================>.....] - ETA: 25:58 - accuracy: 0.9120 - loss: 0.1987\n",
      " training set -> batch:1825 loss:0.20557942986488342 and acc: 0.911821722984314\n",
      "1825/2105 [=========================>....] - ETA: 25:52 - accuracy: 0.9118 - loss: 0.2056\n",
      " training set -> batch:1826 loss:0.20020630955696106 and acc: 0.9135338068008423\n",
      "1826/2105 [=========================>....] - ETA: 25:46 - accuracy: 0.9135 - loss: 0.2002\n",
      " training set -> batch:1827 loss:0.1954904943704605 and acc: 0.915145993232727\n",
      "1827/2105 [=========================>....] - ETA: 25:40 - accuracy: 0.9151 - loss: 0.1955\n",
      " training set -> batch:1828 loss:0.19859404861927032 and acc: 0.9131205677986145\n",
      "1828/2105 [=========================>....] - ETA: 25:34 - accuracy: 0.9131 - loss: 0.1986\n",
      " training set -> batch:1829 loss:0.20022518932819366 and acc: 0.9129310250282288\n",
      "1829/2105 [=========================>....] - ETA: 25:28 - accuracy: 0.9129 - loss: 0.2002\n",
      " training set -> batch:1830 loss:0.19839073717594147 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:1830 val loss:0.2087710201740265 and val acc: 0.9197247624397278\n",
      "1830/2105 [=========================>....] - ETA: 25:27 - accuracy: 0.9128 - loss: 0.1984\n",
      " training set -> batch:1831 loss:0.2032506912946701 and acc: 0.9214601516723633\n",
      "1831/2105 [=========================>....] - ETA: 25:21 - accuracy: 0.9215 - loss: 0.2033\n",
      " training set -> batch:1832 loss:0.20096361637115479 and acc: 0.9220085740089417\n",
      "1832/2105 [=========================>....] - ETA: 25:15 - accuracy: 0.9220 - loss: 0.2010\n",
      " training set -> batch:1833 loss:0.19590291380882263 and acc: 0.9235537052154541\n",
      "1833/2105 [=========================>....] - ETA: 25:09 - accuracy: 0.9236 - loss: 0.1959\n",
      " training set -> batch:1834 loss:0.19380827248096466 and acc: 0.9229999780654907\n",
      "1834/2105 [=========================>....] - ETA: 25:03 - accuracy: 0.9230 - loss: 0.1938\n",
      " training set -> batch:1835 loss:0.19557304680347443 and acc: 0.9234496355056763\n",
      "1835/2105 [=========================>....] - ETA: 24:57 - accuracy: 0.9234 - loss: 0.1956\n",
      " training set -> batch:1836 loss:0.19892549514770508 and acc: 0.9219924807548523\n",
      "1836/2105 [=========================>....] - ETA: 24:51 - accuracy: 0.9220 - loss: 0.1989\n",
      " training set -> batch:1837 loss:0.19734959304332733 and acc: 0.9233576655387878\n",
      "1837/2105 [=========================>....] - ETA: 24:45 - accuracy: 0.9234 - loss: 0.1973\n",
      " training set -> batch:1838 loss:0.19680285453796387 and acc: 0.9228723645210266\n",
      "1838/2105 [=========================>....] - ETA: 24:39 - accuracy: 0.9229 - loss: 0.1968\n",
      " training set -> batch:1839 loss:0.19432048499584198 and acc: 0.9241379499435425\n",
      "1839/2105 [=========================>....] - ETA: 24:33 - accuracy: 0.9241 - loss: 0.1943\n",
      " training set -> batch:1840 loss:0.19222088158130646 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1840 val loss:0.21727405488491058 and val acc: 0.9151375889778137\n",
      "1840/2105 [=========================>....] - ETA: 24:31 - accuracy: 0.9253 - loss: 0.1922\n",
      " training set -> batch:1841 loss:0.22154836356639862 and acc: 0.9137167930603027\n",
      "1841/2105 [=========================>....] - ETA: 24:25 - accuracy: 0.9137 - loss: 0.2215\n",
      " training set -> batch:1842 loss:0.21141520142555237 and acc: 0.9155982732772827\n",
      "1842/2105 [=========================>....] - ETA: 24:19 - accuracy: 0.9156 - loss: 0.2114\n",
      " training set -> batch:1843 loss:0.21886953711509705 and acc: 0.913223147392273\n",
      "1843/2105 [=========================>....] - ETA: 24:13 - accuracy: 0.9132 - loss: 0.2189\n",
      " training set -> batch:1844 loss:0.23002086579799652 and acc: 0.9110000133514404\n",
      "1844/2105 [=========================>....] - ETA: 24:07 - accuracy: 0.9110 - loss: 0.2300\n",
      " training set -> batch:1845 loss:0.22697852551937103 and acc: 0.911821722984314\n",
      "1845/2105 [=========================>....] - ETA: 24:01 - accuracy: 0.9118 - loss: 0.2270\n",
      " training set -> batch:1846 loss:0.2333732545375824 and acc: 0.9088345766067505\n",
      "1846/2105 [=========================>....] - ETA: 23:55 - accuracy: 0.9088 - loss: 0.2334\n",
      " training set -> batch:1847 loss:0.22935771942138672 and acc: 0.9105839133262634\n",
      "1847/2105 [=========================>....] - ETA: 23:49 - accuracy: 0.9106 - loss: 0.2294\n",
      " training set -> batch:1848 loss:0.2255462259054184 and acc: 0.9122340679168701\n",
      "1848/2105 [=========================>....] - ETA: 23:43 - accuracy: 0.9122 - loss: 0.2255\n",
      " training set -> batch:1849 loss:0.2214626669883728 and acc: 0.9129310250282288\n",
      "1849/2105 [=========================>....] - ETA: 23:37 - accuracy: 0.9129 - loss: 0.2215\n",
      " training set -> batch:1850 loss:0.22333814203739166 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:1850 val loss:0.22308091819286346 and val acc: 0.91399085521698\n",
      "1850/2105 [=========================>....] - ETA: 23:35 - accuracy: 0.9119 - loss: 0.2233\n",
      " training set -> batch:1851 loss:0.21566329896450043 and acc: 0.9170354008674622\n",
      "1851/2105 [=========================>....] - ETA: 23:30 - accuracy: 0.9170 - loss: 0.2157\n",
      " training set -> batch:1852 loss:0.2195213884115219 and acc: 0.9188033938407898\n",
      "1852/2105 [=========================>....] - ETA: 23:24 - accuracy: 0.9188 - loss: 0.2195\n",
      " training set -> batch:1853 loss:0.21063025295734406 and acc: 0.9214876294136047\n",
      "1853/2105 [=========================>....] - ETA: 23:18 - accuracy: 0.9215 - loss: 0.2106\n",
      " training set -> batch:1854 loss:0.21224378049373627 and acc: 0.9200000166893005\n",
      "1854/2105 [=========================>....] - ETA: 23:12 - accuracy: 0.9200 - loss: 0.2122\n",
      " training set -> batch:1855 loss:0.20503774285316467 and acc: 0.9224806427955627\n",
      "1855/2105 [=========================>....] - ETA: 23:06 - accuracy: 0.9225 - loss: 0.2050\n",
      " training set -> batch:1856 loss:0.20510491728782654 and acc: 0.9229323267936707\n",
      "1856/2105 [=========================>....] - ETA: 23:00 - accuracy: 0.9229 - loss: 0.2051\n",
      " training set -> batch:1857 loss:0.20216520130634308 and acc: 0.9242700934410095\n",
      "1857/2105 [=========================>....] - ETA: 22:54 - accuracy: 0.9243 - loss: 0.2022\n",
      " training set -> batch:1858 loss:0.20479275286197662 and acc: 0.9246453642845154\n",
      "1858/2105 [=========================>....] - ETA: 22:48 - accuracy: 0.9246 - loss: 0.2048\n",
      " training set -> batch:1859 loss:0.20217441022396088 and acc: 0.925000011920929\n",
      "1859/2105 [=========================>....] - ETA: 22:42 - accuracy: 0.9250 - loss: 0.2022\n",
      " training set -> batch:1860 loss:0.19516855478286743 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1860 val loss:0.21326944231987 and val acc: 0.911697268486023\n",
      "1860/2105 [=========================>....] - ETA: 22:40 - accuracy: 0.9270 - loss: 0.1952\n",
      " training set -> batch:1861 loss:0.20300112664699554 and acc: 0.9148229956626892\n",
      "1861/2105 [=========================>....] - ETA: 22:34 - accuracy: 0.9148 - loss: 0.2030\n",
      " training set -> batch:1862 loss:0.21838676929473877 and acc: 0.9134615659713745\n",
      "1862/2105 [=========================>....] - ETA: 22:28 - accuracy: 0.9135 - loss: 0.2184\n",
      " training set -> batch:1863 loss:0.21477442979812622 and acc: 0.91425621509552\n",
      "1863/2105 [=========================>....] - ETA: 22:22 - accuracy: 0.9143 - loss: 0.2148\n",
      " training set -> batch:1864 loss:0.20817391574382782 and acc: 0.9160000085830688\n",
      "1864/2105 [=========================>....] - ETA: 22:16 - accuracy: 0.9160 - loss: 0.2082\n",
      " training set -> batch:1865 loss:0.20859260857105255 and acc: 0.9147287011146545\n",
      "1865/2105 [=========================>....] - ETA: 22:10 - accuracy: 0.9147 - loss: 0.2086\n",
      " training set -> batch:1866 loss:0.2024473398923874 and acc: 0.9163534045219421\n",
      "1866/2105 [=========================>....] - ETA: 22:04 - accuracy: 0.9164 - loss: 0.2024\n",
      " training set -> batch:1867 loss:0.2009812295436859 and acc: 0.9169707894325256\n",
      "1867/2105 [=========================>....] - ETA: 21:58 - accuracy: 0.9170 - loss: 0.2010\n",
      " training set -> batch:1868 loss:0.1942795217037201 and acc: 0.9193262457847595\n",
      "1868/2105 [=========================>....] - ETA: 21:52 - accuracy: 0.9193 - loss: 0.1943\n",
      " training set -> batch:1869 loss:0.19349214434623718 and acc: 0.9198275804519653\n",
      "1869/2105 [=========================>....] - ETA: 21:46 - accuracy: 0.9198 - loss: 0.1935\n",
      " training set -> batch:1870 loss:0.1881314367055893 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1870 val loss:0.22876989841461182 and val acc: 0.9105504751205444\n",
      "1870/2105 [=========================>....] - ETA: 21:44 - accuracy: 0.9203 - loss: 0.1881\n",
      " training set -> batch:1871 loss:0.2203029841184616 and acc: 0.9126105904579163\n",
      "1871/2105 [=========================>....] - ETA: 21:38 - accuracy: 0.9126 - loss: 0.2203\n",
      " training set -> batch:1872 loss:0.2147923856973648 and acc: 0.9145299196243286\n",
      "1872/2105 [=========================>....] - ETA: 21:32 - accuracy: 0.9145 - loss: 0.2148\n",
      " training set -> batch:1873 loss:0.20670731365680695 and acc: 0.9163222908973694\n",
      "1873/2105 [=========================>....] - ETA: 21:27 - accuracy: 0.9163 - loss: 0.2067\n",
      " training set -> batch:1874 loss:0.20247170329093933 and acc: 0.9169999957084656\n",
      "1874/2105 [=========================>....] - ETA: 21:21 - accuracy: 0.9170 - loss: 0.2025\n",
      " training set -> batch:1875 loss:0.20229944586753845 and acc: 0.9176356792449951\n",
      "1875/2105 [=========================>....] - ETA: 21:15 - accuracy: 0.9176 - loss: 0.2023\n",
      " training set -> batch:1876 loss:0.19733096659183502 and acc: 0.9191729426383972\n",
      "1876/2105 [=========================>....] - ETA: 21:09 - accuracy: 0.9192 - loss: 0.1973\n",
      " training set -> batch:1877 loss:0.2011898308992386 and acc: 0.918795645236969\n",
      "1877/2105 [=========================>....] - ETA: 21:03 - accuracy: 0.9188 - loss: 0.2012\n",
      " training set -> batch:1878 loss:0.19449691474437714 and acc: 0.9202127456665039\n",
      "1878/2105 [=========================>....] - ETA: 20:57 - accuracy: 0.9202 - loss: 0.1945\n",
      " training set -> batch:1879 loss:0.1928335428237915 and acc: 0.9215517044067383\n",
      "1879/2105 [=========================>....] - ETA: 20:51 - accuracy: 0.9216 - loss: 0.1928\n",
      " training set -> batch:1880 loss:0.19287581741809845 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1880 val loss:0.2563634216785431 and val acc: 0.9059633016586304\n",
      "1880/2105 [=========================>....] - ETA: 20:49 - accuracy: 0.9220 - loss: 0.1929\n",
      " training set -> batch:1881 loss:0.27202051877975464 and acc: 0.9048672318458557\n",
      "1881/2105 [=========================>....] - ETA: 20:43 - accuracy: 0.9049 - loss: 0.2720\n",
      " training set -> batch:1882 loss:0.26734718680381775 and acc: 0.9049145579338074\n",
      "1882/2105 [=========================>....] - ETA: 20:37 - accuracy: 0.9049 - loss: 0.2673\n",
      " training set -> batch:1883 loss:0.25762638449668884 and acc: 0.9059917330741882\n",
      "1883/2105 [=========================>....] - ETA: 20:31 - accuracy: 0.9060 - loss: 0.2576\n",
      " training set -> batch:1884 loss:0.25494515895843506 and acc: 0.9070000052452087\n",
      "1884/2105 [=========================>....] - ETA: 20:25 - accuracy: 0.9070 - loss: 0.2549\n",
      " training set -> batch:1885 loss:0.2511254847049713 and acc: 0.9069767594337463\n",
      "1885/2105 [=========================>....] - ETA: 20:19 - accuracy: 0.9070 - loss: 0.2511\n",
      " training set -> batch:1886 loss:0.2508222758769989 and acc: 0.9078947305679321\n",
      "1886/2105 [=========================>....] - ETA: 20:13 - accuracy: 0.9079 - loss: 0.2508\n",
      " training set -> batch:1887 loss:0.25784793496131897 and acc: 0.9060218930244446\n",
      "1887/2105 [=========================>....] - ETA: 20:07 - accuracy: 0.9060 - loss: 0.2578\n",
      " training set -> batch:1888 loss:0.25572556257247925 and acc: 0.9078013896942139\n",
      "1888/2105 [=========================>....] - ETA: 20:01 - accuracy: 0.9078 - loss: 0.2557\n",
      " training set -> batch:1889 loss:0.24763156473636627 and acc: 0.9103448390960693\n",
      "1889/2105 [=========================>....] - ETA: 19:56 - accuracy: 0.9103 - loss: 0.2476\n",
      " training set -> batch:1890 loss:0.24790243804454803 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:1890 val loss:0.22241155803203583 and val acc: 0.9151375889778137\n",
      "1890/2105 [=========================>....] - ETA: 19:53 - accuracy: 0.9102 - loss: 0.2479\n",
      " training set -> batch:1891 loss:0.21779298782348633 and acc: 0.9159291982650757\n",
      "1891/2105 [=========================>....] - ETA: 19:47 - accuracy: 0.9159 - loss: 0.2178\n",
      " training set -> batch:1892 loss:0.2152535319328308 and acc: 0.9155982732772827\n",
      "1892/2105 [=========================>....] - ETA: 19:41 - accuracy: 0.9156 - loss: 0.2153\n",
      " training set -> batch:1893 loss:0.21044854819774628 and acc: 0.9163222908973694\n",
      "1893/2105 [=========================>....] - ETA: 19:35 - accuracy: 0.9163 - loss: 0.2104\n",
      " training set -> batch:1894 loss:0.20387955009937286 and acc: 0.9179999828338623\n",
      "1894/2105 [=========================>....] - ETA: 19:29 - accuracy: 0.9180 - loss: 0.2039\n",
      " training set -> batch:1895 loss:0.20360608398914337 and acc: 0.9186046719551086\n",
      "1895/2105 [==========================>...] - ETA: 19:24 - accuracy: 0.9186 - loss: 0.2036\n",
      " training set -> batch:1896 loss:0.2055777609348297 and acc: 0.9182330965995789\n",
      "1896/2105 [==========================>...] - ETA: 19:18 - accuracy: 0.9182 - loss: 0.2056\n",
      " training set -> batch:1897 loss:0.2042931318283081 and acc: 0.9178832173347473\n",
      "1897/2105 [==========================>...] - ETA: 19:12 - accuracy: 0.9179 - loss: 0.2043\n",
      " training set -> batch:1898 loss:0.2005324810743332 and acc: 0.9193262457847595\n",
      "1898/2105 [==========================>...] - ETA: 19:06 - accuracy: 0.9193 - loss: 0.2005\n",
      " training set -> batch:1899 loss:0.19956620037555695 and acc: 0.9198275804519653\n",
      "1899/2105 [==========================>...] - ETA: 19:00 - accuracy: 0.9198 - loss: 0.1996\n",
      " training set -> batch:1900 loss:0.19662272930145264 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1900 val loss:0.21460625529289246 and val acc: 0.9231651425361633\n",
      "1900/2105 [==========================>...] - ETA: 18:57 - accuracy: 0.9211 - loss: 0.1966\n",
      " training set -> batch:1901 loss:0.217672660946846 and acc: 0.9225663542747498\n",
      "1901/2105 [==========================>...] - ETA: 18:52 - accuracy: 0.9226 - loss: 0.2177\n",
      " training set -> batch:1902 loss:0.22083401679992676 and acc: 0.9220085740089417\n",
      "1902/2105 [==========================>...] - ETA: 18:46 - accuracy: 0.9220 - loss: 0.2208\n",
      " training set -> batch:1903 loss:0.21830597519874573 and acc: 0.9214876294136047\n",
      "1903/2105 [==========================>...] - ETA: 18:40 - accuracy: 0.9215 - loss: 0.2183\n",
      " training set -> batch:1904 loss:0.21635794639587402 and acc: 0.9210000038146973\n",
      "1904/2105 [==========================>...] - ETA: 18:34 - accuracy: 0.9210 - loss: 0.2164\n",
      " training set -> batch:1905 loss:0.2201777994632721 and acc: 0.9195736646652222\n",
      "1905/2105 [==========================>...] - ETA: 18:28 - accuracy: 0.9196 - loss: 0.2202\n",
      " training set -> batch:1906 loss:0.21430888772010803 and acc: 0.9201127886772156\n",
      "1906/2105 [==========================>...] - ETA: 18:22 - accuracy: 0.9201 - loss: 0.2143\n",
      " training set -> batch:1907 loss:0.20609524846076965 and acc: 0.9224452376365662\n",
      "1907/2105 [==========================>...] - ETA: 18:16 - accuracy: 0.9224 - loss: 0.2061\n",
      " training set -> batch:1908 loss:0.20209790766239166 and acc: 0.923758864402771\n",
      "1908/2105 [==========================>...] - ETA: 18:10 - accuracy: 0.9238 - loss: 0.2021\n",
      " training set -> batch:1909 loss:0.20181243121623993 and acc: 0.925000011920929\n",
      "1909/2105 [==========================>...] - ETA: 18:05 - accuracy: 0.9250 - loss: 0.2018\n",
      " training set -> batch:1910 loss:0.2048802524805069 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1910 val loss:0.21365739405155182 and val acc: 0.911697268486023\n",
      "1910/2105 [==========================>...] - ETA: 18:02 - accuracy: 0.9237 - loss: 0.2049\n",
      " training set -> batch:1911 loss:0.21109946072101593 and acc: 0.9126105904579163\n",
      "1911/2105 [==========================>...] - ETA: 17:56 - accuracy: 0.9126 - loss: 0.2111\n",
      " training set -> batch:1912 loss:0.20859482884407043 and acc: 0.9145299196243286\n",
      "1912/2105 [==========================>...] - ETA: 17:50 - accuracy: 0.9145 - loss: 0.2086\n",
      " training set -> batch:1913 loss:0.1989646553993225 and acc: 0.9173553586006165\n",
      "1913/2105 [==========================>...] - ETA: 17:44 - accuracy: 0.9174 - loss: 0.1990\n",
      " training set -> batch:1914 loss:0.19302275776863098 and acc: 0.9200000166893005\n",
      "1914/2105 [==========================>...] - ETA: 17:38 - accuracy: 0.9200 - loss: 0.1930\n",
      " training set -> batch:1915 loss:0.1907554268836975 and acc: 0.9205426573753357\n",
      "1915/2105 [==========================>...] - ETA: 17:33 - accuracy: 0.9205 - loss: 0.1908\n",
      " training set -> batch:1916 loss:0.19328229129314423 and acc: 0.9210526347160339\n",
      "1916/2105 [==========================>...] - ETA: 17:27 - accuracy: 0.9211 - loss: 0.1933\n",
      " training set -> batch:1917 loss:0.18639004230499268 and acc: 0.9224452376365662\n",
      "1917/2105 [==========================>...] - ETA: 17:21 - accuracy: 0.9224 - loss: 0.1864\n",
      " training set -> batch:1918 loss:0.1793772131204605 and acc: 0.9246453642845154\n",
      "1918/2105 [==========================>...] - ETA: 17:15 - accuracy: 0.9246 - loss: 0.1794\n",
      " training set -> batch:1919 loss:0.17907504737377167 and acc: 0.9258620738983154\n",
      "1919/2105 [==========================>...] - ETA: 17:09 - accuracy: 0.9259 - loss: 0.1791\n",
      " training set -> batch:1920 loss:0.17566077411174774 and acc: 0.926174521446228\n",
      "\n",
      " validation set -> batch:1920 val loss:0.22878816723823547 and val acc: 0.9059633016586304\n",
      "1920/2105 [==========================>...] - ETA: 17:06 - accuracy: 0.9262 - loss: 0.1757\n",
      " training set -> batch:1921 loss:0.22068388760089874 and acc: 0.9070796370506287\n",
      "1921/2105 [==========================>...] - ETA: 17:00 - accuracy: 0.9071 - loss: 0.2207\n",
      " training set -> batch:1922 loss:0.21363982558250427 and acc: 0.9081196784973145\n",
      "1922/2105 [==========================>...] - ETA: 16:55 - accuracy: 0.9081 - loss: 0.2136\n",
      " training set -> batch:1923 loss:0.21533945202827454 and acc: 0.9080578684806824\n",
      "1923/2105 [==========================>...] - ETA: 16:49 - accuracy: 0.9081 - loss: 0.2153\n",
      " training set -> batch:1924 loss:0.20582780241966248 and acc: 0.9110000133514404\n",
      "1924/2105 [==========================>...] - ETA: 16:43 - accuracy: 0.9110 - loss: 0.2058\n",
      " training set -> batch:1925 loss:0.21774743497371674 and acc: 0.9098837375640869\n",
      "1925/2105 [==========================>...] - ETA: 16:37 - accuracy: 0.9099 - loss: 0.2177\n",
      " training set -> batch:1926 loss:0.21860869228839874 and acc: 0.9097744226455688\n",
      "1926/2105 [==========================>...] - ETA: 16:31 - accuracy: 0.9098 - loss: 0.2186\n",
      " training set -> batch:1927 loss:0.21021707355976105 and acc: 0.9124087691307068\n",
      "1927/2105 [==========================>...] - ETA: 16:25 - accuracy: 0.9124 - loss: 0.2102\n",
      " training set -> batch:1928 loss:0.20459842681884766 and acc: 0.9131205677986145\n",
      "1928/2105 [==========================>...] - ETA: 16:20 - accuracy: 0.9131 - loss: 0.2046\n",
      " training set -> batch:1929 loss:0.203868106007576 and acc: 0.9129310250282288\n",
      "1929/2105 [==========================>...] - ETA: 16:14 - accuracy: 0.9129 - loss: 0.2039\n",
      " training set -> batch:1930 loss:0.20355069637298584 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:1930 val loss:0.20931671559810638 and val acc: 0.9174311757087708\n",
      "1930/2105 [==========================>...] - ETA: 16:11 - accuracy: 0.9128 - loss: 0.2036\n",
      " training set -> batch:1931 loss:0.20543035864830017 and acc: 0.9192478060722351\n",
      "1931/2105 [==========================>...] - ETA: 16:05 - accuracy: 0.9192 - loss: 0.2054\n",
      " training set -> batch:1932 loss:0.2072915881872177 and acc: 0.9188033938407898\n",
      "1932/2105 [==========================>...] - ETA: 15:59 - accuracy: 0.9188 - loss: 0.2073\n",
      " training set -> batch:1933 loss:0.22359491884708405 and acc: 0.9152892827987671\n",
      "1933/2105 [==========================>...] - ETA: 15:53 - accuracy: 0.9153 - loss: 0.2236\n",
      " training set -> batch:1934 loss:0.22211545705795288 and acc: 0.9160000085830688\n",
      "1934/2105 [==========================>...] - ETA: 15:47 - accuracy: 0.9160 - loss: 0.2221\n",
      " training set -> batch:1935 loss:0.22443121671676636 and acc: 0.9166666865348816\n",
      "1935/2105 [==========================>...] - ETA: 15:42 - accuracy: 0.9167 - loss: 0.2244\n",
      " training set -> batch:1936 loss:0.22210188210010529 and acc: 0.9172932505607605\n",
      "1936/2105 [==========================>...] - ETA: 15:36 - accuracy: 0.9173 - loss: 0.2221\n",
      " training set -> batch:1937 loss:0.21726171672344208 and acc: 0.918795645236969\n",
      "1937/2105 [==========================>...] - ETA: 15:30 - accuracy: 0.9188 - loss: 0.2173\n",
      " training set -> batch:1938 loss:0.21978121995925903 and acc: 0.9193262457847595\n",
      "1938/2105 [==========================>...] - ETA: 15:24 - accuracy: 0.9193 - loss: 0.2198\n",
      " training set -> batch:1939 loss:0.21468469500541687 and acc: 0.9198275804519653\n",
      "1939/2105 [==========================>...] - ETA: 15:18 - accuracy: 0.9198 - loss: 0.2147\n",
      " training set -> batch:1940 loss:0.2077925056219101 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1940 val loss:0.212121844291687 and val acc: 0.91399085521698\n",
      "1940/2105 [==========================>...] - ETA: 15:15 - accuracy: 0.9220 - loss: 0.2078\n",
      " training set -> batch:1941 loss:0.2098822146654129 and acc: 0.9137167930603027\n",
      "1941/2105 [==========================>...] - ETA: 15:09 - accuracy: 0.9137 - loss: 0.2099\n",
      " training set -> batch:1942 loss:0.20405213534832 and acc: 0.9155982732772827\n",
      "1942/2105 [==========================>...] - ETA: 15:04 - accuracy: 0.9156 - loss: 0.2041\n",
      " training set -> batch:1943 loss:0.20230264961719513 and acc: 0.9152892827987671\n",
      "1943/2105 [==========================>...] - ETA: 14:58 - accuracy: 0.9153 - loss: 0.2023\n",
      " training set -> batch:1944 loss:0.2003665268421173 and acc: 0.9160000085830688\n",
      "1944/2105 [==========================>...] - ETA: 14:52 - accuracy: 0.9160 - loss: 0.2004\n",
      " training set -> batch:1945 loss:0.19290857017040253 and acc: 0.9186046719551086\n",
      "1945/2105 [==========================>...] - ETA: 14:46 - accuracy: 0.9186 - loss: 0.1929\n",
      " training set -> batch:1946 loss:0.19401463866233826 and acc: 0.9163534045219421\n",
      "1946/2105 [==========================>...] - ETA: 14:40 - accuracy: 0.9164 - loss: 0.1940\n",
      " training set -> batch:1947 loss:0.19405439496040344 and acc: 0.9160584211349487\n",
      "1947/2105 [==========================>...] - ETA: 14:35 - accuracy: 0.9161 - loss: 0.1941\n",
      " training set -> batch:1948 loss:0.19012746214866638 and acc: 0.917553186416626\n",
      "1948/2105 [==========================>...] - ETA: 14:29 - accuracy: 0.9176 - loss: 0.1901\n",
      " training set -> batch:1949 loss:0.18821918964385986 and acc: 0.9189655184745789\n",
      "1949/2105 [==========================>...] - ETA: 14:23 - accuracy: 0.9190 - loss: 0.1882\n",
      " training set -> batch:1950 loss:0.18593977391719818 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1950 val loss:0.21936063468456268 and val acc: 0.91399085521698\n",
      "1950/2105 [==========================>...] - ETA: 14:20 - accuracy: 0.9203 - loss: 0.1859\n",
      " training set -> batch:1951 loss:0.22825311124324799 and acc: 0.9137167930603027\n",
      "1951/2105 [==========================>...] - ETA: 14:14 - accuracy: 0.9137 - loss: 0.2283\n",
      " training set -> batch:1952 loss:0.2261427640914917 and acc: 0.9145299196243286\n",
      "1952/2105 [==========================>...] - ETA: 14:08 - accuracy: 0.9145 - loss: 0.2261\n",
      " training set -> batch:1953 loss:0.225092351436615 and acc: 0.91425621509552\n",
      "1953/2105 [==========================>...] - ETA: 14:02 - accuracy: 0.9143 - loss: 0.2251\n",
      " training set -> batch:1954 loss:0.21720470488071442 and acc: 0.9160000085830688\n",
      "1954/2105 [==========================>...] - ETA: 13:57 - accuracy: 0.9160 - loss: 0.2172\n",
      " training set -> batch:1955 loss:0.21431046724319458 and acc: 0.9176356792449951\n",
      "1955/2105 [==========================>...] - ETA: 13:51 - accuracy: 0.9176 - loss: 0.2143\n",
      " training set -> batch:1956 loss:0.21002721786499023 and acc: 0.9182330965995789\n",
      "1956/2105 [==========================>...] - ETA: 13:45 - accuracy: 0.9182 - loss: 0.2100\n",
      " training set -> batch:1957 loss:0.20562072098255157 and acc: 0.9197080135345459\n",
      "1957/2105 [==========================>...] - ETA: 13:39 - accuracy: 0.9197 - loss: 0.2056\n",
      " training set -> batch:1958 loss:0.19998079538345337 and acc: 0.9219858050346375\n",
      "1958/2105 [==========================>...] - ETA: 13:33 - accuracy: 0.9220 - loss: 0.2000\n",
      " training set -> batch:1959 loss:0.20180703699588776 and acc: 0.9215517044067383\n",
      "1959/2105 [==========================>...] - ETA: 13:28 - accuracy: 0.9216 - loss: 0.2018\n",
      " training set -> batch:1960 loss:0.19719798862934113 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1960 val loss:0.22954913973808289 and val acc: 0.9162843823432922\n",
      "1960/2105 [==========================>...] - ETA: 13:24 - accuracy: 0.9228 - loss: 0.1972\n",
      " training set -> batch:1961 loss:0.23468194901943207 and acc: 0.9159291982650757\n",
      "1961/2105 [==========================>...] - ETA: 13:18 - accuracy: 0.9159 - loss: 0.2347\n",
      " training set -> batch:1962 loss:0.2255246788263321 and acc: 0.9177350401878357\n",
      "1962/2105 [==========================>...] - ETA: 13:13 - accuracy: 0.9177 - loss: 0.2255\n",
      " training set -> batch:1963 loss:0.23585090041160583 and acc: 0.913223147392273\n",
      "1963/2105 [==========================>...] - ETA: 13:07 - accuracy: 0.9132 - loss: 0.2359\n",
      " training set -> batch:1964 loss:0.23192071914672852 and acc: 0.9129999876022339\n",
      "1964/2105 [==========================>...] - ETA: 13:01 - accuracy: 0.9130 - loss: 0.2319\n",
      " training set -> batch:1965 loss:0.2265530824661255 and acc: 0.913759708404541\n",
      "1965/2105 [===========================>..] - ETA: 12:55 - accuracy: 0.9138 - loss: 0.2266\n",
      " training set -> batch:1966 loss:0.2204369306564331 and acc: 0.9154135584831238\n",
      "1966/2105 [===========================>..] - ETA: 12:50 - accuracy: 0.9154 - loss: 0.2204\n",
      " training set -> batch:1967 loss:0.2142026275396347 and acc: 0.9178832173347473\n",
      "1967/2105 [===========================>..] - ETA: 12:44 - accuracy: 0.9179 - loss: 0.2142\n",
      " training set -> batch:1968 loss:0.21542483568191528 and acc: 0.917553186416626\n",
      "1968/2105 [===========================>..] - ETA: 12:38 - accuracy: 0.9176 - loss: 0.2154\n",
      " training set -> batch:1969 loss:0.2098487764596939 and acc: 0.9189655184745789\n",
      "1969/2105 [===========================>..] - ETA: 12:32 - accuracy: 0.9190 - loss: 0.2098\n",
      " training set -> batch:1970 loss:0.20595425367355347 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1970 val loss:0.2804976999759674 and val acc: 0.9013761281967163\n",
      "1970/2105 [===========================>..] - ETA: 12:29 - accuracy: 0.9195 - loss: 0.2060\n",
      " training set -> batch:1971 loss:0.27357402443885803 and acc: 0.9015486836433411\n",
      "1971/2105 [===========================>..] - ETA: 12:23 - accuracy: 0.9015 - loss: 0.2736\n",
      " training set -> batch:1972 loss:0.26572272181510925 and acc: 0.9038461446762085\n",
      "1972/2105 [===========================>..] - ETA: 12:17 - accuracy: 0.9038 - loss: 0.2657\n",
      " training set -> batch:1973 loss:0.2612231373786926 and acc: 0.9049586653709412\n",
      "1973/2105 [===========================>..] - ETA: 12:11 - accuracy: 0.9050 - loss: 0.2612\n",
      " training set -> batch:1974 loss:0.25048887729644775 and acc: 0.9079999923706055\n",
      "1974/2105 [===========================>..] - ETA: 12:06 - accuracy: 0.9080 - loss: 0.2505\n",
      " training set -> batch:1975 loss:0.2622930109500885 and acc: 0.9069767594337463\n",
      "1975/2105 [===========================>..] - ETA: 12:00 - accuracy: 0.9070 - loss: 0.2623\n",
      " training set -> batch:1976 loss:0.2549390196800232 and acc: 0.9088345766067505\n",
      "1976/2105 [===========================>..] - ETA: 11:54 - accuracy: 0.9088 - loss: 0.2549\n",
      " training set -> batch:1977 loss:0.2549540102481842 and acc: 0.9087591171264648\n",
      "1977/2105 [===========================>..] - ETA: 11:49 - accuracy: 0.9088 - loss: 0.2550\n",
      " training set -> batch:1978 loss:0.2454291433095932 and acc: 0.911347508430481\n",
      "1978/2105 [===========================>..] - ETA: 11:43 - accuracy: 0.9113 - loss: 0.2454\n",
      " training set -> batch:1979 loss:0.2459571212530136 and acc: 0.9103448390960693\n",
      "1979/2105 [===========================>..] - ETA: 11:37 - accuracy: 0.9103 - loss: 0.2460\n",
      " training set -> batch:1980 loss:0.24287503957748413 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1980 val loss:0.266771137714386 and val acc: 0.9048165082931519\n",
      "1980/2105 [===========================>..] - ETA: 11:33 - accuracy: 0.9111 - loss: 0.2429\n",
      " training set -> batch:1981 loss:0.25201573967933655 and acc: 0.9081858396530151\n",
      "1981/2105 [===========================>..] - ETA: 11:28 - accuracy: 0.9082 - loss: 0.2520\n",
      " training set -> batch:1982 loss:0.24978500604629517 and acc: 0.9070512652397156\n",
      "1982/2105 [===========================>..] - ETA: 11:22 - accuracy: 0.9071 - loss: 0.2498\n",
      " training set -> batch:1983 loss:0.23976026475429535 and acc: 0.9090909361839294\n",
      "1983/2105 [===========================>..] - ETA: 11:16 - accuracy: 0.9091 - loss: 0.2398\n",
      " training set -> batch:1984 loss:0.23363740742206573 and acc: 0.9110000133514404\n",
      "1984/2105 [===========================>..] - ETA: 11:10 - accuracy: 0.9110 - loss: 0.2336\n",
      " training set -> batch:1985 loss:0.2414131462574005 and acc: 0.9108527302742004\n",
      "1985/2105 [===========================>..] - ETA: 11:05 - accuracy: 0.9109 - loss: 0.2414\n",
      " training set -> batch:1986 loss:0.23814256489276886 and acc: 0.9116541147232056\n",
      "1986/2105 [===========================>..] - ETA: 10:59 - accuracy: 0.9117 - loss: 0.2381\n",
      " training set -> batch:1987 loss:0.23293931782245636 and acc: 0.9124087691307068\n",
      "1987/2105 [===========================>..] - ETA: 10:53 - accuracy: 0.9124 - loss: 0.2329\n",
      " training set -> batch:1988 loss:0.22971811890602112 and acc: 0.9140070676803589\n",
      "1988/2105 [===========================>..] - ETA: 10:47 - accuracy: 0.9140 - loss: 0.2297\n",
      " training set -> batch:1989 loss:0.22619442641735077 and acc: 0.9137930870056152\n",
      "1989/2105 [===========================>..] - ETA: 10:42 - accuracy: 0.9138 - loss: 0.2262\n",
      " training set -> batch:1990 loss:0.2237989902496338 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:1990 val loss:0.23447632789611816 and val acc: 0.911697268486023\n",
      "1990/2105 [===========================>..] - ETA: 10:38 - accuracy: 0.9136 - loss: 0.2238\n",
      " training set -> batch:1991 loss:0.22630524635314941 and acc: 0.9115044474601746\n",
      "1991/2105 [===========================>..] - ETA: 10:32 - accuracy: 0.9115 - loss: 0.2263\n",
      " training set -> batch:1992 loss:0.21724162995815277 and acc: 0.9123931527137756\n",
      "1992/2105 [===========================>..] - ETA: 10:26 - accuracy: 0.9124 - loss: 0.2172\n",
      " training set -> batch:1993 loss:0.20672626793384552 and acc: 0.9152892827987671\n",
      "1993/2105 [===========================>..] - ETA: 10:21 - accuracy: 0.9153 - loss: 0.2067\n",
      " training set -> batch:1994 loss:0.19893048703670502 and acc: 0.9179999828338623\n",
      "1994/2105 [===========================>..] - ETA: 10:15 - accuracy: 0.9180 - loss: 0.1989\n",
      " training set -> batch:1995 loss:0.20193065702915192 and acc: 0.9176356792449951\n",
      "1995/2105 [===========================>..] - ETA: 10:09 - accuracy: 0.9176 - loss: 0.2019\n",
      " training set -> batch:1996 loss:0.19966448843479156 and acc: 0.9182330965995789\n",
      "1996/2105 [===========================>..] - ETA: 10:03 - accuracy: 0.9182 - loss: 0.1997\n",
      " training set -> batch:1997 loss:0.19401109218597412 and acc: 0.9197080135345459\n",
      "1997/2105 [===========================>..] - ETA: 9:58 - accuracy: 0.9197 - loss: 0.1940 \n",
      " training set -> batch:1998 loss:0.19086605310440063 and acc: 0.9202127456665039\n",
      "1998/2105 [===========================>..] - ETA: 9:52 - accuracy: 0.9202 - loss: 0.1909\n",
      " training set -> batch:1999 loss:0.18687506020069122 and acc: 0.9215517044067383\n",
      "1999/2105 [===========================>..] - ETA: 9:46 - accuracy: 0.9216 - loss: 0.1869\n",
      " training set -> batch:2000 loss:0.18331514298915863 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:2000 val loss:0.23683638870716095 and val acc: 0.9094036817550659\n",
      "2000/2105 [===========================>..] - ETA: 9:42 - accuracy: 0.9228 - loss: 0.1833\n",
      " training set -> batch:2001 loss:0.22793594002723694 and acc: 0.9103982448577881\n",
      "2001/2105 [===========================>..] - ETA: 9:37 - accuracy: 0.9104 - loss: 0.2279\n",
      " training set -> batch:2002 loss:0.22250480949878693 and acc: 0.9113247990608215\n",
      "2002/2105 [===========================>..] - ETA: 9:31 - accuracy: 0.9113 - loss: 0.2225\n",
      " training set -> batch:2003 loss:0.2202906608581543 and acc: 0.9111570119857788\n",
      "2003/2105 [===========================>..] - ETA: 9:25 - accuracy: 0.9112 - loss: 0.2203\n",
      " training set -> batch:2004 loss:0.21182295680046082 and acc: 0.9129999876022339\n",
      "2004/2105 [===========================>..] - ETA: 9:19 - accuracy: 0.9130 - loss: 0.2118\n",
      " training set -> batch:2005 loss:0.20233717560768127 and acc: 0.9156976938247681\n",
      "2005/2105 [===========================>..] - ETA: 9:14 - accuracy: 0.9157 - loss: 0.2023\n",
      " training set -> batch:2006 loss:0.1944209188222885 and acc: 0.9182330965995789\n",
      "2006/2105 [===========================>..] - ETA: 9:08 - accuracy: 0.9182 - loss: 0.1944\n",
      " training set -> batch:2007 loss:0.19356068968772888 and acc: 0.918795645236969\n",
      "2007/2105 [===========================>..] - ETA: 9:02 - accuracy: 0.9188 - loss: 0.1936\n",
      " training set -> batch:2008 loss:0.19202446937561035 and acc: 0.9193262457847595\n",
      "2008/2105 [===========================>..] - ETA: 8:57 - accuracy: 0.9193 - loss: 0.1920\n",
      " training set -> batch:2009 loss:0.18580472469329834 and acc: 0.9215517044067383\n",
      "2009/2105 [===========================>..] - ETA: 8:51 - accuracy: 0.9216 - loss: 0.1858\n",
      " training set -> batch:2010 loss:0.18024511635303497 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:2010 val loss:0.31881433725357056 and val acc: 0.8864678740501404\n",
      "2010/2105 [===========================>..] - ETA: 8:47 - accuracy: 0.9237 - loss: 0.1802\n",
      " training set -> batch:2011 loss:0.30075010657310486 and acc: 0.8904867172241211\n",
      "2011/2105 [===========================>..] - ETA: 8:41 - accuracy: 0.8905 - loss: 0.3008\n",
      " training set -> batch:2012 loss:0.294079065322876 and acc: 0.8920940160751343\n",
      "2012/2105 [===========================>..] - ETA: 8:35 - accuracy: 0.8921 - loss: 0.2941\n",
      " training set -> batch:2013 loss:0.2938889265060425 and acc: 0.8935950398445129\n",
      "2013/2105 [===========================>..] - ETA: 8:30 - accuracy: 0.8936 - loss: 0.2939\n",
      " training set -> batch:2014 loss:0.28641965985298157 and acc: 0.8960000276565552\n",
      "2014/2105 [===========================>..] - ETA: 8:24 - accuracy: 0.8960 - loss: 0.2864\n",
      " training set -> batch:2015 loss:0.27812203764915466 and acc: 0.8972868323326111\n",
      "2015/2105 [===========================>..] - ETA: 8:18 - accuracy: 0.8973 - loss: 0.2781\n",
      " training set -> batch:2016 loss:0.27189892530441284 and acc: 0.8984962701797485\n",
      "2016/2105 [===========================>..] - ETA: 8:13 - accuracy: 0.8985 - loss: 0.2719\n",
      " training set -> batch:2017 loss:0.26407966017723083 and acc: 0.8996350169181824\n",
      "2017/2105 [===========================>..] - ETA: 8:07 - accuracy: 0.8996 - loss: 0.2641\n",
      " training set -> batch:2018 loss:0.25926217436790466 and acc: 0.8998227119445801\n",
      "2018/2105 [===========================>..] - ETA: 8:01 - accuracy: 0.8998 - loss: 0.2593\n",
      " training set -> batch:2019 loss:0.25379154086112976 and acc: 0.9017241597175598\n",
      "2019/2105 [===========================>..] - ETA: 7:56 - accuracy: 0.9017 - loss: 0.2538\n",
      " training set -> batch:2020 loss:0.25061681866645813 and acc: 0.9018456339836121\n",
      "\n",
      " validation set -> batch:2020 val loss:0.26907283067703247 and val acc: 0.9094036817550659\n",
      "2020/2105 [===========================>..] - ETA: 7:51 - accuracy: 0.9018 - loss: 0.2506\n",
      " training set -> batch:2021 loss:0.2738916277885437 and acc: 0.9092920422554016\n",
      "2021/2105 [===========================>..] - ETA: 7:46 - accuracy: 0.9093 - loss: 0.2739\n",
      " training set -> batch:2022 loss:0.2586401700973511 and acc: 0.9123931527137756\n",
      "2022/2105 [===========================>..] - ETA: 7:40 - accuracy: 0.9124 - loss: 0.2586\n",
      " training set -> batch:2023 loss:0.2534087002277374 and acc: 0.91425621509552\n",
      "2023/2105 [===========================>..] - ETA: 7:34 - accuracy: 0.9143 - loss: 0.2534\n",
      " training set -> batch:2024 loss:0.25875017046928406 and acc: 0.9129999876022339\n",
      "2024/2105 [===========================>..] - ETA: 7:29 - accuracy: 0.9130 - loss: 0.2588\n",
      " training set -> batch:2025 loss:0.2552413046360016 and acc: 0.913759708404541\n",
      "2025/2105 [===========================>..] - ETA: 7:23 - accuracy: 0.9138 - loss: 0.2552\n",
      " training set -> batch:2026 loss:0.25147148966789246 and acc: 0.9135338068008423\n",
      "2026/2105 [===========================>..] - ETA: 7:17 - accuracy: 0.9135 - loss: 0.2515\n",
      " training set -> batch:2027 loss:0.24398469924926758 and acc: 0.9142335653305054\n",
      "2027/2105 [===========================>..] - ETA: 7:12 - accuracy: 0.9142 - loss: 0.2440\n",
      " training set -> batch:2028 loss:0.2338206171989441 and acc: 0.9166666865348816\n",
      "2028/2105 [===========================>..] - ETA: 7:06 - accuracy: 0.9167 - loss: 0.2338\n",
      " training set -> batch:2029 loss:0.23270733654499054 and acc: 0.9172413945198059\n",
      "2029/2105 [===========================>..] - ETA: 7:00 - accuracy: 0.9172 - loss: 0.2327\n",
      " training set -> batch:2030 loss:0.23108254373073578 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:2030 val loss:0.27756986021995544 and val acc: 0.9036697149276733\n",
      "2030/2105 [===========================>..] - ETA: 6:56 - accuracy: 0.9169 - loss: 0.2311\n",
      " training set -> batch:2031 loss:0.2617146670818329 and acc: 0.9070796370506287\n",
      "2031/2105 [===========================>..] - ETA: 6:50 - accuracy: 0.9071 - loss: 0.2617\n",
      " training set -> batch:2032 loss:0.25560081005096436 and acc: 0.9081196784973145\n",
      "2032/2105 [===========================>..] - ETA: 6:44 - accuracy: 0.9081 - loss: 0.2556\n",
      " training set -> batch:2033 loss:0.2442627251148224 and acc: 0.9101239442825317\n",
      "2033/2105 [===========================>..] - ETA: 6:39 - accuracy: 0.9101 - loss: 0.2443\n",
      " training set -> batch:2034 loss:0.26246559619903564 and acc: 0.9049999713897705\n",
      "2034/2105 [===========================>..] - ETA: 6:33 - accuracy: 0.9050 - loss: 0.2625\n",
      " training set -> batch:2035 loss:0.260088175535202 and acc: 0.9069767594337463\n",
      "2035/2105 [============================>.] - ETA: 6:27 - accuracy: 0.9070 - loss: 0.2601\n",
      " training set -> batch:2036 loss:0.25806358456611633 and acc: 0.9078947305679321\n",
      "2036/2105 [============================>.] - ETA: 6:22 - accuracy: 0.9079 - loss: 0.2581\n",
      " training set -> batch:2037 loss:0.2542354166507721 and acc: 0.9087591171264648\n",
      "2037/2105 [============================>.] - ETA: 6:16 - accuracy: 0.9088 - loss: 0.2542\n",
      " training set -> batch:2038 loss:0.24374118447303772 and acc: 0.911347508430481\n",
      "2038/2105 [============================>.] - ETA: 6:11 - accuracy: 0.9113 - loss: 0.2437\n",
      " training set -> batch:2039 loss:0.23901832103729248 and acc: 0.9129310250282288\n",
      "2039/2105 [============================>.] - ETA: 6:05 - accuracy: 0.9129 - loss: 0.2390\n",
      " training set -> batch:2040 loss:0.23080331087112427 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:2040 val loss:0.2211008369922638 and val acc: 0.9105504751205444\n",
      "2040/2105 [============================>.] - ETA: 6:00 - accuracy: 0.9153 - loss: 0.2308\n",
      " training set -> batch:2041 loss:0.21579241752624512 and acc: 0.9115044474601746\n",
      "2041/2105 [============================>.] - ETA: 5:55 - accuracy: 0.9115 - loss: 0.2158\n",
      " training set -> batch:2042 loss:0.20787015557289124 and acc: 0.9134615659713745\n",
      "2042/2105 [============================>.] - ETA: 5:49 - accuracy: 0.9135 - loss: 0.2079\n",
      " training set -> batch:2043 loss:0.20233812928199768 and acc: 0.9152892827987671\n",
      "2043/2105 [============================>.] - ETA: 5:43 - accuracy: 0.9153 - loss: 0.2023\n",
      " training set -> batch:2044 loss:0.19436156749725342 and acc: 0.9169999957084656\n",
      "2044/2105 [============================>.] - ETA: 5:38 - accuracy: 0.9170 - loss: 0.1944\n",
      " training set -> batch:2045 loss:0.2068050503730774 and acc: 0.913759708404541\n",
      "2045/2105 [============================>.] - ETA: 5:32 - accuracy: 0.9138 - loss: 0.2068\n",
      " training set -> batch:2046 loss:0.2032805234193802 and acc: 0.9144737124443054\n",
      "2046/2105 [============================>.] - ETA: 5:26 - accuracy: 0.9145 - loss: 0.2033\n",
      " training set -> batch:2047 loss:0.20073305070400238 and acc: 0.915145993232727\n",
      "2047/2105 [============================>.] - ETA: 5:21 - accuracy: 0.9151 - loss: 0.2007\n",
      " training set -> batch:2048 loss:0.1981121003627777 and acc: 0.9166666865348816\n",
      "2048/2105 [============================>.] - ETA: 5:15 - accuracy: 0.9167 - loss: 0.1981\n",
      " training set -> batch:2049 loss:0.19643321633338928 and acc: 0.9163793325424194\n",
      "2049/2105 [============================>.] - ETA: 5:10 - accuracy: 0.9164 - loss: 0.1964\n",
      " training set -> batch:2050 loss:0.19729937613010406 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:2050 val loss:0.20815441012382507 and val acc: 0.9151375889778137\n",
      "2050/2105 [============================>.] - ETA: 5:05 - accuracy: 0.9169 - loss: 0.1973\n",
      " training set -> batch:2051 loss:0.2053815871477127 and acc: 0.9159291982650757\n",
      "2051/2105 [============================>.] - ETA: 4:59 - accuracy: 0.9159 - loss: 0.2054\n",
      " training set -> batch:2052 loss:0.20778179168701172 and acc: 0.9145299196243286\n",
      "2052/2105 [============================>.] - ETA: 4:54 - accuracy: 0.9145 - loss: 0.2078\n",
      " training set -> batch:2053 loss:0.20258460938930511 and acc: 0.9163222908973694\n",
      "2053/2105 [============================>.] - ETA: 4:48 - accuracy: 0.9163 - loss: 0.2026\n",
      " training set -> batch:2054 loss:0.19826604425907135 and acc: 0.9169999957084656\n",
      "2054/2105 [============================>.] - ETA: 4:42 - accuracy: 0.9170 - loss: 0.1983\n",
      " training set -> batch:2055 loss:0.1994374543428421 and acc: 0.9166666865348816\n",
      "2055/2105 [============================>.] - ETA: 4:37 - accuracy: 0.9167 - loss: 0.1994\n",
      " training set -> batch:2056 loss:0.19572684168815613 and acc: 0.9182330965995789\n",
      "2056/2105 [============================>.] - ETA: 4:31 - accuracy: 0.9182 - loss: 0.1957\n",
      " training set -> batch:2057 loss:0.19324631989002228 and acc: 0.918795645236969\n",
      "2057/2105 [============================>.] - ETA: 4:25 - accuracy: 0.9188 - loss: 0.1932\n",
      " training set -> batch:2058 loss:0.19004860520362854 and acc: 0.9202127456665039\n",
      "2058/2105 [============================>.] - ETA: 4:20 - accuracy: 0.9202 - loss: 0.1900\n",
      " training set -> batch:2059 loss:0.18429797887802124 and acc: 0.9224137663841248\n",
      "2059/2105 [============================>.] - ETA: 4:14 - accuracy: 0.9224 - loss: 0.1843\n",
      " training set -> batch:2060 loss:0.18308131396770477 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:2060 val loss:0.21580322086811066 and val acc: 0.9082568883895874\n",
      "2060/2105 [============================>.] - ETA: 4:09 - accuracy: 0.9220 - loss: 0.1831\n",
      " training set -> batch:2061 loss:0.21490468084812164 and acc: 0.9070796370506287\n",
      "2061/2105 [============================>.] - ETA: 4:04 - accuracy: 0.9071 - loss: 0.2149\n",
      " training set -> batch:2062 loss:0.22514306008815765 and acc: 0.9059829115867615\n",
      "2062/2105 [============================>.] - ETA: 3:58 - accuracy: 0.9060 - loss: 0.2251\n",
      " training set -> batch:2063 loss:0.22022394835948944 and acc: 0.9080578684806824\n",
      "2063/2105 [============================>.] - ETA: 3:52 - accuracy: 0.9081 - loss: 0.2202\n",
      " training set -> batch:2064 loss:0.21204577386379242 and acc: 0.9110000133514404\n",
      "2064/2105 [============================>.] - ETA: 3:47 - accuracy: 0.9110 - loss: 0.2120\n",
      " training set -> batch:2065 loss:0.20732319355010986 and acc: 0.9127907156944275\n",
      "2065/2105 [============================>.] - ETA: 3:41 - accuracy: 0.9128 - loss: 0.2073\n",
      " training set -> batch:2066 loss:0.20422320067882538 and acc: 0.9125939607620239\n",
      "2066/2105 [============================>.] - ETA: 3:36 - accuracy: 0.9126 - loss: 0.2042\n",
      " training set -> batch:2067 loss:0.19897526502609253 and acc: 0.9142335653305054\n",
      "2067/2105 [============================>.] - ETA: 3:30 - accuracy: 0.9142 - loss: 0.1990\n",
      " training set -> batch:2068 loss:0.1927611380815506 and acc: 0.9166666865348816\n",
      "2068/2105 [============================>.] - ETA: 3:24 - accuracy: 0.9167 - loss: 0.1928\n",
      " training set -> batch:2069 loss:0.1915978342294693 and acc: 0.9181034564971924\n",
      "2069/2105 [============================>.] - ETA: 3:19 - accuracy: 0.9181 - loss: 0.1916\n",
      " training set -> batch:2070 loss:0.18960797786712646 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:2070 val loss:0.22007808089256287 and val acc: 0.9094036817550659\n",
      "2070/2105 [============================>.] - ETA: 3:14 - accuracy: 0.9178 - loss: 0.1896\n",
      " training set -> batch:2071 loss:0.2186329960823059 and acc: 0.9092920422554016\n",
      "2071/2105 [============================>.] - ETA: 3:08 - accuracy: 0.9093 - loss: 0.2186\n",
      " training set -> batch:2072 loss:0.21438753604888916 and acc: 0.9102563858032227\n",
      "2072/2105 [============================>.] - ETA: 3:03 - accuracy: 0.9103 - loss: 0.2144\n",
      " training set -> batch:2073 loss:0.2040082961320877 and acc: 0.913223147392273\n",
      "2073/2105 [============================>.] - ETA: 2:57 - accuracy: 0.9132 - loss: 0.2040\n",
      " training set -> batch:2074 loss:0.19403213262557983 and acc: 0.9160000085830688\n",
      "2074/2105 [============================>.] - ETA: 2:51 - accuracy: 0.9160 - loss: 0.1940\n",
      " training set -> batch:2075 loss:0.19861185550689697 and acc: 0.9166666865348816\n",
      "2075/2105 [============================>.] - ETA: 2:46 - accuracy: 0.9167 - loss: 0.1986\n",
      " training set -> batch:2076 loss:0.19572940468788147 and acc: 0.9163534045219421\n",
      "2076/2105 [============================>.] - ETA: 2:40 - accuracy: 0.9164 - loss: 0.1957\n",
      " training set -> batch:2077 loss:0.19602146744728088 and acc: 0.9169707894325256\n",
      "2077/2105 [============================>.] - ETA: 2:35 - accuracy: 0.9170 - loss: 0.1960\n",
      " training set -> batch:2078 loss:0.19927288591861725 and acc: 0.9166666865348816\n",
      "2078/2105 [============================>.] - ETA: 2:29 - accuracy: 0.9167 - loss: 0.1993\n",
      " training set -> batch:2079 loss:0.19338315725326538 and acc: 0.9189655184745789\n",
      "2079/2105 [============================>.] - ETA: 2:24 - accuracy: 0.9190 - loss: 0.1934\n",
      " training set -> batch:2080 loss:0.19586442410945892 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:2080 val loss:0.23206470906734467 and val acc: 0.9071100950241089\n",
      "2080/2105 [============================>.] - ETA: 2:18 - accuracy: 0.9186 - loss: 0.1959\n",
      " training set -> batch:2081 loss:0.22250832617282867 and acc: 0.9092920422554016\n",
      "2081/2105 [============================>.] - ETA: 2:13 - accuracy: 0.9093 - loss: 0.2225\n",
      " training set -> batch:2082 loss:0.21549393236637115 and acc: 0.9113247990608215\n",
      "2082/2105 [============================>.] - ETA: 2:07 - accuracy: 0.9113 - loss: 0.2155\n",
      " training set -> batch:2083 loss:0.21023069322109222 and acc: 0.9121900796890259\n",
      "2083/2105 [============================>.] - ETA: 2:02 - accuracy: 0.9122 - loss: 0.2102\n",
      " training set -> batch:2084 loss:0.20782610774040222 and acc: 0.9129999876022339\n",
      "2084/2105 [============================>.] - ETA: 1:56 - accuracy: 0.9130 - loss: 0.2078\n",
      " training set -> batch:2085 loss:0.20450201630592346 and acc: 0.9127907156944275\n",
      "2085/2105 [============================>.] - ETA: 1:50 - accuracy: 0.9128 - loss: 0.2045\n",
      " training set -> batch:2086 loss:0.20916183292865753 and acc: 0.9125939607620239\n",
      "2086/2105 [============================>.] - ETA: 1:45 - accuracy: 0.9126 - loss: 0.2092\n",
      " training set -> batch:2087 loss:0.2049437314271927 and acc: 0.9142335653305054\n",
      "2087/2105 [============================>.] - ETA: 1:39 - accuracy: 0.9142 - loss: 0.2049\n",
      " training set -> batch:2088 loss:0.21339017152786255 and acc: 0.9131205677986145\n",
      "2088/2105 [============================>.] - ETA: 1:34 - accuracy: 0.9131 - loss: 0.2134\n",
      " training set -> batch:2089 loss:0.2086390256881714 and acc: 0.9137930870056152\n",
      "2089/2105 [============================>.] - ETA: 1:28 - accuracy: 0.9138 - loss: 0.2086\n",
      " training set -> batch:2090 loss:0.2094898223876953 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:2090 val loss:0.23618121445178986 and val acc: 0.9082568883895874\n",
      "2090/2105 [============================>.] - ETA: 1:23 - accuracy: 0.9136 - loss: 0.2095\n",
      " training set -> batch:2091 loss:0.23072049021720886 and acc: 0.9081858396530151\n",
      "2091/2105 [============================>.] - ETA: 1:17 - accuracy: 0.9082 - loss: 0.2307\n",
      " training set -> batch:2092 loss:0.22406482696533203 and acc: 0.9091880321502686\n",
      "2092/2105 [============================>.] - ETA: 1:12 - accuracy: 0.9092 - loss: 0.2241\n",
      " training set -> batch:2093 loss:0.22629667818546295 and acc: 0.9101239442825317\n",
      "2093/2105 [============================>.] - ETA: 1:06 - accuracy: 0.9101 - loss: 0.2263\n",
      " training set -> batch:2094 loss:0.2188752293586731 and acc: 0.9120000004768372\n",
      "2094/2105 [============================>.] - ETA: 1:01 - accuracy: 0.9120 - loss: 0.2189\n",
      " training set -> batch:2095 loss:0.2307649850845337 and acc: 0.9098837375640869\n",
      "2095/2105 [============================>.] - ETA: 55s - accuracy: 0.9099 - loss: 0.2308 \n",
      " training set -> batch:2096 loss:0.23185038566589355 and acc: 0.9107142686843872\n",
      "2096/2105 [============================>.] - ETA: 49s - accuracy: 0.9107 - loss: 0.2319\n",
      " training set -> batch:2097 loss:0.22438791394233704 and acc: 0.9124087691307068\n",
      "2097/2105 [============================>.] - ETA: 44s - accuracy: 0.9124 - loss: 0.2244\n",
      " training set -> batch:2098 loss:0.22294165194034576 and acc: 0.9131205677986145\n",
      "2098/2105 [============================>.] - ETA: 38s - accuracy: 0.9131 - loss: 0.2229\n",
      " training set -> batch:2099 loss:0.21704837679862976 and acc: 0.9146551489830017\n",
      "2099/2105 [============================>.] - ETA: 33s - accuracy: 0.9147 - loss: 0.2170\n",
      " training set -> batch:2100 loss:0.21413521468639374 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:2100 val loss:0.22115501761436462 and val acc: 0.9128440618515015\n",
      "2100/2105 [============================>.] - ETA: 27s - accuracy: 0.9153 - loss: 0.2141\n",
      " training set -> batch:2101 loss:0.21171195805072784 and acc: 0.9148229956626892\n",
      "2101/2105 [============================>.] - ETA: 22s - accuracy: 0.9148 - loss: 0.2117\n",
      " training set -> batch:2102 loss:0.20225392282009125 and acc: 0.9166666865348816\n",
      "2102/2105 [============================>.] - ETA: 16s - accuracy: 0.9167 - loss: 0.2023\n",
      " training set -> batch:2103 loss:0.2028355747461319 and acc: 0.9152892827987671\n",
      "2103/2105 [============================>.] - ETA: 11s - accuracy: 0.9153 - loss: 0.2028\n",
      " training set -> batch:2104 loss:0.205835223197937 and acc: 0.9150000214576721\n",
      "2104/2105 [============================>.] - ETA: 5s - accuracy: 0.9150 - loss: 0.2058 \n",
      " training set -> batch:2105 loss:0.20173782110214233 and acc: 0.9147894382476807\n",
      "2105/2105 [==============================] - 11700s 6s/step - accuracy: 0.9148 - loss: 0.2017 - val_accuracy: 0.9479 - val_loss: 0.2135\n",
      "\n",
      "execution time: 3:15:19\n"
     ]
    }
   ],
   "source": [
    "# time the function\n",
    "start_time = time.time()\n",
    "\n",
    "# making the transformation here since insude model.fit it create a lot of warnings\n",
    "data_train = data_feature_extraction(train_dataset, model.name)\n",
    "data_val = data_feature_extraction(valid_dataset, model.name)\n",
    "histories_per_step = History_per_step(data_val, 10)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(data_train, \n",
    "                    epochs=1, \n",
    "                    steps_per_epoch=STEP_EPOCH_TRAIN,\n",
    "                    validation_data=data_val,\n",
    "                    validation_steps=3,\n",
    "                    callbacks=[tensorboard_callback,\n",
    "                               *checkpoint_callback,\n",
    "                               histories_per_step])\n",
    "\n",
    "# print execution time\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "print('\\nexecution time: {}'.format(timedelta(seconds=round(elapsed_time_secs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "  - loss [training dataset]: 0.202\n",
      "  - loss [validation dataset: 0.221\n",
      "\n",
      "Accuracy:\n",
      "  - accuracy [training dataset]: 91.48%\n",
      "  - accuracy [validation dataset: 91.28%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x300 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.plot_acc_loss(steps_loss_train=histories_per_step.steps, loss_train=histories_per_step.losses,\n",
    "                 steps_acc_train=histories_per_step.steps, accuracy_train=histories_per_step.accuracies,\n",
    "                 steps_loss_eval=histories_per_step.val_steps, loss_eval=histories_per_step.val_losses,\n",
    "                 steps_acc_eval=histories_per_step.val_steps, accuracy_eval=histories_per_step.val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Get more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.metrics.Mean object at 0x7fbe103c6ad0>, <tensorflow.python.keras.metrics.SparseCategoricalAccuracy object at 0x7fbe10433750>]\n",
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 1, 'steps': 2105}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# dir(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Exploration of the model's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,\n",
    "                          'model.png',\n",
    "                          show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.modeling_tf_bert.TFBertMainLayer at 0x7fbe10433390>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fbdd0113f50>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fbe103c62d0>]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert [] []\n",
      "dropout_37 [] []\n",
      "classifier [] []\n"
     ]
    }
   ],
   "source": [
    "# _inbound_nodes and inbound_nodes give the same !\n",
    "# to see method available: dir(model.layers[2])\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer._inbound_nodes, layer._outbound_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Validation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard model: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "# get probablility for each classes\n",
    "if model.name=='custom_tf_bert_classification':\n",
    "        print('custom model: {}'.format(model.name))\n",
    "        y_pred = tf.nn.softmax(model.predict(valid_dataset))\n",
    "elif model.name=='tf_bert_classification':\n",
    "        print('standard model: {}'.format(model.name))\n",
    "        y_pred = tf.squeeze(tf.nn.softmax(model.predict(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([872, 2])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# get predicted classes\n",
    "y_pred_argmax = tf.math.argmax(y_pred, axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([872])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred_argmax).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Extracting true classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# extracting and flatten true classes\n",
    "y_true_tf=valid_dataset.map(pp.label_extraction).flat_map(lambda x: valid_dataset.from_tensor_slices(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_true=list(y_true_tf.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872, 872)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true), len(y_pred_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Model performanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.93      0.87      0.90       428\n",
      "    positive       0.89      0.94      0.91       444\n",
      "\n",
      "    accuracy                           0.91       872\n",
      "   macro avg       0.91      0.91      0.91       872\n",
      "weighted avg       0.91      0.91      0.91       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_argmax, target_names=info.features[\"label\"].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on the  dataset:\n",
      "   Metric             \n",
      "accuracy...........   0.9071\n",
      "recall.............   0.9392\n",
      "auc................   0.9065\n",
      "precision (p=0.5)..   0.8854\n",
      "precision (avg)....   0.8625\n",
      "precision (micro)..   0.9071\n",
      "precision (macro)..   0.9090\n",
      "f1.................    0.9115\n",
      "r2.................    0.6283\n"
     ]
    }
   ],
   "source": [
    "mm.print_metrics(y_true, y_pred_argmax, mode='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.plot_confusion_matrix(confusion_matrix(y_true, y_pred_argmax), info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.roc_curves(to_categorical(y_true), y_pred.numpy(), info.features[\"label\"].names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_test]",
   "language": "python",
   "name": "conda-env-env_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
