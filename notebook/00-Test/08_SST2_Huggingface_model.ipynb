{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export REGION=your_region`  \n",
       "`export MODEL_DIR_ESTIMATOR_PATH=your_path_to_save_model` \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    "    TFBertForSequenceClassification,\n",
    "    glue_convert_examples_to_features,\n",
    "    glue_processors\n",
    ")\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import preprocessing.preprocessing as pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading a data from Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/Users/tarrade/tensorflow_datasets/glue/sst2/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /Users/tarrade/tensorflow_datasets/glue/sst2/1.0.0\n"
     ]
    }
   ],
   "source": [
    "data, info = tensorflow_datasets.load(name='glue/sst2',\n",
    "                                      data_dir=data_dir,\n",
    "                                      with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking baics info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    version=1.0.0,\n",
       "    description='GLUE, the General Language Understanding Evaluation benchmark\n",
       "(https://gluebenchmark.com/) is a collection of resources for training,\n",
       "evaluating, and analyzing natural language understanding systems.\n",
       "\n",
       "            The Stanford Sentiment Treebank consists of sentences from movie reviews and\n",
       "            human annotations of their sentiment. The task is to predict the sentiment of a\n",
       "            given sentence. We use the two-way (positive/negative) class split, and use only\n",
       "            sentence-level labels.',\n",
       "    homepage='https://nlp.stanford.edu/sentiment/index.html',\n",
       "    features=FeaturesDict({\n",
       "        'idx': tf.int32,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'sentence': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=70042,\n",
       "    splits={\n",
       "        'test': 1821,\n",
       "        'train': 67349,\n",
       "        'validation': 872,\n",
       "    },\n",
       "    supervised_keys=None,\n",
       "    citation=\"\"\"@inproceedings{socher2013recursive,\n",
       "                  title={Recursive deep models for semantic compositionality over a sentiment treebank},\n",
       "                  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},\n",
       "                  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},\n",
       "                  pages={1631--1642},\n",
       "                  year={2013}\n",
       "                }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "      ['negative', 'positive']\n",
      "\n",
      "Number of label:\n",
      "      2\n",
      "\n",
      "Structure of the data:\n",
      "      dict_keys(['sentence', 'label', 'idx'])\n",
      "\n",
      "Number of entries:\n",
      "   Train dataset: 67349\n",
      "   Test dataset:  1821\n",
      "   Valid dataset: 872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_dataset(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking baics info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'train': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'validation': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   {'idx': TensorShape([]), 'label': TensorShape([]), 'sentence': TensorShape([])}\n",
      "\n",
      "# Output types of one entry:\n",
      "   {'idx': tf.int32, 'label': tf.int64, 'sentence': tf.string}\n",
      "\n",
      "# Output typesof one entry:\n",
      "   {'idx': <class 'tensorflow.python.framework.ops.Tensor'>, 'label': <class 'tensorflow.python.framework.ops.Tensor'>, 'sentence': <class 'tensorflow.python.framework.ops.Tensor'>}\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (67349,)\n",
      "   ---> 67349 entries\n",
      "   ---> 1 dim\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [idx       / label     / sentence ]\n",
      "           [()        / ()        / ()       ]\n",
      "           [int32     / int64     / bytes    ]\n",
      "\n",
      "\n",
      "# Examples of data:\n",
      "{'idx': 16399,\n",
      " 'label': 0,\n",
      " 'sentence': b'for the uninitiated plays better on video with the sound '}\n",
      "{'idx': 1680,\n",
      " 'label': 0,\n",
      " 'sentence': b'like a giant commercial for universal studios , where much of th'\n",
      "             b'e action takes place '}\n",
      "{'idx': 47917,\n",
      " 'label': 1,\n",
      " 'sentence': b'company once again dazzle and delight us '}\n",
      "{'idx': 17307,\n",
      " 'label': 1,\n",
      " 'sentence': b\"'s no surprise that as a director washington demands and receive\"\n",
      "             b's excellent performances , from himself and from newcomer derek '\n",
      "             b'luke '}\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:           67349/  1821/   872\n",
      "Batch size:                32/    32/    64\n",
      "Step per epoch:          2105/    57/    29\n",
      "Total number of batch:   6315/   171/    87\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "BATCH_SIZE_VALID = 64\n",
    "EPOCH = 2\n",
    "\n",
    "# extract parameters\n",
    "size_train_dataset = info.splits['train'].num_examples\n",
    "size_test_dataset = info.splits['test'].num_examples\n",
    "size_valid_dataset = info.splits['validation'].num_examples\n",
    "number_label = info.features[\"label\"].num_classes\n",
    "\n",
    "# computer parameter\n",
    "STEP_EPOCH_TRAIN = math.ceil(size_train_dataset/BATCH_SIZE_TRAIN)\n",
    "STEP_EPOCH_TEST = math.ceil(size_test_dataset/BATCH_SIZE_TEST)\n",
    "STEP_EPOCH_VALID = math.ceil(size_test_dataset/BATCH_SIZE_VALID)\n",
    "\n",
    "\n",
    "print('Dataset size:          {:6}/{:6}/{:6}'.format(size_train_dataset, size_test_dataset, size_valid_dataset))\n",
    "print('Batch size:            {:6}/{:6}/{:6}'.format(BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, BATCH_SIZE_VALID))\n",
    "print('Step per epoch:        {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN, STEP_EPOCH_TEST, STEP_EPOCH_VALID))\n",
    "print('Total number of batch: {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN*(EPOCH+1), STEP_EPOCH_TEST*(EPOCH+1), STEP_EPOCH_VALID*(EPOCH+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tokenizer and prepare data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare data for BERT\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')\n",
    "test_dataset = glue_convert_examples_to_features(data['test'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# set shuffle and batch size\n",
    "train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE_TRAIN).repeat(EPOCH+1)\n",
    "test_dataset = test_dataset.shuffle(100).batch(BATCH_SIZE_TEST).repeat(EPOCH+1)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   ({'input_ids': TensorShape([None, None]), 'attention_mask': TensorShape([None, None]), 'token_type_ids': TensorShape([None, None])}, TensorShape([None]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   ({'input_ids': <class 'tensorflow.python.framework.ops.Tensor'>, 'attention_mask': <class 'tensorflow.python.framework.ops.Tensor'>, 'token_type_ids': <class 'tensorflow.python.framework.ops.Tensor'>}, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (6315, 2)\n",
      "   ---> 6315 batches\n",
      "   ---> 2 dim\n",
      "        label\n",
      "           shape: (32,)\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [input_ids       / attention_mask  / token_type_ids ]\n",
      "           [(32, 128)       / (32, 128)       / (32, 128)      ]\n",
      "           [ndarray         / ndarray         / ndarray        ]\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(train_dataset,print_example=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_ids     ---->    attention_mask    token_type_ids    modified text                 \n",
      "\n",
      "       101     ---->           1                 1          [ C L S ]                     \n",
      "     22572     ---->           1                 1          c h                           \n",
      "      8870     ---->           1                 1          # # e e s                     \n",
      "      1183     ---->           1                 1          # # y                         \n",
      "       171     ---->           1                 1          b                             \n",
      "       118     ---->           1                 1          -                             \n",
      "      2523     ---->           1                 1          m o v i e                     \n",
      "      1773     ---->           1                 1          p l a y i n g                 \n",
      "       102     ---->           1                 1          [ S E P ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n"
     ]
    }
   ],
   "source": [
    "pp.print_detail_tokeniser(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define optimizerm loss and metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Define some parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, \n",
    "                                     epsilon=1e-08, \n",
    "                                     clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define distribute strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Uses the tf.distribute.MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define the callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                       save_weights_only=True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Function for decaying the learning rate.\n",
    "def decay(epoch):\n",
    "    if epoch < 3:\n",
    "        return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "decay_callback = tf.keras.callbacks.LearningRateScheduler(decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Print learning rate at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# checking existing folders\n",
    "for i in os.listdir(tensorboard_dir):\n",
    "    if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# clean old TensorBoard directory \n",
    "for i in os.listdir(tensorboard_dir):\n",
    "        if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "            print(i)\n",
    "            shutil.rmtree(tensorboard_dir+'/'+i, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "log_dir=tensorboard_dir+'/'+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1, \n",
    "                                                      profile_batch = 3,\n",
    "                                                      write_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Create callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint_callback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-1cd326afec20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m callbacks = [tensorboard_callback,\n\u001b[0;32m----> 2\u001b[0;31m              checkpoint_callback]\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#callbacks = [tensorboard_callback,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#             checkpoint_callback,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint_callback' is not defined"
     ]
    }
   ],
   "source": [
    "callbacks = [tensorboard_callback,\n",
    "             checkpoint_callback]\n",
    "\n",
    "#callbacks = [tensorboard_callback,\n",
    "#             checkpoint_callback,\n",
    "#             decay_callback,\n",
    "#             PrintLR()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Use TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy.scope():\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-cased',\n",
    "                                                            num_labels=number_label)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss=loss, \n",
    "                  metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_113 (Dropout)        multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 2 steps\n",
      "2/2 [==============================] - 57s 28s/step - loss: 0.7287 - accuracy: 0.4688\n",
      "\n",
      "execution time: 0:01:00\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the model\n",
    "\n",
    "# time the function\n",
    "start_time = time.time()\n",
    "\n",
    "# train the model\n",
    "history = model.fit(train_dataset, \n",
    "                    epochs=1, \n",
    "                    steps_per_epoch=2)\n",
    "\n",
    "# print execution time\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "print('\\nexecution time: {}'.format(timedelta(seconds=round(elapsed_time_secs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attention_mask': <tf.Tensor 'attention_mask:0' shape=(None, 128) dtype=int32>,\n",
       " 'input_ids': <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>,\n",
       " 'token_type_ids': <tf.Tensor 'token_type_ids:0' shape=(None, 128) dtype=int32>}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'tf_bert_for_sequence_classification_1/Identity:0' shape=(None, 2) dtype=float32>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.modeling_tf_bert.TFBertMainLayer at 0x1a37a540d0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x1a38a97310>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x1a38a97910>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert [] []\n",
      "dropout_75 [] []\n",
      "classifier [] []\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    print(layer.name, layer._inbound_nodes, layer._outbound_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._inbound_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].inbound_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for i in model.layers:\n",
    "    print(i.inbound_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#dir(model.layers[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "out_val=model.predict(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(out_val)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_pred_argmax = tf.math.argmax(y_pred, axis=1)\n",
    "y_pred_argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building a classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def keras_building_blocks(number_label):\n",
    "\n",
    "    # create model\n",
    "    model = TFBertForSequenceClassification.from_pretrained('bert-base-cased', num_labels=number_label)\n",
    "\n",
    "    # hidden layer\n",
    "    model.add(tf.keras.layers.Dense(512,\n",
    "                                    input_dim=dim_input,\n",
    "                                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                                    activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(512,\n",
    "                                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                                    activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "    # last layer\n",
    "    model.add(tf.keras.layers.Dense(num_classes,\n",
    "                                    kernel_initializer=tf.keras.initializers.he_normal(),\n",
    "                                    bias_initializer=tf.keras.initializers.Zeros(),\n",
    "                                    activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_layer = tf.keras.Input(shape = (128,), dtype='int64')  \n",
    "#input_layer = tf.keras.Input({'attention_mask': <tf.Tensor 'attention_mask:0' shape=(None, 128) dtype=int32>,\n",
    "# 'input_ids': <tf.Tensor 'input_ids:0' shape=(None, 128) dtype=int32>,\n",
    "# 'token_type_ids': <tf.Tensor 'token_type_ids:0' shape=(None, 128) dtype=int32>})\n",
    "#in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "#in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "#in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "#bert_inputs = [in_id, in_mask, in_segment]   \n",
    "bert_ini = TFBertModel.from_pretrained('bert-base-cased') (input_layer)\n",
    "bert = bert_ini[1]    \n",
    "dropout = tf.keras.layers.Dropout(0.1)(bert)\n",
    "flat = tf.keras.layers.Flatten()(dropout)\n",
    "classifier = tf.keras.layers.Dense(units=2)(flat)                  \n",
    "model2 = tf.keras.Model(inputs=input_layer, outputs=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bert_ini[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bert_ini[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model2.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model2.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for i in train_dataset.take(1):\n",
    "    print(i[1])\n",
    "    print(i[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare training: Compile tf.keras model with optimizer, loss and learning rate schedule\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model2.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "history = model2.fit(train_dataset, \n",
    "                     epochs=2, \n",
    "                     steps_per_epoch=115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Train and evaluate using tf.keras.Model.fit()\n",
    "history = model.fit(train_dataset, epochs=2, steps_per_epoch=115,\n",
    "                    validation_data=valid_dataset, validation_steps=7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_multilingual_class]",
   "language": "python",
   "name": "conda-env-env_multilingual_class-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
