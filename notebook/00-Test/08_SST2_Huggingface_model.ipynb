{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# The Stanford Sentiment Treebank \n",
    "The Stanford Sentiment Treebank consists of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence. We use the two-way (positive/negative) class split, and use only sentence-level labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Environment variables that need to be defined:   \n",
       "`export DIR_PROJ=your_path_git_repository`  \n",
       "`export PYTHONPATH=$DIR_PROJ/src`  \n",
       "`export PATH_TENSORBOARD=your_path_tensorboard`  \n",
       "`export PATH_DATASETS=your_path_datasets`  \n",
       "`export PROJECT_ID=your_gcp_project_id`  \n",
       "`export BUCKET_NAME=your_gcp_gs_bucket_name`  \n",
       "`export REGION=your_region`  \n",
       "`export PATH_SAVE_MODEL=your_path_to_save_model` \n",
       "\n",
       "- Use local Jupyter Lab \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment created [link](local_jupyter_lab_installation.md) \n",
       "    - you need to have the `jupyter-notebook` Anaconda python environment activated [link](local_jupyter_lab_installation.md) \n",
       "    - then define the environment variables above (copy and paste) \n",
       "    - you need to have the `env_multilingual_class` Anaconda python environment created [link](local_jupyter_lab_installation.md)  \n",
       "    - start Jupyter Lab:  `jupyter lab` \n",
       "    - open a Jupyter Lab notebook from `notebook/` \n",
       "     - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - choose the proper Anaconda python environment:  `Python [conda env:env_multilingual_class]` [link](conda_env.md) \n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "\n",
       "\n",
       "- Use GCP Jupyter Lab \n",
       "    - Go on GCP\n",
       "    - open a Cloud Shell\n",
       "    - `ssh-keygen -t rsa -b 4096 -C firstName_lastName`\n",
       "    - `cp .ssh/id_rsa.pub .`\n",
       "    - use Cloud Editor to edit this file `id_rsa.pub` and copy the full content\n",
       "    - Go on Compute Engine -> Metadata\n",
       "    - Click SSH Keys\n",
       "    - Click Edit\n",
       "    - Click + Add item, copy the content of `id_rsa.pub`\n",
       "    - You should see firstName_lastName of the left\n",
       "    - Click Save\n",
       "    - you need to start a AI Platform instance \n",
       "    - open a Jupyter Lab terminal and got to `/home/gcp_user_name/`\n",
       "    - clone this repositiory: `git clone https://github.com/tarrade/proj_multilingual_text_classification.git`\n",
       "    - then `cd proj_multilingual_text_classification/`\n",
       "    - create the Anacond Python environment `conda env create -f env/environment.yml`\n",
       "    - create a file `config.sh` in `/home` with the following information: \n",
       "    ```\n",
       "    #!/bin/bash\n",
       "    \n",
       "    echo \"applying some configuration ...\"\n",
       "    git config --global user.email user_email\n",
       "    git config --global user.name user_name\n",
       "    git config --global credential.helper store\n",
       "        \n",
       "    # Add here the enviroment variables from above below\n",
       "    # [EDIT ME]\n",
       "    export DIR_PROJ=your_path_git_repository\n",
       "    export PYTHONPATH=$DIR_PROJ/src\n",
       "  \n",
       "    cd /home/gcp_user_name/\n",
       "    \n",
       "    conda activate env_multilingual_class\n",
       "\n",
       "    export PS1='\\[\\e[91m\\]\\u@:\\[\\e[32m\\]\\w\\[\\e[0m\\]$'\n",
       "    ```\n",
       "    - Got to AI Platform Notebook, select your instance and click \"Reset\".\n",
       "    - Wait and reshreh you Web browser with the Notebook\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "with open('../../doc/env_variables_setup.md', 'r') as fh:\n",
    "    content = fh.read()\n",
    "display(Markdown(content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertModel,\n",
    "    TFBertForSequenceClassification,\n",
    "    glue_convert_examples_to_features,\n",
    "    glue_processors\n",
    ")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import shutil\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.2.0-rc1-34-ge6e5d6df2a 2.2.0-rc2\n"
     ]
    }
   ],
   "source": [
    "print(tf.version.GIT_VERSION, tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: /physical_device:GPU:0   Type: GPU\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if len(gpus)>0:\n",
    "    for gpu in gpus:\n",
    "        print('Name:', gpu.name, '  Type:', gpu.device_type)\n",
    "else:\n",
    "    print('No GPU available !!!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    data_dir=os.environ['PATH_DATASETS']\n",
    "except KeyError:\n",
    "    print('missing PATH_DATASETS')\n",
    "try:   \n",
    "    tensorboard_dir=os.environ['PATH_TENSORBOARD']\n",
    "except KeyError:\n",
    "    print('missing PATH_TENSORBOARD')\n",
    "try:   \n",
    "    checkpoint_dir=os.environ['PATH_SAVE_MODEL']\n",
    "except KeyError:\n",
    "    print('missing PATH_SAVE_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Import local packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import preprocessing.preprocessing as pp\n",
    "import utils.model_metrics as mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(pp);\n",
    "importlib.reload(mm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Loading a data from Tensorflow Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Overwrite dataset info from restored data version.\n",
      "INFO:absl:Reusing dataset glue (/home/fabien_tarrade/data/glue/sst2/1.0.0)\n",
      "INFO:absl:Constructing tf.data.Dataset for split None, from /home/fabien_tarrade/data/glue/sst2/1.0.0\n"
     ]
    }
   ],
   "source": [
    "data, info = tensorflow_datasets.load(name='glue/sst2',\n",
    "                                      data_dir=data_dir,\n",
    "                                      with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking baics info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='glue',\n",
       "    version=1.0.0,\n",
       "    description='GLUE, the General Language Understanding Evaluation benchmark\n",
       "(https://gluebenchmark.com/) is a collection of resources for training,\n",
       "evaluating, and analyzing natural language understanding systems.\n",
       "\n",
       "            The Stanford Sentiment Treebank consists of sentences from movie reviews and\n",
       "            human annotations of their sentiment. The task is to predict the sentiment of a\n",
       "            given sentence. We use the two-way (positive/negative) class split, and use only\n",
       "            sentence-level labels.',\n",
       "    homepage='https://nlp.stanford.edu/sentiment/index.html',\n",
       "    features=FeaturesDict({\n",
       "        'idx': tf.int32,\n",
       "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=2),\n",
       "        'sentence': Text(shape=(), dtype=tf.string),\n",
       "    }),\n",
       "    total_num_examples=70042,\n",
       "    splits={\n",
       "        'test': 1821,\n",
       "        'train': 67349,\n",
       "        'validation': 872,\n",
       "    },\n",
       "    supervised_keys=None,\n",
       "    citation=\"\"\"@inproceedings{socher2013recursive,\n",
       "                  title={Recursive deep models for semantic compositionality over a sentiment treebank},\n",
       "                  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},\n",
       "                  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},\n",
       "                  pages={1631--1642},\n",
       "                  year={2013}\n",
       "                }\n",
       "    @inproceedings{wang2019glue,\n",
       "      title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},\n",
       "      author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},\n",
       "      note={In the Proceedings of ICLR.},\n",
       "      year={2019}\n",
       "    }\n",
       "    \n",
       "    Note that each GLUE dataset has its own citation. Please see the source to see\n",
       "    the correct citation for each contained dataset.\"\"\",\n",
       "    redistribution_info=,\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:\n",
      "      ['negative', 'positive']\n",
      "\n",
      "Number of label:\n",
      "      2\n",
      "\n",
      "Structure of the data:\n",
      "      dict_keys(['sentence', 'label', 'idx'])\n",
      "\n",
      "Number of entries:\n",
      "   Train dataset: 67349\n",
      "   Test dataset:  1821\n",
      "   Valid dataset: 872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_dataset(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checking baics info from the metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'train': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>,\n",
       " 'validation': <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['test', 'train', 'validation'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   {'idx': TensorShape([]), 'label': TensorShape([]), 'sentence': TensorShape([])}\n",
      "\n",
      "# Output types of one entry:\n",
      "   {'idx': tf.int32, 'label': tf.int64, 'sentence': tf.string}\n",
      "\n",
      "# Output typesof one entry:\n",
      "   {'idx': <class 'tensorflow.python.framework.ops.Tensor'>, 'label': <class 'tensorflow.python.framework.ops.Tensor'>, 'sentence': <class 'tensorflow.python.framework.ops.Tensor'>}\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (67349,)\n",
      "   ---> 67349 entries\n",
      "   ---> 1 dim\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [idx       / label     / sentence ]\n",
      "           [()        / ()        / ()       ]\n",
      "           [int32     / int64     / bytes    ]\n",
      "\n",
      "\n",
      "# Examples of data:\n",
      "{'idx': 16399,\n",
      " 'label': 0,\n",
      " 'sentence': b'for the uninitiated plays better on video with the sound '}\n",
      "{'idx': 1680,\n",
      " 'label': 0,\n",
      " 'sentence': b'like a giant commercial for universal studios , where much of th'\n",
      "             b'e action takes place '}\n",
      "{'idx': 47917,\n",
      " 'label': 1,\n",
      " 'sentence': b'company once again dazzle and delight us '}\n",
      "{'idx': 17307,\n",
      " 'label': 1,\n",
      " 'sentence': b\"'s no surprise that as a director washington demands and receive\"\n",
      "             b's excellent performances , from himself and from newcomer derek '\n",
      "             b'luke '}\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(data['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Define parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:           67349/  1821/   872\n",
      "Batch size:                32/    32/    64\n",
      "Step per epoch:          2105/    57/    29\n",
      "Total number of batch:   6315/   171/    87\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "BATCH_SIZE_TEST = 32\n",
    "BATCH_SIZE_VALID = 64\n",
    "EPOCH = 2\n",
    "\n",
    "# extract parameters\n",
    "size_train_dataset = info.splits['train'].num_examples\n",
    "size_test_dataset = info.splits['test'].num_examples\n",
    "size_valid_dataset = info.splits['validation'].num_examples\n",
    "number_label = info.features[\"label\"].num_classes\n",
    "\n",
    "# computer parameter\n",
    "STEP_EPOCH_TRAIN = math.ceil(size_train_dataset/BATCH_SIZE_TRAIN)\n",
    "STEP_EPOCH_TEST = math.ceil(size_test_dataset/BATCH_SIZE_TEST)\n",
    "STEP_EPOCH_VALID = math.ceil(size_test_dataset/BATCH_SIZE_VALID)\n",
    "\n",
    "\n",
    "print('Dataset size:          {:6}/{:6}/{:6}'.format(size_train_dataset, size_test_dataset, size_valid_dataset))\n",
    "print('Batch size:            {:6}/{:6}/{:6}'.format(BATCH_SIZE_TRAIN, BATCH_SIZE_TEST, BATCH_SIZE_VALID))\n",
    "print('Step per epoch:        {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN, STEP_EPOCH_TEST, STEP_EPOCH_VALID))\n",
    "print('Total number of batch: {:6}/{:6}/{:6}'.format(STEP_EPOCH_TRAIN*(EPOCH+1), STEP_EPOCH_TEST*(EPOCH+1), STEP_EPOCH_VALID*(EPOCH+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Tokenizer and prepare data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: {idx: (), label: (), sentence: ()}, types: {idx: tf.int32, label: tf.int64, sentence: tf.string}>\n",
      "tf.Tensor(67349, shape=(), dtype=int64)\n",
      "tf.Tensor(1821, shape=(), dtype=int64)\n",
      "tf.Tensor(872, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# recap of input dataset\n",
    "print(data['train'])\n",
    "print(tf.data.experimental.cardinality(data['train']))\n",
    "print(tf.data.experimental.cardinality(data['test']))\n",
    "print(tf.data.experimental.cardinality(data['validation']))\n",
    "# super slow since looping over all data\n",
    "#print(len(list(data['train'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Prepare data for BERT\n",
    "train_dataset = glue_convert_examples_to_features(data['train'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')\n",
    "test_dataset = glue_convert_examples_to_features(data['test'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')\n",
    "valid_dataset = glue_convert_examples_to_features(data['validation'], \n",
    "                                                  tokenizer, \n",
    "                                                  max_length=128, \n",
    "                                                  task='sst-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FlatMapDataset shapes: ({input_ids: (None,), attention_mask: (None,), token_type_ids: (None,)}, ()), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "tf.Tensor(-2, shape=(), dtype=int64)\n",
      "67349\n"
     ]
    }
   ],
   "source": [
    "# recap of pre processing dataset\n",
    "print(train_dataset)\n",
    "print(tf.data.experimental.cardinality(train_dataset))\n",
    "print(tf.data.experimental.cardinality(test_dataset))\n",
    "print(tf.data.experimental.cardinality(valid_dataset))\n",
    "# super slow since looping over all data\n",
    "print(len(list(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# set shuffle and batch size\n",
    "train_dataset = train_dataset.shuffle(100).batch(BATCH_SIZE_TRAIN).repeat(EPOCH+1)\n",
    "test_dataset = test_dataset.shuffle(100).batch(BATCH_SIZE_TEST).repeat(EPOCH+1)\n",
    "valid_dataset = valid_dataset.batch(BATCH_SIZE_VALID) #.repeat(EPOCH+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Check the final data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structure of the data:\n",
      "\n",
      "   <RepeatDataset shapes: ({input_ids: (None, None), attention_mask: (None, None), token_type_ids: (None, None)}, (None,)), types: ({input_ids: tf.int32, attention_mask: tf.int32, token_type_ids: tf.int32}, tf.int64)>\n",
      "\n",
      "# Output shape of one entry:\n",
      "   ({'input_ids': TensorShape([None, None]), 'attention_mask': TensorShape([None, None]), 'token_type_ids': TensorShape([None, None])}, TensorShape([None]))\n",
      "\n",
      "# Output types of one entry:\n",
      "   ({'input_ids': tf.int32, 'attention_mask': tf.int32, 'token_type_ids': tf.int32}, tf.int64)\n",
      "\n",
      "# Output typesof one entry:\n",
      "   ({'input_ids': <class 'tensorflow.python.framework.ops.Tensor'>, 'attention_mask': <class 'tensorflow.python.framework.ops.Tensor'>, 'token_type_ids': <class 'tensorflow.python.framework.ops.Tensor'>}, <class 'tensorflow.python.framework.ops.Tensor'>)\n",
      " \n",
      "\n",
      "# Shape of the data:\n",
      "\n",
      "   (6315, 2)\n",
      "   ---> 6315 batches\n",
      "   ---> 2 dim\n",
      "        label\n",
      "           shape: (32,)\n",
      "        dict structure\n",
      "           dim: 3\n",
      "           [input_ids       / attention_mask  / token_type_ids ]\n",
      "           [(32, 128)       / (32, 128)       / (32, 128)      ]\n",
      "           [ndarray         / ndarray         / ndarray        ]\n"
     ]
    }
   ],
   "source": [
    "pp.print_info_data(train_dataset,print_example=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " input_ids     ---->    attention_mask    token_type_ids    modified text                 \n",
      "\n",
      "       101     ---->           1                 1          [ C L S ]                     \n",
      "      1103     ---->           1                 1          t h e                         \n",
      "      2322     ---->           1                 1          c a m p a i g n               \n",
      "       118     ---->           1                 1          -                             \n",
      "      5126     ---->           1                 1          t r a i l                     \n",
      "      3181     ---->           1                 1          p r e s s                     \n",
      "       102     ---->           1                 1          [ S E P ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n",
      "         0     ---->           0                 0          [ P A D ]                     \n"
     ]
    }
   ],
   "source": [
    "pp.print_detail_tokeniser(train_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Building a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Define the callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Define the checkpoint directory to store the checkpoints\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
    "                                                         save_weights_only=True),"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Decaying learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Function for decaying the learning rate.\n",
    "def decay(epoch):\n",
    "    if epoch < 3:\n",
    "        return 1e-3\n",
    "    elif epoch >= 3 and epoch < 7:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "decay_callback = tf.keras.callbacks.LearningRateScheduler(decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Print learning rate at the end of each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# Callback for printing the LR at the end of each epoch.\n",
    "class PrintLR(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200401-131500\n"
     ]
    }
   ],
   "source": [
    "# checking existing folders\n",
    "for i in os.listdir(tensorboard_dir):\n",
    "    if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20200401-131500\n"
     ]
    }
   ],
   "source": [
    "# clean old TensorBoard directory \n",
    "for i in os.listdir(tensorboard_dir):\n",
    "        if os.path.isdir(tensorboard_dir+'/'+i):\n",
    "            print(i)\n",
    "            shutil.rmtree(tensorboard_dir+'/'+i, ignore_errors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "log_dir=tensorboard_dir+'/'+datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "os.mkdir(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n",
    "                                                      histogram_freq=1, \n",
    "                                                      embeddings_freq=1,\n",
    "                                                      write_graph=True,\n",
    "                                                      update_freq='batch',\n",
    "                                                      profile_batch=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Loss and efficiency per step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class History_per_step(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, validation_data, N):\n",
    "        self.validation_data = validation_data\n",
    "        self.N = N\n",
    "        self.batch = 1\n",
    "\n",
    "    def on_train_begin(self, validation_data, logs={}):\n",
    "        self.steps = []\n",
    "        self.losses = []\n",
    "        self.accuracies = []\n",
    "        self.val_steps = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs={}):\n",
    "        self.losses.append(logs.get('loss'))\n",
    "        self.accuracies.append(logs.get('accuracy'))\n",
    "        self.steps.append(self.batch)\n",
    "        print('\\n training set -> batch:{} loss:{} and acc: {}'.format(self.batch,logs.get('loss'),logs.get('accuracy')))\n",
    "        \n",
    "        if self.batch % self.N == 0:\n",
    "            loss_val, acc_val = self.model.evaluate(self.validation_data, verbose=0)\n",
    "            self.val_losses.append(loss_val)\n",
    "            self.val_accuracies.append(acc_val)\n",
    "            self.val_steps.append(self.batch)\n",
    "            print('\\n validation set -> batch:{} val loss:{} and val acc: {}'.format(self.batch,loss_val, acc_val))\n",
    "\n",
    "        self.batch += 1\n",
    "    \n",
    "    def on_test_batch_end(self, batch, logs={}):    \n",
    "        #print('{}\\n'.format(logs))\n",
    "        return\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}): \n",
    "        #print('{}\\n'.format(logs))\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Checks callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelCheckpoint need to unpack this tuple by adding *\n"
     ]
    }
   ],
   "source": [
    "list_callback = [tensorboard_callback, checkpoint_callback, decay_callback]\n",
    "for cb in list_callback:\n",
    "    if type(cb).__name__=='tuple':\n",
    "        print(cb[0].__class__.__name__, 'need to unpack this tuple by adding *')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Use TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "# Gradient clipping in the optimizer (by setting clipnorm or clipvalue) is currently unsupported when using a distribution strategy\n",
    "# clipnorm=1.0\n",
    "\n",
    "# loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Uses the tf.distribute.MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine\n",
    "strategy_model_1 = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy_model_1.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy_model_1.scope():\n",
    "    # metric\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    \n",
    "    # model\n",
    "    model_1 = TFBertForSequenceClassification.from_pretrained('bert-base-cased',num_labels=number_label)\n",
    "    #model.layers[-1].activation = tf.keras.activations.softmax\n",
    "    model_1._name='tf_bert_classification'\n",
    "    model_1.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  108310272 \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Building a custom classification model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def custom_keras_model(number_classes, bert_model):\n",
    "\n",
    "    # create model\n",
    "    input_layer = tf.keras.Input(shape = (128,), dtype='int64')    \n",
    "    bert_ini = TFBertModel.from_pretrained('bert-base-cased') (input_layer)\n",
    "    # This is because in a bert pretraining progress, there are two tasks: \n",
    "    # masked token prediction and next sentence predition . \n",
    "    # The first needs hidden state of each tokens ( shape: [batch_size, sequence_length, hidden_size]) \n",
    "    # the second needs the embedding of the whole sequence (shape : [batch_size, hidden_size] ) .\n",
    "    bert = bert_ini[1]    \n",
    "    dropout = tf.keras.layers.Dropout(0.1)(bert)\n",
    "    flat = tf.keras.layers.Flatten()(dropout)\n",
    "    classifier = tf.keras.layers.Dense(units=number_classes )(flat) # activation='softmax'               \n",
    "    \n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=classifier, name='custom_tf_bert_classification')\n",
    "\n",
    "    return model, bert_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of devices: 1\n"
     ]
    }
   ],
   "source": [
    "# Define some parameters\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08)\n",
    "# Gradient clipping in the optimizer (by setting clipnorm or clipvalue) is currently unsupported when using a distribution strategy\n",
    "# clipnorm=1.0\n",
    "\n",
    "# loss\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Uses the tf.distribute.MirroredStrategy, which does in-graph replication with synchronous training on many GPUs on one machine\n",
    "strategy_model_2 = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy_model_1.num_replicas_in_sync))\n",
    "\n",
    "# create and compile the Keras model in the context of strategy.scope\n",
    "with strategy_model_2.scope():\n",
    "    # metric\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    \n",
    "    # model\n",
    "    model_2, bert_ini = custom_keras_model(number_label, 'bert-base-cased')\n",
    "    model_2.compile(optimizer=optimizer,\n",
    "                    loss=loss, \n",
    "                    metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'tf_bert_model/Identity:0' shape=(None, 128, 768) dtype=float32>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ini[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'tf_bert_model/Identity_1:0' shape=(None, 768) dtype=float32>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ini[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_tf_bert_classification\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 128)]             0         \n",
      "_________________________________________________________________\n",
      "tf_bert_model (TFBertModel)  ((None, 128, 768), (None, 108310272 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 1538      \n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 108,311,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Choose the model you want to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model's name: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "model=model_1\n",
    "print('model\\'s name: {}'.format(model.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a2df25e3a7de39d1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a2df25e3a7de39d1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard\n",
    "%tensorboard  --logdir   {log_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Final feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def data_feature_extraction(data, name):\n",
    "    if name=='custom_tf_bert_classification':\n",
    "        print('custom model: {}'.format(name))\n",
    "        return data.map(pp.feature_selection)\n",
    "    elif name=='tf_bert_classification':\n",
    "        print('standard model: {}'.format(name))\n",
    "        return data\n",
    "    else:\n",
    "        print('!!! non defined model !!!!')\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard model: tf_bert_classification\n",
      "standard model: tf_bert_classification\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " training set -> batch:1 loss:0.724179744720459 and acc: 0.5\n",
      "   1/2105 [..............................] - ETA: 0s - accuracy: 0.5000 - loss: 0.7242\n",
      " training set -> batch:2 loss:0.7333883047103882 and acc: 0.46875\n",
      "   2/2105 [..............................] - ETA: 8:32 - accuracy: 0.4688 - loss: 0.7334\n",
      " training set -> batch:3 loss:0.722629964351654 and acc: 0.5\n",
      "   3/2105 [..............................] - ETA: 11:06 - accuracy: 0.5000 - loss: 0.7226\n",
      " training set -> batch:4 loss:0.7129441499710083 and acc: 0.5078125\n",
      "   4/2105 [..............................] - ETA: 12:23 - accuracy: 0.5078 - loss: 0.7129\n",
      " training set -> batch:5 loss:0.7117813229560852 and acc: 0.518750011920929\n",
      "   5/2105 [..............................] - ETA: 13:09 - accuracy: 0.5188 - loss: 0.7118\n",
      " training set -> batch:6 loss:0.7003054022789001 and acc: 0.5364583134651184\n",
      "   6/2105 [..............................] - ETA: 13:40 - accuracy: 0.5365 - loss: 0.7003\n",
      " training set -> batch:7 loss:0.706764280796051 and acc: 0.53125\n",
      "   7/2105 [..............................] - ETA: 14:01 - accuracy: 0.5312 - loss: 0.7068\n",
      " training set -> batch:8 loss:0.6988481879234314 and acc: 0.5546875\n",
      "   8/2105 [..............................] - ETA: 14:17 - accuracy: 0.5547 - loss: 0.6988\n",
      " training set -> batch:9 loss:0.6894538402557373 and acc: 0.5729166865348816\n",
      "   9/2105 [..............................] - ETA: 14:29 - accuracy: 0.5729 - loss: 0.6895\n",
      " training set -> batch:10 loss:0.6898026466369629 and acc: 0.5718749761581421\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " validation set -> batch:10 val loss:0.6937678456306458 and val acc: 0.5091742873191833\n",
      "  10/2105 [..............................] - ETA: 42:30 - accuracy: 0.5719 - loss: 0.6898\n",
      " training set -> batch:11 loss:0.6917678713798523 and acc: 0.5121681690216064\n",
      "  11/2105 [..............................] - ETA: 40:05 - accuracy: 0.5122 - loss: 0.6918\n",
      " training set -> batch:12 loss:0.6912035942077637 and acc: 0.5149572491645813\n",
      "  12/2105 [..............................] - ETA: 38:05 - accuracy: 0.5150 - loss: 0.6912\n",
      " training set -> batch:13 loss:0.6928752064704895 and acc: 0.5154958963394165\n",
      "  13/2105 [..............................] - ETA: 36:22 - accuracy: 0.5155 - loss: 0.6929\n",
      " training set -> batch:14 loss:0.6933264136314392 and acc: 0.515999972820282\n",
      "  14/2105 [..............................] - ETA: 34:55 - accuracy: 0.5160 - loss: 0.6933\n",
      " training set -> batch:15 loss:0.6951079964637756 and acc: 0.5125969052314758\n",
      "  15/2105 [..............................] - ETA: 33:39 - accuracy: 0.5126 - loss: 0.6951\n",
      " training set -> batch:16 loss:0.6938676238059998 and acc: 0.5131579041481018\n",
      "  16/2105 [..............................] - ETA: 32:32 - accuracy: 0.5132 - loss: 0.6939\n",
      " training set -> batch:17 loss:0.6927505731582642 and acc: 0.5118613243103027\n",
      "  17/2105 [..............................] - ETA: 31:33 - accuracy: 0.5119 - loss: 0.6928\n",
      " training set -> batch:18 loss:0.6900516152381897 and acc: 0.5177304744720459\n",
      "  18/2105 [..............................] - ETA: 30:41 - accuracy: 0.5177 - loss: 0.6901\n",
      " training set -> batch:19 loss:0.6878761053085327 and acc: 0.5198276042938232\n",
      "  19/2105 [..............................] - ETA: 29:54 - accuracy: 0.5198 - loss: 0.6879\n",
      " training set -> batch:20 loss:0.6864914894104004 and acc: 0.5234899520874023\n",
      "\n",
      " validation set -> batch:20 val loss:0.6468473672866821 and val acc: 0.7614678740501404\n",
      "  20/2105 [..............................] - ETA: 43:44 - accuracy: 0.5235 - loss: 0.6865\n",
      " training set -> batch:21 loss:0.6459929347038269 and acc: 0.7533186078071594\n",
      "  21/2105 [..............................] - ETA: 42:24 - accuracy: 0.7533 - loss: 0.6460\n",
      " training set -> batch:22 loss:0.6477506160736084 and acc: 0.7467948794364929\n",
      "  22/2105 [..............................] - ETA: 41:11 - accuracy: 0.7468 - loss: 0.6478\n",
      " training set -> batch:23 loss:0.6449240446090698 and acc: 0.7469007968902588\n",
      "  23/2105 [..............................] - ETA: 40:05 - accuracy: 0.7469 - loss: 0.6449\n",
      " training set -> batch:24 loss:0.6436407566070557 and acc: 0.7490000128746033\n",
      "  24/2105 [..............................] - ETA: 39:04 - accuracy: 0.7490 - loss: 0.6436\n",
      " training set -> batch:25 loss:0.6371721029281616 and acc: 0.75\n",
      "  25/2105 [..............................] - ETA: 38:07 - accuracy: 0.7500 - loss: 0.6372\n",
      " training set -> batch:26 loss:0.6370604038238525 and acc: 0.7434210777282715\n",
      "  26/2105 [..............................] - ETA: 37:16 - accuracy: 0.7434 - loss: 0.6371\n",
      " training set -> batch:27 loss:0.6370235681533813 and acc: 0.7372262477874756\n",
      "  27/2105 [..............................] - ETA: 36:28 - accuracy: 0.7372 - loss: 0.6370\n",
      " training set -> batch:28 loss:0.6314599514007568 and acc: 0.73758864402771\n",
      "  28/2105 [..............................] - ETA: 35:43 - accuracy: 0.7376 - loss: 0.6315\n",
      " training set -> batch:29 loss:0.6252702474594116 and acc: 0.7413793206214905\n",
      "  29/2105 [..............................] - ETA: 35:01 - accuracy: 0.7414 - loss: 0.6253\n",
      " training set -> batch:30 loss:0.6223271489143372 and acc: 0.7407718300819397\n",
      "\n",
      " validation set -> batch:30 val loss:0.4998639225959778 and val acc: 0.7901375889778137\n",
      "  30/2105 [..............................] - ETA: 43:23 - accuracy: 0.7408 - loss: 0.6223\n",
      " training set -> batch:31 loss:0.5009639263153076 and acc: 0.7876105904579163\n",
      "  31/2105 [..............................] - ETA: 42:29 - accuracy: 0.7876 - loss: 0.5010\n",
      " training set -> batch:32 loss:0.4969373941421509 and acc: 0.7905982732772827\n",
      "  32/2105 [..............................] - ETA: 41:38 - accuracy: 0.7906 - loss: 0.4969\n",
      " training set -> batch:33 loss:0.49509161710739136 and acc: 0.7902892827987671\n",
      "  33/2105 [..............................] - ETA: 40:50 - accuracy: 0.7903 - loss: 0.4951\n",
      " training set -> batch:34 loss:0.49599727988243103 and acc: 0.7860000133514404\n",
      "  34/2105 [..............................] - ETA: 40:05 - accuracy: 0.7860 - loss: 0.4960\n",
      " training set -> batch:35 loss:0.4962650537490845 and acc: 0.7858527302742004\n",
      "  35/2105 [..............................] - ETA: 39:23 - accuracy: 0.7859 - loss: 0.4963\n",
      " training set -> batch:36 loss:0.4886254668235779 and acc: 0.7894737124443054\n",
      "  36/2105 [..............................] - ETA: 38:43 - accuracy: 0.7895 - loss: 0.4886\n",
      " training set -> batch:37 loss:0.5024517178535461 and acc: 0.7837591171264648\n",
      "  37/2105 [..............................] - ETA: 38:05 - accuracy: 0.7838 - loss: 0.5025\n",
      " training set -> batch:38 loss:0.4989641010761261 and acc: 0.7828013896942139\n",
      "  38/2105 [..............................] - ETA: 37:29 - accuracy: 0.7828 - loss: 0.4990\n",
      " training set -> batch:39 loss:0.4950472414493561 and acc: 0.7827585935592651\n",
      "  39/2105 [..............................] - ETA: 36:55 - accuracy: 0.7828 - loss: 0.4950\n",
      " training set -> batch:40 loss:0.4854811131954193 and acc: 0.786912739276886\n",
      "\n",
      " validation set -> batch:40 val loss:0.3942175507545471 and val acc: 0.8337156176567078\n",
      "  40/2105 [..............................] - ETA: 43:11 - accuracy: 0.7869 - loss: 0.4855\n",
      " training set -> batch:41 loss:0.3989655077457428 and acc: 0.8307521939277649\n",
      "  41/2105 [..............................] - ETA: 42:30 - accuracy: 0.8308 - loss: 0.3990\n",
      " training set -> batch:42 loss:0.3864281177520752 and acc: 0.8354700803756714\n",
      "  42/2105 [..............................] - ETA: 41:50 - accuracy: 0.8355 - loss: 0.3864\n",
      " training set -> batch:43 loss:0.3863835036754608 and acc: 0.8336777091026306\n",
      "  43/2105 [..............................] - ETA: 41:13 - accuracy: 0.8337 - loss: 0.3864\n",
      " training set -> batch:44 loss:0.38526999950408936 and acc: 0.8339999914169312\n",
      "  44/2105 [..............................] - ETA: 40:37 - accuracy: 0.8340 - loss: 0.3853\n",
      " training set -> batch:45 loss:0.3790629208087921 and acc: 0.836240291595459\n",
      "  45/2105 [..............................] - ETA: 40:03 - accuracy: 0.8362 - loss: 0.3791\n",
      " training set -> batch:46 loss:0.3739837408065796 and acc: 0.8392857313156128\n",
      "  46/2105 [..............................] - ETA: 39:31 - accuracy: 0.8393 - loss: 0.3740\n",
      " training set -> batch:47 loss:0.3761959671974182 and acc: 0.8385036587715149\n",
      "  47/2105 [..............................] - ETA: 38:59 - accuracy: 0.8385 - loss: 0.3762\n",
      " training set -> batch:48 loss:0.3749289810657501 and acc: 0.838652491569519\n",
      "  48/2105 [..............................] - ETA: 38:29 - accuracy: 0.8387 - loss: 0.3749\n",
      " training set -> batch:49 loss:0.3672337234020233 and acc: 0.8422414064407349\n",
      "  49/2105 [..............................] - ETA: 38:01 - accuracy: 0.8422 - loss: 0.3672\n",
      " training set -> batch:50 loss:0.36743295192718506 and acc: 0.8422818779945374\n",
      "\n",
      " validation set -> batch:50 val loss:0.3685525357723236 and val acc: 0.8428899049758911\n",
      "  50/2105 [..............................] - ETA: 42:59 - accuracy: 0.8423 - loss: 0.3674\n",
      " training set -> batch:51 loss:0.36588096618652344 and acc: 0.8440265655517578\n",
      "  51/2105 [..............................] - ETA: 42:26 - accuracy: 0.8440 - loss: 0.3659\n",
      " training set -> batch:52 loss:0.36047402024269104 and acc: 0.8472222089767456\n",
      "  52/2105 [..............................] - ETA: 41:54 - accuracy: 0.8472 - loss: 0.3605\n",
      " training set -> batch:53 loss:0.348292738199234 and acc: 0.8512396812438965\n",
      "  53/2105 [..............................] - ETA: 41:23 - accuracy: 0.8512 - loss: 0.3483\n",
      " training set -> batch:54 loss:0.35602492094039917 and acc: 0.8510000109672546\n",
      "  54/2105 [..............................] - ETA: 40:54 - accuracy: 0.8510 - loss: 0.3560\n",
      " training set -> batch:55 loss:0.3520241677761078 and acc: 0.8517441749572754\n",
      "  55/2105 [..............................] - ETA: 40:25 - accuracy: 0.8517 - loss: 0.3520\n",
      " training set -> batch:56 loss:0.35049933195114136 and acc: 0.853383481502533\n",
      "  56/2105 [..............................] - ETA: 39:58 - accuracy: 0.8534 - loss: 0.3505\n",
      " training set -> batch:57 loss:0.3535624146461487 and acc: 0.8521897792816162\n",
      "  57/2105 [..............................] - ETA: 39:31 - accuracy: 0.8522 - loss: 0.3536\n",
      " training set -> batch:58 loss:0.3447350263595581 and acc: 0.8554964661598206\n",
      "  58/2105 [..............................] - ETA: 39:05 - accuracy: 0.8555 - loss: 0.3447\n",
      " training set -> batch:59 loss:0.33643838763237 and acc: 0.8586207032203674\n",
      "  59/2105 [..............................] - ETA: 38:41 - accuracy: 0.8586 - loss: 0.3364\n",
      " training set -> batch:60 loss:0.3310079574584961 and acc: 0.8615771532058716\n",
      "\n",
      " validation set -> batch:60 val loss:0.3169082999229431 and val acc: 0.872706413269043\n",
      "  60/2105 [..............................] - ETA: 42:44 - accuracy: 0.8616 - loss: 0.3310\n",
      " training set -> batch:61 loss:0.31096261739730835 and acc: 0.875\n",
      "  61/2105 [..............................] - ETA: 42:16 - accuracy: 0.8750 - loss: 0.3110\n",
      " training set -> batch:62 loss:0.3122881352901459 and acc: 0.8739316463470459\n",
      "  62/2105 [..............................] - ETA: 41:49 - accuracy: 0.8739 - loss: 0.3123\n",
      " training set -> batch:63 loss:0.30348241329193115 and acc: 0.875\n",
      "  63/2105 [..............................] - ETA: 41:23 - accuracy: 0.8750 - loss: 0.3035\n",
      " training set -> batch:64 loss:0.31277045607566833 and acc: 0.871999979019165\n",
      "  64/2105 [..............................] - ETA: 40:58 - accuracy: 0.8720 - loss: 0.3128\n",
      " training set -> batch:65 loss:0.3123531937599182 and acc: 0.8720930218696594\n",
      "  65/2105 [..............................] - ETA: 40:34 - accuracy: 0.8721 - loss: 0.3124\n",
      " training set -> batch:66 loss:0.30914247035980225 and acc: 0.8740601539611816\n",
      "  66/2105 [..............................] - ETA: 40:10 - accuracy: 0.8741 - loss: 0.3091\n",
      " training set -> batch:67 loss:0.3074633479118347 and acc: 0.875\n",
      "  67/2105 [..............................] - ETA: 39:47 - accuracy: 0.8750 - loss: 0.3075\n",
      " training set -> batch:68 loss:0.3100723922252655 and acc: 0.8741135001182556\n",
      "  68/2105 [..............................] - ETA: 39:25 - accuracy: 0.8741 - loss: 0.3101\n",
      " training set -> batch:69 loss:0.3112729787826538 and acc: 0.873275876045227\n",
      "  69/2105 [..............................] - ETA: 39:03 - accuracy: 0.8733 - loss: 0.3113\n",
      " training set -> batch:70 loss:0.3082619905471802 and acc: 0.8741610646247864\n",
      "\n",
      " validation set -> batch:70 val loss:0.30843570828437805 and val acc: 0.872706413269043\n",
      "  70/2105 [..............................] - ETA: 42:31 - accuracy: 0.8742 - loss: 0.3083\n",
      " training set -> batch:71 loss:0.31245115399360657 and acc: 0.8716813921928406\n",
      "  71/2105 [>.............................] - ETA: 42:07 - accuracy: 0.8717 - loss: 0.3125\n",
      " training set -> batch:72 loss:0.317179411649704 and acc: 0.870726466178894\n",
      "  72/2105 [>.............................] - ETA: 41:44 - accuracy: 0.8707 - loss: 0.3172\n",
      " training set -> batch:73 loss:0.32095664739608765 and acc: 0.8698347210884094\n",
      "  73/2105 [>.............................] - ETA: 41:21 - accuracy: 0.8698 - loss: 0.3210\n",
      " training set -> batch:74 loss:0.3213258385658264 and acc: 0.8700000047683716\n",
      "  74/2105 [>.............................] - ETA: 40:59 - accuracy: 0.8700 - loss: 0.3213\n",
      " training set -> batch:75 loss:0.32199597358703613 and acc: 0.8691860437393188\n",
      "  75/2105 [>.............................] - ETA: 40:38 - accuracy: 0.8692 - loss: 0.3220\n",
      " training set -> batch:76 loss:0.326690137386322 and acc: 0.8674812316894531\n",
      "  76/2105 [>.............................] - ETA: 40:17 - accuracy: 0.8675 - loss: 0.3267\n",
      " training set -> batch:77 loss:0.32011935114860535 and acc: 0.8704379796981812\n",
      "  77/2105 [>.............................] - ETA: 39:57 - accuracy: 0.8704 - loss: 0.3201\n",
      " training set -> batch:78 loss:0.31691503524780273 and acc: 0.8714538812637329\n",
      "  78/2105 [>.............................] - ETA: 39:37 - accuracy: 0.8715 - loss: 0.3169\n",
      " training set -> batch:79 loss:0.3186028003692627 and acc: 0.8706896305084229\n",
      "  79/2105 [>.............................] - ETA: 39:18 - accuracy: 0.8707 - loss: 0.3186\n",
      " training set -> batch:80 loss:0.3174269497394562 and acc: 0.8716443181037903\n",
      "\n",
      " validation set -> batch:80 val loss:0.40137696266174316 and val acc: 0.8268348574638367\n",
      "  80/2105 [>.............................] - ETA: 42:18 - accuracy: 0.8716 - loss: 0.3174\n",
      " training set -> batch:81 loss:0.4004167914390564 and acc: 0.8252212405204773\n",
      "  81/2105 [>.............................] - ETA: 41:57 - accuracy: 0.8252 - loss: 0.4004\n",
      " training set -> batch:82 loss:0.3926727771759033 and acc: 0.8279914259910583\n",
      "  82/2105 [>.............................] - ETA: 41:37 - accuracy: 0.8280 - loss: 0.3927\n",
      " training set -> batch:83 loss:0.3889820873737335 and acc: 0.8285123705863953\n",
      "  83/2105 [>.............................] - ETA: 41:17 - accuracy: 0.8285 - loss: 0.3890\n",
      " training set -> batch:84 loss:0.380844384431839 and acc: 0.8309999704360962\n",
      "  84/2105 [>.............................] - ETA: 40:57 - accuracy: 0.8310 - loss: 0.3808\n",
      " training set -> batch:85 loss:0.3837948143482208 and acc: 0.8284883499145508\n",
      "  85/2105 [>.............................] - ETA: 40:38 - accuracy: 0.8285 - loss: 0.3838\n",
      " training set -> batch:86 loss:0.3906399607658386 and acc: 0.8251879811286926\n",
      "  86/2105 [>.............................] - ETA: 40:20 - accuracy: 0.8252 - loss: 0.3906\n",
      " training set -> batch:87 loss:0.3907473087310791 and acc: 0.8248175382614136\n",
      "  87/2105 [>.............................] - ETA: 40:01 - accuracy: 0.8248 - loss: 0.3907\n",
      " training set -> batch:88 loss:0.38982608914375305 and acc: 0.8244680762290955\n",
      "  88/2105 [>.............................] - ETA: 39:43 - accuracy: 0.8245 - loss: 0.3898\n",
      " training set -> batch:89 loss:0.390727162361145 and acc: 0.824999988079071\n",
      "  89/2105 [>.............................] - ETA: 39:26 - accuracy: 0.8250 - loss: 0.3907\n",
      " training set -> batch:90 loss:0.38509318232536316 and acc: 0.8271812200546265\n",
      "\n",
      " validation set -> batch:90 val loss:0.293438583612442 and val acc: 0.8795871734619141\n",
      "  90/2105 [>.............................] - ETA: 42:20 - accuracy: 0.8272 - loss: 0.3851\n",
      " training set -> batch:91 loss:0.29017555713653564 and acc: 0.8794247508049011\n",
      "  91/2105 [>.............................] - ETA: 42:01 - accuracy: 0.8794 - loss: 0.2902\n",
      " training set -> batch:92 loss:0.28784626722335815 and acc: 0.8814102411270142\n",
      "  92/2105 [>.............................] - ETA: 41:43 - accuracy: 0.8814 - loss: 0.2878\n",
      " training set -> batch:93 loss:0.286942720413208 and acc: 0.8832644820213318\n",
      "  93/2105 [>.............................] - ETA: 41:24 - accuracy: 0.8833 - loss: 0.2869\n",
      " training set -> batch:94 loss:0.290000319480896 and acc: 0.8799999952316284\n",
      "  94/2105 [>.............................] - ETA: 41:07 - accuracy: 0.8800 - loss: 0.2900\n",
      " training set -> batch:95 loss:0.30094072222709656 and acc: 0.8779069781303406\n",
      "  95/2105 [>.............................] - ETA: 40:49 - accuracy: 0.8779 - loss: 0.3009\n",
      " training set -> batch:96 loss:0.30884790420532227 and acc: 0.875\n",
      "  96/2105 [>.............................] - ETA: 40:32 - accuracy: 0.8750 - loss: 0.3088\n",
      " training set -> batch:97 loss:0.3135780394077301 and acc: 0.8731752038002014\n",
      "  97/2105 [>.............................] - ETA: 40:16 - accuracy: 0.8732 - loss: 0.3136\n",
      " training set -> batch:98 loss:0.31589630246162415 and acc: 0.8714538812637329\n",
      "  98/2105 [>.............................] - ETA: 39:59 - accuracy: 0.8715 - loss: 0.3159\n",
      " training set -> batch:99 loss:0.31458234786987305 and acc: 0.8706896305084229\n",
      "  99/2105 [>.............................] - ETA: 39:43 - accuracy: 0.8707 - loss: 0.3146\n",
      " training set -> batch:100 loss:0.31194007396698 and acc: 0.8724831938743591\n",
      "\n",
      " validation set -> batch:100 val loss:0.3075471520423889 and val acc: 0.8830274939537048\n",
      " 100/2105 [>.............................] - ETA: 42:06 - accuracy: 0.8725 - loss: 0.3119\n",
      " training set -> batch:101 loss:0.3048679530620575 and acc: 0.8849557638168335\n",
      " 101/2105 [>.............................] - ETA: 41:49 - accuracy: 0.8850 - loss: 0.3049\n",
      " training set -> batch:102 loss:0.315562903881073 and acc: 0.8814102411270142\n",
      " 102/2105 [>.............................] - ETA: 41:32 - accuracy: 0.8814 - loss: 0.3156\n",
      " training set -> batch:103 loss:0.30946215987205505 and acc: 0.8832644820213318\n",
      " 103/2105 [>.............................] - ETA: 41:16 - accuracy: 0.8833 - loss: 0.3095\n",
      " training set -> batch:104 loss:0.30981847643852234 and acc: 0.8830000162124634\n",
      " 104/2105 [>.............................] - ETA: 41:00 - accuracy: 0.8830 - loss: 0.3098\n",
      " training set -> batch:105 loss:0.3142966628074646 and acc: 0.8808139562606812\n",
      " 105/2105 [>.............................] - ETA: 40:44 - accuracy: 0.8808 - loss: 0.3143\n",
      " training set -> batch:106 loss:0.3099169135093689 and acc: 0.8806390762329102\n",
      " 106/2105 [>.............................] - ETA: 40:28 - accuracy: 0.8806 - loss: 0.3099\n",
      " training set -> batch:107 loss:0.31197789311408997 and acc: 0.8777372241020203\n",
      " 107/2105 [>.............................] - ETA: 40:13 - accuracy: 0.8777 - loss: 0.3120\n",
      " training set -> batch:108 loss:0.31368687748908997 and acc: 0.8767730593681335\n",
      " 108/2105 [>.............................] - ETA: 39:58 - accuracy: 0.8768 - loss: 0.3137\n",
      " training set -> batch:109 loss:0.3075442910194397 and acc: 0.8784482479095459\n",
      " 109/2105 [>.............................] - ETA: 39:43 - accuracy: 0.8784 - loss: 0.3075\n",
      " training set -> batch:110 loss:0.3139919936656952 and acc: 0.8758389353752136\n",
      "\n",
      " validation set -> batch:110 val loss:0.3117053508758545 and val acc: 0.872706413269043\n",
      " 110/2105 [>.............................] - ETA: 41:52 - accuracy: 0.8758 - loss: 0.3140\n",
      " training set -> batch:111 loss:0.3122911751270294 and acc: 0.872787594795227\n",
      " 111/2105 [>.............................] - ETA: 41:37 - accuracy: 0.8728 - loss: 0.3123\n",
      " training set -> batch:112 loss:0.3141031563282013 and acc: 0.872863233089447\n",
      " 112/2105 [>.............................] - ETA: 41:21 - accuracy: 0.8729 - loss: 0.3141\n",
      " training set -> batch:113 loss:0.3139340877532959 and acc: 0.8719007968902588\n",
      " 113/2105 [>.............................] - ETA: 41:06 - accuracy: 0.8719 - loss: 0.3139\n",
      " training set -> batch:114 loss:0.30338770151138306 and acc: 0.875\n",
      " 114/2105 [>.............................] - ETA: 40:52 - accuracy: 0.8750 - loss: 0.3034\n",
      " training set -> batch:115 loss:0.30302149057388306 and acc: 0.8740310072898865\n",
      " 115/2105 [>.............................] - ETA: 40:37 - accuracy: 0.8740 - loss: 0.3030\n",
      " training set -> batch:116 loss:0.3035958409309387 and acc: 0.875\n",
      " 116/2105 [>.............................] - ETA: 40:23 - accuracy: 0.8750 - loss: 0.3036\n",
      " training set -> batch:117 loss:0.3014591634273529 and acc: 0.8759124279022217\n",
      " 117/2105 [>.............................] - ETA: 40:09 - accuracy: 0.8759 - loss: 0.3015\n",
      " training set -> batch:118 loss:0.2967158555984497 and acc: 0.8776595592498779\n",
      " 118/2105 [>.............................] - ETA: 39:55 - accuracy: 0.8777 - loss: 0.2967\n",
      " training set -> batch:119 loss:0.2910752296447754 and acc: 0.8801724314689636\n",
      " 119/2105 [>.............................] - ETA: 39:42 - accuracy: 0.8802 - loss: 0.2911\n",
      " training set -> batch:120 loss:0.2860843241214752 and acc: 0.8808724880218506\n",
      "\n",
      " validation set -> batch:120 val loss:0.27856606245040894 and val acc: 0.8864678740501404\n",
      " 120/2105 [>.............................] - ETA: 41:40 - accuracy: 0.8809 - loss: 0.2861\n",
      " training set -> batch:121 loss:0.27781835198402405 and acc: 0.88606196641922\n",
      " 121/2105 [>.............................] - ETA: 41:26 - accuracy: 0.8861 - loss: 0.2778\n",
      " training set -> batch:122 loss:0.27199727296829224 and acc: 0.8878205418586731\n",
      " 122/2105 [>.............................] - ETA: 41:12 - accuracy: 0.8878 - loss: 0.2720\n",
      " training set -> batch:123 loss:0.26965683698654175 and acc: 0.8884297609329224\n",
      " 123/2105 [>.............................] - ETA: 40:58 - accuracy: 0.8884 - loss: 0.2697\n",
      " training set -> batch:124 loss:0.27440255880355835 and acc: 0.8880000114440918\n",
      " 124/2105 [>.............................] - ETA: 40:45 - accuracy: 0.8880 - loss: 0.2744\n",
      " training set -> batch:125 loss:0.2691717743873596 and acc: 0.8885658979415894\n",
      " 125/2105 [>.............................] - ETA: 40:31 - accuracy: 0.8886 - loss: 0.2692\n",
      " training set -> batch:126 loss:0.27995672821998596 and acc: 0.8872180581092834\n",
      " 126/2105 [>.............................] - ETA: 40:18 - accuracy: 0.8872 - loss: 0.2800\n",
      " training set -> batch:127 loss:0.2751130759716034 and acc: 0.8886861205101013\n",
      " 127/2105 [>.............................] - ETA: 40:05 - accuracy: 0.8887 - loss: 0.2751\n",
      " training set -> batch:128 loss:0.27628079056739807 and acc: 0.8865247964859009\n",
      " 128/2105 [>.............................] - ETA: 39:52 - accuracy: 0.8865 - loss: 0.2763\n",
      " training set -> batch:129 loss:0.2730439305305481 and acc: 0.8870689868927002\n",
      " 129/2105 [>.............................] - ETA: 39:39 - accuracy: 0.8871 - loss: 0.2730\n",
      " training set -> batch:130 loss:0.2677093744277954 and acc: 0.8892617225646973\n",
      "\n",
      " validation set -> batch:130 val loss:0.27320101857185364 and val acc: 0.89449542760849\n",
      " 130/2105 [>.............................] - ETA: 41:27 - accuracy: 0.8893 - loss: 0.2677\n",
      " training set -> batch:131 loss:0.26637065410614014 and acc: 0.8960176706314087\n",
      " 131/2105 [>.............................] - ETA: 41:13 - accuracy: 0.8960 - loss: 0.2664\n",
      " training set -> batch:132 loss:0.26581546664237976 and acc: 0.8963675498962402\n",
      " 132/2105 [>.............................] - ETA: 41:00 - accuracy: 0.8964 - loss: 0.2658\n",
      " training set -> batch:133 loss:0.27418142557144165 and acc: 0.8925619721412659\n",
      " 133/2105 [>.............................] - ETA: 40:47 - accuracy: 0.8926 - loss: 0.2742\n",
      " training set -> batch:134 loss:0.26286253333091736 and acc: 0.8960000276565552\n",
      " 134/2105 [>.............................] - ETA: 40:35 - accuracy: 0.8960 - loss: 0.2629\n",
      " training set -> batch:135 loss:0.26609301567077637 and acc: 0.8943798542022705\n",
      " 135/2105 [>.............................] - ETA: 40:22 - accuracy: 0.8944 - loss: 0.2661\n",
      " training set -> batch:136 loss:0.27208560705184937 and acc: 0.8928571343421936\n",
      " 136/2105 [>.............................] - ETA: 40:10 - accuracy: 0.8929 - loss: 0.2721\n",
      " training set -> batch:137 loss:0.2780158519744873 and acc: 0.8923357725143433\n",
      " 137/2105 [>.............................] - ETA: 39:58 - accuracy: 0.8923 - loss: 0.2780\n",
      " training set -> batch:138 loss:0.28217020630836487 and acc: 0.890070915222168\n",
      " 138/2105 [>.............................] - ETA: 39:46 - accuracy: 0.8901 - loss: 0.2822\n",
      " training set -> batch:139 loss:0.28323572874069214 and acc: 0.8905172348022461\n",
      " 139/2105 [>.............................] - ETA: 39:34 - accuracy: 0.8905 - loss: 0.2832\n",
      " training set -> batch:140 loss:0.27760937809944153 and acc: 0.8917785286903381\n",
      "\n",
      " validation set -> batch:140 val loss:0.2690039277076721 and val acc: 0.8899082541465759\n",
      " 140/2105 [>.............................] - ETA: 41:12 - accuracy: 0.8918 - loss: 0.2776\n",
      " training set -> batch:141 loss:0.26724135875701904 and acc: 0.8882743120193481\n",
      " 141/2105 [=>............................] - ETA: 41:00 - accuracy: 0.8883 - loss: 0.2672\n",
      " training set -> batch:142 loss:0.26766884326934814 and acc: 0.8856837749481201\n",
      " 142/2105 [=>............................] - ETA: 40:48 - accuracy: 0.8857 - loss: 0.2677\n",
      " training set -> batch:143 loss:0.26258715987205505 and acc: 0.8873966932296753\n",
      " 143/2105 [=>............................] - ETA: 40:36 - accuracy: 0.8874 - loss: 0.2626\n",
      " training set -> batch:144 loss:0.2609756588935852 and acc: 0.8870000243186951\n",
      " 144/2105 [=>............................] - ETA: 40:24 - accuracy: 0.8870 - loss: 0.2610\n",
      " training set -> batch:145 loss:0.2589803636074066 and acc: 0.8885658979415894\n",
      " 145/2105 [=>............................] - ETA: 40:12 - accuracy: 0.8886 - loss: 0.2590\n",
      " training set -> batch:146 loss:0.25955504179000854 and acc: 0.8890977501869202\n",
      " 146/2105 [=>............................] - ETA: 40:01 - accuracy: 0.8891 - loss: 0.2596\n",
      " training set -> batch:147 loss:0.25774940848350525 and acc: 0.889598548412323\n",
      " 147/2105 [=>............................] - ETA: 39:50 - accuracy: 0.8896 - loss: 0.2577\n",
      " training set -> batch:148 loss:0.253466933965683 and acc: 0.890070915222168\n",
      " 148/2105 [=>............................] - ETA: 39:38 - accuracy: 0.8901 - loss: 0.2535\n",
      " training set -> batch:149 loss:0.2604217529296875 and acc: 0.8879310488700867\n",
      " 149/2105 [=>............................] - ETA: 39:27 - accuracy: 0.8879 - loss: 0.2604\n",
      " training set -> batch:150 loss:0.2691943049430847 and acc: 0.8850671052932739\n",
      "\n",
      " validation set -> batch:150 val loss:0.29733601212501526 and val acc: 0.8715596199035645\n",
      " 150/2105 [=>............................] - ETA: 40:59 - accuracy: 0.8851 - loss: 0.2692\n",
      " training set -> batch:151 loss:0.318082720041275 and acc: 0.8672566413879395\n",
      " 151/2105 [=>............................] - ETA: 40:47 - accuracy: 0.8673 - loss: 0.3181\n",
      " training set -> batch:152 loss:0.31875085830688477 and acc: 0.867521345615387\n",
      " 152/2105 [=>............................] - ETA: 40:36 - accuracy: 0.8675 - loss: 0.3188\n",
      " training set -> batch:153 loss:0.3199809193611145 and acc: 0.8688016533851624\n",
      " 153/2105 [=>............................] - ETA: 40:25 - accuracy: 0.8688 - loss: 0.3200\n",
      " training set -> batch:154 loss:0.3197084963321686 and acc: 0.8690000176429749\n",
      " 154/2105 [=>............................] - ETA: 40:13 - accuracy: 0.8690 - loss: 0.3197\n",
      " training set -> batch:155 loss:0.32388123869895935 and acc: 0.8682170510292053\n",
      " 155/2105 [=>............................] - ETA: 40:02 - accuracy: 0.8682 - loss: 0.3239\n",
      " training set -> batch:156 loss:0.3263917863368988 and acc: 0.8646616339683533\n",
      " 156/2105 [=>............................] - ETA: 39:52 - accuracy: 0.8647 - loss: 0.3264\n",
      " training set -> batch:157 loss:0.32259565591812134 and acc: 0.8667883276939392\n",
      " 157/2105 [=>............................] - ETA: 39:41 - accuracy: 0.8668 - loss: 0.3226\n",
      " training set -> batch:158 loss:0.32348310947418213 and acc: 0.866134762763977\n",
      " 158/2105 [=>............................] - ETA: 39:30 - accuracy: 0.8661 - loss: 0.3235\n",
      " training set -> batch:159 loss:0.3257399797439575 and acc: 0.865517258644104\n",
      " 159/2105 [=>............................] - ETA: 39:20 - accuracy: 0.8655 - loss: 0.3257\n",
      " training set -> batch:160 loss:0.3252761662006378 and acc: 0.8649328947067261\n",
      "\n",
      " validation set -> batch:160 val loss:0.26831820607185364 and val acc: 0.8979358077049255\n",
      " 160/2105 [=>............................] - ETA: 40:33 - accuracy: 0.8649 - loss: 0.3253\n",
      " training set -> batch:161 loss:0.27890345454216003 and acc: 0.8971238732337952\n",
      " 161/2105 [=>............................] - ETA: 40:23 - accuracy: 0.8971 - loss: 0.2789\n",
      " training set -> batch:162 loss:0.2830989360809326 and acc: 0.8963675498962402\n",
      " 162/2105 [=>............................] - ETA: 40:12 - accuracy: 0.8964 - loss: 0.2831\n",
      " training set -> batch:163 loss:0.2794111669063568 and acc: 0.8977272510528564\n",
      " 163/2105 [=>............................] - ETA: 40:02 - accuracy: 0.8977 - loss: 0.2794\n",
      " training set -> batch:164 loss:0.2814730405807495 and acc: 0.8960000276565552\n",
      " 164/2105 [=>............................] - ETA: 39:51 - accuracy: 0.8960 - loss: 0.2815\n",
      " training set -> batch:165 loss:0.27685338258743286 and acc: 0.8982558250427246\n",
      " 165/2105 [=>............................] - ETA: 39:41 - accuracy: 0.8983 - loss: 0.2769\n",
      " training set -> batch:166 loss:0.2777365446090698 and acc: 0.8975563645362854\n",
      " 166/2105 [=>............................] - ETA: 39:31 - accuracy: 0.8976 - loss: 0.2777\n",
      " training set -> batch:167 loss:0.27513331174850464 and acc: 0.8978102207183838\n",
      " 167/2105 [=>............................] - ETA: 39:21 - accuracy: 0.8978 - loss: 0.2751\n",
      " training set -> batch:168 loss:0.27426761388778687 and acc: 0.8989361524581909\n",
      " 168/2105 [=>............................] - ETA: 39:11 - accuracy: 0.8989 - loss: 0.2743\n",
      " training set -> batch:169 loss:0.2815552055835724 and acc: 0.8965517282485962\n",
      " 169/2105 [=>............................] - ETA: 39:01 - accuracy: 0.8966 - loss: 0.2816\n",
      " training set -> batch:170 loss:0.27799201011657715 and acc: 0.8951342105865479\n",
      "\n",
      " validation set -> batch:170 val loss:0.2935750484466553 and val acc: 0.8761467933654785\n",
      " 170/2105 [=>............................] - ETA: 40:21 - accuracy: 0.8951 - loss: 0.2780\n",
      " training set -> batch:171 loss:0.30480238795280457 and acc: 0.8738937973976135\n",
      " 171/2105 [=>............................] - ETA: 40:11 - accuracy: 0.8739 - loss: 0.3048\n",
      " training set -> batch:172 loss:0.31235677003860474 and acc: 0.870726466178894\n",
      " 172/2105 [=>............................] - ETA: 40:01 - accuracy: 0.8707 - loss: 0.3124\n",
      " training set -> batch:173 loss:0.30200424790382385 and acc: 0.875\n",
      " 173/2105 [=>............................] - ETA: 39:51 - accuracy: 0.8750 - loss: 0.3020\n",
      " training set -> batch:174 loss:0.2995552718639374 and acc: 0.875\n",
      " 174/2105 [=>............................] - ETA: 39:41 - accuracy: 0.8750 - loss: 0.2996\n",
      " training set -> batch:175 loss:0.3075420558452606 and acc: 0.873062014579773\n",
      " 175/2105 [=>............................] - ETA: 39:31 - accuracy: 0.8731 - loss: 0.3075\n",
      " training set -> batch:176 loss:0.30870679020881653 and acc: 0.8740601539611816\n",
      " 176/2105 [=>............................] - ETA: 39:22 - accuracy: 0.8741 - loss: 0.3087\n",
      " training set -> batch:177 loss:0.3030979037284851 and acc: 0.8759124279022217\n",
      " 177/2105 [=>............................] - ETA: 39:12 - accuracy: 0.8759 - loss: 0.3031\n",
      " training set -> batch:178 loss:0.315319299697876 and acc: 0.8723404407501221\n",
      " 178/2105 [=>............................] - ETA: 39:03 - accuracy: 0.8723 - loss: 0.3153\n",
      " training set -> batch:179 loss:0.30993935465812683 and acc: 0.873275876045227\n",
      " 179/2105 [=>............................] - ETA: 38:54 - accuracy: 0.8733 - loss: 0.3099\n",
      " training set -> batch:180 loss:0.30655696988105774 and acc: 0.875\n",
      "\n",
      " validation set -> batch:180 val loss:0.2666056752204895 and val acc: 0.8910550475120544\n",
      " 180/2105 [=>............................] - ETA: 40:18 - accuracy: 0.8750 - loss: 0.3066\n",
      " training set -> batch:181 loss:0.2747873067855835 and acc: 0.8882743120193481\n",
      " 181/2105 [=>............................] - ETA: 40:08 - accuracy: 0.8883 - loss: 0.2748\n",
      " training set -> batch:182 loss:0.27027440071105957 and acc: 0.8888888955116272\n",
      " 182/2105 [=>............................] - ETA: 39:59 - accuracy: 0.8889 - loss: 0.2703\n",
      " training set -> batch:183 loss:0.26439327001571655 and acc: 0.8904958963394165\n",
      " 183/2105 [=>............................] - ETA: 39:49 - accuracy: 0.8905 - loss: 0.2644\n",
      " training set -> batch:184 loss:0.26016557216644287 and acc: 0.8930000066757202\n",
      " 184/2105 [=>............................] - ETA: 39:40 - accuracy: 0.8930 - loss: 0.2602\n",
      " training set -> batch:185 loss:0.2549125552177429 and acc: 0.8943798542022705\n",
      " 185/2105 [=>............................] - ETA: 39:31 - accuracy: 0.8944 - loss: 0.2549\n",
      " training set -> batch:186 loss:0.2691425085067749 and acc: 0.8919172883033752\n",
      " 186/2105 [=>............................] - ETA: 39:21 - accuracy: 0.8919 - loss: 0.2691\n",
      " training set -> batch:187 loss:0.2647722363471985 and acc: 0.8905109763145447\n",
      " 187/2105 [=>............................] - ETA: 39:12 - accuracy: 0.8905 - loss: 0.2648\n",
      " training set -> batch:188 loss:0.26460620760917664 and acc: 0.890070915222168\n",
      " 188/2105 [=>............................] - ETA: 39:03 - accuracy: 0.8901 - loss: 0.2646\n",
      " training set -> batch:189 loss:0.2643623352050781 and acc: 0.8905172348022461\n",
      " 189/2105 [=>............................] - ETA: 38:54 - accuracy: 0.8905 - loss: 0.2644\n",
      " training set -> batch:190 loss:0.2687291204929352 and acc: 0.8892617225646973\n",
      "\n",
      " validation set -> batch:190 val loss:0.2837878167629242 and val acc: 0.8795871734619141\n",
      " 190/2105 [=>............................] - ETA: 40:05 - accuracy: 0.8893 - loss: 0.2687\n",
      " training set -> batch:191 loss:0.28257519006729126 and acc: 0.8794247508049011\n",
      " 191/2105 [=>............................] - ETA: 39:56 - accuracy: 0.8794 - loss: 0.2826\n",
      " training set -> batch:192 loss:0.2757139205932617 and acc: 0.882478654384613\n",
      " 192/2105 [=>............................] - ETA: 39:47 - accuracy: 0.8825 - loss: 0.2757\n",
      " training set -> batch:193 loss:0.284077912569046 and acc: 0.8822314143180847\n",
      " 193/2105 [=>............................] - ETA: 39:38 - accuracy: 0.8822 - loss: 0.2841\n",
      " training set -> batch:194 loss:0.28454819321632385 and acc: 0.8830000162124634\n",
      " 194/2105 [=>............................] - ETA: 39:29 - accuracy: 0.8830 - loss: 0.2845\n",
      " training set -> batch:195 loss:0.29310712218284607 and acc: 0.8798449635505676\n",
      " 195/2105 [=>............................] - ETA: 39:20 - accuracy: 0.8798 - loss: 0.2931\n",
      " training set -> batch:196 loss:0.2898569405078888 and acc: 0.8796992301940918\n",
      " 196/2105 [=>............................] - ETA: 39:12 - accuracy: 0.8797 - loss: 0.2899\n",
      " training set -> batch:197 loss:0.2911427319049835 and acc: 0.8795620203018188\n",
      " 197/2105 [=>............................] - ETA: 39:03 - accuracy: 0.8796 - loss: 0.2911\n",
      " training set -> batch:198 loss:0.28703105449676514 and acc: 0.8803191781044006\n",
      " 198/2105 [=>............................] - ETA: 38:54 - accuracy: 0.8803 - loss: 0.2870\n",
      " training set -> batch:199 loss:0.2826994061470032 and acc: 0.882758617401123\n",
      " 199/2105 [=>............................] - ETA: 38:46 - accuracy: 0.8828 - loss: 0.2827\n",
      " training set -> batch:200 loss:0.27796700596809387 and acc: 0.8850671052932739\n",
      "\n",
      " validation set -> batch:200 val loss:0.2699889838695526 and val acc: 0.8841742873191833\n",
      " 200/2105 [=>............................] - ETA: 39:55 - accuracy: 0.8851 - loss: 0.2780\n",
      " training set -> batch:201 loss:0.2638099491596222 and acc: 0.8849557638168335\n",
      " 201/2105 [=>............................] - ETA: 39:46 - accuracy: 0.8850 - loss: 0.2638\n",
      " training set -> batch:202 loss:0.26150938868522644 and acc: 0.8867521286010742\n",
      " 202/2105 [=>............................] - ETA: 39:37 - accuracy: 0.8868 - loss: 0.2615\n",
      " training set -> batch:203 loss:0.2617982029914856 and acc: 0.8873966932296753\n",
      " 203/2105 [=>............................] - ETA: 39:29 - accuracy: 0.8874 - loss: 0.2618\n",
      " training set -> batch:204 loss:0.2525813579559326 and acc: 0.890999972820282\n",
      " 204/2105 [=>............................] - ETA: 39:20 - accuracy: 0.8910 - loss: 0.2526\n",
      " training set -> batch:205 loss:0.26192986965179443 and acc: 0.8895348906517029\n",
      " 205/2105 [=>............................] - ETA: 39:12 - accuracy: 0.8895 - loss: 0.2619\n",
      " training set -> batch:206 loss:0.25793421268463135 and acc: 0.8909774422645569\n",
      " 206/2105 [=>............................] - ETA: 39:04 - accuracy: 0.8910 - loss: 0.2579\n",
      " training set -> batch:207 loss:0.2689121961593628 and acc: 0.8886861205101013\n",
      " 207/2105 [=>............................] - ETA: 38:55 - accuracy: 0.8887 - loss: 0.2689\n",
      " training set -> batch:208 loss:0.2706087529659271 and acc: 0.88741135597229\n",
      " 208/2105 [=>............................] - ETA: 38:47 - accuracy: 0.8874 - loss: 0.2706\n",
      " training set -> batch:209 loss:0.2723524570465088 and acc: 0.8862069249153137\n",
      " 209/2105 [=>............................] - ETA: 38:39 - accuracy: 0.8862 - loss: 0.2724\n",
      " training set -> batch:210 loss:0.27641361951828003 and acc: 0.8850671052932739\n",
      "\n",
      " validation set -> batch:210 val loss:0.25901415944099426 and val acc: 0.892201840877533\n",
      " 210/2105 [=>............................] - ETA: 39:43 - accuracy: 0.8851 - loss: 0.2764\n",
      " training set -> batch:211 loss:0.25829142332077026 and acc: 0.892699122428894\n",
      " 211/2105 [==>...........................] - ETA: 39:35 - accuracy: 0.8927 - loss: 0.2583\n",
      " training set -> batch:212 loss:0.26827478408813477 and acc: 0.8910256624221802\n",
      " 212/2105 [==>...........................] - ETA: 39:27 - accuracy: 0.8910 - loss: 0.2683\n",
      " training set -> batch:213 loss:0.26245641708374023 and acc: 0.8925619721412659\n",
      " 213/2105 [==>...........................] - ETA: 39:18 - accuracy: 0.8926 - loss: 0.2625\n",
      " training set -> batch:214 loss:0.26064831018447876 and acc: 0.8930000066757202\n",
      " 214/2105 [==>...........................] - ETA: 39:10 - accuracy: 0.8930 - loss: 0.2606\n",
      " training set -> batch:215 loss:0.25596553087234497 and acc: 0.8943798542022705\n",
      " 215/2105 [==>...........................] - ETA: 39:02 - accuracy: 0.8944 - loss: 0.2560\n",
      " training set -> batch:216 loss:0.25504085421562195 and acc: 0.8947368264198303\n",
      " 216/2105 [==>...........................] - ETA: 38:54 - accuracy: 0.8947 - loss: 0.2550\n",
      " training set -> batch:217 loss:0.26149752736091614 and acc: 0.8941605687141418\n",
      " 217/2105 [==>...........................] - ETA: 38:46 - accuracy: 0.8942 - loss: 0.2615\n",
      " training set -> batch:218 loss:0.26270791888237 and acc: 0.8936170339584351\n",
      " 218/2105 [==>...........................] - ETA: 38:38 - accuracy: 0.8936 - loss: 0.2627\n",
      " training set -> batch:219 loss:0.2580074965953827 and acc: 0.8948276042938232\n",
      " 219/2105 [==>...........................] - ETA: 38:30 - accuracy: 0.8948 - loss: 0.2580\n",
      " training set -> batch:220 loss:0.2590219974517822 and acc: 0.8942952752113342\n",
      "\n",
      " validation set -> batch:220 val loss:0.2800203561782837 and val acc: 0.8910550475120544\n",
      " 220/2105 [==>...........................] - ETA: 39:31 - accuracy: 0.8943 - loss: 0.2590\n",
      " training set -> batch:221 loss:0.2815383970737457 and acc: 0.8904867172241211\n",
      " 221/2105 [==>...........................] - ETA: 39:23 - accuracy: 0.8905 - loss: 0.2815\n",
      " training set -> batch:222 loss:0.28281283378601074 and acc: 0.8899572491645813\n",
      " 222/2105 [==>...........................] - ETA: 39:15 - accuracy: 0.8900 - loss: 0.2828\n",
      " training set -> batch:223 loss:0.28879815340042114 and acc: 0.8863636255264282\n",
      " 223/2105 [==>...........................] - ETA: 39:07 - accuracy: 0.8864 - loss: 0.2888\n",
      " training set -> batch:224 loss:0.30725085735321045 and acc: 0.8830000162124634\n",
      " 224/2105 [==>...........................] - ETA: 38:59 - accuracy: 0.8830 - loss: 0.3073\n",
      " training set -> batch:225 loss:0.3081546127796173 and acc: 0.8817829489707947\n",
      " 225/2105 [==>...........................] - ETA: 38:51 - accuracy: 0.8818 - loss: 0.3082\n",
      " training set -> batch:226 loss:0.3024607002735138 and acc: 0.8825187683105469\n",
      " 226/2105 [==>...........................] - ETA: 38:44 - accuracy: 0.8825 - loss: 0.3025\n",
      " training set -> batch:227 loss:0.30027976632118225 and acc: 0.8822992444038391\n",
      " 227/2105 [==>...........................] - ETA: 38:36 - accuracy: 0.8823 - loss: 0.3003\n",
      " training set -> batch:228 loss:0.30101048946380615 and acc: 0.8829787373542786\n",
      " 228/2105 [==>...........................] - ETA: 38:28 - accuracy: 0.8830 - loss: 0.3010\n",
      " training set -> batch:229 loss:0.2983042895793915 and acc: 0.8836206793785095\n",
      " 229/2105 [==>...........................] - ETA: 38:21 - accuracy: 0.8836 - loss: 0.2983\n",
      " training set -> batch:230 loss:0.29460078477859497 and acc: 0.8842281699180603\n",
      "\n",
      " validation set -> batch:230 val loss:0.24733544886112213 and val acc: 0.9048165082931519\n",
      " 230/2105 [==>...........................] - ETA: 39:18 - accuracy: 0.8842 - loss: 0.2946\n",
      " training set -> batch:231 loss:0.25338587164878845 and acc: 0.9015486836433411\n",
      " 231/2105 [==>...........................] - ETA: 39:11 - accuracy: 0.9015 - loss: 0.2534\n",
      " training set -> batch:232 loss:0.25070229172706604 and acc: 0.9027777910232544\n",
      " 232/2105 [==>...........................] - ETA: 39:03 - accuracy: 0.9028 - loss: 0.2507\n",
      " training set -> batch:233 loss:0.25951215624809265 and acc: 0.8987603187561035\n",
      " 233/2105 [==>...........................] - ETA: 38:55 - accuracy: 0.8988 - loss: 0.2595\n",
      " training set -> batch:234 loss:0.25876370072364807 and acc: 0.8970000147819519\n",
      " 234/2105 [==>...........................] - ETA: 38:48 - accuracy: 0.8970 - loss: 0.2588\n",
      " training set -> batch:235 loss:0.2542281150817871 and acc: 0.8992248177528381\n",
      " 235/2105 [==>...........................] - ETA: 38:40 - accuracy: 0.8992 - loss: 0.2542\n",
      " training set -> batch:236 loss:0.25441330671310425 and acc: 0.8975563645362854\n",
      " 236/2105 [==>...........................] - ETA: 38:33 - accuracy: 0.8976 - loss: 0.2544\n",
      " training set -> batch:237 loss:0.2550983428955078 and acc: 0.8978102207183838\n",
      " 237/2105 [==>...........................] - ETA: 38:26 - accuracy: 0.8978 - loss: 0.2551\n",
      " training set -> batch:238 loss:0.2476382851600647 and acc: 0.9007092118263245\n",
      " 238/2105 [==>...........................] - ETA: 38:18 - accuracy: 0.9007 - loss: 0.2476\n",
      " training set -> batch:239 loss:0.25269919633865356 and acc: 0.8991379141807556\n",
      " 239/2105 [==>...........................] - ETA: 38:11 - accuracy: 0.8991 - loss: 0.2527\n",
      " training set -> batch:240 loss:0.25198808312416077 and acc: 0.9001677632331848\n",
      "\n",
      " validation set -> batch:240 val loss:0.23925602436065674 and val acc: 0.9082568883895874\n",
      " 240/2105 [==>...........................] - ETA: 39:06 - accuracy: 0.9002 - loss: 0.2520\n",
      " training set -> batch:241 loss:0.23927274346351624 and acc: 0.9081858396530151\n",
      " 241/2105 [==>...........................] - ETA: 38:58 - accuracy: 0.9082 - loss: 0.2393\n",
      " training set -> batch:242 loss:0.24150924384593964 and acc: 0.9081196784973145\n",
      " 242/2105 [==>...........................] - ETA: 38:51 - accuracy: 0.9081 - loss: 0.2415\n",
      " training set -> batch:243 loss:0.24978764355182648 and acc: 0.9070248007774353\n",
      " 243/2105 [==>...........................] - ETA: 38:44 - accuracy: 0.9070 - loss: 0.2498\n",
      " training set -> batch:244 loss:0.2488437294960022 and acc: 0.9079999923706055\n",
      " 244/2105 [==>...........................] - ETA: 38:37 - accuracy: 0.9080 - loss: 0.2488\n",
      " training set -> batch:245 loss:0.24560099840164185 and acc: 0.9089147448539734\n",
      " 245/2105 [==>...........................] - ETA: 38:29 - accuracy: 0.9089 - loss: 0.2456\n",
      " training set -> batch:246 loss:0.2494184523820877 and acc: 0.9088345766067505\n",
      " 246/2105 [==>...........................] - ETA: 38:22 - accuracy: 0.9088 - loss: 0.2494\n",
      " training set -> batch:247 loss:0.24988950788974762 and acc: 0.9078466892242432\n",
      " 247/2105 [==>...........................] - ETA: 38:15 - accuracy: 0.9078 - loss: 0.2499\n",
      " training set -> batch:248 loss:0.2458728700876236 and acc: 0.908687949180603\n",
      " 248/2105 [==>...........................] - ETA: 38:08 - accuracy: 0.9087 - loss: 0.2459\n",
      " training set -> batch:249 loss:0.2400527000427246 and acc: 0.9103448390960693\n",
      " 249/2105 [==>...........................] - ETA: 38:01 - accuracy: 0.9103 - loss: 0.2401\n",
      " training set -> batch:250 loss:0.24064771831035614 and acc: 0.9085570573806763\n",
      "\n",
      " validation set -> batch:250 val loss:0.24902786314487457 and val acc: 0.896789014339447\n",
      " 250/2105 [==>...........................] - ETA: 38:54 - accuracy: 0.9086 - loss: 0.2406\n",
      " training set -> batch:251 loss:0.25466176867485046 and acc: 0.8960176706314087\n",
      " 251/2105 [==>...........................] - ETA: 38:47 - accuracy: 0.8960 - loss: 0.2547\n",
      " training set -> batch:252 loss:0.2503299117088318 and acc: 0.8974359035491943\n",
      " 252/2105 [==>...........................] - ETA: 38:39 - accuracy: 0.8974 - loss: 0.2503\n",
      " training set -> batch:253 loss:0.2479282021522522 and acc: 0.8977272510528564\n",
      " 253/2105 [==>...........................] - ETA: 38:32 - accuracy: 0.8977 - loss: 0.2479\n",
      " training set -> batch:254 loss:0.24843719601631165 and acc: 0.8989999890327454\n",
      " 254/2105 [==>...........................] - ETA: 38:26 - accuracy: 0.8990 - loss: 0.2484\n",
      " training set -> batch:255 loss:0.24489952623844147 and acc: 0.8992248177528381\n",
      " 255/2105 [==>...........................] - ETA: 38:19 - accuracy: 0.8992 - loss: 0.2449\n",
      " training set -> batch:256 loss:0.24116496741771698 and acc: 0.9013158082962036\n",
      " 256/2105 [==>...........................] - ETA: 38:12 - accuracy: 0.9013 - loss: 0.2412\n",
      " training set -> batch:257 loss:0.24361065030097961 and acc: 0.900547444820404\n",
      " 257/2105 [==>...........................] - ETA: 38:05 - accuracy: 0.9005 - loss: 0.2436\n",
      " training set -> batch:258 loss:0.2384078949689865 and acc: 0.9015957713127136\n",
      " 258/2105 [==>...........................] - ETA: 37:58 - accuracy: 0.9016 - loss: 0.2384\n",
      " training set -> batch:259 loss:0.23721325397491455 and acc: 0.9008620977401733\n",
      " 259/2105 [==>...........................] - ETA: 37:51 - accuracy: 0.9009 - loss: 0.2372\n",
      " training set -> batch:260 loss:0.23273904621601105 and acc: 0.9018456339836121\n",
      "\n",
      " validation set -> batch:260 val loss:0.2580609917640686 and val acc: 0.8933486342430115\n",
      " 260/2105 [==>...........................] - ETA: 38:42 - accuracy: 0.9018 - loss: 0.2327\n",
      " training set -> batch:261 loss:0.26393571496009827 and acc: 0.892699122428894\n",
      " 261/2105 [==>...........................] - ETA: 38:35 - accuracy: 0.8927 - loss: 0.2639\n",
      " training set -> batch:262 loss:0.26121971011161804 and acc: 0.8942307829856873\n",
      " 262/2105 [==>...........................] - ETA: 38:28 - accuracy: 0.8942 - loss: 0.2612\n",
      " training set -> batch:263 loss:0.2659204304218292 and acc: 0.8935950398445129\n",
      " 263/2105 [==>...........................] - ETA: 38:21 - accuracy: 0.8936 - loss: 0.2659\n",
      " training set -> batch:264 loss:0.2626560628414154 and acc: 0.8949999809265137\n",
      " 264/2105 [==>...........................] - ETA: 38:14 - accuracy: 0.8950 - loss: 0.2627\n",
      " training set -> batch:265 loss:0.27744489908218384 and acc: 0.8914728760719299\n",
      " 265/2105 [==>...........................] - ETA: 38:08 - accuracy: 0.8915 - loss: 0.2774\n",
      " training set -> batch:266 loss:0.2695024609565735 and acc: 0.893796980381012\n",
      " 266/2105 [==>...........................] - ETA: 38:01 - accuracy: 0.8938 - loss: 0.2695\n",
      " training set -> batch:267 loss:0.26985517144203186 and acc: 0.8941605687141418\n",
      " 267/2105 [==>...........................] - ETA: 37:54 - accuracy: 0.8942 - loss: 0.2699\n",
      " training set -> batch:268 loss:0.26610642671585083 and acc: 0.8945035338401794\n",
      " 268/2105 [==>...........................] - ETA: 37:48 - accuracy: 0.8945 - loss: 0.2661\n",
      " training set -> batch:269 loss:0.26536262035369873 and acc: 0.8948276042938232\n",
      " 269/2105 [==>...........................] - ETA: 37:41 - accuracy: 0.8948 - loss: 0.2654\n",
      " training set -> batch:270 loss:0.26485952734947205 and acc: 0.8951342105865479\n",
      "\n",
      " validation set -> batch:270 val loss:0.24803891777992249 and val acc: 0.8956422209739685\n",
      " 270/2105 [==>...........................] - ETA: 38:30 - accuracy: 0.8951 - loss: 0.2649\n",
      " training set -> batch:271 loss:0.2640437185764313 and acc: 0.8904867172241211\n",
      " 271/2105 [==>...........................] - ETA: 38:23 - accuracy: 0.8905 - loss: 0.2640\n",
      " training set -> batch:272 loss:0.25225478410720825 and acc: 0.8942307829856873\n",
      " 272/2105 [==>...........................] - ETA: 38:17 - accuracy: 0.8942 - loss: 0.2523\n",
      " training set -> batch:273 loss:0.2554852366447449 and acc: 0.8935950398445129\n",
      " 273/2105 [==>...........................] - ETA: 38:10 - accuracy: 0.8936 - loss: 0.2555\n",
      " training set -> batch:274 loss:0.25318846106529236 and acc: 0.8949999809265137\n",
      " 274/2105 [==>...........................] - ETA: 38:04 - accuracy: 0.8950 - loss: 0.2532\n",
      " training set -> batch:275 loss:0.25203150510787964 and acc: 0.8943798542022705\n",
      " 275/2105 [==>...........................] - ETA: 37:57 - accuracy: 0.8944 - loss: 0.2520\n",
      " training set -> batch:276 loss:0.24925562739372253 and acc: 0.8956766724586487\n",
      " 276/2105 [==>...........................] - ETA: 37:51 - accuracy: 0.8957 - loss: 0.2493\n",
      " training set -> batch:277 loss:0.2519274353981018 and acc: 0.8941605687141418\n",
      " 277/2105 [==>...........................] - ETA: 37:44 - accuracy: 0.8942 - loss: 0.2519\n",
      " training set -> batch:278 loss:0.2470981627702713 and acc: 0.896276593208313\n",
      " 278/2105 [==>...........................] - ETA: 37:38 - accuracy: 0.8963 - loss: 0.2471\n",
      " training set -> batch:279 loss:0.25151437520980835 and acc: 0.8939655423164368\n",
      " 279/2105 [==>...........................] - ETA: 37:32 - accuracy: 0.8940 - loss: 0.2515\n",
      " training set -> batch:280 loss:0.2516610622406006 and acc: 0.8951342105865479\n",
      "\n",
      " validation set -> batch:280 val loss:0.2547454833984375 and val acc: 0.8899082541465759\n",
      " 280/2105 [==>...........................] - ETA: 38:23 - accuracy: 0.8951 - loss: 0.2517\n",
      " training set -> batch:281 loss:0.25578925013542175 and acc: 0.8893805146217346\n",
      " 281/2105 [===>..........................] - ETA: 38:17 - accuracy: 0.8894 - loss: 0.2558\n",
      " training set -> batch:282 loss:0.26189181208610535 and acc: 0.8888888955116272\n",
      " 282/2105 [===>..........................] - ETA: 38:11 - accuracy: 0.8889 - loss: 0.2619\n",
      " training set -> batch:283 loss:0.2587125301361084 and acc: 0.8904958963394165\n",
      " 283/2105 [===>..........................] - ETA: 38:04 - accuracy: 0.8905 - loss: 0.2587\n",
      " training set -> batch:284 loss:0.25862571597099304 and acc: 0.8920000195503235\n",
      " 284/2105 [===>..........................] - ETA: 37:58 - accuracy: 0.8920 - loss: 0.2586\n",
      " training set -> batch:285 loss:0.26900067925453186 and acc: 0.8914728760719299\n",
      " 285/2105 [===>..........................] - ETA: 37:52 - accuracy: 0.8915 - loss: 0.2690\n",
      " training set -> batch:286 loss:0.26609277725219727 and acc: 0.8919172883033752\n",
      " 286/2105 [===>..........................] - ETA: 37:45 - accuracy: 0.8919 - loss: 0.2661\n",
      " training set -> batch:287 loss:0.26634183526039124 and acc: 0.8923357725143433\n",
      " 287/2105 [===>..........................] - ETA: 37:39 - accuracy: 0.8923 - loss: 0.2663\n",
      " training set -> batch:288 loss:0.26908108592033386 and acc: 0.8927304744720459\n",
      " 288/2105 [===>..........................] - ETA: 37:33 - accuracy: 0.8927 - loss: 0.2691\n",
      " training set -> batch:289 loss:0.26506277918815613 and acc: 0.8948276042938232\n",
      " 289/2105 [===>..........................] - ETA: 37:27 - accuracy: 0.8948 - loss: 0.2651\n",
      " training set -> batch:290 loss:0.26937487721443176 and acc: 0.8926174640655518\n",
      "\n",
      " validation set -> batch:290 val loss:0.25724664330482483 and val acc: 0.9048165082931519\n",
      " 290/2105 [===>..........................] - ETA: 38:11 - accuracy: 0.8926 - loss: 0.2694\n",
      " training set -> batch:291 loss:0.2532835900783539 and acc: 0.9059734344482422\n",
      " 291/2105 [===>..........................] - ETA: 38:05 - accuracy: 0.9060 - loss: 0.2533\n",
      " training set -> batch:292 loss:0.24820664525032043 and acc: 0.9070512652397156\n",
      " 292/2105 [===>..........................] - ETA: 37:59 - accuracy: 0.9071 - loss: 0.2482\n",
      " training set -> batch:293 loss:0.24531367421150208 and acc: 0.9080578684806824\n",
      " 293/2105 [===>..........................] - ETA: 37:52 - accuracy: 0.9081 - loss: 0.2453\n",
      " training set -> batch:294 loss:0.24083958566188812 and acc: 0.9100000262260437\n",
      " 294/2105 [===>..........................] - ETA: 37:46 - accuracy: 0.9100 - loss: 0.2408\n",
      " training set -> batch:295 loss:0.23777572810649872 and acc: 0.9108527302742004\n",
      " 295/2105 [===>..........................] - ETA: 37:40 - accuracy: 0.9109 - loss: 0.2378\n",
      " training set -> batch:296 loss:0.23497433960437775 and acc: 0.9107142686843872\n",
      " 296/2105 [===>..........................] - ETA: 37:34 - accuracy: 0.9107 - loss: 0.2350\n",
      " training set -> batch:297 loss:0.2333008199930191 and acc: 0.9105839133262634\n",
      " 297/2105 [===>..........................] - ETA: 37:28 - accuracy: 0.9106 - loss: 0.2333\n",
      " training set -> batch:298 loss:0.2337658852338791 and acc: 0.911347508430481\n",
      " 298/2105 [===>..........................] - ETA: 37:22 - accuracy: 0.9113 - loss: 0.2338\n",
      " training set -> batch:299 loss:0.2301057130098343 and acc: 0.9120689630508423\n",
      " 299/2105 [===>..........................] - ETA: 37:16 - accuracy: 0.9121 - loss: 0.2301\n",
      " training set -> batch:300 loss:0.22446978092193604 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:300 val loss:0.2660302221775055 and val acc: 0.8990825414657593\n",
      " 300/2105 [===>..........................] - ETA: 37:59 - accuracy: 0.9136 - loss: 0.2245\n",
      " training set -> batch:301 loss:0.2606474459171295 and acc: 0.8993362784385681\n",
      " 301/2105 [===>..........................] - ETA: 37:53 - accuracy: 0.8993 - loss: 0.2606\n",
      " training set -> batch:302 loss:0.284877747297287 and acc: 0.8942307829856873\n",
      " 302/2105 [===>..........................] - ETA: 37:47 - accuracy: 0.8942 - loss: 0.2849\n",
      " training set -> batch:303 loss:0.2954777181148529 and acc: 0.8925619721412659\n",
      " 303/2105 [===>..........................] - ETA: 37:41 - accuracy: 0.8926 - loss: 0.2955\n",
      " training set -> batch:304 loss:0.29139137268066406 and acc: 0.8939999938011169\n",
      " 304/2105 [===>..........................] - ETA: 37:35 - accuracy: 0.8940 - loss: 0.2914\n",
      " training set -> batch:305 loss:0.2904385030269623 and acc: 0.893410861492157\n",
      " 305/2105 [===>..........................] - ETA: 37:29 - accuracy: 0.8934 - loss: 0.2904\n",
      " training set -> batch:306 loss:0.28472039103507996 and acc: 0.8947368264198303\n",
      " 306/2105 [===>..........................] - ETA: 37:23 - accuracy: 0.8947 - loss: 0.2847\n",
      " training set -> batch:307 loss:0.2899532616138458 and acc: 0.8950729966163635\n",
      " 307/2105 [===>..........................] - ETA: 37:17 - accuracy: 0.8951 - loss: 0.2900\n",
      " training set -> batch:308 loss:0.2810197174549103 and acc: 0.8971630930900574\n",
      " 308/2105 [===>..........................] - ETA: 37:11 - accuracy: 0.8972 - loss: 0.2810\n",
      " training set -> batch:309 loss:0.2883152365684509 and acc: 0.8939655423164368\n",
      " 309/2105 [===>..........................] - ETA: 37:06 - accuracy: 0.8940 - loss: 0.2883\n",
      " training set -> batch:310 loss:0.2810695171356201 and acc: 0.8959731459617615\n",
      "\n",
      " validation set -> batch:310 val loss:0.3032614290714264 and val acc: 0.8704128265380859\n",
      " 310/2105 [===>..........................] - ETA: 37:46 - accuracy: 0.8960 - loss: 0.2811\n",
      " training set -> batch:311 loss:0.29546675086021423 and acc: 0.8716813921928406\n",
      " 311/2105 [===>..........................] - ETA: 37:40 - accuracy: 0.8717 - loss: 0.2955\n",
      " training set -> batch:312 loss:0.2886219024658203 and acc: 0.8739316463470459\n",
      " 312/2105 [===>..........................] - ETA: 37:34 - accuracy: 0.8739 - loss: 0.2886\n",
      " training set -> batch:313 loss:0.29056984186172485 and acc: 0.8739669322967529\n",
      " 313/2105 [===>..........................] - ETA: 37:28 - accuracy: 0.8740 - loss: 0.2906\n",
      " training set -> batch:314 loss:0.2853202819824219 and acc: 0.875\n",
      " 314/2105 [===>..........................] - ETA: 37:23 - accuracy: 0.8750 - loss: 0.2853\n",
      " training set -> batch:315 loss:0.2957070767879486 and acc: 0.8701550364494324\n",
      " 315/2105 [===>..........................] - ETA: 37:17 - accuracy: 0.8702 - loss: 0.2957\n",
      " training set -> batch:316 loss:0.2950477600097656 and acc: 0.8703007698059082\n",
      " 316/2105 [===>..........................] - ETA: 37:11 - accuracy: 0.8703 - loss: 0.2950\n",
      " training set -> batch:317 loss:0.29507124423980713 and acc: 0.8704379796981812\n",
      " 317/2105 [===>..........................] - ETA: 37:06 - accuracy: 0.8704 - loss: 0.2951\n",
      " training set -> batch:318 loss:0.29909008741378784 and acc: 0.8679078221321106\n",
      " 318/2105 [===>..........................] - ETA: 37:00 - accuracy: 0.8679 - loss: 0.2991\n",
      " training set -> batch:319 loss:0.298442542552948 and acc: 0.867241382598877\n",
      " 319/2105 [===>..........................] - ETA: 36:54 - accuracy: 0.8672 - loss: 0.2984\n",
      " training set -> batch:320 loss:0.2928714454174042 and acc: 0.869966447353363\n",
      "\n",
      " validation set -> batch:320 val loss:0.24842743575572968 and val acc: 0.8933486342430115\n",
      " 320/2105 [===>..........................] - ETA: 37:33 - accuracy: 0.8700 - loss: 0.2929\n",
      " training set -> batch:321 loss:0.243386372923851 and acc: 0.894911527633667\n",
      " 321/2105 [===>..........................] - ETA: 37:27 - accuracy: 0.8949 - loss: 0.2434\n",
      " training set -> batch:322 loss:0.2525665760040283 and acc: 0.8942307829856873\n",
      " 322/2105 [===>..........................] - ETA: 37:22 - accuracy: 0.8942 - loss: 0.2526\n",
      " training set -> batch:323 loss:0.26546430587768555 and acc: 0.8884297609329224\n",
      " 323/2105 [===>..........................] - ETA: 37:16 - accuracy: 0.8884 - loss: 0.2655\n",
      " training set -> batch:324 loss:0.2677103877067566 and acc: 0.8889999985694885\n",
      " 324/2105 [===>..........................] - ETA: 37:10 - accuracy: 0.8890 - loss: 0.2677\n",
      " training set -> batch:325 loss:0.26978376507759094 and acc: 0.8875969052314758\n",
      " 325/2105 [===>..........................] - ETA: 37:05 - accuracy: 0.8876 - loss: 0.2698\n",
      " training set -> batch:326 loss:0.27267616987228394 and acc: 0.8872180581092834\n",
      " 326/2105 [===>..........................] - ETA: 36:59 - accuracy: 0.8872 - loss: 0.2727\n",
      " training set -> batch:327 loss:0.28041520714759827 and acc: 0.8850364685058594\n",
      " 327/2105 [===>..........................] - ETA: 36:54 - accuracy: 0.8850 - loss: 0.2804\n",
      " training set -> batch:328 loss:0.27471110224723816 and acc: 0.8865247964859009\n",
      " 328/2105 [===>..........................] - ETA: 36:48 - accuracy: 0.8865 - loss: 0.2747\n",
      " training set -> batch:329 loss:0.27526789903640747 and acc: 0.8870689868927002\n",
      " 329/2105 [===>..........................] - ETA: 36:43 - accuracy: 0.8871 - loss: 0.2753\n",
      " training set -> batch:330 loss:0.2794090807437897 and acc: 0.8842281699180603\n",
      "\n",
      " validation set -> batch:330 val loss:0.248369961977005 and val acc: 0.9013761281967163\n",
      " 330/2105 [===>..........................] - ETA: 37:20 - accuracy: 0.8842 - loss: 0.2794\n",
      " training set -> batch:331 loss:0.24953414499759674 and acc: 0.9015486836433411\n",
      " 331/2105 [===>..........................] - ETA: 37:15 - accuracy: 0.9015 - loss: 0.2495\n",
      " training set -> batch:332 loss:0.2481917440891266 and acc: 0.8995726704597473\n",
      " 332/2105 [===>..........................] - ETA: 37:09 - accuracy: 0.8996 - loss: 0.2482\n",
      " training set -> batch:333 loss:0.24892745912075043 and acc: 0.8997933864593506\n",
      " 333/2105 [===>..........................] - ETA: 37:04 - accuracy: 0.8998 - loss: 0.2489\n",
      " training set -> batch:334 loss:0.2477329820394516 and acc: 0.9010000228881836\n",
      " 334/2105 [===>..........................] - ETA: 36:58 - accuracy: 0.9010 - loss: 0.2477\n",
      " training set -> batch:335 loss:0.24093742668628693 and acc: 0.9040697813034058\n",
      " 335/2105 [===>..........................] - ETA: 36:53 - accuracy: 0.9041 - loss: 0.2409\n",
      " training set -> batch:336 loss:0.24501386284828186 and acc: 0.9003759622573853\n",
      " 336/2105 [===>..........................] - ETA: 36:48 - accuracy: 0.9004 - loss: 0.2450\n",
      " training set -> batch:337 loss:0.2453746497631073 and acc: 0.8996350169181824\n",
      " 337/2105 [===>..........................] - ETA: 36:42 - accuracy: 0.8996 - loss: 0.2454\n",
      " training set -> batch:338 loss:0.2454683929681778 and acc: 0.8989361524581909\n",
      " 338/2105 [===>..........................] - ETA: 36:37 - accuracy: 0.8989 - loss: 0.2455\n",
      " training set -> batch:339 loss:0.24522070586681366 and acc: 0.8982758522033691\n",
      " 339/2105 [===>..........................] - ETA: 36:32 - accuracy: 0.8983 - loss: 0.2452\n",
      " training set -> batch:340 loss:0.24455232918262482 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:340 val loss:0.242862731218338 and val acc: 0.8990825414657593\n",
      " 340/2105 [===>..........................] - ETA: 37:08 - accuracy: 0.8993 - loss: 0.2446\n",
      " training set -> batch:341 loss:0.24151650071144104 and acc: 0.8982300758361816\n",
      " 341/2105 [===>..........................] - ETA: 37:03 - accuracy: 0.8982 - loss: 0.2415\n",
      " training set -> batch:342 loss:0.2406546026468277 and acc: 0.8985042572021484\n",
      " 342/2105 [===>..........................] - ETA: 36:57 - accuracy: 0.8985 - loss: 0.2407\n",
      " training set -> batch:343 loss:0.2354709655046463 and acc: 0.8997933864593506\n",
      " 343/2105 [===>..........................] - ETA: 36:52 - accuracy: 0.8998 - loss: 0.2355\n",
      " training set -> batch:344 loss:0.24122503399848938 and acc: 0.8999999761581421\n",
      " 344/2105 [===>..........................] - ETA: 36:47 - accuracy: 0.9000 - loss: 0.2412\n",
      " training set -> batch:345 loss:0.23612073063850403 and acc: 0.9011628031730652\n",
      " 345/2105 [===>..........................] - ETA: 36:41 - accuracy: 0.9012 - loss: 0.2361\n",
      " training set -> batch:346 loss:0.2359117567539215 and acc: 0.9003759622573853\n",
      " 346/2105 [===>..........................] - ETA: 36:36 - accuracy: 0.9004 - loss: 0.2359\n",
      " training set -> batch:347 loss:0.23364374041557312 and acc: 0.9014598727226257\n",
      " 347/2105 [===>..........................] - ETA: 36:31 - accuracy: 0.9015 - loss: 0.2336\n",
      " training set -> batch:348 loss:0.23104505240917206 and acc: 0.9015957713127136\n",
      " 348/2105 [===>..........................] - ETA: 36:26 - accuracy: 0.9016 - loss: 0.2310\n",
      " training set -> batch:349 loss:0.24624891579151154 and acc: 0.8974137902259827\n",
      " 349/2105 [===>..........................] - ETA: 36:21 - accuracy: 0.8974 - loss: 0.2462\n",
      " training set -> batch:350 loss:0.24614234268665314 and acc: 0.8976510167121887\n",
      "\n",
      " validation set -> batch:350 val loss:0.27430638670921326 and val acc: 0.89449542760849\n",
      " 350/2105 [===>..........................] - ETA: 36:55 - accuracy: 0.8977 - loss: 0.2461\n",
      " training set -> batch:351 loss:0.2681155204772949 and acc: 0.894911527633667\n",
      " 351/2105 [====>.........................] - ETA: 36:50 - accuracy: 0.8949 - loss: 0.2681\n",
      " training set -> batch:352 loss:0.26184818148612976 and acc: 0.8952991366386414\n",
      " 352/2105 [====>.........................] - ETA: 36:45 - accuracy: 0.8953 - loss: 0.2618\n",
      " training set -> batch:353 loss:0.26162102818489075 and acc: 0.8956611752510071\n",
      " 353/2105 [====>.........................] - ETA: 36:40 - accuracy: 0.8957 - loss: 0.2616\n",
      " training set -> batch:354 loss:0.25890278816223145 and acc: 0.8970000147819519\n",
      " 354/2105 [====>.........................] - ETA: 36:35 - accuracy: 0.8970 - loss: 0.2589\n",
      " training set -> batch:355 loss:0.2507246434688568 and acc: 0.9001938104629517\n",
      " 355/2105 [====>.........................] - ETA: 36:29 - accuracy: 0.9002 - loss: 0.2507\n",
      " training set -> batch:356 loss:0.2505706548690796 and acc: 0.9003759622573853\n",
      " 356/2105 [====>.........................] - ETA: 36:24 - accuracy: 0.9004 - loss: 0.2506\n",
      " training set -> batch:357 loss:0.24690701067447662 and acc: 0.900547444820404\n",
      " 357/2105 [====>.........................] - ETA: 36:19 - accuracy: 0.9005 - loss: 0.2469\n",
      " training set -> batch:358 loss:0.2421741932630539 and acc: 0.902482271194458\n",
      " 358/2105 [====>.........................] - ETA: 36:14 - accuracy: 0.9025 - loss: 0.2422\n",
      " training set -> batch:359 loss:0.23781587183475494 and acc: 0.9043103456497192\n",
      " 359/2105 [====>.........................] - ETA: 36:09 - accuracy: 0.9043 - loss: 0.2378\n",
      " training set -> batch:360 loss:0.23143750429153442 and acc: 0.906879186630249\n",
      "\n",
      " validation set -> batch:360 val loss:0.25344371795654297 and val acc: 0.9013761281967163\n",
      " 360/2105 [====>.........................] - ETA: 36:38 - accuracy: 0.9069 - loss: 0.2314\n",
      " training set -> batch:361 loss:0.26889321208000183 and acc: 0.8993362784385681\n",
      " 361/2105 [====>.........................] - ETA: 36:33 - accuracy: 0.8993 - loss: 0.2689\n",
      " training set -> batch:362 loss:0.2634128928184509 and acc: 0.8995726704597473\n",
      " 362/2105 [====>.........................] - ETA: 36:28 - accuracy: 0.8996 - loss: 0.2634\n",
      " training set -> batch:363 loss:0.25225940346717834 and acc: 0.9028925895690918\n",
      " 363/2105 [====>.........................] - ETA: 36:23 - accuracy: 0.9029 - loss: 0.2523\n",
      " training set -> batch:364 loss:0.2469247430562973 and acc: 0.9039999842643738\n",
      " 364/2105 [====>.........................] - ETA: 36:18 - accuracy: 0.9040 - loss: 0.2469\n",
      " training set -> batch:365 loss:0.2474856823682785 and acc: 0.9040697813034058\n",
      " 365/2105 [====>.........................] - ETA: 36:13 - accuracy: 0.9041 - loss: 0.2475\n",
      " training set -> batch:366 loss:0.24594390392303467 and acc: 0.905075192451477\n",
      " 366/2105 [====>.........................] - ETA: 36:08 - accuracy: 0.9051 - loss: 0.2459\n",
      " training set -> batch:367 loss:0.24551834166049957 and acc: 0.904197096824646\n",
      " 367/2105 [====>.........................] - ETA: 36:03 - accuracy: 0.9042 - loss: 0.2455\n",
      " training set -> batch:368 loss:0.24003224074840546 and acc: 0.9051418304443359\n",
      " 368/2105 [====>.........................] - ETA: 35:58 - accuracy: 0.9051 - loss: 0.2400\n",
      " training set -> batch:369 loss:0.23937788605690002 and acc: 0.9051724076271057\n",
      " 369/2105 [====>.........................] - ETA: 35:53 - accuracy: 0.9052 - loss: 0.2394\n",
      " training set -> batch:370 loss:0.24357186257839203 and acc: 0.9043624401092529\n",
      "\n",
      " validation set -> batch:370 val loss:0.25849661231040955 and val acc: 0.896789014339447\n",
      " 370/2105 [====>.........................] - ETA: 36:26 - accuracy: 0.9044 - loss: 0.2436\n",
      " training set -> batch:371 loss:0.24805425107479095 and acc: 0.8993362784385681\n",
      " 371/2105 [====>.........................] - ETA: 36:21 - accuracy: 0.8993 - loss: 0.2481\n",
      " training set -> batch:372 loss:0.24657535552978516 and acc: 0.8995726704597473\n",
      " 372/2105 [====>.........................] - ETA: 36:16 - accuracy: 0.8996 - loss: 0.2466\n",
      " training set -> batch:373 loss:0.2415684461593628 and acc: 0.8997933864593506\n",
      " 373/2105 [====>.........................] - ETA: 36:11 - accuracy: 0.8998 - loss: 0.2416\n",
      " training set -> batch:374 loss:0.24050790071487427 and acc: 0.8999999761581421\n",
      " 374/2105 [====>.........................] - ETA: 36:06 - accuracy: 0.9000 - loss: 0.2405\n",
      " training set -> batch:375 loss:0.24554701149463654 and acc: 0.9001938104629517\n",
      " 375/2105 [====>.........................] - ETA: 36:01 - accuracy: 0.9002 - loss: 0.2455\n",
      " training set -> batch:376 loss:0.23862624168395996 and acc: 0.902255654335022\n",
      " 376/2105 [====>.........................] - ETA: 35:56 - accuracy: 0.9023 - loss: 0.2386\n",
      " training set -> batch:377 loss:0.23465025424957275 and acc: 0.9023722410202026\n",
      " 377/2105 [====>.........................] - ETA: 35:51 - accuracy: 0.9024 - loss: 0.2347\n",
      " training set -> batch:378 loss:0.22963948547840118 and acc: 0.9042553305625916\n",
      " 378/2105 [====>.........................] - ETA: 35:47 - accuracy: 0.9043 - loss: 0.2296\n",
      " training set -> batch:379 loss:0.23370593786239624 and acc: 0.9043103456497192\n",
      " 379/2105 [====>.........................] - ETA: 35:42 - accuracy: 0.9043 - loss: 0.2337\n",
      " training set -> batch:380 loss:0.23093722760677338 and acc: 0.9052013158798218\n",
      "\n",
      " validation set -> batch:380 val loss:0.2570071518421173 and val acc: 0.89449542760849\n",
      " 380/2105 [====>.........................] - ETA: 36:13 - accuracy: 0.9052 - loss: 0.2309\n",
      " training set -> batch:381 loss:0.250934898853302 and acc: 0.8960176706314087\n",
      " 381/2105 [====>.........................] - ETA: 36:09 - accuracy: 0.8960 - loss: 0.2509\n",
      " training set -> batch:382 loss:0.24427025020122528 and acc: 0.8985042572021484\n",
      " 382/2105 [====>.........................] - ETA: 36:04 - accuracy: 0.8985 - loss: 0.2443\n",
      " training set -> batch:383 loss:0.24796807765960693 and acc: 0.8987603187561035\n",
      " 383/2105 [====>.........................] - ETA: 35:59 - accuracy: 0.8988 - loss: 0.2480\n",
      " training set -> batch:384 loss:0.24751298129558563 and acc: 0.8999999761581421\n",
      " 384/2105 [====>.........................] - ETA: 35:54 - accuracy: 0.9000 - loss: 0.2475\n",
      " training set -> batch:385 loss:0.24308982491493225 and acc: 0.9001938104629517\n",
      " 385/2105 [====>.........................] - ETA: 35:49 - accuracy: 0.9002 - loss: 0.2431\n",
      " training set -> batch:386 loss:0.24667520821094513 and acc: 0.8994361162185669\n",
      " 386/2105 [====>.........................] - ETA: 35:45 - accuracy: 0.8994 - loss: 0.2467\n",
      " training set -> batch:387 loss:0.24682703614234924 and acc: 0.8996350169181824\n",
      " 387/2105 [====>.........................] - ETA: 35:40 - accuracy: 0.8996 - loss: 0.2468\n",
      " training set -> batch:388 loss:0.24884194135665894 and acc: 0.8989361524581909\n",
      " 388/2105 [====>.........................] - ETA: 35:35 - accuracy: 0.8989 - loss: 0.2488\n",
      " training set -> batch:389 loss:0.25419899821281433 and acc: 0.8974137902259827\n",
      " 389/2105 [====>.........................] - ETA: 35:31 - accuracy: 0.8974 - loss: 0.2542\n",
      " training set -> batch:390 loss:0.24898816645145416 and acc: 0.8984899520874023\n",
      "\n",
      " validation set -> batch:390 val loss:0.258132666349411 and val acc: 0.896789014339447\n",
      " 390/2105 [====>.........................] - ETA: 36:01 - accuracy: 0.8985 - loss: 0.2490\n",
      " training set -> batch:391 loss:0.25586792826652527 and acc: 0.8982300758361816\n",
      " 391/2105 [====>.........................] - ETA: 35:56 - accuracy: 0.8982 - loss: 0.2559\n",
      " training set -> batch:392 loss:0.25044217705726624 and acc: 0.8995726704597473\n",
      " 392/2105 [====>.........................] - ETA: 35:51 - accuracy: 0.8996 - loss: 0.2504\n",
      " training set -> batch:393 loss:0.2617378830909729 and acc: 0.8966942429542542\n",
      " 393/2105 [====>.........................] - ETA: 35:47 - accuracy: 0.8967 - loss: 0.2617\n",
      " training set -> batch:394 loss:0.25602567195892334 and acc: 0.8980000019073486\n",
      " 394/2105 [====>.........................] - ETA: 35:42 - accuracy: 0.8980 - loss: 0.2560\n",
      " training set -> batch:395 loss:0.2535569369792938 and acc: 0.8972868323326111\n",
      " 395/2105 [====>.........................] - ETA: 35:37 - accuracy: 0.8973 - loss: 0.2536\n",
      " training set -> batch:396 loss:0.24832463264465332 and acc: 0.8994361162185669\n",
      " 396/2105 [====>.........................] - ETA: 35:33 - accuracy: 0.8994 - loss: 0.2483\n",
      " training set -> batch:397 loss:0.2537537217140198 and acc: 0.8987226486206055\n",
      " 397/2105 [====>.........................] - ETA: 35:28 - accuracy: 0.8987 - loss: 0.2538\n",
      " training set -> batch:398 loss:0.2536870241165161 and acc: 0.8989361524581909\n",
      " 398/2105 [====>.........................] - ETA: 35:23 - accuracy: 0.8989 - loss: 0.2537\n",
      " training set -> batch:399 loss:0.2530832588672638 and acc: 0.8991379141807556\n",
      " 399/2105 [====>.........................] - ETA: 35:19 - accuracy: 0.8991 - loss: 0.2531\n",
      " training set -> batch:400 loss:0.25088533759117126 and acc: 0.9001677632331848\n",
      "\n",
      " validation set -> batch:400 val loss:0.2449507862329483 and val acc: 0.8956422209739685\n",
      " 400/2105 [====>.........................] - ETA: 35:48 - accuracy: 0.9002 - loss: 0.2509\n",
      " training set -> batch:401 loss:0.24629554152488708 and acc: 0.8960176706314087\n",
      " 401/2105 [====>.........................] - ETA: 35:43 - accuracy: 0.8960 - loss: 0.2463\n",
      " training set -> batch:402 loss:0.24569182097911835 and acc: 0.8963675498962402\n",
      " 402/2105 [====>.........................] - ETA: 35:39 - accuracy: 0.8964 - loss: 0.2457\n",
      " training set -> batch:403 loss:0.2440321296453476 and acc: 0.8977272510528564\n",
      " 403/2105 [====>.........................] - ETA: 35:34 - accuracy: 0.8977 - loss: 0.2440\n",
      " training set -> batch:404 loss:0.23934824764728546 and acc: 0.8999999761581421\n",
      " 404/2105 [====>.........................] - ETA: 35:30 - accuracy: 0.9000 - loss: 0.2393\n",
      " training set -> batch:405 loss:0.2350941002368927 and acc: 0.9001938104629517\n",
      " 405/2105 [====>.........................] - ETA: 35:25 - accuracy: 0.9002 - loss: 0.2351\n",
      " training set -> batch:406 loss:0.24166631698608398 and acc: 0.8984962701797485\n",
      " 406/2105 [====>.........................] - ETA: 35:21 - accuracy: 0.8985 - loss: 0.2417\n",
      " training set -> batch:407 loss:0.24379031360149384 and acc: 0.8978102207183838\n",
      " 407/2105 [====>.........................] - ETA: 35:16 - accuracy: 0.8978 - loss: 0.2438\n",
      " training set -> batch:408 loss:0.24693606793880463 and acc: 0.8980496525764465\n",
      " 408/2105 [====>.........................] - ETA: 35:12 - accuracy: 0.8980 - loss: 0.2469\n",
      " training set -> batch:409 loss:0.2395152449607849 and acc: 0.9008620977401733\n",
      " 409/2105 [====>.........................] - ETA: 35:07 - accuracy: 0.9009 - loss: 0.2395\n",
      " training set -> batch:410 loss:0.2435079962015152 and acc: 0.9001677632331848\n",
      "\n",
      " validation set -> batch:410 val loss:0.2517107129096985 and val acc: 0.9002293348312378\n",
      " 410/2105 [====>.........................] - ETA: 35:40 - accuracy: 0.9002 - loss: 0.2435\n",
      " training set -> batch:411 loss:0.2514522075653076 and acc: 0.9004424810409546\n",
      " 411/2105 [====>.........................] - ETA: 35:35 - accuracy: 0.9004 - loss: 0.2515\n",
      " training set -> batch:412 loss:0.24736498296260834 and acc: 0.9017093777656555\n",
      " 412/2105 [====>.........................] - ETA: 35:31 - accuracy: 0.9017 - loss: 0.2474\n",
      " training set -> batch:413 loss:0.24201326072216034 and acc: 0.9028925895690918\n",
      " 413/2105 [====>.........................] - ETA: 35:26 - accuracy: 0.9029 - loss: 0.2420\n",
      " training set -> batch:414 loss:0.2462363988161087 and acc: 0.9010000228881836\n",
      " 414/2105 [====>.........................] - ETA: 35:22 - accuracy: 0.9010 - loss: 0.2462\n",
      " training set -> batch:415 loss:0.25199344754219055 and acc: 0.8982558250427246\n",
      " 415/2105 [====>.........................] - ETA: 35:17 - accuracy: 0.8983 - loss: 0.2520\n",
      " training set -> batch:416 loss:0.25050050020217896 and acc: 0.8994361162185669\n",
      " 416/2105 [====>.........................] - ETA: 35:13 - accuracy: 0.8994 - loss: 0.2505\n",
      " training set -> batch:417 loss:0.24853919446468353 and acc: 0.8996350169181824\n",
      " 417/2105 [====>.........................] - ETA: 35:09 - accuracy: 0.8996 - loss: 0.2485\n",
      " training set -> batch:418 loss:0.2473040521144867 and acc: 0.8998227119445801\n",
      " 418/2105 [====>.........................] - ETA: 35:04 - accuracy: 0.8998 - loss: 0.2473\n",
      " training set -> batch:419 loss:0.25132182240486145 and acc: 0.8982758522033691\n",
      " 419/2105 [====>.........................] - ETA: 35:00 - accuracy: 0.8983 - loss: 0.2513\n",
      " training set -> batch:420 loss:0.24663145840168 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:420 val loss:0.2405569702386856 and val acc: 0.9059633016586304\n",
      " 420/2105 [====>.........................] - ETA: 35:27 - accuracy: 0.8993 - loss: 0.2466\n",
      " training set -> batch:421 loss:0.236986443400383 and acc: 0.9070796370506287\n",
      " 421/2105 [=====>........................] - ETA: 35:23 - accuracy: 0.9071 - loss: 0.2370\n",
      " training set -> batch:422 loss:0.22729913890361786 and acc: 0.9091880321502686\n",
      " 422/2105 [=====>........................] - ETA: 35:18 - accuracy: 0.9092 - loss: 0.2273\n",
      " training set -> batch:423 loss:0.21909023821353912 and acc: 0.9121900796890259\n",
      " 423/2105 [=====>........................] - ETA: 35:14 - accuracy: 0.9122 - loss: 0.2191\n",
      " training set -> batch:424 loss:0.22852028906345367 and acc: 0.9100000262260437\n",
      " 424/2105 [=====>........................] - ETA: 35:10 - accuracy: 0.9100 - loss: 0.2285\n",
      " training set -> batch:425 loss:0.23079662024974823 and acc: 0.9089147448539734\n",
      " 425/2105 [=====>........................] - ETA: 35:05 - accuracy: 0.9089 - loss: 0.2308\n",
      " training set -> batch:426 loss:0.2259618043899536 and acc: 0.9107142686843872\n",
      " 426/2105 [=====>........................] - ETA: 35:01 - accuracy: 0.9107 - loss: 0.2260\n",
      " training set -> batch:427 loss:0.2318054884672165 and acc: 0.9096715450286865\n",
      " 427/2105 [=====>........................] - ETA: 34:56 - accuracy: 0.9097 - loss: 0.2318\n",
      " training set -> batch:428 loss:0.2355489581823349 and acc: 0.9095744490623474\n",
      " 428/2105 [=====>........................] - ETA: 34:52 - accuracy: 0.9096 - loss: 0.2355\n",
      " training set -> batch:429 loss:0.23883840441703796 and acc: 0.9077585935592651\n",
      " 429/2105 [=====>........................] - ETA: 34:48 - accuracy: 0.9078 - loss: 0.2388\n",
      " training set -> batch:430 loss:0.23786687850952148 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:430 val loss:0.24314503371715546 and val acc: 0.9048165082931519\n",
      " 430/2105 [=====>........................] - ETA: 35:14 - accuracy: 0.9077 - loss: 0.2379\n",
      " training set -> batch:431 loss:0.24626027047634125 and acc: 0.9026548862457275\n",
      " 431/2105 [=====>........................] - ETA: 35:10 - accuracy: 0.9027 - loss: 0.2463\n",
      " training set -> batch:432 loss:0.24614793062210083 and acc: 0.9017093777656555\n",
      " 432/2105 [=====>........................] - ETA: 35:06 - accuracy: 0.9017 - loss: 0.2461\n",
      " training set -> batch:433 loss:0.24704515933990479 and acc: 0.9028925895690918\n",
      " 433/2105 [=====>........................] - ETA: 35:01 - accuracy: 0.9029 - loss: 0.2470\n",
      " training set -> batch:434 loss:0.24943190813064575 and acc: 0.8999999761581421\n",
      " 434/2105 [=====>........................] - ETA: 34:57 - accuracy: 0.9000 - loss: 0.2494\n",
      " training set -> batch:435 loss:0.2530950605869293 and acc: 0.8992248177528381\n",
      " 435/2105 [=====>........................] - ETA: 34:53 - accuracy: 0.8992 - loss: 0.2531\n",
      " training set -> batch:436 loss:0.24944885075092316 and acc: 0.8994361162185669\n",
      " 436/2105 [=====>........................] - ETA: 34:49 - accuracy: 0.8994 - loss: 0.2494\n",
      " training set -> batch:437 loss:0.24502454698085785 and acc: 0.900547444820404\n",
      " 437/2105 [=====>........................] - ETA: 34:44 - accuracy: 0.9005 - loss: 0.2450\n",
      " training set -> batch:438 loss:0.24052201211452484 and acc: 0.9015957713127136\n",
      " 438/2105 [=====>........................] - ETA: 34:40 - accuracy: 0.9016 - loss: 0.2405\n",
      " training set -> batch:439 loss:0.23615065217018127 and acc: 0.9034482836723328\n",
      " 439/2105 [=====>........................] - ETA: 34:36 - accuracy: 0.9034 - loss: 0.2362\n",
      " training set -> batch:440 loss:0.23557810485363007 and acc: 0.9035235047340393\n",
      "\n",
      " validation set -> batch:440 val loss:0.25701576471328735 and val acc: 0.9002293348312378\n",
      " 440/2105 [=====>........................] - ETA: 34:58 - accuracy: 0.9035 - loss: 0.2356\n",
      " training set -> batch:441 loss:0.25971719622612 and acc: 0.8993362784385681\n",
      " 441/2105 [=====>........................] - ETA: 34:53 - accuracy: 0.8993 - loss: 0.2597\n",
      " training set -> batch:442 loss:0.25586017966270447 and acc: 0.9006410241127014\n",
      " 442/2105 [=====>........................] - ETA: 34:49 - accuracy: 0.9006 - loss: 0.2559\n",
      " training set -> batch:443 loss:0.24825410544872284 and acc: 0.9028925895690918\n",
      " 443/2105 [=====>........................] - ETA: 34:45 - accuracy: 0.9029 - loss: 0.2483\n",
      " training set -> batch:444 loss:0.24210421741008759 and acc: 0.9039999842643738\n",
      " 444/2105 [=====>........................] - ETA: 34:41 - accuracy: 0.9040 - loss: 0.2421\n",
      " training set -> batch:445 loss:0.23421739041805267 and acc: 0.9069767594337463\n",
      " 445/2105 [=====>........................] - ETA: 34:37 - accuracy: 0.9070 - loss: 0.2342\n",
      " training set -> batch:446 loss:0.2297375500202179 and acc: 0.9097744226455688\n",
      " 446/2105 [=====>........................] - ETA: 34:32 - accuracy: 0.9098 - loss: 0.2297\n",
      " training set -> batch:447 loss:0.22698372602462769 and acc: 0.9096715450286865\n",
      " 447/2105 [=====>........................] - ETA: 34:28 - accuracy: 0.9097 - loss: 0.2270\n",
      " training set -> batch:448 loss:0.2354884296655655 and acc: 0.9095744490623474\n",
      " 448/2105 [=====>........................] - ETA: 34:24 - accuracy: 0.9096 - loss: 0.2355\n",
      " training set -> batch:449 loss:0.23338402807712555 and acc: 0.9103448390960693\n",
      " 449/2105 [=====>........................] - ETA: 34:20 - accuracy: 0.9103 - loss: 0.2334\n",
      " training set -> batch:450 loss:0.23022042214870453 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:450 val loss:0.3186165690422058 and val acc: 0.8876146674156189\n",
      " 450/2105 [=====>........................] - ETA: 34:45 - accuracy: 0.9119 - loss: 0.2302\n",
      " training set -> batch:451 loss:0.3054766356945038 and acc: 0.8893805146217346\n",
      " 451/2105 [=====>........................] - ETA: 34:41 - accuracy: 0.8894 - loss: 0.3055\n",
      " training set -> batch:452 loss:0.30871376395225525 and acc: 0.8888888955116272\n",
      " 452/2105 [=====>........................] - ETA: 34:37 - accuracy: 0.8889 - loss: 0.3087\n",
      " training set -> batch:453 loss:0.31962907314300537 and acc: 0.8884297609329224\n",
      " 453/2105 [=====>........................] - ETA: 34:32 - accuracy: 0.8884 - loss: 0.3196\n",
      " training set -> batch:454 loss:0.3178682327270508 and acc: 0.8870000243186951\n",
      " 454/2105 [=====>........................] - ETA: 34:28 - accuracy: 0.8870 - loss: 0.3179\n",
      " training set -> batch:455 loss:0.3159017562866211 and acc: 0.8875969052314758\n",
      " 455/2105 [=====>........................] - ETA: 34:24 - accuracy: 0.8876 - loss: 0.3159\n",
      " training set -> batch:456 loss:0.32207006216049194 and acc: 0.8872180581092834\n",
      " 456/2105 [=====>........................] - ETA: 34:20 - accuracy: 0.8872 - loss: 0.3221\n",
      " training set -> batch:457 loss:0.3165489733219147 and acc: 0.8886861205101013\n",
      " 457/2105 [=====>........................] - ETA: 34:16 - accuracy: 0.8887 - loss: 0.3165\n",
      " training set -> batch:458 loss:0.3106188476085663 and acc: 0.8891844153404236\n",
      " 458/2105 [=====>........................] - ETA: 34:12 - accuracy: 0.8892 - loss: 0.3106\n",
      " training set -> batch:459 loss:0.3036472201347351 and acc: 0.8905172348022461\n",
      " 459/2105 [=====>........................] - ETA: 34:08 - accuracy: 0.8905 - loss: 0.3036\n",
      " training set -> batch:460 loss:0.295194536447525 and acc: 0.8926174640655518\n",
      "\n",
      " validation set -> batch:460 val loss:0.3071206510066986 and val acc: 0.8864678740501404\n",
      " 460/2105 [=====>........................] - ETA: 34:32 - accuracy: 0.8926 - loss: 0.2952\n",
      " training set -> batch:461 loss:0.30242040753364563 and acc: 0.88606196641922\n",
      " 461/2105 [=====>........................] - ETA: 34:28 - accuracy: 0.8861 - loss: 0.3024\n",
      " training set -> batch:462 loss:0.30959880352020264 and acc: 0.882478654384613\n",
      " 462/2105 [=====>........................] - ETA: 34:24 - accuracy: 0.8825 - loss: 0.3096\n",
      " training set -> batch:463 loss:0.30010828375816345 and acc: 0.8842975497245789\n",
      " 463/2105 [=====>........................] - ETA: 34:20 - accuracy: 0.8843 - loss: 0.3001\n",
      " training set -> batch:464 loss:0.2970060110092163 and acc: 0.8849999904632568\n",
      " 464/2105 [=====>........................] - ETA: 34:16 - accuracy: 0.8850 - loss: 0.2970\n",
      " training set -> batch:465 loss:0.292636901140213 and acc: 0.8846899271011353\n",
      " 465/2105 [=====>........................] - ETA: 34:12 - accuracy: 0.8847 - loss: 0.2926\n",
      " training set -> batch:466 loss:0.2934066355228424 and acc: 0.8853383660316467\n",
      " 466/2105 [=====>........................] - ETA: 34:08 - accuracy: 0.8853 - loss: 0.2934\n",
      " training set -> batch:467 loss:0.2868640422821045 and acc: 0.8850364685058594\n",
      " 467/2105 [=====>........................] - ETA: 34:04 - accuracy: 0.8850 - loss: 0.2869\n",
      " training set -> batch:468 loss:0.28102850914001465 and acc: 0.88741135597229\n",
      " 468/2105 [=====>........................] - ETA: 34:00 - accuracy: 0.8874 - loss: 0.2810\n",
      " training set -> batch:469 loss:0.28019362688064575 and acc: 0.8879310488700867\n",
      " 469/2105 [=====>........................] - ETA: 33:56 - accuracy: 0.8879 - loss: 0.2802\n",
      " training set -> batch:470 loss:0.2769908010959625 and acc: 0.8901006579399109\n",
      "\n",
      " validation set -> batch:470 val loss:0.24663639068603516 and val acc: 0.9048165082931519\n",
      " 470/2105 [=====>........................] - ETA: 34:19 - accuracy: 0.8901 - loss: 0.2770\n",
      " training set -> batch:471 loss:0.2426845133304596 and acc: 0.9059734344482422\n",
      " 471/2105 [=====>........................] - ETA: 34:15 - accuracy: 0.9060 - loss: 0.2427\n",
      " training set -> batch:472 loss:0.2435893565416336 and acc: 0.9059829115867615\n",
      " 472/2105 [=====>........................] - ETA: 34:11 - accuracy: 0.9060 - loss: 0.2436\n",
      " training set -> batch:473 loss:0.23675450682640076 and acc: 0.9080578684806824\n",
      " 473/2105 [=====>........................] - ETA: 34:07 - accuracy: 0.9081 - loss: 0.2368\n",
      " training set -> batch:474 loss:0.23504246771335602 and acc: 0.9079999923706055\n",
      " 474/2105 [=====>........................] - ETA: 34:03 - accuracy: 0.9080 - loss: 0.2350\n",
      " training set -> batch:475 loss:0.23365512490272522 and acc: 0.9069767594337463\n",
      " 475/2105 [=====>........................] - ETA: 33:59 - accuracy: 0.9070 - loss: 0.2337\n",
      " training set -> batch:476 loss:0.22970810532569885 and acc: 0.9088345766067505\n",
      " 476/2105 [=====>........................] - ETA: 33:55 - accuracy: 0.9088 - loss: 0.2297\n",
      " training set -> batch:477 loss:0.22907742857933044 and acc: 0.9087591171264648\n",
      " 477/2105 [=====>........................] - ETA: 33:52 - accuracy: 0.9088 - loss: 0.2291\n",
      " training set -> batch:478 loss:0.23513959348201752 and acc: 0.9069148898124695\n",
      " 478/2105 [=====>........................] - ETA: 33:48 - accuracy: 0.9069 - loss: 0.2351\n",
      " training set -> batch:479 loss:0.23824399709701538 and acc: 0.9060344696044922\n",
      " 479/2105 [=====>........................] - ETA: 33:44 - accuracy: 0.9060 - loss: 0.2382\n",
      " training set -> batch:480 loss:0.23991452157497406 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:480 val loss:0.24575354158878326 and val acc: 0.9105504751205444\n",
      " 480/2105 [=====>........................] - ETA: 34:06 - accuracy: 0.9060 - loss: 0.2399\n",
      " training set -> batch:481 loss:0.24457451701164246 and acc: 0.9115044474601746\n",
      " 481/2105 [=====>........................] - ETA: 34:03 - accuracy: 0.9115 - loss: 0.2446\n",
      " training set -> batch:482 loss:0.2481568455696106 and acc: 0.9091880321502686\n",
      " 482/2105 [=====>........................] - ETA: 33:59 - accuracy: 0.9092 - loss: 0.2482\n",
      " training set -> batch:483 loss:0.24531050026416779 and acc: 0.9111570119857788\n",
      " 483/2105 [=====>........................] - ETA: 33:55 - accuracy: 0.9112 - loss: 0.2453\n",
      " training set -> batch:484 loss:0.250611811876297 and acc: 0.9110000133514404\n",
      " 484/2105 [=====>........................] - ETA: 33:51 - accuracy: 0.9110 - loss: 0.2506\n",
      " training set -> batch:485 loss:0.24674181640148163 and acc: 0.9108527302742004\n",
      " 485/2105 [=====>........................] - ETA: 33:47 - accuracy: 0.9109 - loss: 0.2467\n",
      " training set -> batch:486 loss:0.24249103665351868 and acc: 0.9116541147232056\n",
      " 486/2105 [=====>........................] - ETA: 33:43 - accuracy: 0.9117 - loss: 0.2425\n",
      " training set -> batch:487 loss:0.2504218518733978 and acc: 0.9096715450286865\n",
      " 487/2105 [=====>........................] - ETA: 33:39 - accuracy: 0.9097 - loss: 0.2504\n",
      " training set -> batch:488 loss:0.24707414209842682 and acc: 0.9104610085487366\n",
      " 488/2105 [=====>........................] - ETA: 33:35 - accuracy: 0.9105 - loss: 0.2471\n",
      " training set -> batch:489 loss:0.2417399138212204 and acc: 0.9120689630508423\n",
      " 489/2105 [=====>........................] - ETA: 33:31 - accuracy: 0.9121 - loss: 0.2417\n",
      " training set -> batch:490 loss:0.2413034439086914 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:490 val loss:0.23899976909160614 and val acc: 0.9025229215621948\n",
      " 490/2105 [=====>........................] - ETA: 33:54 - accuracy: 0.9119 - loss: 0.2413\n",
      " training set -> batch:491 loss:0.2529943883419037 and acc: 0.8993362784385681\n",
      " 491/2105 [=====>........................] - ETA: 33:50 - accuracy: 0.8993 - loss: 0.2530\n",
      " training set -> batch:492 loss:0.263742595911026 and acc: 0.8974359035491943\n",
      " 492/2105 [======>.......................] - ETA: 33:46 - accuracy: 0.8974 - loss: 0.2637\n",
      " training set -> batch:493 loss:0.262838214635849 and acc: 0.8977272510528564\n",
      " 493/2105 [======>.......................] - ETA: 33:42 - accuracy: 0.8977 - loss: 0.2628\n",
      " training set -> batch:494 loss:0.26475390791893005 and acc: 0.8970000147819519\n",
      " 494/2105 [======>.......................] - ETA: 33:38 - accuracy: 0.8970 - loss: 0.2648\n",
      " training set -> batch:495 loss:0.2614246606826782 and acc: 0.8982558250427246\n",
      " 495/2105 [======>.......................] - ETA: 33:34 - accuracy: 0.8983 - loss: 0.2614\n",
      " training set -> batch:496 loss:0.25609856843948364 and acc: 0.8984962701797485\n",
      " 496/2105 [======>.......................] - ETA: 33:31 - accuracy: 0.8985 - loss: 0.2561\n",
      " training set -> batch:497 loss:0.2521681785583496 and acc: 0.8996350169181824\n",
      " 497/2105 [======>.......................] - ETA: 33:27 - accuracy: 0.8996 - loss: 0.2522\n",
      " training set -> batch:498 loss:0.2500561475753784 and acc: 0.9015957713127136\n",
      " 498/2105 [======>.......................] - ETA: 33:23 - accuracy: 0.9016 - loss: 0.2501\n",
      " training set -> batch:499 loss:0.24641534686088562 and acc: 0.9025862216949463\n",
      " 499/2105 [======>.......................] - ETA: 33:19 - accuracy: 0.9026 - loss: 0.2464\n",
      " training set -> batch:500 loss:0.24383775889873505 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:500 val loss:0.23789167404174805 and val acc: 0.8990825414657593\n",
      " 500/2105 [======>.......................] - ETA: 33:41 - accuracy: 0.9027 - loss: 0.2438\n",
      " training set -> batch:501 loss:0.23330146074295044 and acc: 0.8993362784385681\n",
      " 501/2105 [======>.......................] - ETA: 33:37 - accuracy: 0.8993 - loss: 0.2333\n",
      " training set -> batch:502 loss:0.23875105381011963 and acc: 0.8995726704597473\n",
      " 502/2105 [======>.......................] - ETA: 33:33 - accuracy: 0.8996 - loss: 0.2388\n",
      " training set -> batch:503 loss:0.24333679676055908 and acc: 0.8987603187561035\n",
      " 503/2105 [======>.......................] - ETA: 33:30 - accuracy: 0.8988 - loss: 0.2433\n",
      " training set -> batch:504 loss:0.23978526890277863 and acc: 0.8989999890327454\n",
      " 504/2105 [======>.......................] - ETA: 33:26 - accuracy: 0.8990 - loss: 0.2398\n",
      " training set -> batch:505 loss:0.2339905947446823 and acc: 0.9001938104629517\n",
      " 505/2105 [======>.......................] - ETA: 33:22 - accuracy: 0.9002 - loss: 0.2340\n",
      " training set -> batch:506 loss:0.23503009974956512 and acc: 0.9003759622573853\n",
      " 506/2105 [======>.......................] - ETA: 33:18 - accuracy: 0.9004 - loss: 0.2350\n",
      " training set -> batch:507 loss:0.23362207412719727 and acc: 0.9014598727226257\n",
      " 507/2105 [======>.......................] - ETA: 33:15 - accuracy: 0.9015 - loss: 0.2336\n",
      " training set -> batch:508 loss:0.23020409047603607 and acc: 0.902482271194458\n",
      " 508/2105 [======>.......................] - ETA: 33:11 - accuracy: 0.9025 - loss: 0.2302\n",
      " training set -> batch:509 loss:0.2290637344121933 and acc: 0.9034482836723328\n",
      " 509/2105 [======>.......................] - ETA: 33:07 - accuracy: 0.9034 - loss: 0.2291\n",
      " training set -> batch:510 loss:0.2272213101387024 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:510 val loss:0.24922554194927216 and val acc: 0.9002293348312378\n",
      " 510/2105 [======>.......................] - ETA: 33:28 - accuracy: 0.9027 - loss: 0.2272\n",
      " training set -> batch:511 loss:0.23847289383411407 and acc: 0.9026548862457275\n",
      " 511/2105 [======>.......................] - ETA: 33:25 - accuracy: 0.9027 - loss: 0.2385\n",
      " training set -> batch:512 loss:0.24009951949119568 and acc: 0.9017093777656555\n",
      " 512/2105 [======>.......................] - ETA: 33:21 - accuracy: 0.9017 - loss: 0.2401\n",
      " training set -> batch:513 loss:0.23872922360897064 and acc: 0.9018595218658447\n",
      " 513/2105 [======>.......................] - ETA: 33:17 - accuracy: 0.9019 - loss: 0.2387\n",
      " training set -> batch:514 loss:0.24276791512966156 and acc: 0.9010000228881836\n",
      " 514/2105 [======>.......................] - ETA: 33:13 - accuracy: 0.9010 - loss: 0.2428\n",
      " training set -> batch:515 loss:0.25308331847190857 and acc: 0.9001938104629517\n",
      " 515/2105 [======>.......................] - ETA: 33:10 - accuracy: 0.9002 - loss: 0.2531\n",
      " training set -> batch:516 loss:0.25108569860458374 and acc: 0.9003759622573853\n",
      " 516/2105 [======>.......................] - ETA: 33:06 - accuracy: 0.9004 - loss: 0.2511\n",
      " training set -> batch:517 loss:0.24969482421875 and acc: 0.9023722410202026\n",
      " 517/2105 [======>.......................] - ETA: 33:02 - accuracy: 0.9024 - loss: 0.2497\n",
      " training set -> batch:518 loss:0.24544191360473633 and acc: 0.9042553305625916\n",
      " 518/2105 [======>.......................] - ETA: 32:59 - accuracy: 0.9043 - loss: 0.2454\n",
      " training set -> batch:519 loss:0.24692672491073608 and acc: 0.9034482836723328\n",
      " 519/2105 [======>.......................] - ETA: 32:55 - accuracy: 0.9034 - loss: 0.2469\n",
      " training set -> batch:520 loss:0.24663694202899933 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:520 val loss:0.24944260716438293 and val acc: 0.9071100950241089\n",
      " 520/2105 [======>.......................] - ETA: 33:15 - accuracy: 0.9027 - loss: 0.2466\n",
      " training set -> batch:521 loss:0.24548028409481049 and acc: 0.9081858396530151\n",
      " 521/2105 [======>.......................] - ETA: 33:12 - accuracy: 0.9082 - loss: 0.2455\n",
      " training set -> batch:522 loss:0.24942882359027863 and acc: 0.9070512652397156\n",
      " 522/2105 [======>.......................] - ETA: 33:08 - accuracy: 0.9071 - loss: 0.2494\n",
      " training set -> batch:523 loss:0.2502129375934601 and acc: 0.9070248007774353\n",
      " 523/2105 [======>.......................] - ETA: 33:04 - accuracy: 0.9070 - loss: 0.2502\n",
      " training set -> batch:524 loss:0.25110748410224915 and acc: 0.906000018119812\n",
      " 524/2105 [======>.......................] - ETA: 33:01 - accuracy: 0.9060 - loss: 0.2511\n",
      " training set -> batch:525 loss:0.24870288372039795 and acc: 0.9069767594337463\n",
      " 525/2105 [======>.......................] - ETA: 32:57 - accuracy: 0.9070 - loss: 0.2487\n",
      " training set -> batch:526 loss:0.24900174140930176 and acc: 0.9069548845291138\n",
      " 526/2105 [======>.......................] - ETA: 32:53 - accuracy: 0.9070 - loss: 0.2490\n",
      " training set -> batch:527 loss:0.24304024875164032 and acc: 0.9087591171264648\n",
      " 527/2105 [======>.......................] - ETA: 32:50 - accuracy: 0.9088 - loss: 0.2430\n",
      " training set -> batch:528 loss:0.24550507962703705 and acc: 0.9069148898124695\n",
      " 528/2105 [======>.......................] - ETA: 32:46 - accuracy: 0.9069 - loss: 0.2455\n",
      " training set -> batch:529 loss:0.2454448789358139 and acc: 0.9068965315818787\n",
      " 529/2105 [======>.......................] - ETA: 32:43 - accuracy: 0.9069 - loss: 0.2454\n",
      " training set -> batch:530 loss:0.2418997436761856 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:530 val loss:0.2616567015647888 and val acc: 0.89449542760849\n",
      " 530/2105 [======>.......................] - ETA: 33:03 - accuracy: 0.9077 - loss: 0.2419\n",
      " training set -> batch:531 loss:0.2572633624076843 and acc: 0.8960176706314087\n",
      " 531/2105 [======>.......................] - ETA: 32:59 - accuracy: 0.8960 - loss: 0.2573\n",
      " training set -> batch:532 loss:0.24915245175361633 and acc: 0.8985042572021484\n",
      " 532/2105 [======>.......................] - ETA: 32:55 - accuracy: 0.8985 - loss: 0.2492\n",
      " training set -> batch:533 loss:0.24623048305511475 and acc: 0.8987603187561035\n",
      " 533/2105 [======>.......................] - ETA: 32:52 - accuracy: 0.8988 - loss: 0.2462\n",
      " training set -> batch:534 loss:0.2420642077922821 and acc: 0.8999999761581421\n",
      " 534/2105 [======>.......................] - ETA: 32:48 - accuracy: 0.9000 - loss: 0.2421\n",
      " training set -> batch:535 loss:0.25084570050239563 and acc: 0.8963178396224976\n",
      " 535/2105 [======>.......................] - ETA: 32:45 - accuracy: 0.8963 - loss: 0.2508\n",
      " training set -> batch:536 loss:0.24823543429374695 and acc: 0.8975563645362854\n",
      " 536/2105 [======>.......................] - ETA: 32:41 - accuracy: 0.8976 - loss: 0.2482\n",
      " training set -> batch:537 loss:0.25626713037490845 and acc: 0.8959854245185852\n",
      " 537/2105 [======>.......................] - ETA: 32:37 - accuracy: 0.8960 - loss: 0.2563\n",
      " training set -> batch:538 loss:0.25854384899139404 and acc: 0.8945035338401794\n",
      " 538/2105 [======>.......................] - ETA: 32:34 - accuracy: 0.8945 - loss: 0.2585\n",
      " training set -> batch:539 loss:0.25786691904067993 and acc: 0.8948276042938232\n",
      " 539/2105 [======>.......................] - ETA: 32:30 - accuracy: 0.8948 - loss: 0.2579\n",
      " training set -> batch:540 loss:0.2584863007068634 and acc: 0.8942952752113342\n",
      "\n",
      " validation set -> batch:540 val loss:0.24903273582458496 and val acc: 0.89449542760849\n",
      " 540/2105 [======>.......................] - ETA: 32:50 - accuracy: 0.8943 - loss: 0.2585\n",
      " training set -> batch:541 loss:0.24440717697143555 and acc: 0.8960176706314087\n",
      " 541/2105 [======>.......................] - ETA: 32:46 - accuracy: 0.8960 - loss: 0.2444\n",
      " training set -> batch:542 loss:0.24664469063282013 and acc: 0.8963675498962402\n",
      " 542/2105 [======>.......................] - ETA: 32:43 - accuracy: 0.8964 - loss: 0.2466\n",
      " training set -> batch:543 loss:0.24796807765960693 and acc: 0.8956611752510071\n",
      " 543/2105 [======>.......................] - ETA: 32:39 - accuracy: 0.8957 - loss: 0.2480\n",
      " training set -> batch:544 loss:0.24331945180892944 and acc: 0.8970000147819519\n",
      " 544/2105 [======>.......................] - ETA: 32:36 - accuracy: 0.8970 - loss: 0.2433\n",
      " training set -> batch:545 loss:0.2397637665271759 and acc: 0.8972868323326111\n",
      " 545/2105 [======>.......................] - ETA: 32:32 - accuracy: 0.8973 - loss: 0.2398\n",
      " training set -> batch:546 loss:0.24027188122272491 and acc: 0.896616518497467\n",
      " 546/2105 [======>.......................] - ETA: 32:29 - accuracy: 0.8966 - loss: 0.2403\n",
      " training set -> batch:547 loss:0.24755537509918213 and acc: 0.8959854245185852\n",
      " 547/2105 [======>.......................] - ETA: 32:25 - accuracy: 0.8960 - loss: 0.2476\n",
      " training set -> batch:548 loss:0.2481309026479721 and acc: 0.8971630930900574\n",
      " 548/2105 [======>.......................] - ETA: 32:22 - accuracy: 0.8972 - loss: 0.2481\n",
      " training set -> batch:549 loss:0.2463996559381485 and acc: 0.8991379141807556\n",
      " 549/2105 [======>.......................] - ETA: 32:18 - accuracy: 0.8991 - loss: 0.2464\n",
      " training set -> batch:550 loss:0.2440163940191269 and acc: 0.9001677632331848\n",
      "\n",
      " validation set -> batch:550 val loss:0.23276209831237793 and val acc: 0.8933486342430115\n",
      " 550/2105 [======>.......................] - ETA: 32:37 - accuracy: 0.9002 - loss: 0.2440\n",
      " training set -> batch:551 loss:0.22963176667690277 and acc: 0.8938053250312805\n",
      " 551/2105 [======>.......................] - ETA: 32:33 - accuracy: 0.8938 - loss: 0.2296\n",
      " training set -> batch:552 loss:0.22636519372463226 and acc: 0.8952991366386414\n",
      " 552/2105 [======>.......................] - ETA: 32:30 - accuracy: 0.8953 - loss: 0.2264\n",
      " training set -> batch:553 loss:0.23387399315834045 and acc: 0.89462810754776\n",
      " 553/2105 [======>.......................] - ETA: 32:27 - accuracy: 0.8946 - loss: 0.2339\n",
      " training set -> batch:554 loss:0.22992685437202454 and acc: 0.8970000147819519\n",
      " 554/2105 [======>.......................] - ETA: 32:23 - accuracy: 0.8970 - loss: 0.2299\n",
      " training set -> batch:555 loss:0.22655878961086273 and acc: 0.8982558250427246\n",
      " 555/2105 [======>.......................] - ETA: 32:20 - accuracy: 0.8983 - loss: 0.2266\n",
      " training set -> batch:556 loss:0.22306375205516815 and acc: 0.8994361162185669\n",
      " 556/2105 [======>.......................] - ETA: 32:16 - accuracy: 0.8994 - loss: 0.2231\n",
      " training set -> batch:557 loss:0.22615912556648254 and acc: 0.8996350169181824\n",
      " 557/2105 [======>.......................] - ETA: 32:13 - accuracy: 0.8996 - loss: 0.2262\n",
      " training set -> batch:558 loss:0.2247384637594223 and acc: 0.9007092118263245\n",
      " 558/2105 [======>.......................] - ETA: 32:09 - accuracy: 0.9007 - loss: 0.2247\n",
      " training set -> batch:559 loss:0.21908578276634216 and acc: 0.9025862216949463\n",
      " 559/2105 [======>.......................] - ETA: 32:06 - accuracy: 0.9026 - loss: 0.2191\n",
      " training set -> batch:560 loss:0.21801941096782684 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:560 val loss:0.23094996809959412 and val acc: 0.9094036817550659\n",
      " 560/2105 [======>.......................] - ETA: 32:25 - accuracy: 0.9027 - loss: 0.2180\n",
      " training set -> batch:561 loss:0.2241850197315216 and acc: 0.9103982448577881\n",
      " 561/2105 [======>.......................] - ETA: 32:21 - accuracy: 0.9104 - loss: 0.2242\n",
      " training set -> batch:562 loss:0.22290588915348053 and acc: 0.9102563858032227\n",
      " 562/2105 [=======>......................] - ETA: 32:18 - accuracy: 0.9103 - loss: 0.2229\n",
      " training set -> batch:563 loss:0.2242746651172638 and acc: 0.9101239442825317\n",
      " 563/2105 [=======>......................] - ETA: 32:15 - accuracy: 0.9101 - loss: 0.2243\n",
      " training set -> batch:564 loss:0.22088469564914703 and acc: 0.9100000262260437\n",
      " 564/2105 [=======>......................] - ETA: 32:11 - accuracy: 0.9100 - loss: 0.2209\n",
      " training set -> batch:565 loss:0.21773649752140045 and acc: 0.911821722984314\n",
      " 565/2105 [=======>......................] - ETA: 32:08 - accuracy: 0.9118 - loss: 0.2177\n",
      " training set -> batch:566 loss:0.23121985793113708 and acc: 0.9069548845291138\n",
      " 566/2105 [=======>......................] - ETA: 32:04 - accuracy: 0.9070 - loss: 0.2312\n",
      " training set -> batch:567 loss:0.23211851716041565 and acc: 0.9069343209266663\n",
      " 567/2105 [=======>......................] - ETA: 32:01 - accuracy: 0.9069 - loss: 0.2321\n",
      " training set -> batch:568 loss:0.23239582777023315 and acc: 0.9060283899307251\n",
      " 568/2105 [=======>......................] - ETA: 31:58 - accuracy: 0.9060 - loss: 0.2324\n",
      " training set -> batch:569 loss:0.23108737170696259 and acc: 0.9068965315818787\n",
      " 569/2105 [=======>......................] - ETA: 31:54 - accuracy: 0.9069 - loss: 0.2311\n",
      " training set -> batch:570 loss:0.23352886736392975 and acc: 0.9052013158798218\n",
      "\n",
      " validation set -> batch:570 val loss:0.23831970989704132 and val acc: 0.9105504751205444\n",
      " 570/2105 [=======>......................] - ETA: 32:17 - accuracy: 0.9052 - loss: 0.2335\n",
      " training set -> batch:571 loss:0.23084229230880737 and acc: 0.9115044474601746\n",
      " 571/2105 [=======>......................] - ETA: 32:13 - accuracy: 0.9115 - loss: 0.2308\n",
      " training set -> batch:572 loss:0.22627031803131104 and acc: 0.9134615659713745\n",
      " 572/2105 [=======>......................] - ETA: 32:10 - accuracy: 0.9135 - loss: 0.2263\n",
      " training set -> batch:573 loss:0.22090540826320648 and acc: 0.9152892827987671\n",
      " 573/2105 [=======>......................] - ETA: 32:06 - accuracy: 0.9153 - loss: 0.2209\n",
      " training set -> batch:574 loss:0.21643300354480743 and acc: 0.9160000085830688\n",
      " 574/2105 [=======>......................] - ETA: 32:03 - accuracy: 0.9160 - loss: 0.2164\n",
      " training set -> batch:575 loss:0.21960222721099854 and acc: 0.9147287011146545\n",
      " 575/2105 [=======>......................] - ETA: 32:00 - accuracy: 0.9147 - loss: 0.2196\n",
      " training set -> batch:576 loss:0.21615366637706757 and acc: 0.9154135584831238\n",
      " 576/2105 [=======>......................] - ETA: 31:56 - accuracy: 0.9154 - loss: 0.2162\n",
      " training set -> batch:577 loss:0.21214108169078827 and acc: 0.9169707894325256\n",
      " 577/2105 [=======>......................] - ETA: 31:53 - accuracy: 0.9170 - loss: 0.2121\n",
      " training set -> batch:578 loss:0.21188126504421234 and acc: 0.9157801270484924\n",
      " 578/2105 [=======>......................] - ETA: 31:50 - accuracy: 0.9158 - loss: 0.2119\n",
      " training set -> batch:579 loss:0.20922523736953735 and acc: 0.9163793325424194\n",
      " 579/2105 [=======>......................] - ETA: 31:46 - accuracy: 0.9164 - loss: 0.2092\n",
      " training set -> batch:580 loss:0.20843829214572906 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:580 val loss:0.26780787110328674 and val acc: 0.8990825414657593\n",
      " 580/2105 [=======>......................] - ETA: 32:04 - accuracy: 0.9161 - loss: 0.2084\n",
      " training set -> batch:581 loss:0.27123117446899414 and acc: 0.8993362784385681\n",
      " 581/2105 [=======>......................] - ETA: 32:01 - accuracy: 0.8993 - loss: 0.2712\n",
      " training set -> batch:582 loss:0.26978424191474915 and acc: 0.8974359035491943\n",
      " 582/2105 [=======>......................] - ETA: 31:57 - accuracy: 0.8974 - loss: 0.2698\n",
      " training set -> batch:583 loss:0.27086141705513 and acc: 0.8956611752510071\n",
      " 583/2105 [=======>......................] - ETA: 31:54 - accuracy: 0.8957 - loss: 0.2709\n",
      " training set -> batch:584 loss:0.2626592516899109 and acc: 0.8980000019073486\n",
      " 584/2105 [=======>......................] - ETA: 31:51 - accuracy: 0.8980 - loss: 0.2627\n",
      " training set -> batch:585 loss:0.26049354672431946 and acc: 0.8992248177528381\n",
      " 585/2105 [=======>......................] - ETA: 31:47 - accuracy: 0.8992 - loss: 0.2605\n",
      " training set -> batch:586 loss:0.2658410966396332 and acc: 0.8994361162185669\n",
      " 586/2105 [=======>......................] - ETA: 31:44 - accuracy: 0.8994 - loss: 0.2658\n",
      " training set -> batch:587 loss:0.26349586248397827 and acc: 0.8996350169181824\n",
      " 587/2105 [=======>......................] - ETA: 31:41 - accuracy: 0.8996 - loss: 0.2635\n",
      " training set -> batch:588 loss:0.2640198767185211 and acc: 0.8998227119445801\n",
      " 588/2105 [=======>......................] - ETA: 31:38 - accuracy: 0.8998 - loss: 0.2640\n",
      " training set -> batch:589 loss:0.26802176237106323 and acc: 0.8999999761581421\n",
      " 589/2105 [=======>......................] - ETA: 31:34 - accuracy: 0.9000 - loss: 0.2680\n",
      " training set -> batch:590 loss:0.26480019092559814 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:590 val loss:0.26324060559272766 and val acc: 0.9002293348312378\n",
      " 590/2105 [=======>......................] - ETA: 31:52 - accuracy: 0.8993 - loss: 0.2648\n",
      " training set -> batch:591 loss:0.25856003165245056 and acc: 0.9015486836433411\n",
      " 591/2105 [=======>......................] - ETA: 31:48 - accuracy: 0.9015 - loss: 0.2586\n",
      " training set -> batch:592 loss:0.25677868723869324 and acc: 0.9006410241127014\n",
      " 592/2105 [=======>......................] - ETA: 31:45 - accuracy: 0.9006 - loss: 0.2568\n",
      " training set -> batch:593 loss:0.2517971098423004 and acc: 0.9018595218658447\n",
      " 593/2105 [=======>......................] - ETA: 31:42 - accuracy: 0.9019 - loss: 0.2518\n",
      " training set -> batch:594 loss:0.24874228239059448 and acc: 0.9020000100135803\n",
      " 594/2105 [=======>......................] - ETA: 31:38 - accuracy: 0.9020 - loss: 0.2487\n",
      " training set -> batch:595 loss:0.24424733221530914 and acc: 0.9031007885932922\n",
      " 595/2105 [=======>......................] - ETA: 31:35 - accuracy: 0.9031 - loss: 0.2442\n",
      " training set -> batch:596 loss:0.2611002027988434 and acc: 0.8994361162185669\n",
      " 596/2105 [=======>......................] - ETA: 31:32 - accuracy: 0.8994 - loss: 0.2611\n",
      " training set -> batch:597 loss:0.26949211955070496 and acc: 0.8968977928161621\n",
      " 597/2105 [=======>......................] - ETA: 31:29 - accuracy: 0.8969 - loss: 0.2695\n",
      " training set -> batch:598 loss:0.2628856301307678 and acc: 0.8980496525764465\n",
      " 598/2105 [=======>......................] - ETA: 31:25 - accuracy: 0.8980 - loss: 0.2629\n",
      " training set -> batch:599 loss:0.2604949474334717 and acc: 0.8999999761581421\n",
      " 599/2105 [=======>......................] - ETA: 31:22 - accuracy: 0.9000 - loss: 0.2605\n",
      " training set -> batch:600 loss:0.25602319836616516 and acc: 0.9010066986083984\n",
      "\n",
      " validation set -> batch:600 val loss:0.23606674373149872 and val acc: 0.8979358077049255\n",
      " 600/2105 [=======>......................] - ETA: 31:39 - accuracy: 0.9010 - loss: 0.2560\n",
      " training set -> batch:601 loss:0.2330579310655594 and acc: 0.8993362784385681\n",
      " 601/2105 [=======>......................] - ETA: 31:36 - accuracy: 0.8993 - loss: 0.2331\n",
      " training set -> batch:602 loss:0.2349817305803299 and acc: 0.8995726704597473\n",
      " 602/2105 [=======>......................] - ETA: 31:32 - accuracy: 0.8996 - loss: 0.2350\n",
      " training set -> batch:603 loss:0.23316967487335205 and acc: 0.8997933864593506\n",
      " 603/2105 [=======>......................] - ETA: 31:29 - accuracy: 0.8998 - loss: 0.2332\n",
      " training set -> batch:604 loss:0.22728048264980316 and acc: 0.9020000100135803\n",
      " 604/2105 [=======>......................] - ETA: 31:26 - accuracy: 0.9020 - loss: 0.2273\n",
      " training set -> batch:605 loss:0.22316378355026245 and acc: 0.9040697813034058\n",
      " 605/2105 [=======>......................] - ETA: 31:23 - accuracy: 0.9041 - loss: 0.2232\n",
      " training set -> batch:606 loss:0.2256782501935959 and acc: 0.9013158082962036\n",
      " 606/2105 [=======>......................] - ETA: 31:19 - accuracy: 0.9013 - loss: 0.2257\n",
      " training set -> batch:607 loss:0.2302495390176773 and acc: 0.8996350169181824\n",
      " 607/2105 [=======>......................] - ETA: 31:16 - accuracy: 0.8996 - loss: 0.2302\n",
      " training set -> batch:608 loss:0.22678762674331665 and acc: 0.9007092118263245\n",
      " 608/2105 [=======>......................] - ETA: 31:13 - accuracy: 0.9007 - loss: 0.2268\n",
      " training set -> batch:609 loss:0.22421623766422272 and acc: 0.9017241597175598\n",
      " 609/2105 [=======>......................] - ETA: 31:10 - accuracy: 0.9017 - loss: 0.2242\n",
      " training set -> batch:610 loss:0.21852189302444458 and acc: 0.9043624401092529\n",
      "\n",
      " validation set -> batch:610 val loss:0.236228808760643 and val acc: 0.9082568883895874\n",
      " 610/2105 [=======>......................] - ETA: 31:27 - accuracy: 0.9044 - loss: 0.2185\n",
      " training set -> batch:611 loss:0.2306530624628067 and acc: 0.9103982448577881\n",
      " 611/2105 [=======>......................] - ETA: 31:23 - accuracy: 0.9104 - loss: 0.2307\n",
      " training set -> batch:612 loss:0.22503049671649933 and acc: 0.9123931527137756\n",
      " 612/2105 [=======>......................] - ETA: 31:20 - accuracy: 0.9124 - loss: 0.2250\n",
      " training set -> batch:613 loss:0.22446660697460175 and acc: 0.913223147392273\n",
      " 613/2105 [=======>......................] - ETA: 31:17 - accuracy: 0.9132 - loss: 0.2245\n",
      " training set -> batch:614 loss:0.21503113210201263 and acc: 0.9160000085830688\n",
      " 614/2105 [=======>......................] - ETA: 31:14 - accuracy: 0.9160 - loss: 0.2150\n",
      " training set -> batch:615 loss:0.21198023855686188 and acc: 0.9166666865348816\n",
      " 615/2105 [=======>......................] - ETA: 31:11 - accuracy: 0.9167 - loss: 0.2120\n",
      " training set -> batch:616 loss:0.2085300236940384 and acc: 0.9172932505607605\n",
      " 616/2105 [=======>......................] - ETA: 31:08 - accuracy: 0.9173 - loss: 0.2085\n",
      " training set -> batch:617 loss:0.21257460117340088 and acc: 0.9169707894325256\n",
      " 617/2105 [=======>......................] - ETA: 31:04 - accuracy: 0.9170 - loss: 0.2126\n",
      " training set -> batch:618 loss:0.21466164290905 and acc: 0.917553186416626\n",
      " 618/2105 [=======>......................] - ETA: 31:01 - accuracy: 0.9176 - loss: 0.2147\n",
      " training set -> batch:619 loss:0.20896708965301514 and acc: 0.9189655184745789\n",
      " 619/2105 [=======>......................] - ETA: 30:58 - accuracy: 0.9190 - loss: 0.2090\n",
      " training set -> batch:620 loss:0.2049911767244339 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:620 val loss:0.267760306596756 and val acc: 0.9059633016586304\n",
      " 620/2105 [=======>......................] - ETA: 31:14 - accuracy: 0.9203 - loss: 0.2050\n",
      " training set -> batch:621 loss:0.2523864805698395 and acc: 0.9092920422554016\n",
      " 621/2105 [=======>......................] - ETA: 31:11 - accuracy: 0.9093 - loss: 0.2524\n",
      " training set -> batch:622 loss:0.2539002597332001 and acc: 0.9070512652397156\n",
      " 622/2105 [=======>......................] - ETA: 31:08 - accuracy: 0.9071 - loss: 0.2539\n",
      " training set -> batch:623 loss:0.2554958164691925 and acc: 0.9070248007774353\n",
      " 623/2105 [=======>......................] - ETA: 31:05 - accuracy: 0.9070 - loss: 0.2555\n",
      " training set -> batch:624 loss:0.2601504623889923 and acc: 0.906000018119812\n",
      " 624/2105 [=======>......................] - ETA: 31:02 - accuracy: 0.9060 - loss: 0.2602\n",
      " training set -> batch:625 loss:0.26242944598197937 and acc: 0.9050387740135193\n",
      " 625/2105 [=======>......................] - ETA: 30:58 - accuracy: 0.9050 - loss: 0.2624\n",
      " training set -> batch:626 loss:0.2542935013771057 and acc: 0.9069548845291138\n",
      " 626/2105 [=======>......................] - ETA: 30:55 - accuracy: 0.9070 - loss: 0.2543\n",
      " training set -> batch:627 loss:0.259259432554245 and acc: 0.904197096824646\n",
      " 627/2105 [=======>......................] - ETA: 30:52 - accuracy: 0.9042 - loss: 0.2593\n",
      " training set -> batch:628 loss:0.25031331181526184 and acc: 0.9069148898124695\n",
      " 628/2105 [=======>......................] - ETA: 30:49 - accuracy: 0.9069 - loss: 0.2503\n",
      " training set -> batch:629 loss:0.2592756152153015 and acc: 0.9043103456497192\n",
      " 629/2105 [=======>......................] - ETA: 30:46 - accuracy: 0.9043 - loss: 0.2593\n",
      " training set -> batch:630 loss:0.25285983085632324 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:630 val loss:0.2561039328575134 and val acc: 0.9082568883895874\n",
      " 630/2105 [=======>......................] - ETA: 31:02 - accuracy: 0.9060 - loss: 0.2529\n",
      " training set -> batch:631 loss:0.2753300666809082 and acc: 0.9048672318458557\n",
      " 631/2105 [=======>......................] - ETA: 30:58 - accuracy: 0.9049 - loss: 0.2753\n",
      " training set -> batch:632 loss:0.2673434317111969 and acc: 0.9059829115867615\n",
      " 632/2105 [========>.....................] - ETA: 30:55 - accuracy: 0.9060 - loss: 0.2673\n",
      " training set -> batch:633 loss:0.26475492119789124 and acc: 0.9049586653709412\n",
      " 633/2105 [========>.....................] - ETA: 30:52 - accuracy: 0.9050 - loss: 0.2648\n",
      " training set -> batch:634 loss:0.26201561093330383 and acc: 0.9049999713897705\n",
      " 634/2105 [========>.....................] - ETA: 30:49 - accuracy: 0.9050 - loss: 0.2620\n",
      " training set -> batch:635 loss:0.2546304762363434 and acc: 0.9060077667236328\n",
      " 635/2105 [========>.....................] - ETA: 30:46 - accuracy: 0.9060 - loss: 0.2546\n",
      " training set -> batch:636 loss:0.2492552548646927 and acc: 0.9078947305679321\n",
      " 636/2105 [========>.....................] - ETA: 30:43 - accuracy: 0.9079 - loss: 0.2493\n",
      " training set -> batch:637 loss:0.243636816740036 and acc: 0.9096715450286865\n",
      " 637/2105 [========>.....................] - ETA: 30:40 - accuracy: 0.9097 - loss: 0.2436\n",
      " training set -> batch:638 loss:0.2423069328069687 and acc: 0.9095744490623474\n",
      " 638/2105 [========>.....................] - ETA: 30:37 - accuracy: 0.9096 - loss: 0.2423\n",
      " training set -> batch:639 loss:0.23912455141544342 and acc: 0.9112069010734558\n",
      " 639/2105 [========>.....................] - ETA: 30:34 - accuracy: 0.9112 - loss: 0.2391\n",
      " training set -> batch:640 loss:0.24232260882854462 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:640 val loss:0.24143186211585999 and val acc: 0.896789014339447\n",
      " 640/2105 [========>.....................] - ETA: 30:49 - accuracy: 0.9111 - loss: 0.2423\n",
      " training set -> batch:641 loss:0.24593967199325562 and acc: 0.8971238732337952\n",
      " 641/2105 [========>.....................] - ETA: 30:46 - accuracy: 0.8971 - loss: 0.2459\n",
      " training set -> batch:642 loss:0.24465131759643555 and acc: 0.8985042572021484\n",
      " 642/2105 [========>.....................] - ETA: 30:43 - accuracy: 0.8985 - loss: 0.2447\n",
      " training set -> batch:643 loss:0.24145549535751343 and acc: 0.8997933864593506\n",
      " 643/2105 [========>.....................] - ETA: 30:40 - accuracy: 0.8998 - loss: 0.2415\n",
      " training set -> batch:644 loss:0.24776850640773773 and acc: 0.8970000147819519\n",
      " 644/2105 [========>.....................] - ETA: 30:37 - accuracy: 0.8970 - loss: 0.2478\n",
      " training set -> batch:645 loss:0.25216996669769287 and acc: 0.895348846912384\n",
      " 645/2105 [========>.....................] - ETA: 30:34 - accuracy: 0.8953 - loss: 0.2522\n",
      " training set -> batch:646 loss:0.25404971837997437 and acc: 0.8956766724586487\n",
      " 646/2105 [========>.....................] - ETA: 30:30 - accuracy: 0.8957 - loss: 0.2540\n",
      " training set -> batch:647 loss:0.2461872100830078 and acc: 0.8987226486206055\n",
      " 647/2105 [========>.....................] - ETA: 30:27 - accuracy: 0.8987 - loss: 0.2462\n",
      " training set -> batch:648 loss:0.2468225508928299 and acc: 0.8989361524581909\n",
      " 648/2105 [========>.....................] - ETA: 30:24 - accuracy: 0.8989 - loss: 0.2468\n",
      " training set -> batch:649 loss:0.24393567442893982 and acc: 0.8999999761581421\n",
      " 649/2105 [========>.....................] - ETA: 30:21 - accuracy: 0.9000 - loss: 0.2439\n",
      " training set -> batch:650 loss:0.24249029159545898 and acc: 0.9018456339836121\n",
      "\n",
      " validation set -> batch:650 val loss:0.24050988256931305 and val acc: 0.9071100950241089\n",
      " 650/2105 [========>.....................] - ETA: 30:36 - accuracy: 0.9018 - loss: 0.2425\n",
      " training set -> batch:651 loss:0.23930135369300842 and acc: 0.9081858396530151\n",
      " 651/2105 [========>.....................] - ETA: 30:33 - accuracy: 0.9082 - loss: 0.2393\n",
      " training set -> batch:652 loss:0.23141951858997345 and acc: 0.9091880321502686\n",
      " 652/2105 [========>.....................] - ETA: 30:30 - accuracy: 0.9092 - loss: 0.2314\n",
      " training set -> batch:653 loss:0.22765634953975677 and acc: 0.9101239442825317\n",
      " 653/2105 [========>.....................] - ETA: 30:27 - accuracy: 0.9101 - loss: 0.2277\n",
      " training set -> batch:654 loss:0.22816316783428192 and acc: 0.9089999794960022\n",
      " 654/2105 [========>.....................] - ETA: 30:24 - accuracy: 0.9090 - loss: 0.2282\n",
      " training set -> batch:655 loss:0.2236575037240982 and acc: 0.9098837375640869\n",
      " 655/2105 [========>.....................] - ETA: 30:21 - accuracy: 0.9099 - loss: 0.2237\n",
      " training set -> batch:656 loss:0.22182326018810272 and acc: 0.9097744226455688\n",
      " 656/2105 [========>.....................] - ETA: 30:18 - accuracy: 0.9098 - loss: 0.2218\n",
      " training set -> batch:657 loss:0.22174523770809174 and acc: 0.9096715450286865\n",
      " 657/2105 [========>.....................] - ETA: 30:15 - accuracy: 0.9097 - loss: 0.2217\n",
      " training set -> batch:658 loss:0.21738500893115997 and acc: 0.911347508430481\n",
      " 658/2105 [========>.....................] - ETA: 30:12 - accuracy: 0.9113 - loss: 0.2174\n",
      " training set -> batch:659 loss:0.21926572918891907 and acc: 0.9103448390960693\n",
      " 659/2105 [========>.....................] - ETA: 30:09 - accuracy: 0.9103 - loss: 0.2193\n",
      " training set -> batch:660 loss:0.21298615634441376 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:660 val loss:0.23626764118671417 and val acc: 0.9048165082931519\n",
      " 660/2105 [========>.....................] - ETA: 30:24 - accuracy: 0.9119 - loss: 0.2130\n",
      " training set -> batch:661 loss:0.23003454506397247 and acc: 0.9070796370506287\n",
      " 661/2105 [========>.....................] - ETA: 30:21 - accuracy: 0.9071 - loss: 0.2300\n",
      " training set -> batch:662 loss:0.2275029569864273 and acc: 0.9070512652397156\n",
      " 662/2105 [========>.....................] - ETA: 30:18 - accuracy: 0.9071 - loss: 0.2275\n",
      " training set -> batch:663 loss:0.23363792896270752 and acc: 0.9070248007774353\n",
      " 663/2105 [========>.....................] - ETA: 30:15 - accuracy: 0.9070 - loss: 0.2336\n",
      " training set -> batch:664 loss:0.22341704368591309 and acc: 0.9100000262260437\n",
      " 664/2105 [========>.....................] - ETA: 30:12 - accuracy: 0.9100 - loss: 0.2234\n",
      " training set -> batch:665 loss:0.22947080433368683 and acc: 0.9079457521438599\n",
      " 665/2105 [========>.....................] - ETA: 30:09 - accuracy: 0.9079 - loss: 0.2295\n",
      " training set -> batch:666 loss:0.2317388504743576 and acc: 0.9069548845291138\n",
      " 666/2105 [========>.....................] - ETA: 30:06 - accuracy: 0.9070 - loss: 0.2317\n",
      " training set -> batch:667 loss:0.22740250825881958 and acc: 0.9087591171264648\n",
      " 667/2105 [========>.....................] - ETA: 30:03 - accuracy: 0.9088 - loss: 0.2274\n",
      " training set -> batch:668 loss:0.23046918213367462 and acc: 0.9078013896942139\n",
      " 668/2105 [========>.....................] - ETA: 30:00 - accuracy: 0.9078 - loss: 0.2305\n",
      " training set -> batch:669 loss:0.23265844583511353 and acc: 0.9077585935592651\n",
      " 669/2105 [========>.....................] - ETA: 29:57 - accuracy: 0.9078 - loss: 0.2327\n",
      " training set -> batch:670 loss:0.22808556258678436 and acc: 0.9085570573806763\n",
      "\n",
      " validation set -> batch:670 val loss:0.2730059325695038 and val acc: 0.896789014339447\n",
      " 670/2105 [========>.....................] - ETA: 30:11 - accuracy: 0.9086 - loss: 0.2281\n",
      " training set -> batch:671 loss:0.27056822180747986 and acc: 0.8971238732337952\n",
      " 671/2105 [========>.....................] - ETA: 30:08 - accuracy: 0.8971 - loss: 0.2706\n",
      " training set -> batch:672 loss:0.26662999391555786 and acc: 0.8963675498962402\n",
      " 672/2105 [========>.....................] - ETA: 30:05 - accuracy: 0.8964 - loss: 0.2666\n",
      " training set -> batch:673 loss:0.25506138801574707 and acc: 0.8997933864593506\n",
      " 673/2105 [========>.....................] - ETA: 30:02 - accuracy: 0.8998 - loss: 0.2551\n",
      " training set -> batch:674 loss:0.24511756002902985 and acc: 0.9020000100135803\n",
      " 674/2105 [========>.....................] - ETA: 29:59 - accuracy: 0.9020 - loss: 0.2451\n",
      " training set -> batch:675 loss:0.25482913851737976 and acc: 0.9001938104629517\n",
      " 675/2105 [========>.....................] - ETA: 29:56 - accuracy: 0.9002 - loss: 0.2548\n",
      " training set -> batch:676 loss:0.253248393535614 and acc: 0.8984962701797485\n",
      " 676/2105 [========>.....................] - ETA: 29:53 - accuracy: 0.8985 - loss: 0.2532\n",
      " training set -> batch:677 loss:0.24840828776359558 and acc: 0.8987226486206055\n",
      " 677/2105 [========>.....................] - ETA: 29:50 - accuracy: 0.8987 - loss: 0.2484\n",
      " training set -> batch:678 loss:0.24136872589588165 and acc: 0.9015957713127136\n",
      " 678/2105 [========>.....................] - ETA: 29:47 - accuracy: 0.9016 - loss: 0.2414\n",
      " training set -> batch:679 loss:0.23602141439914703 and acc: 0.9034482836723328\n",
      " 679/2105 [========>.....................] - ETA: 29:44 - accuracy: 0.9034 - loss: 0.2360\n",
      " training set -> batch:680 loss:0.2327888160943985 and acc: 0.9043624401092529\n",
      "\n",
      " validation set -> batch:680 val loss:0.27086469531059265 and val acc: 0.8887614607810974\n",
      " 680/2105 [========>.....................] - ETA: 29:58 - accuracy: 0.9044 - loss: 0.2328\n",
      " training set -> batch:681 loss:0.2898278534412384 and acc: 0.88606196641922\n",
      " 681/2105 [========>.....................] - ETA: 29:55 - accuracy: 0.8861 - loss: 0.2898\n",
      " training set -> batch:682 loss:0.28113430738449097 and acc: 0.8878205418586731\n",
      " 682/2105 [========>.....................] - ETA: 29:52 - accuracy: 0.8878 - loss: 0.2811\n",
      " training set -> batch:683 loss:0.2793523967266083 and acc: 0.8894628286361694\n",
      " 683/2105 [========>.....................] - ETA: 29:49 - accuracy: 0.8895 - loss: 0.2794\n",
      " training set -> batch:684 loss:0.27458450198173523 and acc: 0.8920000195503235\n",
      " 684/2105 [========>.....................] - ETA: 29:47 - accuracy: 0.8920 - loss: 0.2746\n",
      " training set -> batch:685 loss:0.27264779806137085 and acc: 0.893410861492157\n",
      " 685/2105 [========>.....................] - ETA: 29:44 - accuracy: 0.8934 - loss: 0.2726\n",
      " training set -> batch:686 loss:0.2681291103363037 and acc: 0.8947368264198303\n",
      " 686/2105 [========>.....................] - ETA: 29:41 - accuracy: 0.8947 - loss: 0.2681\n",
      " training set -> batch:687 loss:0.271566778421402 and acc: 0.8932482004165649\n",
      " 687/2105 [========>.....................] - ETA: 29:38 - accuracy: 0.8932 - loss: 0.2716\n",
      " training set -> batch:688 loss:0.26685404777526855 and acc: 0.8945035338401794\n",
      " 688/2105 [========>.....................] - ETA: 29:35 - accuracy: 0.8945 - loss: 0.2669\n",
      " training set -> batch:689 loss:0.26037830114364624 and acc: 0.8948276042938232\n",
      " 689/2105 [========>.....................] - ETA: 29:32 - accuracy: 0.8948 - loss: 0.2604\n",
      " training set -> batch:690 loss:0.26364096999168396 and acc: 0.8934563994407654\n",
      "\n",
      " validation set -> batch:690 val loss:0.26153093576431274 and val acc: 0.9036697149276733\n",
      " 690/2105 [========>.....................] - ETA: 29:46 - accuracy: 0.8935 - loss: 0.2636\n",
      " training set -> batch:691 loss:0.2634001076221466 and acc: 0.9026548862457275\n",
      " 691/2105 [========>.....................] - ETA: 29:43 - accuracy: 0.9027 - loss: 0.2634\n",
      " training set -> batch:692 loss:0.25471797585487366 and acc: 0.9049145579338074\n",
      " 692/2105 [========>.....................] - ETA: 29:40 - accuracy: 0.9049 - loss: 0.2547\n",
      " training set -> batch:693 loss:0.2571284770965576 and acc: 0.9049586653709412\n",
      " 693/2105 [========>.....................] - ETA: 29:37 - accuracy: 0.9050 - loss: 0.2571\n",
      " training set -> batch:694 loss:0.2619321346282959 and acc: 0.902999997138977\n",
      " 694/2105 [========>.....................] - ETA: 29:34 - accuracy: 0.9030 - loss: 0.2619\n",
      " training set -> batch:695 loss:0.2656092941761017 and acc: 0.9021317958831787\n",
      " 695/2105 [========>.....................] - ETA: 29:31 - accuracy: 0.9021 - loss: 0.2656\n",
      " training set -> batch:696 loss:0.26427602767944336 and acc: 0.902255654335022\n",
      " 696/2105 [========>.....................] - ETA: 29:28 - accuracy: 0.9023 - loss: 0.2643\n",
      " training set -> batch:697 loss:0.26432833075523376 and acc: 0.900547444820404\n",
      " 697/2105 [========>.....................] - ETA: 29:26 - accuracy: 0.9005 - loss: 0.2643\n",
      " training set -> batch:698 loss:0.26027432084083557 and acc: 0.9015957713127136\n",
      " 698/2105 [========>.....................] - ETA: 29:23 - accuracy: 0.9016 - loss: 0.2603\n",
      " training set -> batch:699 loss:0.2609294056892395 and acc: 0.9017241597175598\n",
      " 699/2105 [========>.....................] - ETA: 29:20 - accuracy: 0.9017 - loss: 0.2609\n",
      " training set -> batch:700 loss:0.2569330036640167 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:700 val loss:0.24778330326080322 and val acc: 0.9048165082931519\n",
      " 700/2105 [========>.....................] - ETA: 29:33 - accuracy: 0.9027 - loss: 0.2569\n",
      " training set -> batch:701 loss:0.24209147691726685 and acc: 0.9070796370506287\n",
      " 701/2105 [========>.....................] - ETA: 29:30 - accuracy: 0.9071 - loss: 0.2421\n",
      " training set -> batch:702 loss:0.23907609283924103 and acc: 0.9081196784973145\n",
      " 702/2105 [=========>....................] - ETA: 29:27 - accuracy: 0.9081 - loss: 0.2391\n",
      " training set -> batch:703 loss:0.2391694039106369 and acc: 0.9080578684806824\n",
      " 703/2105 [=========>....................] - ETA: 29:24 - accuracy: 0.9081 - loss: 0.2392\n",
      " training set -> batch:704 loss:0.24429672956466675 and acc: 0.906000018119812\n",
      " 704/2105 [=========>....................] - ETA: 29:22 - accuracy: 0.9060 - loss: 0.2443\n",
      " training set -> batch:705 loss:0.2450440376996994 and acc: 0.9031007885932922\n",
      " 705/2105 [=========>....................] - ETA: 29:19 - accuracy: 0.9031 - loss: 0.2450\n",
      " training set -> batch:706 loss:0.2482442557811737 and acc: 0.9013158082962036\n",
      " 706/2105 [=========>....................] - ETA: 29:16 - accuracy: 0.9013 - loss: 0.2482\n",
      " training set -> batch:707 loss:0.2526968717575073 and acc: 0.8996350169181824\n",
      " 707/2105 [=========>....................] - ETA: 29:13 - accuracy: 0.8996 - loss: 0.2527\n",
      " training set -> batch:708 loss:0.2553139925003052 and acc: 0.8989361524581909\n",
      " 708/2105 [=========>....................] - ETA: 29:10 - accuracy: 0.8989 - loss: 0.2553\n",
      " training set -> batch:709 loss:0.25543540716171265 and acc: 0.8991379141807556\n",
      " 709/2105 [=========>....................] - ETA: 29:07 - accuracy: 0.8991 - loss: 0.2554\n",
      " training set -> batch:710 loss:0.2582356631755829 and acc: 0.899328887462616\n",
      "\n",
      " validation set -> batch:710 val loss:0.26907700300216675 and val acc: 0.89449542760849\n",
      " 710/2105 [=========>....................] - ETA: 29:20 - accuracy: 0.8993 - loss: 0.2582\n",
      " training set -> batch:711 loss:0.2682798206806183 and acc: 0.892699122428894\n",
      " 711/2105 [=========>....................] - ETA: 29:17 - accuracy: 0.8927 - loss: 0.2683\n",
      " training set -> batch:712 loss:0.25855985283851624 and acc: 0.8952991366386414\n",
      " 712/2105 [=========>....................] - ETA: 29:15 - accuracy: 0.8953 - loss: 0.2586\n",
      " training set -> batch:713 loss:0.25107836723327637 and acc: 0.8966942429542542\n",
      " 713/2105 [=========>....................] - ETA: 29:12 - accuracy: 0.8967 - loss: 0.2511\n",
      " training set -> batch:714 loss:0.24558837711811066 and acc: 0.8980000019073486\n",
      " 714/2105 [=========>....................] - ETA: 29:09 - accuracy: 0.8980 - loss: 0.2456\n",
      " training set -> batch:715 loss:0.23904059827327728 and acc: 0.9001938104629517\n",
      " 715/2105 [=========>....................] - ETA: 29:06 - accuracy: 0.9002 - loss: 0.2390\n",
      " training set -> batch:716 loss:0.23915788531303406 and acc: 0.8984962701797485\n",
      " 716/2105 [=========>....................] - ETA: 29:03 - accuracy: 0.8985 - loss: 0.2392\n",
      " training set -> batch:717 loss:0.2386324554681778 and acc: 0.8996350169181824\n",
      " 717/2105 [=========>....................] - ETA: 29:01 - accuracy: 0.8996 - loss: 0.2386\n",
      " training set -> batch:718 loss:0.2465762197971344 and acc: 0.8980496525764465\n",
      " 718/2105 [=========>....................] - ETA: 28:58 - accuracy: 0.8980 - loss: 0.2466\n",
      " training set -> batch:719 loss:0.24030615389347076 and acc: 0.8999999761581421\n",
      " 719/2105 [=========>....................] - ETA: 28:55 - accuracy: 0.9000 - loss: 0.2403\n",
      " training set -> batch:720 loss:0.23868989944458008 and acc: 0.9010066986083984\n",
      "\n",
      " validation set -> batch:720 val loss:0.2548801898956299 and val acc: 0.9013761281967163\n",
      " 720/2105 [=========>....................] - ETA: 29:08 - accuracy: 0.9010 - loss: 0.2387\n",
      " training set -> batch:721 loss:0.24230654537677765 and acc: 0.9048672318458557\n",
      " 721/2105 [=========>....................] - ETA: 29:05 - accuracy: 0.9049 - loss: 0.2423\n",
      " training set -> batch:722 loss:0.2348479926586151 and acc: 0.9070512652397156\n",
      " 722/2105 [=========>....................] - ETA: 29:02 - accuracy: 0.9071 - loss: 0.2348\n",
      " training set -> batch:723 loss:0.22894954681396484 and acc: 0.9070248007774353\n",
      " 723/2105 [=========>....................] - ETA: 28:59 - accuracy: 0.9070 - loss: 0.2289\n",
      " training set -> batch:724 loss:0.22073858976364136 and acc: 0.9089999794960022\n",
      " 724/2105 [=========>....................] - ETA: 28:56 - accuracy: 0.9090 - loss: 0.2207\n",
      " training set -> batch:725 loss:0.2145664542913437 and acc: 0.9098837375640869\n",
      " 725/2105 [=========>....................] - ETA: 28:54 - accuracy: 0.9099 - loss: 0.2146\n",
      " training set -> batch:726 loss:0.2134251892566681 and acc: 0.9116541147232056\n",
      " 726/2105 [=========>....................] - ETA: 28:51 - accuracy: 0.9117 - loss: 0.2134\n",
      " training set -> batch:727 loss:0.21633656322956085 and acc: 0.9114963412284851\n",
      " 727/2105 [=========>....................] - ETA: 28:48 - accuracy: 0.9115 - loss: 0.2163\n",
      " training set -> batch:728 loss:0.22216233611106873 and acc: 0.908687949180603\n",
      " 728/2105 [=========>....................] - ETA: 28:45 - accuracy: 0.9087 - loss: 0.2222\n",
      " training set -> batch:729 loss:0.22603735327720642 and acc: 0.9086207151412964\n",
      " 729/2105 [=========>....................] - ETA: 28:43 - accuracy: 0.9086 - loss: 0.2260\n",
      " training set -> batch:730 loss:0.21953873336315155 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:730 val loss:0.24193325638771057 and val acc: 0.9048165082931519\n",
      " 730/2105 [=========>....................] - ETA: 28:55 - accuracy: 0.9102 - loss: 0.2195\n",
      " training set -> batch:731 loss:0.25099998712539673 and acc: 0.9015486836433411\n",
      " 731/2105 [=========>....................] - ETA: 28:52 - accuracy: 0.9015 - loss: 0.2510\n",
      " training set -> batch:732 loss:0.24580645561218262 and acc: 0.9027777910232544\n",
      " 732/2105 [=========>....................] - ETA: 28:49 - accuracy: 0.9028 - loss: 0.2458\n",
      " training set -> batch:733 loss:0.245523139834404 and acc: 0.9018595218658447\n",
      " 733/2105 [=========>....................] - ETA: 28:47 - accuracy: 0.9019 - loss: 0.2455\n",
      " training set -> batch:734 loss:0.241733580827713 and acc: 0.902999997138977\n",
      " 734/2105 [=========>....................] - ETA: 28:44 - accuracy: 0.9030 - loss: 0.2417\n",
      " training set -> batch:735 loss:0.23587414622306824 and acc: 0.9050387740135193\n",
      " 735/2105 [=========>....................] - ETA: 28:41 - accuracy: 0.9050 - loss: 0.2359\n",
      " training set -> batch:736 loss:0.23710432648658752 and acc: 0.905075192451477\n",
      " 736/2105 [=========>....................] - ETA: 28:38 - accuracy: 0.9051 - loss: 0.2371\n",
      " training set -> batch:737 loss:0.24040304124355316 and acc: 0.904197096824646\n",
      " 737/2105 [=========>....................] - ETA: 28:36 - accuracy: 0.9042 - loss: 0.2404\n",
      " training set -> batch:738 loss:0.237528458237648 and acc: 0.9051418304443359\n",
      " 738/2105 [=========>....................] - ETA: 28:33 - accuracy: 0.9051 - loss: 0.2375\n",
      " training set -> batch:739 loss:0.23475277423858643 and acc: 0.9060344696044922\n",
      " 739/2105 [=========>....................] - ETA: 28:30 - accuracy: 0.9060 - loss: 0.2348\n",
      " training set -> batch:740 loss:0.233199343085289 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:740 val loss:0.22623459994792938 and val acc: 0.9105504751205444\n",
      " 740/2105 [=========>....................] - ETA: 28:42 - accuracy: 0.9060 - loss: 0.2332\n",
      " training set -> batch:741 loss:0.2258748561143875 and acc: 0.9103982448577881\n",
      " 741/2105 [=========>....................] - ETA: 28:39 - accuracy: 0.9104 - loss: 0.2259\n",
      " training set -> batch:742 loss:0.22221852838993073 and acc: 0.9113247990608215\n",
      " 742/2105 [=========>....................] - ETA: 28:37 - accuracy: 0.9113 - loss: 0.2222\n",
      " training set -> batch:743 loss:0.23401369154453278 and acc: 0.9101239442825317\n",
      " 743/2105 [=========>....................] - ETA: 28:34 - accuracy: 0.9101 - loss: 0.2340\n",
      " training set -> batch:744 loss:0.2330392748117447 and acc: 0.9100000262260437\n",
      " 744/2105 [=========>....................] - ETA: 28:31 - accuracy: 0.9100 - loss: 0.2330\n",
      " training set -> batch:745 loss:0.2365727424621582 and acc: 0.9089147448539734\n",
      " 745/2105 [=========>....................] - ETA: 28:29 - accuracy: 0.9089 - loss: 0.2366\n",
      " training set -> batch:746 loss:0.23291699588298798 and acc: 0.9088345766067505\n",
      " 746/2105 [=========>....................] - ETA: 28:26 - accuracy: 0.9088 - loss: 0.2329\n",
      " training set -> batch:747 loss:0.23882193863391876 and acc: 0.9069343209266663\n",
      " 747/2105 [=========>....................] - ETA: 28:23 - accuracy: 0.9069 - loss: 0.2388\n",
      " training set -> batch:748 loss:0.23187847435474396 and acc: 0.908687949180603\n",
      " 748/2105 [=========>....................] - ETA: 28:20 - accuracy: 0.9087 - loss: 0.2319\n",
      " training set -> batch:749 loss:0.23289601504802704 and acc: 0.9068965315818787\n",
      " 749/2105 [=========>....................] - ETA: 28:18 - accuracy: 0.9069 - loss: 0.2329\n",
      " training set -> batch:750 loss:0.23271618783473969 and acc: 0.906879186630249\n",
      "\n",
      " validation set -> batch:750 val loss:0.23357941210269928 and val acc: 0.9082568883895874\n",
      " 750/2105 [=========>....................] - ETA: 28:29 - accuracy: 0.9069 - loss: 0.2327\n",
      " training set -> batch:751 loss:0.23172752559185028 and acc: 0.9092920422554016\n",
      " 751/2105 [=========>....................] - ETA: 28:27 - accuracy: 0.9093 - loss: 0.2317\n",
      " training set -> batch:752 loss:0.225498229265213 and acc: 0.9113247990608215\n",
      " 752/2105 [=========>....................] - ETA: 28:24 - accuracy: 0.9113 - loss: 0.2255\n",
      " training set -> batch:753 loss:0.2199764996767044 and acc: 0.913223147392273\n",
      " 753/2105 [=========>....................] - ETA: 28:21 - accuracy: 0.9132 - loss: 0.2200\n",
      " training set -> batch:754 loss:0.22742073237895966 and acc: 0.9120000004768372\n",
      " 754/2105 [=========>....................] - ETA: 28:19 - accuracy: 0.9120 - loss: 0.2274\n",
      " training set -> batch:755 loss:0.2292858213186264 and acc: 0.911821722984314\n",
      " 755/2105 [=========>....................] - ETA: 28:16 - accuracy: 0.9118 - loss: 0.2293\n",
      " training set -> batch:756 loss:0.22802035510540009 and acc: 0.9125939607620239\n",
      " 756/2105 [=========>....................] - ETA: 28:13 - accuracy: 0.9126 - loss: 0.2280\n",
      " training set -> batch:757 loss:0.22769062221050262 and acc: 0.9114963412284851\n",
      " 757/2105 [=========>....................] - ETA: 28:11 - accuracy: 0.9115 - loss: 0.2277\n",
      " training set -> batch:758 loss:0.22477596998214722 and acc: 0.9122340679168701\n",
      " 758/2105 [=========>....................] - ETA: 28:08 - accuracy: 0.9122 - loss: 0.2248\n",
      " training set -> batch:759 loss:0.22425837814807892 and acc: 0.9129310250282288\n",
      " 759/2105 [=========>....................] - ETA: 28:05 - accuracy: 0.9129 - loss: 0.2243\n",
      " training set -> batch:760 loss:0.22118620574474335 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:760 val loss:0.2509443759918213 and val acc: 0.9036697149276733\n",
      " 760/2105 [=========>....................] - ETA: 28:20 - accuracy: 0.9136 - loss: 0.2212\n",
      " training set -> batch:761 loss:0.25398537516593933 and acc: 0.903761088848114\n",
      " 761/2105 [=========>....................] - ETA: 28:17 - accuracy: 0.9038 - loss: 0.2540\n",
      " training set -> batch:762 loss:0.24827733635902405 and acc: 0.9059829115867615\n",
      " 762/2105 [=========>....................] - ETA: 28:14 - accuracy: 0.9060 - loss: 0.2483\n",
      " training set -> batch:763 loss:0.2470710426568985 and acc: 0.9049586653709412\n",
      " 763/2105 [=========>....................] - ETA: 28:12 - accuracy: 0.9050 - loss: 0.2471\n",
      " training set -> batch:764 loss:0.2388741672039032 and acc: 0.9070000052452087\n",
      " 764/2105 [=========>....................] - ETA: 28:09 - accuracy: 0.9070 - loss: 0.2389\n",
      " training set -> batch:765 loss:0.23197360336780548 and acc: 0.9079457521438599\n",
      " 765/2105 [=========>....................] - ETA: 28:06 - accuracy: 0.9079 - loss: 0.2320\n",
      " training set -> batch:766 loss:0.2316901981830597 and acc: 0.9088345766067505\n",
      " 766/2105 [=========>....................] - ETA: 28:04 - accuracy: 0.9088 - loss: 0.2317\n",
      " training set -> batch:767 loss:0.23705659806728363 and acc: 0.9069343209266663\n",
      " 767/2105 [=========>....................] - ETA: 28:01 - accuracy: 0.9069 - loss: 0.2371\n",
      " training set -> batch:768 loss:0.24036522209644318 and acc: 0.9069148898124695\n",
      " 768/2105 [=========>....................] - ETA: 27:59 - accuracy: 0.9069 - loss: 0.2404\n",
      " training set -> batch:769 loss:0.2393781691789627 and acc: 0.9068965315818787\n",
      " 769/2105 [=========>....................] - ETA: 27:56 - accuracy: 0.9069 - loss: 0.2394\n",
      " training set -> batch:770 loss:0.2429623156785965 and acc: 0.9043624401092529\n",
      "\n",
      " validation set -> batch:770 val loss:0.27619272470474243 and val acc: 0.9071100950241089\n",
      " 770/2105 [=========>....................] - ETA: 28:07 - accuracy: 0.9044 - loss: 0.2430\n",
      " training set -> batch:771 loss:0.26797813177108765 and acc: 0.9059734344482422\n",
      " 771/2105 [=========>....................] - ETA: 28:04 - accuracy: 0.9060 - loss: 0.2680\n",
      " training set -> batch:772 loss:0.26215675473213196 and acc: 0.9059829115867615\n",
      " 772/2105 [==========>...................] - ETA: 28:02 - accuracy: 0.9060 - loss: 0.2622\n",
      " training set -> batch:773 loss:0.2551822364330292 and acc: 0.9080578684806824\n",
      " 773/2105 [==========>...................] - ETA: 27:59 - accuracy: 0.9081 - loss: 0.2552\n",
      " training set -> batch:774 loss:0.25115731358528137 and acc: 0.9079999923706055\n",
      " 774/2105 [==========>...................] - ETA: 27:57 - accuracy: 0.9080 - loss: 0.2512\n",
      " training set -> batch:775 loss:0.24624285101890564 and acc: 0.9098837375640869\n",
      " 775/2105 [==========>...................] - ETA: 27:54 - accuracy: 0.9099 - loss: 0.2462\n",
      " training set -> batch:776 loss:0.24241948127746582 and acc: 0.9107142686843872\n",
      " 776/2105 [==========>...................] - ETA: 27:51 - accuracy: 0.9107 - loss: 0.2424\n",
      " training set -> batch:777 loss:0.25605884194374084 and acc: 0.9069343209266663\n",
      " 777/2105 [==========>...................] - ETA: 27:49 - accuracy: 0.9069 - loss: 0.2561\n",
      " training set -> batch:778 loss:0.25449860095977783 and acc: 0.9060283899307251\n",
      " 778/2105 [==========>...................] - ETA: 27:46 - accuracy: 0.9060 - loss: 0.2545\n",
      " training set -> batch:779 loss:0.24804140627384186 and acc: 0.9077585935592651\n",
      " 779/2105 [==========>...................] - ETA: 27:43 - accuracy: 0.9078 - loss: 0.2480\n",
      " training set -> batch:780 loss:0.23945285379886627 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:780 val loss:0.24530434608459473 and val acc: 0.9082568883895874\n",
      " 780/2105 [==========>...................] - ETA: 27:54 - accuracy: 0.9102 - loss: 0.2395\n",
      " training set -> batch:781 loss:0.23736871778964996 and acc: 0.9092920422554016\n",
      " 781/2105 [==========>...................] - ETA: 27:52 - accuracy: 0.9093 - loss: 0.2374\n",
      " training set -> batch:782 loss:0.236291766166687 and acc: 0.9102563858032227\n",
      " 782/2105 [==========>...................] - ETA: 27:49 - accuracy: 0.9103 - loss: 0.2363\n",
      " training set -> batch:783 loss:0.23097267746925354 and acc: 0.9111570119857788\n",
      " 783/2105 [==========>...................] - ETA: 27:46 - accuracy: 0.9112 - loss: 0.2310\n",
      " training set -> batch:784 loss:0.2291012406349182 and acc: 0.9110000133514404\n",
      " 784/2105 [==========>...................] - ETA: 27:44 - accuracy: 0.9110 - loss: 0.2291\n",
      " training set -> batch:785 loss:0.2203253209590912 and acc: 0.913759708404541\n",
      " 785/2105 [==========>...................] - ETA: 27:41 - accuracy: 0.9138 - loss: 0.2203\n",
      " training set -> batch:786 loss:0.22099053859710693 and acc: 0.9144737124443054\n",
      " 786/2105 [==========>...................] - ETA: 27:39 - accuracy: 0.9145 - loss: 0.2210\n",
      " training set -> batch:787 loss:0.21343392133712769 and acc: 0.9169707894325256\n",
      " 787/2105 [==========>...................] - ETA: 27:36 - accuracy: 0.9170 - loss: 0.2134\n",
      " training set -> batch:788 loss:0.21718840301036835 and acc: 0.9157801270484924\n",
      " 788/2105 [==========>...................] - ETA: 27:34 - accuracy: 0.9158 - loss: 0.2172\n",
      " training set -> batch:789 loss:0.21046361327171326 and acc: 0.9172413945198059\n",
      " 789/2105 [==========>...................] - ETA: 27:31 - accuracy: 0.9172 - loss: 0.2105\n",
      " training set -> batch:790 loss:0.20961147546768188 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:790 val loss:0.236722931265831 and val acc: 0.9082568883895874\n",
      " 790/2105 [==========>...................] - ETA: 27:42 - accuracy: 0.9169 - loss: 0.2096\n",
      " training set -> batch:791 loss:0.22543030977249146 and acc: 0.9115044474601746\n",
      " 791/2105 [==========>...................] - ETA: 27:39 - accuracy: 0.9115 - loss: 0.2254\n",
      " training set -> batch:792 loss:0.23598803579807281 and acc: 0.9091880321502686\n",
      " 792/2105 [==========>...................] - ETA: 27:36 - accuracy: 0.9092 - loss: 0.2360\n",
      " training set -> batch:793 loss:0.23260490596294403 and acc: 0.9101239442825317\n",
      " 793/2105 [==========>...................] - ETA: 27:34 - accuracy: 0.9101 - loss: 0.2326\n",
      " training set -> batch:794 loss:0.2255520522594452 and acc: 0.9120000004768372\n",
      " 794/2105 [==========>...................] - ETA: 27:31 - accuracy: 0.9120 - loss: 0.2256\n",
      " training set -> batch:795 loss:0.22576922178268433 and acc: 0.911821722984314\n",
      " 795/2105 [==========>...................] - ETA: 27:29 - accuracy: 0.9118 - loss: 0.2258\n",
      " training set -> batch:796 loss:0.22561243176460266 and acc: 0.9116541147232056\n",
      " 796/2105 [==========>...................] - ETA: 27:26 - accuracy: 0.9117 - loss: 0.2256\n",
      " training set -> batch:797 loss:0.22469161450862885 and acc: 0.9114963412284851\n",
      " 797/2105 [==========>...................] - ETA: 27:23 - accuracy: 0.9115 - loss: 0.2247\n",
      " training set -> batch:798 loss:0.21982072293758392 and acc: 0.9140070676803589\n",
      " 798/2105 [==========>...................] - ETA: 27:21 - accuracy: 0.9140 - loss: 0.2198\n",
      " training set -> batch:799 loss:0.23113909363746643 and acc: 0.9120689630508423\n",
      " 799/2105 [==========>...................] - ETA: 27:18 - accuracy: 0.9121 - loss: 0.2311\n",
      " training set -> batch:800 loss:0.2301650494337082 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:800 val loss:0.22035716474056244 and val acc: 0.9151375889778137\n",
      " 800/2105 [==========>...................] - ETA: 27:29 - accuracy: 0.9128 - loss: 0.2302\n",
      " training set -> batch:801 loss:0.21444541215896606 and acc: 0.9159291982650757\n",
      " 801/2105 [==========>...................] - ETA: 27:26 - accuracy: 0.9159 - loss: 0.2144\n",
      " training set -> batch:802 loss:0.21476061642169952 and acc: 0.9155982732772827\n",
      " 802/2105 [==========>...................] - ETA: 27:24 - accuracy: 0.9156 - loss: 0.2148\n",
      " training set -> batch:803 loss:0.2118780016899109 and acc: 0.9163222908973694\n",
      " 803/2105 [==========>...................] - ETA: 27:21 - accuracy: 0.9163 - loss: 0.2119\n",
      " training set -> batch:804 loss:0.20823854207992554 and acc: 0.9169999957084656\n",
      " 804/2105 [==========>...................] - ETA: 27:18 - accuracy: 0.9170 - loss: 0.2082\n",
      " training set -> batch:805 loss:0.20591631531715393 and acc: 0.9176356792449951\n",
      " 805/2105 [==========>...................] - ETA: 27:16 - accuracy: 0.9176 - loss: 0.2059\n",
      " training set -> batch:806 loss:0.2010372132062912 and acc: 0.9191729426383972\n",
      " 806/2105 [==========>...................] - ETA: 27:13 - accuracy: 0.9192 - loss: 0.2010\n",
      " training set -> batch:807 loss:0.20052902400493622 and acc: 0.9197080135345459\n",
      " 807/2105 [==========>...................] - ETA: 27:11 - accuracy: 0.9197 - loss: 0.2005\n",
      " training set -> batch:808 loss:0.2055792659521103 and acc: 0.9193262457847595\n",
      " 808/2105 [==========>...................] - ETA: 27:08 - accuracy: 0.9193 - loss: 0.2056\n",
      " training set -> batch:809 loss:0.2040744423866272 and acc: 0.9181034564971924\n",
      " 809/2105 [==========>...................] - ETA: 27:06 - accuracy: 0.9181 - loss: 0.2041\n",
      " training set -> batch:810 loss:0.19991286098957062 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:810 val loss:0.22080762684345245 and val acc: 0.9185779690742493\n",
      " 810/2105 [==========>...................] - ETA: 27:16 - accuracy: 0.9186 - loss: 0.1999\n",
      " training set -> batch:811 loss:0.21945475041866302 and acc: 0.9181416034698486\n",
      " 811/2105 [==========>...................] - ETA: 27:14 - accuracy: 0.9181 - loss: 0.2195\n",
      " training set -> batch:812 loss:0.21131277084350586 and acc: 0.9209401607513428\n",
      " 812/2105 [==========>...................] - ETA: 27:11 - accuracy: 0.9209 - loss: 0.2113\n",
      " training set -> batch:813 loss:0.21738646924495697 and acc: 0.9204545617103577\n",
      " 813/2105 [==========>...................] - ETA: 27:08 - accuracy: 0.9205 - loss: 0.2174\n",
      " training set -> batch:814 loss:0.21485590934753418 and acc: 0.9210000038146973\n",
      " 814/2105 [==========>...................] - ETA: 27:06 - accuracy: 0.9210 - loss: 0.2149\n",
      " training set -> batch:815 loss:0.21793292462825775 and acc: 0.9205426573753357\n",
      " 815/2105 [==========>...................] - ETA: 27:03 - accuracy: 0.9205 - loss: 0.2179\n",
      " training set -> batch:816 loss:0.2155274897813797 and acc: 0.9201127886772156\n",
      " 816/2105 [==========>...................] - ETA: 27:01 - accuracy: 0.9201 - loss: 0.2155\n",
      " training set -> batch:817 loss:0.2147287130355835 and acc: 0.9197080135345459\n",
      " 817/2105 [==========>...................] - ETA: 26:58 - accuracy: 0.9197 - loss: 0.2147\n",
      " training set -> batch:818 loss:0.211744025349617 and acc: 0.9202127456665039\n",
      " 818/2105 [==========>...................] - ETA: 26:56 - accuracy: 0.9202 - loss: 0.2117\n",
      " training set -> batch:819 loss:0.21900740265846252 and acc: 0.9189655184745789\n",
      " 819/2105 [==========>...................] - ETA: 26:53 - accuracy: 0.9190 - loss: 0.2190\n",
      " training set -> batch:820 loss:0.21209128201007843 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:820 val loss:0.2800877094268799 and val acc: 0.896789014339447\n",
      " 820/2105 [==========>...................] - ETA: 27:03 - accuracy: 0.9211 - loss: 0.2121\n",
      " training set -> batch:821 loss:0.2800576984882355 and acc: 0.894911527633667\n",
      " 821/2105 [==========>...................] - ETA: 27:01 - accuracy: 0.8949 - loss: 0.2801\n",
      " training set -> batch:822 loss:0.27463844418525696 and acc: 0.8952991366386414\n",
      " 822/2105 [==========>...................] - ETA: 26:58 - accuracy: 0.8953 - loss: 0.2746\n",
      " training set -> batch:823 loss:0.2731195390224457 and acc: 0.8956611752510071\n",
      " 823/2105 [==========>...................] - ETA: 26:56 - accuracy: 0.8957 - loss: 0.2731\n",
      " training set -> batch:824 loss:0.2818249464035034 and acc: 0.8939999938011169\n",
      " 824/2105 [==========>...................] - ETA: 26:53 - accuracy: 0.8940 - loss: 0.2818\n",
      " training set -> batch:825 loss:0.27661556005477905 and acc: 0.8943798542022705\n",
      " 825/2105 [==========>...................] - ETA: 26:51 - accuracy: 0.8944 - loss: 0.2766\n",
      " training set -> batch:826 loss:0.26879656314849854 and acc: 0.896616518497467\n",
      " 826/2105 [==========>...................] - ETA: 26:48 - accuracy: 0.8966 - loss: 0.2688\n",
      " training set -> batch:827 loss:0.2623600661754608 and acc: 0.8987226486206055\n",
      " 827/2105 [==========>...................] - ETA: 26:46 - accuracy: 0.8987 - loss: 0.2624\n",
      " training set -> batch:828 loss:0.2532791793346405 and acc: 0.9015957713127136\n",
      " 828/2105 [==========>...................] - ETA: 26:43 - accuracy: 0.9016 - loss: 0.2533\n",
      " training set -> batch:829 loss:0.2522616684436798 and acc: 0.9017241597175598\n",
      " 829/2105 [==========>...................] - ETA: 26:41 - accuracy: 0.9017 - loss: 0.2523\n",
      " training set -> batch:830 loss:0.2510276138782501 and acc: 0.9026845693588257\n",
      "\n",
      " validation set -> batch:830 val loss:0.22777403891086578 and val acc: 0.9094036817550659\n",
      " 830/2105 [==========>...................] - ETA: 26:51 - accuracy: 0.9027 - loss: 0.2510\n",
      " training set -> batch:831 loss:0.23484984040260315 and acc: 0.9081858396530151\n",
      " 831/2105 [==========>...................] - ETA: 26:48 - accuracy: 0.9082 - loss: 0.2348\n",
      " training set -> batch:832 loss:0.2316550314426422 and acc: 0.9091880321502686\n",
      " 832/2105 [==========>...................] - ETA: 26:46 - accuracy: 0.9092 - loss: 0.2317\n",
      " training set -> batch:833 loss:0.23812086880207062 and acc: 0.9101239442825317\n",
      " 833/2105 [==========>...................] - ETA: 26:43 - accuracy: 0.9101 - loss: 0.2381\n",
      " training set -> batch:834 loss:0.24381092190742493 and acc: 0.9079999923706055\n",
      " 834/2105 [==========>...................] - ETA: 26:41 - accuracy: 0.9080 - loss: 0.2438\n",
      " training set -> batch:835 loss:0.23481108248233795 and acc: 0.9098837375640869\n",
      " 835/2105 [==========>...................] - ETA: 26:38 - accuracy: 0.9099 - loss: 0.2348\n",
      " training set -> batch:836 loss:0.23270228505134583 and acc: 0.9097744226455688\n",
      " 836/2105 [==========>...................] - ETA: 26:36 - accuracy: 0.9098 - loss: 0.2327\n",
      " training set -> batch:837 loss:0.2275245487689972 and acc: 0.9114963412284851\n",
      " 837/2105 [==========>...................] - ETA: 26:33 - accuracy: 0.9115 - loss: 0.2275\n",
      " training set -> batch:838 loss:0.22692467272281647 and acc: 0.9104610085487366\n",
      " 838/2105 [==========>...................] - ETA: 26:31 - accuracy: 0.9105 - loss: 0.2269\n",
      " training set -> batch:839 loss:0.22363921999931335 and acc: 0.9112069010734558\n",
      " 839/2105 [==========>...................] - ETA: 26:28 - accuracy: 0.9112 - loss: 0.2236\n",
      " training set -> batch:840 loss:0.22395484149456024 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:840 val loss:0.24888621270656586 and val acc: 0.9025229215621948\n",
      " 840/2105 [==========>...................] - ETA: 26:38 - accuracy: 0.9111 - loss: 0.2240\n",
      " training set -> batch:841 loss:0.24724619090557098 and acc: 0.9026548862457275\n",
      " 841/2105 [==========>...................] - ETA: 26:35 - accuracy: 0.9027 - loss: 0.2472\n",
      " training set -> batch:842 loss:0.2362861931324005 and acc: 0.9059829115867615\n",
      " 842/2105 [===========>..................] - ETA: 26:33 - accuracy: 0.9060 - loss: 0.2363\n",
      " training set -> batch:843 loss:0.230703666806221 and acc: 0.9070248007774353\n",
      " 843/2105 [===========>..................] - ETA: 26:31 - accuracy: 0.9070 - loss: 0.2307\n",
      " training set -> batch:844 loss:0.2255125790834427 and acc: 0.9079999923706055\n",
      " 844/2105 [===========>..................] - ETA: 26:28 - accuracy: 0.9080 - loss: 0.2255\n",
      " training set -> batch:845 loss:0.22227250039577484 and acc: 0.9089147448539734\n",
      " 845/2105 [===========>..................] - ETA: 26:26 - accuracy: 0.9089 - loss: 0.2223\n",
      " training set -> batch:846 loss:0.21596968173980713 and acc: 0.9107142686843872\n",
      " 846/2105 [===========>..................] - ETA: 26:23 - accuracy: 0.9107 - loss: 0.2160\n",
      " training set -> batch:847 loss:0.21428677439689636 and acc: 0.9096715450286865\n",
      " 847/2105 [===========>..................] - ETA: 26:21 - accuracy: 0.9097 - loss: 0.2143\n",
      " training set -> batch:848 loss:0.21834464371204376 and acc: 0.9095744490623474\n",
      " 848/2105 [===========>..................] - ETA: 26:18 - accuracy: 0.9096 - loss: 0.2183\n",
      " training set -> batch:849 loss:0.212197408080101 and acc: 0.9112069010734558\n",
      " 849/2105 [===========>..................] - ETA: 26:16 - accuracy: 0.9112 - loss: 0.2122\n",
      " training set -> batch:850 loss:0.213522270321846 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:850 val loss:0.22840285301208496 and val acc: 0.9071100950241089\n",
      " 850/2105 [===========>..................] - ETA: 26:25 - accuracy: 0.9111 - loss: 0.2135\n",
      " training set -> batch:851 loss:0.22770164906978607 and acc: 0.9081858396530151\n",
      " 851/2105 [===========>..................] - ETA: 26:23 - accuracy: 0.9082 - loss: 0.2277\n",
      " training set -> batch:852 loss:0.2297690063714981 and acc: 0.9059829115867615\n",
      " 852/2105 [===========>..................] - ETA: 26:20 - accuracy: 0.9060 - loss: 0.2298\n",
      " training set -> batch:853 loss:0.23501423001289368 and acc: 0.9028925895690918\n",
      " 853/2105 [===========>..................] - ETA: 26:18 - accuracy: 0.9029 - loss: 0.2350\n",
      " training set -> batch:854 loss:0.2287064641714096 and acc: 0.9039999842643738\n",
      " 854/2105 [===========>..................] - ETA: 26:15 - accuracy: 0.9040 - loss: 0.2287\n",
      " training set -> batch:855 loss:0.22294510900974274 and acc: 0.9050387740135193\n",
      " 855/2105 [===========>..................] - ETA: 26:13 - accuracy: 0.9050 - loss: 0.2229\n",
      " training set -> batch:856 loss:0.21794147789478302 and acc: 0.9069548845291138\n",
      " 856/2105 [===========>..................] - ETA: 26:11 - accuracy: 0.9070 - loss: 0.2179\n",
      " training set -> batch:857 loss:0.22088922560214996 and acc: 0.9051094651222229\n",
      " 857/2105 [===========>..................] - ETA: 26:08 - accuracy: 0.9051 - loss: 0.2209\n",
      " training set -> batch:858 loss:0.21560654044151306 and acc: 0.9069148898124695\n",
      " 858/2105 [===========>..................] - ETA: 26:06 - accuracy: 0.9069 - loss: 0.2156\n",
      " training set -> batch:859 loss:0.21131594479084015 and acc: 0.9077585935592651\n",
      " 859/2105 [===========>..................] - ETA: 26:03 - accuracy: 0.9078 - loss: 0.2113\n",
      " training set -> batch:860 loss:0.21198014914989471 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:860 val loss:0.20071522891521454 and val acc: 0.9185779690742493\n",
      " 860/2105 [===========>..................] - ETA: 26:13 - accuracy: 0.9060 - loss: 0.2120\n",
      " training set -> batch:861 loss:0.20257797837257385 and acc: 0.9170354008674622\n",
      " 861/2105 [===========>..................] - ETA: 26:10 - accuracy: 0.9170 - loss: 0.2026\n",
      " training set -> batch:862 loss:0.22070062160491943 and acc: 0.9155982732772827\n",
      " 862/2105 [===========>..................] - ETA: 26:08 - accuracy: 0.9156 - loss: 0.2207\n",
      " training set -> batch:863 loss:0.21640348434448242 and acc: 0.9163222908973694\n",
      " 863/2105 [===========>..................] - ETA: 26:05 - accuracy: 0.9163 - loss: 0.2164\n",
      " training set -> batch:864 loss:0.20945584774017334 and acc: 0.9179999828338623\n",
      " 864/2105 [===========>..................] - ETA: 26:03 - accuracy: 0.9180 - loss: 0.2095\n",
      " training set -> batch:865 loss:0.20657625794410706 and acc: 0.9176356792449951\n",
      " 865/2105 [===========>..................] - ETA: 26:01 - accuracy: 0.9176 - loss: 0.2066\n",
      " training set -> batch:866 loss:0.21438917517662048 and acc: 0.9144737124443054\n",
      " 866/2105 [===========>..................] - ETA: 25:58 - accuracy: 0.9145 - loss: 0.2144\n",
      " training set -> batch:867 loss:0.20846569538116455 and acc: 0.9169707894325256\n",
      " 867/2105 [===========>..................] - ETA: 25:56 - accuracy: 0.9170 - loss: 0.2085\n",
      " training set -> batch:868 loss:0.21934747695922852 and acc: 0.9140070676803589\n",
      " 868/2105 [===========>..................] - ETA: 25:53 - accuracy: 0.9140 - loss: 0.2193\n",
      " training set -> batch:869 loss:0.21425876021385193 and acc: 0.9163793325424194\n",
      " 869/2105 [===========>..................] - ETA: 25:51 - accuracy: 0.9164 - loss: 0.2143\n",
      " training set -> batch:870 loss:0.2080824226140976 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:870 val loss:0.2376280277967453 and val acc: 0.9036697149276733\n",
      " 870/2105 [===========>..................] - ETA: 26:00 - accuracy: 0.9186 - loss: 0.2081\n",
      " training set -> batch:871 loss:0.2629553973674774 and acc: 0.8993362784385681\n",
      " 871/2105 [===========>..................] - ETA: 25:58 - accuracy: 0.8993 - loss: 0.2630\n",
      " training set -> batch:872 loss:0.2537592649459839 and acc: 0.9017093777656555\n",
      " 872/2105 [===========>..................] - ETA: 25:56 - accuracy: 0.9017 - loss: 0.2538\n",
      " training set -> batch:873 loss:0.24214452505111694 and acc: 0.9049586653709412\n",
      " 873/2105 [===========>..................] - ETA: 25:53 - accuracy: 0.9050 - loss: 0.2421\n",
      " training set -> batch:874 loss:0.24556207656860352 and acc: 0.9039999842643738\n",
      " 874/2105 [===========>..................] - ETA: 25:51 - accuracy: 0.9040 - loss: 0.2456\n",
      " training set -> batch:875 loss:0.23902948200702667 and acc: 0.9040697813034058\n",
      " 875/2105 [===========>..................] - ETA: 25:48 - accuracy: 0.9041 - loss: 0.2390\n",
      " training set -> batch:876 loss:0.23959457874298096 and acc: 0.905075192451477\n",
      " 876/2105 [===========>..................] - ETA: 25:46 - accuracy: 0.9051 - loss: 0.2396\n",
      " training set -> batch:877 loss:0.23474009335041046 and acc: 0.9060218930244446\n",
      " 877/2105 [===========>..................] - ETA: 25:44 - accuracy: 0.9060 - loss: 0.2347\n",
      " training set -> batch:878 loss:0.23594960570335388 and acc: 0.9060283899307251\n",
      " 878/2105 [===========>..................] - ETA: 25:41 - accuracy: 0.9060 - loss: 0.2359\n",
      " training set -> batch:879 loss:0.23106510937213898 and acc: 0.9068965315818787\n",
      " 879/2105 [===========>..................] - ETA: 25:39 - accuracy: 0.9069 - loss: 0.2311\n",
      " training set -> batch:880 loss:0.23050236701965332 and acc: 0.9060402512550354\n",
      "\n",
      " validation set -> batch:880 val loss:0.1959773600101471 and val acc: 0.9185779690742493\n",
      " 880/2105 [===========>..................] - ETA: 25:48 - accuracy: 0.9060 - loss: 0.2305\n",
      " training set -> batch:881 loss:0.20288509130477905 and acc: 0.9159291982650757\n",
      " 881/2105 [===========>..................] - ETA: 25:45 - accuracy: 0.9159 - loss: 0.2029\n",
      " training set -> batch:882 loss:0.20202317833900452 and acc: 0.9155982732772827\n",
      " 882/2105 [===========>..................] - ETA: 25:43 - accuracy: 0.9156 - loss: 0.2020\n",
      " training set -> batch:883 loss:0.20187392830848694 and acc: 0.9163222908973694\n",
      " 883/2105 [===========>..................] - ETA: 25:40 - accuracy: 0.9163 - loss: 0.2019\n",
      " training set -> batch:884 loss:0.20315776765346527 and acc: 0.9150000214576721\n",
      " 884/2105 [===========>..................] - ETA: 25:38 - accuracy: 0.9150 - loss: 0.2032\n",
      " training set -> batch:885 loss:0.20922096073627472 and acc: 0.913759708404541\n",
      " 885/2105 [===========>..................] - ETA: 25:36 - accuracy: 0.9138 - loss: 0.2092\n",
      " training set -> batch:886 loss:0.21151332557201385 and acc: 0.9125939607620239\n",
      " 886/2105 [===========>..................] - ETA: 25:33 - accuracy: 0.9126 - loss: 0.2115\n",
      " training set -> batch:887 loss:0.21521207690238953 and acc: 0.9124087691307068\n",
      " 887/2105 [===========>..................] - ETA: 25:31 - accuracy: 0.9124 - loss: 0.2152\n",
      " training set -> batch:888 loss:0.209747776389122 and acc: 0.9140070676803589\n",
      " 888/2105 [===========>..................] - ETA: 25:29 - accuracy: 0.9140 - loss: 0.2097\n",
      " training set -> batch:889 loss:0.209210604429245 and acc: 0.9146551489830017\n",
      " 889/2105 [===========>..................] - ETA: 25:26 - accuracy: 0.9147 - loss: 0.2092\n",
      " training set -> batch:890 loss:0.2097870260477066 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:890 val loss:0.20261956751346588 and val acc: 0.9151375889778137\n",
      " 890/2105 [===========>..................] - ETA: 25:35 - accuracy: 0.9136 - loss: 0.2098\n",
      " training set -> batch:891 loss:0.19456511735916138 and acc: 0.9170354008674622\n",
      " 891/2105 [===========>..................] - ETA: 25:32 - accuracy: 0.9170 - loss: 0.1946\n",
      " training set -> batch:892 loss:0.19182804226875305 and acc: 0.9177350401878357\n",
      " 892/2105 [===========>..................] - ETA: 25:30 - accuracy: 0.9177 - loss: 0.1918\n",
      " training set -> batch:893 loss:0.18798194825649261 and acc: 0.9183884263038635\n",
      " 893/2105 [===========>..................] - ETA: 25:28 - accuracy: 0.9184 - loss: 0.1880\n",
      " training set -> batch:894 loss:0.20503337681293488 and acc: 0.9150000214576721\n",
      " 894/2105 [===========>..................] - ETA: 25:25 - accuracy: 0.9150 - loss: 0.2050\n",
      " training set -> batch:895 loss:0.20446345210075378 and acc: 0.9147287011146545\n",
      " 895/2105 [===========>..................] - ETA: 25:23 - accuracy: 0.9147 - loss: 0.2045\n",
      " training set -> batch:896 loss:0.20292702317237854 and acc: 0.9135338068008423\n",
      " 896/2105 [===========>..................] - ETA: 25:21 - accuracy: 0.9135 - loss: 0.2029\n",
      " training set -> batch:897 loss:0.20226643979549408 and acc: 0.9133211970329285\n",
      " 897/2105 [===========>..................] - ETA: 25:18 - accuracy: 0.9133 - loss: 0.2023\n",
      " training set -> batch:898 loss:0.20257435739040375 and acc: 0.9140070676803589\n",
      " 898/2105 [===========>..................] - ETA: 25:16 - accuracy: 0.9140 - loss: 0.2026\n",
      " training set -> batch:899 loss:0.19757691025733948 and acc: 0.9146551489830017\n",
      " 899/2105 [===========>..................] - ETA: 25:14 - accuracy: 0.9147 - loss: 0.1976\n",
      " training set -> batch:900 loss:0.19488899409770966 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:900 val loss:0.22128057479858398 and val acc: 0.9071100950241089\n",
      " 900/2105 [===========>..................] - ETA: 25:22 - accuracy: 0.9161 - loss: 0.1949\n",
      " training set -> batch:901 loss:0.2091483473777771 and acc: 0.9103982448577881\n",
      " 901/2105 [===========>..................] - ETA: 25:20 - accuracy: 0.9104 - loss: 0.2091\n",
      " training set -> batch:902 loss:0.20560751855373383 and acc: 0.9091880321502686\n",
      " 902/2105 [===========>..................] - ETA: 25:17 - accuracy: 0.9092 - loss: 0.2056\n",
      " training set -> batch:903 loss:0.21406671404838562 and acc: 0.9080578684806824\n",
      " 903/2105 [===========>..................] - ETA: 25:15 - accuracy: 0.9081 - loss: 0.2141\n",
      " training set -> batch:904 loss:0.21085631847381592 and acc: 0.9079999923706055\n",
      " 904/2105 [===========>..................] - ETA: 25:13 - accuracy: 0.9080 - loss: 0.2109\n",
      " training set -> batch:905 loss:0.2092091590166092 and acc: 0.9089147448539734\n",
      " 905/2105 [===========>..................] - ETA: 25:10 - accuracy: 0.9089 - loss: 0.2092\n",
      " training set -> batch:906 loss:0.2146216630935669 and acc: 0.9078947305679321\n",
      " 906/2105 [===========>..................] - ETA: 25:08 - accuracy: 0.9079 - loss: 0.2146\n",
      " training set -> batch:907 loss:0.2200658917427063 and acc: 0.9078466892242432\n",
      " 907/2105 [===========>..................] - ETA: 25:06 - accuracy: 0.9078 - loss: 0.2201\n",
      " training set -> batch:908 loss:0.21599659323692322 and acc: 0.9095744490623474\n",
      " 908/2105 [===========>..................] - ETA: 25:04 - accuracy: 0.9096 - loss: 0.2160\n",
      " training set -> batch:909 loss:0.21053294837474823 and acc: 0.9112069010734558\n",
      " 909/2105 [===========>..................] - ETA: 25:01 - accuracy: 0.9112 - loss: 0.2105\n",
      " training set -> batch:910 loss:0.20483213663101196 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:910 val loss:0.20232565701007843 and val acc: 0.9162843823432922\n",
      " 910/2105 [===========>..................] - ETA: 25:09 - accuracy: 0.9128 - loss: 0.2048\n",
      " training set -> batch:911 loss:0.1939122974872589 and acc: 0.9181416034698486\n",
      " 911/2105 [===========>..................] - ETA: 25:07 - accuracy: 0.9181 - loss: 0.1939\n",
      " training set -> batch:912 loss:0.19237865507602692 and acc: 0.9188033938407898\n",
      " 912/2105 [===========>..................] - ETA: 25:05 - accuracy: 0.9188 - loss: 0.1924\n",
      " training set -> batch:913 loss:0.18992604315280914 and acc: 0.9183884263038635\n",
      " 913/2105 [============>.................] - ETA: 25:02 - accuracy: 0.9184 - loss: 0.1899\n",
      " training set -> batch:914 loss:0.19270330667495728 and acc: 0.9190000295639038\n",
      " 914/2105 [============>.................] - ETA: 25:00 - accuracy: 0.9190 - loss: 0.1927\n",
      " training set -> batch:915 loss:0.1900051385164261 and acc: 0.9205426573753357\n",
      " 915/2105 [============>.................] - ETA: 24:58 - accuracy: 0.9205 - loss: 0.1900\n",
      " training set -> batch:916 loss:0.18215228617191315 and acc: 0.9229323267936707\n",
      " 916/2105 [============>.................] - ETA: 24:55 - accuracy: 0.9229 - loss: 0.1822\n",
      " training set -> batch:917 loss:0.1759837418794632 and acc: 0.9251824617385864\n",
      " 917/2105 [============>.................] - ETA: 24:53 - accuracy: 0.9252 - loss: 0.1760\n",
      " training set -> batch:918 loss:0.18031150102615356 and acc: 0.9228723645210266\n",
      " 918/2105 [============>.................] - ETA: 24:51 - accuracy: 0.9229 - loss: 0.1803\n",
      " training set -> batch:919 loss:0.18132968246936798 and acc: 0.9224137663841248\n",
      " 919/2105 [============>.................] - ETA: 24:49 - accuracy: 0.9224 - loss: 0.1813\n",
      " training set -> batch:920 loss:0.18172214925289154 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:920 val loss:0.23459896445274353 and val acc: 0.9059633016586304\n",
      " 920/2105 [============>.................] - ETA: 24:57 - accuracy: 0.9228 - loss: 0.1817\n",
      " training set -> batch:921 loss:0.22769808769226074 and acc: 0.9070796370506287\n",
      " 921/2105 [============>.................] - ETA: 24:54 - accuracy: 0.9071 - loss: 0.2277\n",
      " training set -> batch:922 loss:0.2224855124950409 and acc: 0.9091880321502686\n",
      " 922/2105 [============>.................] - ETA: 24:52 - accuracy: 0.9092 - loss: 0.2225\n",
      " training set -> batch:923 loss:0.2176254242658615 and acc: 0.9101239442825317\n",
      " 923/2105 [============>.................] - ETA: 24:50 - accuracy: 0.9101 - loss: 0.2176\n",
      " training set -> batch:924 loss:0.223213329911232 and acc: 0.9100000262260437\n",
      " 924/2105 [============>.................] - ETA: 24:48 - accuracy: 0.9100 - loss: 0.2232\n",
      " training set -> batch:925 loss:0.21746496856212616 and acc: 0.911821722984314\n",
      " 925/2105 [============>.................] - ETA: 24:45 - accuracy: 0.9118 - loss: 0.2175\n",
      " training set -> batch:926 loss:0.2157786637544632 and acc: 0.9116541147232056\n",
      " 926/2105 [============>.................] - ETA: 24:43 - accuracy: 0.9117 - loss: 0.2158\n",
      " training set -> batch:927 loss:0.21035566926002502 and acc: 0.9133211970329285\n",
      " 927/2105 [============>.................] - ETA: 24:41 - accuracy: 0.9133 - loss: 0.2104\n",
      " training set -> batch:928 loss:0.20226500928401947 and acc: 0.9157801270484924\n",
      " 928/2105 [============>.................] - ETA: 24:38 - accuracy: 0.9158 - loss: 0.2023\n",
      " training set -> batch:929 loss:0.19848467409610748 and acc: 0.9163793325424194\n",
      " 929/2105 [============>.................] - ETA: 24:36 - accuracy: 0.9164 - loss: 0.1985\n",
      " training set -> batch:930 loss:0.20353029668331146 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:930 val loss:0.24911527335643768 and val acc: 0.9071100950241089\n",
      " 930/2105 [============>.................] - ETA: 24:44 - accuracy: 0.9153 - loss: 0.2035\n",
      " training set -> batch:931 loss:0.25255128741264343 and acc: 0.9081858396530151\n",
      " 931/2105 [============>.................] - ETA: 24:42 - accuracy: 0.9082 - loss: 0.2526\n",
      " training set -> batch:932 loss:0.24427837133407593 and acc: 0.9102563858032227\n",
      " 932/2105 [============>.................] - ETA: 24:39 - accuracy: 0.9103 - loss: 0.2443\n",
      " training set -> batch:933 loss:0.2510192394256592 and acc: 0.9111570119857788\n",
      " 933/2105 [============>.................] - ETA: 24:37 - accuracy: 0.9112 - loss: 0.2510\n",
      " training set -> batch:934 loss:0.2560718357563019 and acc: 0.9110000133514404\n",
      " 934/2105 [============>.................] - ETA: 24:35 - accuracy: 0.9110 - loss: 0.2561\n",
      " training set -> batch:935 loss:0.2442149817943573 and acc: 0.913759708404541\n",
      " 935/2105 [============>.................] - ETA: 24:33 - accuracy: 0.9138 - loss: 0.2442\n",
      " training set -> batch:936 loss:0.2431027591228485 and acc: 0.9135338068008423\n",
      " 936/2105 [============>.................] - ETA: 24:30 - accuracy: 0.9135 - loss: 0.2431\n",
      " training set -> batch:937 loss:0.2523697018623352 and acc: 0.9087591171264648\n",
      " 937/2105 [============>.................] - ETA: 24:28 - accuracy: 0.9088 - loss: 0.2524\n",
      " training set -> batch:938 loss:0.24510632455348969 and acc: 0.9104610085487366\n",
      " 938/2105 [============>.................] - ETA: 24:26 - accuracy: 0.9105 - loss: 0.2451\n",
      " training set -> batch:939 loss:0.2386629730463028 and acc: 0.9112069010734558\n",
      " 939/2105 [============>.................] - ETA: 24:24 - accuracy: 0.9112 - loss: 0.2387\n",
      " training set -> batch:940 loss:0.24305683374404907 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:940 val loss:0.21594156324863434 and val acc: 0.911697268486023\n",
      " 940/2105 [============>.................] - ETA: 24:31 - accuracy: 0.9102 - loss: 0.2431\n",
      " training set -> batch:941 loss:0.22410735487937927 and acc: 0.9103982448577881\n",
      " 941/2105 [============>.................] - ETA: 24:29 - accuracy: 0.9104 - loss: 0.2241\n",
      " training set -> batch:942 loss:0.21335752308368683 and acc: 0.9134615659713745\n",
      " 942/2105 [============>.................] - ETA: 24:27 - accuracy: 0.9135 - loss: 0.2134\n",
      " training set -> batch:943 loss:0.22230835258960724 and acc: 0.9111570119857788\n",
      " 943/2105 [============>.................] - ETA: 24:24 - accuracy: 0.9112 - loss: 0.2223\n",
      " training set -> batch:944 loss:0.23187120258808136 and acc: 0.9079999923706055\n",
      " 944/2105 [============>.................] - ETA: 24:22 - accuracy: 0.9080 - loss: 0.2319\n",
      " training set -> batch:945 loss:0.2350401133298874 and acc: 0.9060077667236328\n",
      " 945/2105 [============>.................] - ETA: 24:20 - accuracy: 0.9060 - loss: 0.2350\n",
      " training set -> batch:946 loss:0.23681548237800598 and acc: 0.9069548845291138\n",
      " 946/2105 [============>.................] - ETA: 24:18 - accuracy: 0.9070 - loss: 0.2368\n",
      " training set -> batch:947 loss:0.2366422861814499 and acc: 0.9060218930244446\n",
      " 947/2105 [============>.................] - ETA: 24:16 - accuracy: 0.9060 - loss: 0.2366\n",
      " training set -> batch:948 loss:0.24255557358264923 and acc: 0.9051418304443359\n",
      " 948/2105 [============>.................] - ETA: 24:13 - accuracy: 0.9051 - loss: 0.2426\n",
      " training set -> batch:949 loss:0.23646894097328186 and acc: 0.9077585935592651\n",
      " 949/2105 [============>.................] - ETA: 24:11 - accuracy: 0.9078 - loss: 0.2365\n",
      " training set -> batch:950 loss:0.23542071878910065 and acc: 0.9085570573806763\n",
      "\n",
      " validation set -> batch:950 val loss:0.23052041232585907 and val acc: 0.9059633016586304\n",
      " 950/2105 [============>.................] - ETA: 24:18 - accuracy: 0.9086 - loss: 0.2354\n",
      " training set -> batch:951 loss:0.2232629507780075 and acc: 0.9081858396530151\n",
      " 951/2105 [============>.................] - ETA: 24:16 - accuracy: 0.9082 - loss: 0.2233\n",
      " training set -> batch:952 loss:0.22517737746238708 and acc: 0.9091880321502686\n",
      " 952/2105 [============>.................] - ETA: 24:14 - accuracy: 0.9092 - loss: 0.2252\n",
      " training set -> batch:953 loss:0.2199697047472 and acc: 0.9101239442825317\n",
      " 953/2105 [============>.................] - ETA: 24:12 - accuracy: 0.9101 - loss: 0.2200\n",
      " training set -> batch:954 loss:0.2147851586341858 and acc: 0.9110000133514404\n",
      " 954/2105 [============>.................] - ETA: 24:10 - accuracy: 0.9110 - loss: 0.2148\n",
      " training set -> batch:955 loss:0.21095559000968933 and acc: 0.911821722984314\n",
      " 955/2105 [============>.................] - ETA: 24:07 - accuracy: 0.9118 - loss: 0.2110\n",
      " training set -> batch:956 loss:0.21203026175498962 and acc: 0.9116541147232056\n",
      " 956/2105 [============>.................] - ETA: 24:05 - accuracy: 0.9117 - loss: 0.2120\n",
      " training set -> batch:957 loss:0.20541343092918396 and acc: 0.9142335653305054\n",
      " 957/2105 [============>.................] - ETA: 24:03 - accuracy: 0.9142 - loss: 0.2054\n",
      " training set -> batch:958 loss:0.20313391089439392 and acc: 0.914893627166748\n",
      " 958/2105 [============>.................] - ETA: 24:01 - accuracy: 0.9149 - loss: 0.2031\n",
      " training set -> batch:959 loss:0.20230980217456818 and acc: 0.915517270565033\n",
      " 959/2105 [============>.................] - ETA: 23:59 - accuracy: 0.9155 - loss: 0.2023\n",
      " training set -> batch:960 loss:0.20034533739089966 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:960 val loss:0.22257889807224274 and val acc: 0.9082568883895874\n",
      " 960/2105 [============>.................] - ETA: 24:06 - accuracy: 0.9169 - loss: 0.2003\n",
      " training set -> batch:961 loss:0.2202191799879074 and acc: 0.9081858396530151\n",
      " 961/2105 [============>.................] - ETA: 24:04 - accuracy: 0.9082 - loss: 0.2202\n",
      " training set -> batch:962 loss:0.2244541496038437 and acc: 0.9070512652397156\n",
      " 962/2105 [============>.................] - ETA: 24:01 - accuracy: 0.9071 - loss: 0.2245\n",
      " training set -> batch:963 loss:0.22908727824687958 and acc: 0.9059917330741882\n",
      " 963/2105 [============>.................] - ETA: 23:59 - accuracy: 0.9060 - loss: 0.2291\n",
      " training set -> batch:964 loss:0.22604727745056152 and acc: 0.9070000052452087\n",
      " 964/2105 [============>.................] - ETA: 23:57 - accuracy: 0.9070 - loss: 0.2260\n",
      " training set -> batch:965 loss:0.22297929227352142 and acc: 0.9069767594337463\n",
      " 965/2105 [============>.................] - ETA: 23:55 - accuracy: 0.9070 - loss: 0.2230\n",
      " training set -> batch:966 loss:0.23105235397815704 and acc: 0.9069548845291138\n",
      " 966/2105 [============>.................] - ETA: 23:53 - accuracy: 0.9070 - loss: 0.2311\n",
      " training set -> batch:967 loss:0.22247979044914246 and acc: 0.9096715450286865\n",
      " 967/2105 [============>.................] - ETA: 23:50 - accuracy: 0.9097 - loss: 0.2225\n",
      " training set -> batch:968 loss:0.22332152724266052 and acc: 0.908687949180603\n",
      " 968/2105 [============>.................] - ETA: 23:48 - accuracy: 0.9087 - loss: 0.2233\n",
      " training set -> batch:969 loss:0.21809139847755432 and acc: 0.9094827771186829\n",
      " 969/2105 [============>.................] - ETA: 23:46 - accuracy: 0.9095 - loss: 0.2181\n",
      " training set -> batch:970 loss:0.2147885411977768 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:970 val loss:0.2540903389453888 and val acc: 0.8979358077049255\n",
      " 970/2105 [============>.................] - ETA: 23:53 - accuracy: 0.9111 - loss: 0.2148\n",
      " training set -> batch:971 loss:0.25740888714790344 and acc: 0.8960176706314087\n",
      " 971/2105 [============>.................] - ETA: 23:51 - accuracy: 0.8960 - loss: 0.2574\n",
      " training set -> batch:972 loss:0.2579646408557892 and acc: 0.8974359035491943\n",
      " 972/2105 [============>.................] - ETA: 23:49 - accuracy: 0.8974 - loss: 0.2580\n",
      " training set -> batch:973 loss:0.24687111377716064 and acc: 0.9008264541625977\n",
      " 973/2105 [============>.................] - ETA: 23:46 - accuracy: 0.9008 - loss: 0.2469\n",
      " training set -> batch:974 loss:0.24119040369987488 and acc: 0.902999997138977\n",
      " 974/2105 [============>.................] - ETA: 23:44 - accuracy: 0.9030 - loss: 0.2412\n",
      " training set -> batch:975 loss:0.24433831870555878 and acc: 0.9021317958831787\n",
      " 975/2105 [============>.................] - ETA: 23:42 - accuracy: 0.9021 - loss: 0.2443\n",
      " training set -> batch:976 loss:0.23538848757743835 and acc: 0.9041353464126587\n",
      " 976/2105 [============>.................] - ETA: 23:40 - accuracy: 0.9041 - loss: 0.2354\n",
      " training set -> batch:977 loss:0.24113576114177704 and acc: 0.9023722410202026\n",
      " 977/2105 [============>.................] - ETA: 23:38 - accuracy: 0.9024 - loss: 0.2411\n",
      " training set -> batch:978 loss:0.2342553287744522 and acc: 0.9051418304443359\n",
      " 978/2105 [============>.................] - ETA: 23:36 - accuracy: 0.9051 - loss: 0.2343\n",
      " training set -> batch:979 loss:0.23517654836177826 and acc: 0.9051724076271057\n",
      " 979/2105 [============>.................] - ETA: 23:33 - accuracy: 0.9052 - loss: 0.2352\n",
      " training set -> batch:980 loss:0.23560142517089844 and acc: 0.9052013158798218\n",
      "\n",
      " validation set -> batch:980 val loss:0.2177942544221878 and val acc: 0.9105504751205444\n",
      " 980/2105 [============>.................] - ETA: 23:40 - accuracy: 0.9052 - loss: 0.2356\n",
      " training set -> batch:981 loss:0.21566079556941986 and acc: 0.9103982448577881\n",
      " 981/2105 [============>.................] - ETA: 23:38 - accuracy: 0.9104 - loss: 0.2157\n",
      " training set -> batch:982 loss:0.20895053446292877 and acc: 0.9113247990608215\n",
      " 982/2105 [============>.................] - ETA: 23:36 - accuracy: 0.9113 - loss: 0.2090\n",
      " training set -> batch:983 loss:0.20044048130512238 and acc: 0.91425621509552\n",
      " 983/2105 [=============>................] - ETA: 23:34 - accuracy: 0.9143 - loss: 0.2004\n",
      " training set -> batch:984 loss:0.19872087240219116 and acc: 0.9139999747276306\n",
      " 984/2105 [=============>................] - ETA: 23:32 - accuracy: 0.9140 - loss: 0.1987\n",
      " training set -> batch:985 loss:0.19404946267604828 and acc: 0.9156976938247681\n",
      " 985/2105 [=============>................] - ETA: 23:29 - accuracy: 0.9157 - loss: 0.1940\n",
      " training set -> batch:986 loss:0.20102481544017792 and acc: 0.9135338068008423\n",
      " 986/2105 [=============>................] - ETA: 23:27 - accuracy: 0.9135 - loss: 0.2010\n",
      " training set -> batch:987 loss:0.19818279147148132 and acc: 0.915145993232727\n",
      " 987/2105 [=============>................] - ETA: 23:25 - accuracy: 0.9151 - loss: 0.1982\n",
      " training set -> batch:988 loss:0.19574379920959473 and acc: 0.9166666865348816\n",
      " 988/2105 [=============>................] - ETA: 23:23 - accuracy: 0.9167 - loss: 0.1957\n",
      " training set -> batch:989 loss:0.19669999182224274 and acc: 0.9146551489830017\n",
      " 989/2105 [=============>................] - ETA: 23:21 - accuracy: 0.9147 - loss: 0.1967\n",
      " training set -> batch:990 loss:0.1941169947385788 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:990 val loss:0.2550186514854431 and val acc: 0.8979358077049255\n",
      " 990/2105 [=============>................] - ETA: 23:30 - accuracy: 0.9153 - loss: 0.1941\n",
      " training set -> batch:991 loss:0.25565433502197266 and acc: 0.8971238732337952\n",
      " 991/2105 [=============>................] - ETA: 23:28 - accuracy: 0.8971 - loss: 0.2557\n",
      " training set -> batch:992 loss:0.24590030312538147 and acc: 0.8995726704597473\n",
      " 992/2105 [=============>................] - ETA: 23:25 - accuracy: 0.8996 - loss: 0.2459\n",
      " training set -> batch:993 loss:0.2374463975429535 and acc: 0.9008264541625977\n",
      " 993/2105 [=============>................] - ETA: 23:23 - accuracy: 0.9008 - loss: 0.2374\n",
      " training set -> batch:994 loss:0.23941098153591156 and acc: 0.9010000228881836\n",
      " 994/2105 [=============>................] - ETA: 23:21 - accuracy: 0.9010 - loss: 0.2394\n",
      " training set -> batch:995 loss:0.23509101569652557 and acc: 0.9021317958831787\n",
      " 995/2105 [=============>................] - ETA: 23:19 - accuracy: 0.9021 - loss: 0.2351\n",
      " training set -> batch:996 loss:0.2343064546585083 and acc: 0.9031955003738403\n",
      " 996/2105 [=============>................] - ETA: 23:17 - accuracy: 0.9032 - loss: 0.2343\n",
      " training set -> batch:997 loss:0.2321651130914688 and acc: 0.9032846689224243\n",
      " 997/2105 [=============>................] - ETA: 23:15 - accuracy: 0.9033 - loss: 0.2322\n",
      " training set -> batch:998 loss:0.22815531492233276 and acc: 0.9051418304443359\n",
      " 998/2105 [=============>................] - ETA: 23:12 - accuracy: 0.9051 - loss: 0.2282\n",
      " training set -> batch:999 loss:0.2285333275794983 and acc: 0.9060344696044922\n",
      " 999/2105 [=============>................] - ETA: 23:10 - accuracy: 0.9060 - loss: 0.2285\n",
      " training set -> batch:1000 loss:0.22139686346054077 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:1000 val loss:0.23113787174224854 and val acc: 0.91399085521698\n",
      "1000/2105 [=============>................] - ETA: 23:17 - accuracy: 0.9077 - loss: 0.2214\n",
      " training set -> batch:1001 loss:0.2333618849515915 and acc: 0.9126105904579163\n",
      "1001/2105 [=============>................] - ETA: 23:15 - accuracy: 0.9126 - loss: 0.2334\n",
      " training set -> batch:1002 loss:0.2237531840801239 and acc: 0.9145299196243286\n",
      "1002/2105 [=============>................] - ETA: 23:13 - accuracy: 0.9145 - loss: 0.2238\n",
      " training set -> batch:1003 loss:0.22630393505096436 and acc: 0.913223147392273\n",
      "1003/2105 [=============>................] - ETA: 23:10 - accuracy: 0.9132 - loss: 0.2263\n",
      " training set -> batch:1004 loss:0.2355659306049347 and acc: 0.9120000004768372\n",
      "1004/2105 [=============>................] - ETA: 23:08 - accuracy: 0.9120 - loss: 0.2356\n",
      " training set -> batch:1005 loss:0.23732160031795502 and acc: 0.9108527302742004\n",
      "1005/2105 [=============>................] - ETA: 23:06 - accuracy: 0.9109 - loss: 0.2373\n",
      " training set -> batch:1006 loss:0.22722527384757996 and acc: 0.9135338068008423\n",
      "1006/2105 [=============>................] - ETA: 23:04 - accuracy: 0.9135 - loss: 0.2272\n",
      " training set -> batch:1007 loss:0.22342804074287415 and acc: 0.915145993232727\n",
      "1007/2105 [=============>................] - ETA: 23:02 - accuracy: 0.9151 - loss: 0.2234\n",
      " training set -> batch:1008 loss:0.21961082518100739 and acc: 0.9166666865348816\n",
      "1008/2105 [=============>................] - ETA: 23:00 - accuracy: 0.9167 - loss: 0.2196\n",
      " training set -> batch:1009 loss:0.23349590599536896 and acc: 0.9163793325424194\n",
      "1009/2105 [=============>................] - ETA: 22:58 - accuracy: 0.9164 - loss: 0.2335\n",
      " training set -> batch:1010 loss:0.23000741004943848 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1010 val loss:0.26691383123397827 and val acc: 0.8853210806846619\n",
      "1010/2105 [=============>................] - ETA: 23:04 - accuracy: 0.9178 - loss: 0.2300\n",
      " training set -> batch:1011 loss:0.2560606300830841 and acc: 0.8882743120193481\n",
      "1011/2105 [=============>................] - ETA: 23:02 - accuracy: 0.8883 - loss: 0.2561\n",
      " training set -> batch:1012 loss:0.2560000717639923 and acc: 0.8878205418586731\n",
      "1012/2105 [=============>................] - ETA: 23:00 - accuracy: 0.8878 - loss: 0.2560\n",
      " training set -> batch:1013 loss:0.25264763832092285 and acc: 0.8904958963394165\n",
      "1013/2105 [=============>................] - ETA: 22:58 - accuracy: 0.8905 - loss: 0.2526\n",
      " training set -> batch:1014 loss:0.256339430809021 and acc: 0.8889999985694885\n",
      "1014/2105 [=============>................] - ETA: 22:56 - accuracy: 0.8890 - loss: 0.2563\n",
      " training set -> batch:1015 loss:0.25174278020858765 and acc: 0.8895348906517029\n",
      "1015/2105 [=============>................] - ETA: 22:54 - accuracy: 0.8895 - loss: 0.2517\n",
      " training set -> batch:1016 loss:0.24487176537513733 and acc: 0.8919172883033752\n",
      "1016/2105 [=============>................] - ETA: 22:51 - accuracy: 0.8919 - loss: 0.2449\n",
      " training set -> batch:1017 loss:0.2364133596420288 and acc: 0.8950729966163635\n",
      "1017/2105 [=============>................] - ETA: 22:49 - accuracy: 0.8951 - loss: 0.2364\n",
      " training set -> batch:1018 loss:0.23168177902698517 and acc: 0.896276593208313\n",
      "1018/2105 [=============>................] - ETA: 22:47 - accuracy: 0.8963 - loss: 0.2317\n",
      " training set -> batch:1019 loss:0.22440995275974274 and acc: 0.8982758522033691\n",
      "1019/2105 [=============>................] - ETA: 22:45 - accuracy: 0.8983 - loss: 0.2244\n",
      " training set -> batch:1020 loss:0.2198239415884018 and acc: 0.9001677632331848\n",
      "\n",
      " validation set -> batch:1020 val loss:0.22635717689990997 and val acc: 0.9082568883895874\n",
      "1020/2105 [=============>................] - ETA: 22:51 - accuracy: 0.9002 - loss: 0.2198\n",
      " training set -> batch:1021 loss:0.22003936767578125 and acc: 0.9103982448577881\n",
      "1021/2105 [=============>................] - ETA: 22:49 - accuracy: 0.9104 - loss: 0.2200\n",
      " training set -> batch:1022 loss:0.22024878859519958 and acc: 0.9102563858032227\n",
      "1022/2105 [=============>................] - ETA: 22:47 - accuracy: 0.9103 - loss: 0.2202\n",
      " training set -> batch:1023 loss:0.21989163756370544 and acc: 0.9111570119857788\n",
      "1023/2105 [=============>................] - ETA: 22:45 - accuracy: 0.9112 - loss: 0.2199\n",
      " training set -> batch:1024 loss:0.21615402400493622 and acc: 0.9120000004768372\n",
      "1024/2105 [=============>................] - ETA: 22:43 - accuracy: 0.9120 - loss: 0.2162\n",
      " training set -> batch:1025 loss:0.2124420702457428 and acc: 0.9127907156944275\n",
      "1025/2105 [=============>................] - ETA: 22:41 - accuracy: 0.9128 - loss: 0.2124\n",
      " training set -> batch:1026 loss:0.2172451913356781 and acc: 0.9107142686843872\n",
      "1026/2105 [=============>................] - ETA: 22:39 - accuracy: 0.9107 - loss: 0.2172\n",
      " training set -> batch:1027 loss:0.2108466923236847 and acc: 0.9133211970329285\n",
      "1027/2105 [=============>................] - ETA: 22:37 - accuracy: 0.9133 - loss: 0.2108\n",
      " training set -> batch:1028 loss:0.2093448042869568 and acc: 0.9140070676803589\n",
      "1028/2105 [=============>................] - ETA: 22:35 - accuracy: 0.9140 - loss: 0.2093\n",
      " training set -> batch:1029 loss:0.20651525259017944 and acc: 0.9137930870056152\n",
      "1029/2105 [=============>................] - ETA: 22:33 - accuracy: 0.9138 - loss: 0.2065\n",
      " training set -> batch:1030 loss:0.2054702490568161 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:1030 val loss:0.2594808042049408 and val acc: 0.896789014339447\n",
      "1030/2105 [=============>................] - ETA: 22:39 - accuracy: 0.9136 - loss: 0.2055\n",
      " training set -> batch:1031 loss:0.2631951570510864 and acc: 0.8971238732337952\n",
      "1031/2105 [=============>................] - ETA: 22:37 - accuracy: 0.8971 - loss: 0.2632\n",
      " training set -> batch:1032 loss:0.2535247504711151 and acc: 0.9006410241127014\n",
      "1032/2105 [=============>................] - ETA: 22:35 - accuracy: 0.9006 - loss: 0.2535\n",
      " training set -> batch:1033 loss:0.25927260518074036 and acc: 0.9018595218658447\n",
      "1033/2105 [=============>................] - ETA: 22:32 - accuracy: 0.9019 - loss: 0.2593\n",
      " training set -> batch:1034 loss:0.24728897213935852 and acc: 0.9049999713897705\n",
      "1034/2105 [=============>................] - ETA: 22:30 - accuracy: 0.9050 - loss: 0.2473\n",
      " training set -> batch:1035 loss:0.24097171425819397 and acc: 0.9069767594337463\n",
      "1035/2105 [=============>................] - ETA: 22:28 - accuracy: 0.9070 - loss: 0.2410\n",
      " training set -> batch:1036 loss:0.23886103928089142 and acc: 0.9078947305679321\n",
      "1036/2105 [=============>................] - ETA: 22:26 - accuracy: 0.9079 - loss: 0.2389\n",
      " training set -> batch:1037 loss:0.23288951814174652 and acc: 0.9087591171264648\n",
      "1037/2105 [=============>................] - ETA: 22:24 - accuracy: 0.9088 - loss: 0.2329\n",
      " training set -> batch:1038 loss:0.24062897264957428 and acc: 0.908687949180603\n",
      "1038/2105 [=============>................] - ETA: 22:22 - accuracy: 0.9087 - loss: 0.2406\n",
      " training set -> batch:1039 loss:0.2400689274072647 and acc: 0.9094827771186829\n",
      "1039/2105 [=============>................] - ETA: 22:20 - accuracy: 0.9095 - loss: 0.2401\n",
      " training set -> batch:1040 loss:0.24021202325820923 and acc: 0.9093959927558899\n",
      "\n",
      " validation set -> batch:1040 val loss:0.21692423522472382 and val acc: 0.9220183491706848\n",
      "1040/2105 [=============>................] - ETA: 22:26 - accuracy: 0.9094 - loss: 0.2402\n",
      " training set -> batch:1041 loss:0.21692506968975067 and acc: 0.9203540086746216\n",
      "1041/2105 [=============>................] - ETA: 22:24 - accuracy: 0.9204 - loss: 0.2169\n",
      " training set -> batch:1042 loss:0.2097964584827423 and acc: 0.9220085740089417\n",
      "1042/2105 [=============>................] - ETA: 22:22 - accuracy: 0.9220 - loss: 0.2098\n",
      " training set -> batch:1043 loss:0.20111748576164246 and acc: 0.9245867729187012\n",
      "1043/2105 [=============>................] - ETA: 22:20 - accuracy: 0.9246 - loss: 0.2011\n",
      " training set -> batch:1044 loss:0.19907765090465546 and acc: 0.9240000247955322\n",
      "1044/2105 [=============>................] - ETA: 22:18 - accuracy: 0.9240 - loss: 0.1991\n",
      " training set -> batch:1045 loss:0.2025919109582901 and acc: 0.9234496355056763\n",
      "1045/2105 [=============>................] - ETA: 22:16 - accuracy: 0.9234 - loss: 0.2026\n",
      " training set -> batch:1046 loss:0.20098987221717834 and acc: 0.9229323267936707\n",
      "1046/2105 [=============>................] - ETA: 22:14 - accuracy: 0.9229 - loss: 0.2010\n",
      " training set -> batch:1047 loss:0.19562460482120514 and acc: 0.9242700934410095\n",
      "1047/2105 [=============>................] - ETA: 22:11 - accuracy: 0.9243 - loss: 0.1956\n",
      " training set -> batch:1048 loss:0.1924266219139099 and acc: 0.9255319237709045\n",
      "1048/2105 [=============>................] - ETA: 22:09 - accuracy: 0.9255 - loss: 0.1924\n",
      " training set -> batch:1049 loss:0.19275729358196259 and acc: 0.9258620738983154\n",
      "1049/2105 [=============>................] - ETA: 22:07 - accuracy: 0.9259 - loss: 0.1928\n",
      " training set -> batch:1050 loss:0.19176225364208221 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1050 val loss:0.21939325332641602 and val acc: 0.9151375889778137\n",
      "1050/2105 [=============>................] - ETA: 22:13 - accuracy: 0.9270 - loss: 0.1918\n",
      " training set -> batch:1051 loss:0.20700551569461823 and acc: 0.9181416034698486\n",
      "1051/2105 [=============>................] - ETA: 22:11 - accuracy: 0.9181 - loss: 0.2070\n",
      " training set -> batch:1052 loss:0.19684971868991852 and acc: 0.9209401607513428\n",
      "1052/2105 [=============>................] - ETA: 22:09 - accuracy: 0.9209 - loss: 0.1968\n",
      " training set -> batch:1053 loss:0.20341292023658752 and acc: 0.9204545617103577\n",
      "1053/2105 [==============>...............] - ETA: 22:07 - accuracy: 0.9205 - loss: 0.2034\n",
      " training set -> batch:1054 loss:0.2043333202600479 and acc: 0.9200000166893005\n",
      "1054/2105 [==============>...............] - ETA: 22:05 - accuracy: 0.9200 - loss: 0.2043\n",
      " training set -> batch:1055 loss:0.20987729728221893 and acc: 0.9195736646652222\n",
      "1055/2105 [==============>...............] - ETA: 22:03 - accuracy: 0.9196 - loss: 0.2099\n",
      " training set -> batch:1056 loss:0.2242896556854248 and acc: 0.9163534045219421\n",
      "1056/2105 [==============>...............] - ETA: 22:01 - accuracy: 0.9164 - loss: 0.2243\n",
      " training set -> batch:1057 loss:0.21773181855678558 and acc: 0.9178832173347473\n",
      "1057/2105 [==============>...............] - ETA: 21:59 - accuracy: 0.9179 - loss: 0.2177\n",
      " training set -> batch:1058 loss:0.2125309407711029 and acc: 0.9193262457847595\n",
      "1058/2105 [==============>...............] - ETA: 21:57 - accuracy: 0.9193 - loss: 0.2125\n",
      " training set -> batch:1059 loss:0.21388889849185944 and acc: 0.9189655184745789\n",
      "1059/2105 [==============>...............] - ETA: 21:55 - accuracy: 0.9190 - loss: 0.2139\n",
      " training set -> batch:1060 loss:0.21203909814357758 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1060 val loss:0.22538819909095764 and val acc: 0.9128440618515015\n",
      "1060/2105 [==============>...............] - ETA: 22:00 - accuracy: 0.9186 - loss: 0.2120\n",
      " training set -> batch:1061 loss:0.2256975919008255 and acc: 0.9115044474601746\n",
      "1061/2105 [==============>...............] - ETA: 21:58 - accuracy: 0.9115 - loss: 0.2257\n",
      " training set -> batch:1062 loss:0.2331213653087616 and acc: 0.9113247990608215\n",
      "1062/2105 [==============>...............] - ETA: 21:56 - accuracy: 0.9113 - loss: 0.2331\n",
      " training set -> batch:1063 loss:0.22420749068260193 and acc: 0.913223147392273\n",
      "1063/2105 [==============>...............] - ETA: 21:54 - accuracy: 0.9132 - loss: 0.2242\n",
      " training set -> batch:1064 loss:0.21630620956420898 and acc: 0.9160000085830688\n",
      "1064/2105 [==============>...............] - ETA: 21:52 - accuracy: 0.9160 - loss: 0.2163\n",
      " training set -> batch:1065 loss:0.2203998565673828 and acc: 0.9166666865348816\n",
      "1065/2105 [==============>...............] - ETA: 21:50 - accuracy: 0.9167 - loss: 0.2204\n",
      " training set -> batch:1066 loss:0.22229519486427307 and acc: 0.9163534045219421\n",
      "1066/2105 [==============>...............] - ETA: 21:48 - accuracy: 0.9164 - loss: 0.2223\n",
      " training set -> batch:1067 loss:0.22504369914531708 and acc: 0.9142335653305054\n",
      "1067/2105 [==============>...............] - ETA: 21:46 - accuracy: 0.9142 - loss: 0.2250\n",
      " training set -> batch:1068 loss:0.23629584908485413 and acc: 0.9122340679168701\n",
      "1068/2105 [==============>...............] - ETA: 21:44 - accuracy: 0.9122 - loss: 0.2363\n",
      " training set -> batch:1069 loss:0.2404242753982544 and acc: 0.9120689630508423\n",
      "1069/2105 [==============>...............] - ETA: 21:42 - accuracy: 0.9121 - loss: 0.2404\n",
      " training set -> batch:1070 loss:0.24393726885318756 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1070 val loss:0.24255958199501038 and val acc: 0.9025229215621948\n",
      "1070/2105 [==============>...............] - ETA: 21:48 - accuracy: 0.9111 - loss: 0.2439\n",
      " training set -> batch:1071 loss:0.2436642348766327 and acc: 0.9015486836433411\n",
      "1071/2105 [==============>...............] - ETA: 21:46 - accuracy: 0.9015 - loss: 0.2437\n",
      " training set -> batch:1072 loss:0.24163664877414703 and acc: 0.9027777910232544\n",
      "1072/2105 [==============>...............] - ETA: 21:44 - accuracy: 0.9028 - loss: 0.2416\n",
      " training set -> batch:1073 loss:0.2423727661371231 and acc: 0.9018595218658447\n",
      "1073/2105 [==============>...............] - ETA: 21:42 - accuracy: 0.9019 - loss: 0.2424\n",
      " training set -> batch:1074 loss:0.2477816939353943 and acc: 0.8999999761581421\n",
      "1074/2105 [==============>...............] - ETA: 21:40 - accuracy: 0.9000 - loss: 0.2478\n",
      " training set -> batch:1075 loss:0.2455352246761322 and acc: 0.9001938104629517\n",
      "1075/2105 [==============>...............] - ETA: 21:38 - accuracy: 0.9002 - loss: 0.2455\n",
      " training set -> batch:1076 loss:0.24243251979351044 and acc: 0.9013158082962036\n",
      "1076/2105 [==============>...............] - ETA: 21:36 - accuracy: 0.9013 - loss: 0.2424\n",
      " training set -> batch:1077 loss:0.23646952211856842 and acc: 0.9032846689224243\n",
      "1077/2105 [==============>...............] - ETA: 21:34 - accuracy: 0.9033 - loss: 0.2365\n",
      " training set -> batch:1078 loss:0.24032850563526154 and acc: 0.902482271194458\n",
      "1078/2105 [==============>...............] - ETA: 21:32 - accuracy: 0.9025 - loss: 0.2403\n",
      " training set -> batch:1079 loss:0.23924148082733154 and acc: 0.9017241597175598\n",
      "1079/2105 [==============>...............] - ETA: 21:30 - accuracy: 0.9017 - loss: 0.2392\n",
      " training set -> batch:1080 loss:0.23809541761875153 and acc: 0.9018456339836121\n",
      "\n",
      " validation set -> batch:1080 val loss:0.22202685475349426 and val acc: 0.9197247624397278\n",
      "1080/2105 [==============>...............] - ETA: 21:35 - accuracy: 0.9018 - loss: 0.2381\n",
      " training set -> batch:1081 loss:0.22481057047843933 and acc: 0.9181416034698486\n",
      "1081/2105 [==============>...............] - ETA: 21:33 - accuracy: 0.9181 - loss: 0.2248\n",
      " training set -> batch:1082 loss:0.22012951970100403 and acc: 0.9198718070983887\n",
      "1082/2105 [==============>...............] - ETA: 21:31 - accuracy: 0.9199 - loss: 0.2201\n",
      " training set -> batch:1083 loss:0.21673716604709625 and acc: 0.9194214940071106\n",
      "1083/2105 [==============>...............] - ETA: 21:29 - accuracy: 0.9194 - loss: 0.2167\n",
      " training set -> batch:1084 loss:0.22189180552959442 and acc: 0.9190000295639038\n",
      "1084/2105 [==============>...............] - ETA: 21:27 - accuracy: 0.9190 - loss: 0.2219\n",
      " training set -> batch:1085 loss:0.21578921377658844 and acc: 0.9215116500854492\n",
      "1085/2105 [==============>...............] - ETA: 21:25 - accuracy: 0.9215 - loss: 0.2158\n",
      " training set -> batch:1086 loss:0.2130310982465744 and acc: 0.9229323267936707\n",
      "1086/2105 [==============>...............] - ETA: 21:23 - accuracy: 0.9229 - loss: 0.2130\n",
      " training set -> batch:1087 loss:0.20822064578533173 and acc: 0.9242700934410095\n",
      "1087/2105 [==============>...............] - ETA: 21:21 - accuracy: 0.9243 - loss: 0.2082\n",
      " training set -> batch:1088 loss:0.20684975385665894 and acc: 0.9228723645210266\n",
      "1088/2105 [==============>...............] - ETA: 21:19 - accuracy: 0.9229 - loss: 0.2068\n",
      " training set -> batch:1089 loss:0.2023250162601471 and acc: 0.9241379499435425\n",
      "1089/2105 [==============>...............] - ETA: 21:17 - accuracy: 0.9241 - loss: 0.2023\n",
      " training set -> batch:1090 loss:0.2058119773864746 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1090 val loss:0.23619583249092102 and val acc: 0.9048165082931519\n",
      "1090/2105 [==============>...............] - ETA: 21:22 - accuracy: 0.9237 - loss: 0.2058\n",
      " training set -> batch:1091 loss:0.2442423403263092 and acc: 0.9026548862457275\n",
      "1091/2105 [==============>...............] - ETA: 21:20 - accuracy: 0.9027 - loss: 0.2442\n",
      " training set -> batch:1092 loss:0.24253219366073608 and acc: 0.9027777910232544\n",
      "1092/2105 [==============>...............] - ETA: 21:18 - accuracy: 0.9028 - loss: 0.2425\n",
      " training set -> batch:1093 loss:0.2320277988910675 and acc: 0.9059917330741882\n",
      "1093/2105 [==============>...............] - ETA: 21:16 - accuracy: 0.9060 - loss: 0.2320\n",
      " training set -> batch:1094 loss:0.2287948727607727 and acc: 0.9070000052452087\n",
      "1094/2105 [==============>...............] - ETA: 21:14 - accuracy: 0.9070 - loss: 0.2288\n",
      " training set -> batch:1095 loss:0.22508510947227478 and acc: 0.9089147448539734\n",
      "1095/2105 [==============>...............] - ETA: 21:12 - accuracy: 0.9089 - loss: 0.2251\n",
      " training set -> batch:1096 loss:0.22656401991844177 and acc: 0.9097744226455688\n",
      "1096/2105 [==============>...............] - ETA: 21:10 - accuracy: 0.9098 - loss: 0.2266\n",
      " training set -> batch:1097 loss:0.22065900266170502 and acc: 0.9114963412284851\n",
      "1097/2105 [==============>...............] - ETA: 21:08 - accuracy: 0.9115 - loss: 0.2207\n",
      " training set -> batch:1098 loss:0.2151826024055481 and acc: 0.9131205677986145\n",
      "1098/2105 [==============>...............] - ETA: 21:06 - accuracy: 0.9131 - loss: 0.2152\n",
      " training set -> batch:1099 loss:0.20942062139511108 and acc: 0.9146551489830017\n",
      "1099/2105 [==============>...............] - ETA: 21:04 - accuracy: 0.9147 - loss: 0.2094\n",
      " training set -> batch:1100 loss:0.20498646795749664 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:1100 val loss:0.22425124049186707 and val acc: 0.9174311757087708\n",
      "1100/2105 [==============>...............] - ETA: 21:10 - accuracy: 0.9153 - loss: 0.2050\n",
      " training set -> batch:1101 loss:0.21308809518814087 and acc: 0.9203540086746216\n",
      "1101/2105 [==============>...............] - ETA: 21:08 - accuracy: 0.9204 - loss: 0.2131\n",
      " training set -> batch:1102 loss:0.20243817567825317 and acc: 0.9230769276618958\n",
      "1102/2105 [==============>...............] - ETA: 21:06 - accuracy: 0.9231 - loss: 0.2024\n",
      " training set -> batch:1103 loss:0.20929959416389465 and acc: 0.9214876294136047\n",
      "1103/2105 [==============>...............] - ETA: 21:04 - accuracy: 0.9215 - loss: 0.2093\n",
      " training set -> batch:1104 loss:0.20111782848834991 and acc: 0.9229999780654907\n",
      "1104/2105 [==============>...............] - ETA: 21:02 - accuracy: 0.9230 - loss: 0.2011\n",
      " training set -> batch:1105 loss:0.2008279711008072 and acc: 0.9224806427955627\n",
      "1105/2105 [==============>...............] - ETA: 21:00 - accuracy: 0.9225 - loss: 0.2008\n",
      " training set -> batch:1106 loss:0.2008085995912552 and acc: 0.9229323267936707\n",
      "1106/2105 [==============>...............] - ETA: 20:58 - accuracy: 0.9229 - loss: 0.2008\n",
      " training set -> batch:1107 loss:0.19652479887008667 and acc: 0.9242700934410095\n",
      "1107/2105 [==============>...............] - ETA: 20:56 - accuracy: 0.9243 - loss: 0.1965\n",
      " training set -> batch:1108 loss:0.19448445737361908 and acc: 0.9246453642845154\n",
      "1108/2105 [==============>...............] - ETA: 20:54 - accuracy: 0.9246 - loss: 0.1945\n",
      " training set -> batch:1109 loss:0.1979900449514389 and acc: 0.9241379499435425\n",
      "1109/2105 [==============>...............] - ETA: 20:52 - accuracy: 0.9241 - loss: 0.1980\n",
      " training set -> batch:1110 loss:0.1924237757921219 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1110 val loss:0.22404909133911133 and val acc: 0.9162843823432922\n",
      "1110/2105 [==============>...............] - ETA: 20:57 - accuracy: 0.9253 - loss: 0.1924\n",
      " training set -> batch:1111 loss:0.21498730778694153 and acc: 0.9181416034698486\n",
      "1111/2105 [==============>...............] - ETA: 20:55 - accuracy: 0.9181 - loss: 0.2150\n",
      " training set -> batch:1112 loss:0.2166306972503662 and acc: 0.9166666865348816\n",
      "1112/2105 [==============>...............] - ETA: 20:53 - accuracy: 0.9167 - loss: 0.2166\n",
      " training set -> batch:1113 loss:0.21280714869499207 and acc: 0.9173553586006165\n",
      "1113/2105 [==============>...............] - ETA: 20:51 - accuracy: 0.9174 - loss: 0.2128\n",
      " training set -> batch:1114 loss:0.20504996180534363 and acc: 0.9190000295639038\n",
      "1114/2105 [==============>...............] - ETA: 20:49 - accuracy: 0.9190 - loss: 0.2050\n",
      " training set -> batch:1115 loss:0.2177092581987381 and acc: 0.9176356792449951\n",
      "1115/2105 [==============>...............] - ETA: 20:47 - accuracy: 0.9176 - loss: 0.2177\n",
      " training set -> batch:1116 loss:0.21604080498218536 and acc: 0.9191729426383972\n",
      "1116/2105 [==============>...............] - ETA: 20:45 - accuracy: 0.9192 - loss: 0.2160\n",
      " training set -> batch:1117 loss:0.20941859483718872 and acc: 0.9206204414367676\n",
      "1117/2105 [==============>...............] - ETA: 20:43 - accuracy: 0.9206 - loss: 0.2094\n",
      " training set -> batch:1118 loss:0.2091587632894516 and acc: 0.9202127456665039\n",
      "1118/2105 [==============>...............] - ETA: 20:41 - accuracy: 0.9202 - loss: 0.2092\n",
      " training set -> batch:1119 loss:0.20102103054523468 and acc: 0.9224137663841248\n",
      "1119/2105 [==============>...............] - ETA: 20:39 - accuracy: 0.9224 - loss: 0.2010\n",
      " training set -> batch:1120 loss:0.20096836984157562 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1120 val loss:0.2542637288570404 and val acc: 0.9036697149276733\n",
      "1120/2105 [==============>...............] - ETA: 20:44 - accuracy: 0.9237 - loss: 0.2010\n",
      " training set -> batch:1121 loss:0.24581222236156464 and acc: 0.9059734344482422\n",
      "1121/2105 [==============>...............] - ETA: 20:42 - accuracy: 0.9060 - loss: 0.2458\n",
      " training set -> batch:1122 loss:0.23610934615135193 and acc: 0.9070512652397156\n",
      "1122/2105 [==============>...............] - ETA: 20:41 - accuracy: 0.9071 - loss: 0.2361\n",
      " training set -> batch:1123 loss:0.235825315117836 and acc: 0.9059917330741882\n",
      "1123/2105 [===============>..............] - ETA: 20:39 - accuracy: 0.9060 - loss: 0.2358\n",
      " training set -> batch:1124 loss:0.23269611597061157 and acc: 0.906000018119812\n",
      "1124/2105 [===============>..............] - ETA: 20:37 - accuracy: 0.9060 - loss: 0.2327\n",
      " training set -> batch:1125 loss:0.23394536972045898 and acc: 0.9050387740135193\n",
      "1125/2105 [===============>..............] - ETA: 20:35 - accuracy: 0.9050 - loss: 0.2339\n",
      " training set -> batch:1126 loss:0.23227889835834503 and acc: 0.9060150384902954\n",
      "1126/2105 [===============>..............] - ETA: 20:33 - accuracy: 0.9060 - loss: 0.2323\n",
      " training set -> batch:1127 loss:0.23267914354801178 and acc: 0.9069343209266663\n",
      "1127/2105 [===============>..............] - ETA: 20:31 - accuracy: 0.9069 - loss: 0.2327\n",
      " training set -> batch:1128 loss:0.22425609827041626 and acc: 0.9095744490623474\n",
      "1128/2105 [===============>..............] - ETA: 20:29 - accuracy: 0.9096 - loss: 0.2243\n",
      " training set -> batch:1129 loss:0.22094392776489258 and acc: 0.9103448390960693\n",
      "1129/2105 [===============>..............] - ETA: 20:27 - accuracy: 0.9103 - loss: 0.2209\n",
      " training set -> batch:1130 loss:0.21506597101688385 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:1130 val loss:0.21157296001911163 and val acc: 0.9059633016586304\n",
      "1130/2105 [===============>..............] - ETA: 20:32 - accuracy: 0.9119 - loss: 0.2151\n",
      " training set -> batch:1131 loss:0.22157971560955048 and acc: 0.903761088848114\n",
      "1131/2105 [===============>..............] - ETA: 20:30 - accuracy: 0.9038 - loss: 0.2216\n",
      " training set -> batch:1132 loss:0.2124110907316208 and acc: 0.9070512652397156\n",
      "1132/2105 [===============>..............] - ETA: 20:28 - accuracy: 0.9071 - loss: 0.2124\n",
      " training set -> batch:1133 loss:0.21857497096061707 and acc: 0.9059917330741882\n",
      "1133/2105 [===============>..............] - ETA: 20:26 - accuracy: 0.9060 - loss: 0.2186\n",
      " training set -> batch:1134 loss:0.21225868165493011 and acc: 0.9079999923706055\n",
      "1134/2105 [===============>..............] - ETA: 20:24 - accuracy: 0.9080 - loss: 0.2123\n",
      " training set -> batch:1135 loss:0.20398317277431488 and acc: 0.9098837375640869\n",
      "1135/2105 [===============>..............] - ETA: 20:22 - accuracy: 0.9099 - loss: 0.2040\n",
      " training set -> batch:1136 loss:0.20393137633800507 and acc: 0.9097744226455688\n",
      "1136/2105 [===============>..............] - ETA: 20:20 - accuracy: 0.9098 - loss: 0.2039\n",
      " training set -> batch:1137 loss:0.19842880964279175 and acc: 0.9114963412284851\n",
      "1137/2105 [===============>..............] - ETA: 20:18 - accuracy: 0.9115 - loss: 0.1984\n",
      " training set -> batch:1138 loss:0.19918470084667206 and acc: 0.9122340679168701\n",
      "1138/2105 [===============>..............] - ETA: 20:16 - accuracy: 0.9122 - loss: 0.1992\n",
      " training set -> batch:1139 loss:0.19784994423389435 and acc: 0.9137930870056152\n",
      "1139/2105 [===============>..............] - ETA: 20:14 - accuracy: 0.9138 - loss: 0.1978\n",
      " training set -> batch:1140 loss:0.1977328509092331 and acc: 0.9144295454025269\n",
      "\n",
      " validation set -> batch:1140 val loss:0.2064058482646942 and val acc: 0.9197247624397278\n",
      "1140/2105 [===============>..............] - ETA: 20:19 - accuracy: 0.9144 - loss: 0.1977\n",
      " training set -> batch:1141 loss:0.20656706392765045 and acc: 0.9203540086746216\n",
      "1141/2105 [===============>..............] - ETA: 20:17 - accuracy: 0.9204 - loss: 0.2066\n",
      " training set -> batch:1142 loss:0.20931442081928253 and acc: 0.9198718070983887\n",
      "1142/2105 [===============>..............] - ETA: 20:15 - accuracy: 0.9199 - loss: 0.2093\n",
      " training set -> batch:1143 loss:0.20332711935043335 and acc: 0.9214876294136047\n",
      "1143/2105 [===============>..............] - ETA: 20:13 - accuracy: 0.9215 - loss: 0.2033\n",
      " training set -> batch:1144 loss:0.19664667546749115 and acc: 0.9229999780654907\n",
      "1144/2105 [===============>..............] - ETA: 20:11 - accuracy: 0.9230 - loss: 0.1966\n",
      " training set -> batch:1145 loss:0.19878654181957245 and acc: 0.9215116500854492\n",
      "1145/2105 [===============>..............] - ETA: 20:10 - accuracy: 0.9215 - loss: 0.1988\n",
      " training set -> batch:1146 loss:0.20026437938213348 and acc: 0.9210526347160339\n",
      "1146/2105 [===============>..............] - ETA: 20:08 - accuracy: 0.9211 - loss: 0.2003\n",
      " training set -> batch:1147 loss:0.20507287979125977 and acc: 0.9197080135345459\n",
      "1147/2105 [===============>..............] - ETA: 20:06 - accuracy: 0.9197 - loss: 0.2051\n",
      " training set -> batch:1148 loss:0.20813937485218048 and acc: 0.9202127456665039\n",
      "1148/2105 [===============>..............] - ETA: 20:04 - accuracy: 0.9202 - loss: 0.2081\n",
      " training set -> batch:1149 loss:0.2084680199623108 and acc: 0.9198275804519653\n",
      "1149/2105 [===============>..............] - ETA: 20:02 - accuracy: 0.9198 - loss: 0.2085\n",
      " training set -> batch:1150 loss:0.20348389446735382 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1150 val loss:0.20387187600135803 and val acc: 0.9231651425361633\n",
      "1150/2105 [===============>..............] - ETA: 20:07 - accuracy: 0.9203 - loss: 0.2035\n",
      " training set -> batch:1151 loss:0.20164424180984497 and acc: 0.9247787594795227\n",
      "1151/2105 [===============>..............] - ETA: 20:05 - accuracy: 0.9248 - loss: 0.2016\n",
      " training set -> batch:1152 loss:0.20081233978271484 and acc: 0.9252136945724487\n",
      "1152/2105 [===============>..............] - ETA: 20:03 - accuracy: 0.9252 - loss: 0.2008\n",
      " training set -> batch:1153 loss:0.20370663702487946 and acc: 0.9235537052154541\n",
      "1153/2105 [===============>..............] - ETA: 20:01 - accuracy: 0.9236 - loss: 0.2037\n",
      " training set -> batch:1154 loss:0.20104394853115082 and acc: 0.9229999780654907\n",
      "1154/2105 [===============>..............] - ETA: 19:59 - accuracy: 0.9230 - loss: 0.2010\n",
      " training set -> batch:1155 loss:0.19743570685386658 and acc: 0.9234496355056763\n",
      "1155/2105 [===============>..............] - ETA: 19:57 - accuracy: 0.9234 - loss: 0.1974\n",
      " training set -> batch:1156 loss:0.20757226645946503 and acc: 0.9210526347160339\n",
      "1156/2105 [===============>..............] - ETA: 19:55 - accuracy: 0.9211 - loss: 0.2076\n",
      " training set -> batch:1157 loss:0.20508480072021484 and acc: 0.9206204414367676\n",
      "1157/2105 [===============>..............] - ETA: 19:53 - accuracy: 0.9206 - loss: 0.2051\n",
      " training set -> batch:1158 loss:0.20619629323482513 and acc: 0.9193262457847595\n",
      "1158/2105 [===============>..............] - ETA: 19:51 - accuracy: 0.9193 - loss: 0.2062\n",
      " training set -> batch:1159 loss:0.20291398465633392 and acc: 0.9206896424293518\n",
      "1159/2105 [===============>..............] - ETA: 19:50 - accuracy: 0.9207 - loss: 0.2029\n",
      " training set -> batch:1160 loss:0.19969576597213745 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1160 val loss:0.2032664567232132 and val acc: 0.9151375889778137\n",
      "1160/2105 [===============>..............] - ETA: 19:54 - accuracy: 0.9211 - loss: 0.1997\n",
      " training set -> batch:1161 loss:0.1932576447725296 and acc: 0.9170354008674622\n",
      "1161/2105 [===============>..............] - ETA: 19:52 - accuracy: 0.9170 - loss: 0.1933\n",
      " training set -> batch:1162 loss:0.19922898709774017 and acc: 0.9166666865348816\n",
      "1162/2105 [===============>..............] - ETA: 19:50 - accuracy: 0.9167 - loss: 0.1992\n",
      " training set -> batch:1163 loss:0.19274580478668213 and acc: 0.9183884263038635\n",
      "1163/2105 [===============>..............] - ETA: 19:49 - accuracy: 0.9184 - loss: 0.1927\n",
      " training set -> batch:1164 loss:0.20514090359210968 and acc: 0.9160000085830688\n",
      "1164/2105 [===============>..............] - ETA: 19:47 - accuracy: 0.9160 - loss: 0.2051\n",
      " training set -> batch:1165 loss:0.20086689293384552 and acc: 0.9176356792449951\n",
      "1165/2105 [===============>..............] - ETA: 19:45 - accuracy: 0.9176 - loss: 0.2009\n",
      " training set -> batch:1166 loss:0.20461320877075195 and acc: 0.9154135584831238\n",
      "1166/2105 [===============>..............] - ETA: 19:43 - accuracy: 0.9154 - loss: 0.2046\n",
      " training set -> batch:1167 loss:0.2009916752576828 and acc: 0.9169707894325256\n",
      "1167/2105 [===============>..............] - ETA: 19:41 - accuracy: 0.9170 - loss: 0.2010\n",
      " training set -> batch:1168 loss:0.2000523954629898 and acc: 0.917553186416626\n",
      "1168/2105 [===============>..............] - ETA: 19:39 - accuracy: 0.9176 - loss: 0.2001\n",
      " training set -> batch:1169 loss:0.19519391655921936 and acc: 0.9189655184745789\n",
      "1169/2105 [===============>..............] - ETA: 19:37 - accuracy: 0.9190 - loss: 0.1952\n",
      " training set -> batch:1170 loss:0.1998065859079361 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1170 val loss:0.2073633223772049 and val acc: 0.9151375889778137\n",
      "1170/2105 [===============>..............] - ETA: 19:42 - accuracy: 0.9195 - loss: 0.1998\n",
      " training set -> batch:1171 loss:0.20583875477313995 and acc: 0.9148229956626892\n",
      "1171/2105 [===============>..............] - ETA: 19:40 - accuracy: 0.9148 - loss: 0.2058\n",
      " training set -> batch:1172 loss:0.20310847461223602 and acc: 0.9134615659713745\n",
      "1172/2105 [===============>..............] - ETA: 19:38 - accuracy: 0.9135 - loss: 0.2031\n",
      " training set -> batch:1173 loss:0.20801767706871033 and acc: 0.9121900796890259\n",
      "1173/2105 [===============>..............] - ETA: 19:36 - accuracy: 0.9122 - loss: 0.2080\n",
      " training set -> batch:1174 loss:0.20081593096256256 and acc: 0.9150000214576721\n",
      "1174/2105 [===============>..............] - ETA: 19:34 - accuracy: 0.9150 - loss: 0.2008\n",
      " training set -> batch:1175 loss:0.19738228619098663 and acc: 0.9156976938247681\n",
      "1175/2105 [===============>..............] - ETA: 19:32 - accuracy: 0.9157 - loss: 0.1974\n",
      " training set -> batch:1176 loss:0.19365449249744415 and acc: 0.9172932505607605\n",
      "1176/2105 [===============>..............] - ETA: 19:30 - accuracy: 0.9173 - loss: 0.1937\n",
      " training set -> batch:1177 loss:0.19582784175872803 and acc: 0.9160584211349487\n",
      "1177/2105 [===============>..............] - ETA: 19:28 - accuracy: 0.9161 - loss: 0.1958\n",
      " training set -> batch:1178 loss:0.20329028367996216 and acc: 0.9140070676803589\n",
      "1178/2105 [===============>..............] - ETA: 19:27 - accuracy: 0.9140 - loss: 0.2033\n",
      " training set -> batch:1179 loss:0.20030498504638672 and acc: 0.9146551489830017\n",
      "1179/2105 [===============>..............] - ETA: 19:25 - accuracy: 0.9147 - loss: 0.2003\n",
      " training set -> batch:1180 loss:0.19496279954910278 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1180 val loss:0.20078977942466736 and val acc: 0.9220183491706848\n",
      "1180/2105 [===============>..............] - ETA: 19:29 - accuracy: 0.9161 - loss: 0.1950\n",
      " training set -> batch:1181 loss:0.19769126176834106 and acc: 0.9225663542747498\n",
      "1181/2105 [===============>..............] - ETA: 19:27 - accuracy: 0.9226 - loss: 0.1977\n",
      " training set -> batch:1182 loss:0.19457972049713135 and acc: 0.9220085740089417\n",
      "1182/2105 [===============>..............] - ETA: 19:25 - accuracy: 0.9220 - loss: 0.1946\n",
      " training set -> batch:1183 loss:0.20638348162174225 and acc: 0.9214876294136047\n",
      "1183/2105 [===============>..............] - ETA: 19:24 - accuracy: 0.9215 - loss: 0.2064\n",
      " training set -> batch:1184 loss:0.21251657605171204 and acc: 0.9200000166893005\n",
      "1184/2105 [===============>..............] - ETA: 19:22 - accuracy: 0.9200 - loss: 0.2125\n",
      " training set -> batch:1185 loss:0.2036910355091095 and acc: 0.9224806427955627\n",
      "1185/2105 [===============>..............] - ETA: 19:20 - accuracy: 0.9225 - loss: 0.2037\n",
      " training set -> batch:1186 loss:0.20248468220233917 and acc: 0.923872172832489\n",
      "1186/2105 [===============>..............] - ETA: 19:18 - accuracy: 0.9239 - loss: 0.2025\n",
      " training set -> batch:1187 loss:0.1980234831571579 and acc: 0.9260948896408081\n",
      "1187/2105 [===============>..............] - ETA: 19:16 - accuracy: 0.9261 - loss: 0.1980\n",
      " training set -> batch:1188 loss:0.19622613489627838 and acc: 0.9273049831390381\n",
      "1188/2105 [===============>..............] - ETA: 19:14 - accuracy: 0.9273 - loss: 0.1962\n",
      " training set -> batch:1189 loss:0.19270996749401093 and acc: 0.9284482598304749\n",
      "1189/2105 [===============>..............] - ETA: 19:12 - accuracy: 0.9284 - loss: 0.1927\n",
      " training set -> batch:1190 loss:0.1877022385597229 and acc: 0.9303691387176514\n",
      "\n",
      " validation set -> batch:1190 val loss:0.2034977674484253 and val acc: 0.9174311757087708\n",
      "1190/2105 [===============>..............] - ETA: 19:17 - accuracy: 0.9304 - loss: 0.1877\n",
      " training set -> batch:1191 loss:0.20414644479751587 and acc: 0.9170354008674622\n",
      "1191/2105 [===============>..............] - ETA: 19:15 - accuracy: 0.9170 - loss: 0.2041\n",
      " training set -> batch:1192 loss:0.20384900271892548 and acc: 0.9188033938407898\n",
      "1192/2105 [===============>..............] - ETA: 19:13 - accuracy: 0.9188 - loss: 0.2038\n",
      " training set -> batch:1193 loss:0.20152877271175385 and acc: 0.9194214940071106\n",
      "1193/2105 [================>.............] - ETA: 19:11 - accuracy: 0.9194 - loss: 0.2015\n",
      " training set -> batch:1194 loss:0.2085145264863968 and acc: 0.9179999828338623\n",
      "1194/2105 [================>.............] - ETA: 19:09 - accuracy: 0.9180 - loss: 0.2085\n",
      " training set -> batch:1195 loss:0.20229528844356537 and acc: 0.9195736646652222\n",
      "1195/2105 [================>.............] - ETA: 19:07 - accuracy: 0.9196 - loss: 0.2023\n",
      " training set -> batch:1196 loss:0.19468489289283752 and acc: 0.9210526347160339\n",
      "1196/2105 [================>.............] - ETA: 19:05 - accuracy: 0.9211 - loss: 0.1947\n",
      " training set -> batch:1197 loss:0.19678640365600586 and acc: 0.9206204414367676\n",
      "1197/2105 [================>.............] - ETA: 19:04 - accuracy: 0.9206 - loss: 0.1968\n",
      " training set -> batch:1198 loss:0.19934004545211792 and acc: 0.9202127456665039\n",
      "1198/2105 [================>.............] - ETA: 19:02 - accuracy: 0.9202 - loss: 0.1993\n",
      " training set -> batch:1199 loss:0.19274039566516876 and acc: 0.9224137663841248\n",
      "1199/2105 [================>.............] - ETA: 19:00 - accuracy: 0.9224 - loss: 0.1927\n",
      " training set -> batch:1200 loss:0.18916301429271698 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1200 val loss:0.22502171993255615 and val acc: 0.9243119359016418\n",
      "1200/2105 [================>.............] - ETA: 19:04 - accuracy: 0.9228 - loss: 0.1892\n",
      " training set -> batch:1201 loss:0.21592837572097778 and acc: 0.9258849620819092\n",
      "1201/2105 [================>.............] - ETA: 19:02 - accuracy: 0.9259 - loss: 0.2159\n",
      " training set -> batch:1202 loss:0.21692177653312683 and acc: 0.9252136945724487\n",
      "1202/2105 [================>.............] - ETA: 19:00 - accuracy: 0.9252 - loss: 0.2169\n",
      " training set -> batch:1203 loss:0.20895707607269287 and acc: 0.9266529083251953\n",
      "1203/2105 [================>.............] - ETA: 18:59 - accuracy: 0.9267 - loss: 0.2090\n",
      " training set -> batch:1204 loss:0.19926075637340546 and acc: 0.9290000200271606\n",
      "1204/2105 [================>.............] - ETA: 18:57 - accuracy: 0.9290 - loss: 0.1993\n",
      " training set -> batch:1205 loss:0.19649769365787506 and acc: 0.930232584476471\n",
      "1205/2105 [================>.............] - ETA: 18:55 - accuracy: 0.9302 - loss: 0.1965\n",
      " training set -> batch:1206 loss:0.18790902197360992 and acc: 0.932330846786499\n",
      "1206/2105 [================>.............] - ETA: 18:53 - accuracy: 0.9323 - loss: 0.1879\n",
      " training set -> batch:1207 loss:0.1829158067703247 and acc: 0.9333941340446472\n",
      "1207/2105 [================>.............] - ETA: 18:51 - accuracy: 0.9334 - loss: 0.1829\n",
      " training set -> batch:1208 loss:0.1883026659488678 and acc: 0.932624101638794\n",
      "1208/2105 [================>.............] - ETA: 18:49 - accuracy: 0.9326 - loss: 0.1883\n",
      " training set -> batch:1209 loss:0.1853960007429123 and acc: 0.932758629322052\n",
      "1209/2105 [================>.............] - ETA: 18:47 - accuracy: 0.9328 - loss: 0.1854\n",
      " training set -> batch:1210 loss:0.19156505167484283 and acc: 0.931208074092865\n",
      "\n",
      " validation set -> batch:1210 val loss:0.22055332362651825 and val acc: 0.9254587292671204\n",
      "1210/2105 [================>.............] - ETA: 18:52 - accuracy: 0.9312 - loss: 0.1916\n",
      " training set -> batch:1211 loss:0.2174987941980362 and acc: 0.9269911646842957\n",
      "1211/2105 [================>.............] - ETA: 18:50 - accuracy: 0.9270 - loss: 0.2175\n",
      " training set -> batch:1212 loss:0.22539792954921722 and acc: 0.9262820482254028\n",
      "1212/2105 [================>.............] - ETA: 18:48 - accuracy: 0.9263 - loss: 0.2254\n",
      " training set -> batch:1213 loss:0.21746191382408142 and acc: 0.9276859760284424\n",
      "1213/2105 [================>.............] - ETA: 18:46 - accuracy: 0.9277 - loss: 0.2175\n",
      " training set -> batch:1214 loss:0.2259315699338913 and acc: 0.9259999990463257\n",
      "1214/2105 [================>.............] - ETA: 18:44 - accuracy: 0.9260 - loss: 0.2259\n",
      " training set -> batch:1215 loss:0.21672768890857697 and acc: 0.9273256063461304\n",
      "1215/2105 [================>.............] - ETA: 18:42 - accuracy: 0.9273 - loss: 0.2167\n",
      " training set -> batch:1216 loss:0.2145538330078125 and acc: 0.9266917109489441\n",
      "1216/2105 [================>.............] - ETA: 18:40 - accuracy: 0.9267 - loss: 0.2146\n",
      " training set -> batch:1217 loss:0.22193142771720886 and acc: 0.9260948896408081\n",
      "1217/2105 [================>.............] - ETA: 18:39 - accuracy: 0.9261 - loss: 0.2219\n",
      " training set -> batch:1218 loss:0.2311771959066391 and acc: 0.923758864402771\n",
      "1218/2105 [================>.............] - ETA: 18:37 - accuracy: 0.9238 - loss: 0.2312\n",
      " training set -> batch:1219 loss:0.23176313936710358 and acc: 0.9224137663841248\n",
      "1219/2105 [================>.............] - ETA: 18:35 - accuracy: 0.9224 - loss: 0.2318\n",
      " training set -> batch:1220 loss:0.2253427505493164 and acc: 0.9244966506958008\n",
      "\n",
      " validation set -> batch:1220 val loss:0.23353826999664307 and val acc: 0.9036697149276733\n",
      "1220/2105 [================>.............] - ETA: 18:39 - accuracy: 0.9245 - loss: 0.2253\n",
      " training set -> batch:1221 loss:0.22285889089107513 and acc: 0.9059734344482422\n",
      "1221/2105 [================>.............] - ETA: 18:37 - accuracy: 0.9060 - loss: 0.2229\n",
      " training set -> batch:1222 loss:0.2307461053133011 and acc: 0.9049145579338074\n",
      "1222/2105 [================>.............] - ETA: 18:35 - accuracy: 0.9049 - loss: 0.2307\n",
      " training set -> batch:1223 loss:0.22526298463344574 and acc: 0.9059917330741882\n",
      "1223/2105 [================>.............] - ETA: 18:34 - accuracy: 0.9060 - loss: 0.2253\n",
      " training set -> batch:1224 loss:0.22611014544963837 and acc: 0.906000018119812\n",
      "1224/2105 [================>.............] - ETA: 18:32 - accuracy: 0.9060 - loss: 0.2261\n",
      " training set -> batch:1225 loss:0.23135699331760406 and acc: 0.9050387740135193\n",
      "1225/2105 [================>.............] - ETA: 18:30 - accuracy: 0.9050 - loss: 0.2314\n",
      " training set -> batch:1226 loss:0.2265673577785492 and acc: 0.9069548845291138\n",
      "1226/2105 [================>.............] - ETA: 18:28 - accuracy: 0.9070 - loss: 0.2266\n",
      " training set -> batch:1227 loss:0.22744715213775635 and acc: 0.9069343209266663\n",
      "1227/2105 [================>.............] - ETA: 18:26 - accuracy: 0.9069 - loss: 0.2274\n",
      " training set -> batch:1228 loss:0.22604280710220337 and acc: 0.908687949180603\n",
      "1228/2105 [================>.............] - ETA: 18:24 - accuracy: 0.9087 - loss: 0.2260\n",
      " training set -> batch:1229 loss:0.2284201681613922 and acc: 0.9068965315818787\n",
      "1229/2105 [================>.............] - ETA: 18:23 - accuracy: 0.9069 - loss: 0.2284\n",
      " training set -> batch:1230 loss:0.2244095802307129 and acc: 0.9085570573806763\n",
      "\n",
      " validation set -> batch:1230 val loss:0.21890346705913544 and val acc: 0.9151375889778137\n",
      "1230/2105 [================>.............] - ETA: 18:26 - accuracy: 0.9086 - loss: 0.2244\n",
      " training set -> batch:1231 loss:0.22073706984519958 and acc: 0.9137167930603027\n",
      "1231/2105 [================>.............] - ETA: 18:25 - accuracy: 0.9137 - loss: 0.2207\n",
      " training set -> batch:1232 loss:0.21837985515594482 and acc: 0.9145299196243286\n",
      "1232/2105 [================>.............] - ETA: 18:23 - accuracy: 0.9145 - loss: 0.2184\n",
      " training set -> batch:1233 loss:0.2179102748632431 and acc: 0.9163222908973694\n",
      "1233/2105 [================>.............] - ETA: 18:21 - accuracy: 0.9163 - loss: 0.2179\n",
      " training set -> batch:1234 loss:0.21309027075767517 and acc: 0.9179999828338623\n",
      "1234/2105 [================>.............] - ETA: 18:19 - accuracy: 0.9180 - loss: 0.2131\n",
      " training set -> batch:1235 loss:0.21028544008731842 and acc: 0.9176356792449951\n",
      "1235/2105 [================>.............] - ETA: 18:17 - accuracy: 0.9176 - loss: 0.2103\n",
      " training set -> batch:1236 loss:0.20795240998268127 and acc: 0.9191729426383972\n",
      "1236/2105 [================>.............] - ETA: 18:15 - accuracy: 0.9192 - loss: 0.2080\n",
      " training set -> batch:1237 loss:0.2025088369846344 and acc: 0.9206204414367676\n",
      "1237/2105 [================>.............] - ETA: 18:14 - accuracy: 0.9206 - loss: 0.2025\n",
      " training set -> batch:1238 loss:0.20199109613895416 and acc: 0.9202127456665039\n",
      "1238/2105 [================>.............] - ETA: 18:12 - accuracy: 0.9202 - loss: 0.2020\n",
      " training set -> batch:1239 loss:0.19599218666553497 and acc: 0.9224137663841248\n",
      "1239/2105 [================>.............] - ETA: 18:10 - accuracy: 0.9224 - loss: 0.1960\n",
      " training set -> batch:1240 loss:0.19467799365520477 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1240 val loss:0.22713303565979004 and val acc: 0.9002293348312378\n",
      "1240/2105 [================>.............] - ETA: 18:14 - accuracy: 0.9228 - loss: 0.1947\n",
      " training set -> batch:1241 loss:0.2292778640985489 and acc: 0.9004424810409546\n",
      "1241/2105 [================>.............] - ETA: 18:12 - accuracy: 0.9004 - loss: 0.2293\n",
      " training set -> batch:1242 loss:0.2245284914970398 and acc: 0.9017093777656555\n",
      "1242/2105 [================>.............] - ETA: 18:10 - accuracy: 0.9017 - loss: 0.2245\n",
      " training set -> batch:1243 loss:0.2199239432811737 and acc: 0.9028925895690918\n",
      "1243/2105 [================>.............] - ETA: 18:08 - accuracy: 0.9029 - loss: 0.2199\n",
      " training set -> batch:1244 loss:0.22138358652591705 and acc: 0.902999997138977\n",
      "1244/2105 [================>.............] - ETA: 18:07 - accuracy: 0.9030 - loss: 0.2214\n",
      " training set -> batch:1245 loss:0.2188386470079422 and acc: 0.9040697813034058\n",
      "1245/2105 [================>.............] - ETA: 18:05 - accuracy: 0.9041 - loss: 0.2188\n",
      " training set -> batch:1246 loss:0.21884064376354218 and acc: 0.9041353464126587\n",
      "1246/2105 [================>.............] - ETA: 18:03 - accuracy: 0.9041 - loss: 0.2188\n",
      " training set -> batch:1247 loss:0.22158250212669373 and acc: 0.904197096824646\n",
      "1247/2105 [================>.............] - ETA: 18:01 - accuracy: 0.9042 - loss: 0.2216\n",
      " training set -> batch:1248 loss:0.2232927680015564 and acc: 0.9042553305625916\n",
      "1248/2105 [================>.............] - ETA: 17:59 - accuracy: 0.9043 - loss: 0.2233\n",
      " training set -> batch:1249 loss:0.22246412932872772 and acc: 0.9043103456497192\n",
      "1249/2105 [================>.............] - ETA: 17:58 - accuracy: 0.9043 - loss: 0.2225\n",
      " training set -> batch:1250 loss:0.21575404703617096 and acc: 0.906879186630249\n",
      "\n",
      " validation set -> batch:1250 val loss:0.22588644921779633 and val acc: 0.9025229215621948\n",
      "1250/2105 [================>.............] - ETA: 18:01 - accuracy: 0.9069 - loss: 0.2158\n",
      " training set -> batch:1251 loss:0.21952347457408905 and acc: 0.9048672318458557\n",
      "1251/2105 [================>.............] - ETA: 17:59 - accuracy: 0.9049 - loss: 0.2195\n",
      " training set -> batch:1252 loss:0.2086242288351059 and acc: 0.9081196784973145\n",
      "1252/2105 [================>.............] - ETA: 17:58 - accuracy: 0.9081 - loss: 0.2086\n",
      " training set -> batch:1253 loss:0.199136883020401 and acc: 0.9111570119857788\n",
      "1253/2105 [================>.............] - ETA: 17:56 - accuracy: 0.9112 - loss: 0.1991\n",
      " training set -> batch:1254 loss:0.20013335347175598 and acc: 0.9110000133514404\n",
      "1254/2105 [================>.............] - ETA: 17:54 - accuracy: 0.9110 - loss: 0.2001\n",
      " training set -> batch:1255 loss:0.19439955055713654 and acc: 0.9127907156944275\n",
      "1255/2105 [================>.............] - ETA: 17:52 - accuracy: 0.9128 - loss: 0.1944\n",
      " training set -> batch:1256 loss:0.20195111632347107 and acc: 0.9116541147232056\n",
      "1256/2105 [================>.............] - ETA: 17:50 - accuracy: 0.9117 - loss: 0.2020\n",
      " training set -> batch:1257 loss:0.19670552015304565 and acc: 0.9133211970329285\n",
      "1257/2105 [================>.............] - ETA: 17:49 - accuracy: 0.9133 - loss: 0.1967\n",
      " training set -> batch:1258 loss:0.1980157047510147 and acc: 0.9122340679168701\n",
      "1258/2105 [================>.............] - ETA: 17:47 - accuracy: 0.9122 - loss: 0.1980\n",
      " training set -> batch:1259 loss:0.19333811104297638 and acc: 0.9129310250282288\n",
      "1259/2105 [================>.............] - ETA: 17:45 - accuracy: 0.9129 - loss: 0.1933\n",
      " training set -> batch:1260 loss:0.19688771665096283 and acc: 0.911912739276886\n",
      "\n",
      " validation set -> batch:1260 val loss:0.2145853191614151 and val acc: 0.9162843823432922\n",
      "1260/2105 [================>.............] - ETA: 17:49 - accuracy: 0.9119 - loss: 0.1969\n",
      " training set -> batch:1261 loss:0.21484190225601196 and acc: 0.9181416034698486\n",
      "1261/2105 [================>.............] - ETA: 17:47 - accuracy: 0.9181 - loss: 0.2148\n",
      " training set -> batch:1262 loss:0.2226276993751526 and acc: 0.9166666865348816\n",
      "1262/2105 [================>.............] - ETA: 17:45 - accuracy: 0.9167 - loss: 0.2226\n",
      " training set -> batch:1263 loss:0.22437351942062378 and acc: 0.9163222908973694\n",
      "1263/2105 [=================>............] - ETA: 17:43 - accuracy: 0.9163 - loss: 0.2244\n",
      " training set -> batch:1264 loss:0.2185806781053543 and acc: 0.9179999828338623\n",
      "1264/2105 [=================>............] - ETA: 17:42 - accuracy: 0.9180 - loss: 0.2186\n",
      " training set -> batch:1265 loss:0.21933279931545258 and acc: 0.9186046719551086\n",
      "1265/2105 [=================>............] - ETA: 17:40 - accuracy: 0.9186 - loss: 0.2193\n",
      " training set -> batch:1266 loss:0.21058717370033264 and acc: 0.9210526347160339\n",
      "1266/2105 [=================>............] - ETA: 17:38 - accuracy: 0.9211 - loss: 0.2106\n",
      " training set -> batch:1267 loss:0.21099740266799927 and acc: 0.9197080135345459\n",
      "1267/2105 [=================>............] - ETA: 17:36 - accuracy: 0.9197 - loss: 0.2110\n",
      " training set -> batch:1268 loss:0.20887045562267303 and acc: 0.9202127456665039\n",
      "1268/2105 [=================>............] - ETA: 17:34 - accuracy: 0.9202 - loss: 0.2089\n",
      " training set -> batch:1269 loss:0.21294493973255157 and acc: 0.9181034564971924\n",
      "1269/2105 [=================>............] - ETA: 17:33 - accuracy: 0.9181 - loss: 0.2129\n",
      " training set -> batch:1270 loss:0.20577771961688995 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1270 val loss:0.21341940760612488 and val acc: 0.9162843823432922\n",
      "1270/2105 [=================>............] - ETA: 17:36 - accuracy: 0.9203 - loss: 0.2058\n",
      " training set -> batch:1271 loss:0.21539980173110962 and acc: 0.9159291982650757\n",
      "1271/2105 [=================>............] - ETA: 17:34 - accuracy: 0.9159 - loss: 0.2154\n",
      " training set -> batch:1272 loss:0.20608298480510712 and acc: 0.9177350401878357\n",
      "1272/2105 [=================>............] - ETA: 17:33 - accuracy: 0.9177 - loss: 0.2061\n",
      " training set -> batch:1273 loss:0.20073539018630981 and acc: 0.9183884263038635\n",
      "1273/2105 [=================>............] - ETA: 17:31 - accuracy: 0.9184 - loss: 0.2007\n",
      " training set -> batch:1274 loss:0.1990984082221985 and acc: 0.9200000166893005\n",
      "1274/2105 [=================>............] - ETA: 17:29 - accuracy: 0.9200 - loss: 0.1991\n",
      " training set -> batch:1275 loss:0.20120258629322052 and acc: 0.9176356792449951\n",
      "1275/2105 [=================>............] - ETA: 17:27 - accuracy: 0.9176 - loss: 0.2012\n",
      " training set -> batch:1276 loss:0.2038707733154297 and acc: 0.9172932505607605\n",
      "1276/2105 [=================>............] - ETA: 17:25 - accuracy: 0.9173 - loss: 0.2039\n",
      " training set -> batch:1277 loss:0.19980137050151825 and acc: 0.9178832173347473\n",
      "1277/2105 [=================>............] - ETA: 17:24 - accuracy: 0.9179 - loss: 0.1998\n",
      " training set -> batch:1278 loss:0.197811558842659 and acc: 0.917553186416626\n",
      "1278/2105 [=================>............] - ETA: 17:22 - accuracy: 0.9176 - loss: 0.1978\n",
      " training set -> batch:1279 loss:0.2041958123445511 and acc: 0.9163793325424194\n",
      "1279/2105 [=================>............] - ETA: 17:20 - accuracy: 0.9164 - loss: 0.2042\n",
      " training set -> batch:1280 loss:0.20128236711025238 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1280 val loss:0.23093339800834656 and val acc: 0.9036697149276733\n",
      "1280/2105 [=================>............] - ETA: 17:25 - accuracy: 0.9169 - loss: 0.2013\n",
      " training set -> batch:1281 loss:0.23585207760334015 and acc: 0.9015486836433411\n",
      "1281/2105 [=================>............] - ETA: 17:23 - accuracy: 0.9015 - loss: 0.2359\n",
      " training set -> batch:1282 loss:0.24077154695987701 and acc: 0.9006410241127014\n",
      "1282/2105 [=================>............] - ETA: 17:22 - accuracy: 0.9006 - loss: 0.2408\n",
      " training set -> batch:1283 loss:0.2326480746269226 and acc: 0.9028925895690918\n",
      "1283/2105 [=================>............] - ETA: 17:20 - accuracy: 0.9029 - loss: 0.2326\n",
      " training set -> batch:1284 loss:0.22724130749702454 and acc: 0.9039999842643738\n",
      "1284/2105 [=================>............] - ETA: 17:18 - accuracy: 0.9040 - loss: 0.2272\n",
      " training set -> batch:1285 loss:0.23219168186187744 and acc: 0.9040697813034058\n",
      "1285/2105 [=================>............] - ETA: 17:16 - accuracy: 0.9041 - loss: 0.2322\n",
      " training set -> batch:1286 loss:0.2283048927783966 and acc: 0.905075192451477\n",
      "1286/2105 [=================>............] - ETA: 17:15 - accuracy: 0.9051 - loss: 0.2283\n",
      " training set -> batch:1287 loss:0.2219429761171341 and acc: 0.9069343209266663\n",
      "1287/2105 [=================>............] - ETA: 17:13 - accuracy: 0.9069 - loss: 0.2219\n",
      " training set -> batch:1288 loss:0.2185613512992859 and acc: 0.9078013896942139\n",
      "1288/2105 [=================>............] - ETA: 17:11 - accuracy: 0.9078 - loss: 0.2186\n",
      " training set -> batch:1289 loss:0.21352125704288483 and acc: 0.9094827771186829\n",
      "1289/2105 [=================>............] - ETA: 17:09 - accuracy: 0.9095 - loss: 0.2135\n",
      " training set -> batch:1290 loss:0.2179507166147232 and acc: 0.9085570573806763\n",
      "\n",
      " validation set -> batch:1290 val loss:0.21437884867191315 and val acc: 0.9162843823432922\n",
      "1290/2105 [=================>............] - ETA: 17:13 - accuracy: 0.9086 - loss: 0.2180\n",
      " training set -> batch:1291 loss:0.20542220771312714 and acc: 0.9192478060722351\n",
      "1291/2105 [=================>............] - ETA: 17:11 - accuracy: 0.9192 - loss: 0.2054\n",
      " training set -> batch:1292 loss:0.2026478797197342 and acc: 0.9209401607513428\n",
      "1292/2105 [=================>............] - ETA: 17:09 - accuracy: 0.9209 - loss: 0.2026\n",
      " training set -> batch:1293 loss:0.2026243507862091 and acc: 0.9214876294136047\n",
      "1293/2105 [=================>............] - ETA: 17:07 - accuracy: 0.9215 - loss: 0.2026\n",
      " training set -> batch:1294 loss:0.19383062422275543 and acc: 0.9240000247955322\n",
      "1294/2105 [=================>............] - ETA: 17:06 - accuracy: 0.9240 - loss: 0.1938\n",
      " training set -> batch:1295 loss:0.19124434888362885 and acc: 0.9253876209259033\n",
      "1295/2105 [=================>............] - ETA: 17:04 - accuracy: 0.9254 - loss: 0.1912\n",
      " training set -> batch:1296 loss:0.19061656296253204 and acc: 0.9257518649101257\n",
      "1296/2105 [=================>............] - ETA: 17:02 - accuracy: 0.9258 - loss: 0.1906\n",
      " training set -> batch:1297 loss:0.18957650661468506 and acc: 0.9251824617385864\n",
      "1297/2105 [=================>............] - ETA: 17:00 - accuracy: 0.9252 - loss: 0.1896\n",
      " training set -> batch:1298 loss:0.18984848260879517 and acc: 0.9246453642845154\n",
      "1298/2105 [=================>............] - ETA: 16:58 - accuracy: 0.9246 - loss: 0.1898\n",
      " training set -> batch:1299 loss:0.18932899832725525 and acc: 0.925000011920929\n",
      "1299/2105 [=================>............] - ETA: 16:57 - accuracy: 0.9250 - loss: 0.1893\n",
      " training set -> batch:1300 loss:0.1833905130624771 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1300 val loss:0.24233901500701904 and val acc: 0.9094036817550659\n",
      "1300/2105 [=================>............] - ETA: 17:00 - accuracy: 0.9270 - loss: 0.1834\n",
      " training set -> batch:1301 loss:0.2337481677532196 and acc: 0.9115044474601746\n",
      "1301/2105 [=================>............] - ETA: 16:58 - accuracy: 0.9115 - loss: 0.2337\n",
      " training set -> batch:1302 loss:0.22383959591388702 and acc: 0.9134615659713745\n",
      "1302/2105 [=================>............] - ETA: 16:56 - accuracy: 0.9135 - loss: 0.2238\n",
      " training set -> batch:1303 loss:0.21906517446041107 and acc: 0.91425621509552\n",
      "1303/2105 [=================>............] - ETA: 16:55 - accuracy: 0.9143 - loss: 0.2191\n",
      " training set -> batch:1304 loss:0.2258368879556656 and acc: 0.9129999876022339\n",
      "1304/2105 [=================>............] - ETA: 16:53 - accuracy: 0.9130 - loss: 0.2258\n",
      " training set -> batch:1305 loss:0.22406607866287231 and acc: 0.913759708404541\n",
      "1305/2105 [=================>............] - ETA: 16:51 - accuracy: 0.9138 - loss: 0.2241\n",
      " training set -> batch:1306 loss:0.22501620650291443 and acc: 0.9135338068008423\n",
      "1306/2105 [=================>............] - ETA: 16:49 - accuracy: 0.9135 - loss: 0.2250\n",
      " training set -> batch:1307 loss:0.21617035567760468 and acc: 0.915145993232727\n",
      "1307/2105 [=================>............] - ETA: 16:48 - accuracy: 0.9151 - loss: 0.2162\n",
      " training set -> batch:1308 loss:0.22078019380569458 and acc: 0.914893627166748\n",
      "1308/2105 [=================>............] - ETA: 16:46 - accuracy: 0.9149 - loss: 0.2208\n",
      " training set -> batch:1309 loss:0.2155766636133194 and acc: 0.9163793325424194\n",
      "1309/2105 [=================>............] - ETA: 16:44 - accuracy: 0.9164 - loss: 0.2156\n",
      " training set -> batch:1310 loss:0.215984046459198 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1310 val loss:0.22860801219940186 and val acc: 0.911697268486023\n",
      "1310/2105 [=================>............] - ETA: 16:47 - accuracy: 0.9161 - loss: 0.2160\n",
      " training set -> batch:1311 loss:0.22525405883789062 and acc: 0.9103982448577881\n",
      "1311/2105 [=================>............] - ETA: 16:46 - accuracy: 0.9104 - loss: 0.2253\n",
      " training set -> batch:1312 loss:0.21785269677639008 and acc: 0.9113247990608215\n",
      "1312/2105 [=================>............] - ETA: 16:44 - accuracy: 0.9113 - loss: 0.2179\n",
      " training set -> batch:1313 loss:0.22179792821407318 and acc: 0.9121900796890259\n",
      "1313/2105 [=================>............] - ETA: 16:42 - accuracy: 0.9122 - loss: 0.2218\n",
      " training set -> batch:1314 loss:0.2193465381860733 and acc: 0.9139999747276306\n",
      "1314/2105 [=================>............] - ETA: 16:40 - accuracy: 0.9140 - loss: 0.2193\n",
      " training set -> batch:1315 loss:0.2130647897720337 and acc: 0.9147287011146545\n",
      "1315/2105 [=================>............] - ETA: 16:39 - accuracy: 0.9147 - loss: 0.2131\n",
      " training set -> batch:1316 loss:0.21684272587299347 and acc: 0.9144737124443054\n",
      "1316/2105 [=================>............] - ETA: 16:37 - accuracy: 0.9145 - loss: 0.2168\n",
      " training set -> batch:1317 loss:0.21324482560157776 and acc: 0.9160584211349487\n",
      "1317/2105 [=================>............] - ETA: 16:35 - accuracy: 0.9161 - loss: 0.2132\n",
      " training set -> batch:1318 loss:0.20519226789474487 and acc: 0.9184397459030151\n",
      "1318/2105 [=================>............] - ETA: 16:33 - accuracy: 0.9184 - loss: 0.2052\n",
      " training set -> batch:1319 loss:0.19970040023326874 and acc: 0.9206896424293518\n",
      "1319/2105 [=================>............] - ETA: 16:32 - accuracy: 0.9207 - loss: 0.1997\n",
      " training set -> batch:1320 loss:0.20427416265010834 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1320 val loss:0.24072861671447754 and val acc: 0.9082568883895874\n",
      "1320/2105 [=================>............] - ETA: 16:35 - accuracy: 0.9186 - loss: 0.2043\n",
      " training set -> batch:1321 loss:0.23757316172122955 and acc: 0.9081858396530151\n",
      "1321/2105 [=================>............] - ETA: 16:33 - accuracy: 0.9082 - loss: 0.2376\n",
      " training set -> batch:1322 loss:0.22638414800167084 and acc: 0.9102563858032227\n",
      "1322/2105 [=================>............] - ETA: 16:31 - accuracy: 0.9103 - loss: 0.2264\n",
      " training set -> batch:1323 loss:0.22405919432640076 and acc: 0.9090909361839294\n",
      "1323/2105 [=================>............] - ETA: 16:29 - accuracy: 0.9091 - loss: 0.2241\n",
      " training set -> batch:1324 loss:0.21633262932300568 and acc: 0.9110000133514404\n",
      "1324/2105 [=================>............] - ETA: 16:28 - accuracy: 0.9110 - loss: 0.2163\n",
      " training set -> batch:1325 loss:0.21359212696552277 and acc: 0.9108527302742004\n",
      "1325/2105 [=================>............] - ETA: 16:26 - accuracy: 0.9109 - loss: 0.2136\n",
      " training set -> batch:1326 loss:0.21757522225379944 and acc: 0.9097744226455688\n",
      "1326/2105 [=================>............] - ETA: 16:24 - accuracy: 0.9098 - loss: 0.2176\n",
      " training set -> batch:1327 loss:0.2182723879814148 and acc: 0.9105839133262634\n",
      "1327/2105 [=================>............] - ETA: 16:22 - accuracy: 0.9106 - loss: 0.2183\n",
      " training set -> batch:1328 loss:0.21699990332126617 and acc: 0.9122340679168701\n",
      "1328/2105 [=================>............] - ETA: 16:21 - accuracy: 0.9122 - loss: 0.2170\n",
      " training set -> batch:1329 loss:0.22195492684841156 and acc: 0.9094827771186829\n",
      "1329/2105 [=================>............] - ETA: 16:19 - accuracy: 0.9095 - loss: 0.2220\n",
      " training set -> batch:1330 loss:0.22527535259723663 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:1330 val loss:0.2400849312543869 and val acc: 0.9071100950241089\n",
      "1330/2105 [=================>............] - ETA: 16:22 - accuracy: 0.9077 - loss: 0.2253\n",
      " training set -> batch:1331 loss:0.2435740977525711 and acc: 0.9059734344482422\n",
      "1331/2105 [=================>............] - ETA: 16:20 - accuracy: 0.9060 - loss: 0.2436\n",
      " training set -> batch:1332 loss:0.23616021871566772 and acc: 0.9081196784973145\n",
      "1332/2105 [=================>............] - ETA: 16:19 - accuracy: 0.9081 - loss: 0.2362\n",
      " training set -> batch:1333 loss:0.22974146902561188 and acc: 0.9090909361839294\n",
      "1333/2105 [=================>............] - ETA: 16:17 - accuracy: 0.9091 - loss: 0.2297\n",
      " training set -> batch:1334 loss:0.2266225814819336 and acc: 0.9100000262260437\n",
      "1334/2105 [==================>...........] - ETA: 16:15 - accuracy: 0.9100 - loss: 0.2266\n",
      " training set -> batch:1335 loss:0.2242170125246048 and acc: 0.911821722984314\n",
      "1335/2105 [==================>...........] - ETA: 16:13 - accuracy: 0.9118 - loss: 0.2242\n",
      " training set -> batch:1336 loss:0.2207106351852417 and acc: 0.9135338068008423\n",
      "1336/2105 [==================>...........] - ETA: 16:12 - accuracy: 0.9135 - loss: 0.2207\n",
      " training set -> batch:1337 loss:0.21244452893733978 and acc: 0.9160584211349487\n",
      "1337/2105 [==================>...........] - ETA: 16:10 - accuracy: 0.9161 - loss: 0.2124\n",
      " training set -> batch:1338 loss:0.20741382241249084 and acc: 0.9184397459030151\n",
      "1338/2105 [==================>...........] - ETA: 16:08 - accuracy: 0.9184 - loss: 0.2074\n",
      " training set -> batch:1339 loss:0.20132657885551453 and acc: 0.9206896424293518\n",
      "1339/2105 [==================>...........] - ETA: 16:06 - accuracy: 0.9207 - loss: 0.2013\n",
      " training set -> batch:1340 loss:0.19610829651355743 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1340 val loss:0.2282443344593048 and val acc: 0.9105504751205444\n",
      "1340/2105 [==================>...........] - ETA: 16:09 - accuracy: 0.9220 - loss: 0.1961\n",
      " training set -> batch:1341 loss:0.2152511179447174 and acc: 0.9137167930603027\n",
      "1341/2105 [==================>...........] - ETA: 16:08 - accuracy: 0.9137 - loss: 0.2153\n",
      " training set -> batch:1342 loss:0.21091870963573456 and acc: 0.9134615659713745\n",
      "1342/2105 [==================>...........] - ETA: 16:06 - accuracy: 0.9135 - loss: 0.2109\n",
      " training set -> batch:1343 loss:0.20349344611167908 and acc: 0.9152892827987671\n",
      "1343/2105 [==================>...........] - ETA: 16:04 - accuracy: 0.9153 - loss: 0.2035\n",
      " training set -> batch:1344 loss:0.2054474949836731 and acc: 0.9150000214576721\n",
      "1344/2105 [==================>...........] - ETA: 16:03 - accuracy: 0.9150 - loss: 0.2054\n",
      " training set -> batch:1345 loss:0.19842803478240967 and acc: 0.9166666865348816\n",
      "1345/2105 [==================>...........] - ETA: 16:01 - accuracy: 0.9167 - loss: 0.1984\n",
      " training set -> batch:1346 loss:0.20002755522727966 and acc: 0.9154135584831238\n",
      "1346/2105 [==================>...........] - ETA: 15:59 - accuracy: 0.9154 - loss: 0.2000\n",
      " training set -> batch:1347 loss:0.19457592070102692 and acc: 0.9169707894325256\n",
      "1347/2105 [==================>...........] - ETA: 15:57 - accuracy: 0.9170 - loss: 0.1946\n",
      " training set -> batch:1348 loss:0.1887032389640808 and acc: 0.9193262457847595\n",
      "1348/2105 [==================>...........] - ETA: 15:56 - accuracy: 0.9193 - loss: 0.1887\n",
      " training set -> batch:1349 loss:0.1902562379837036 and acc: 0.9189655184745789\n",
      "1349/2105 [==================>...........] - ETA: 15:54 - accuracy: 0.9190 - loss: 0.1903\n",
      " training set -> batch:1350 loss:0.18838565051555634 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1350 val loss:0.24157960712909698 and val acc: 0.91399085521698\n",
      "1350/2105 [==================>...........] - ETA: 15:57 - accuracy: 0.9186 - loss: 0.1884\n",
      " training set -> batch:1351 loss:0.23359403014183044 and acc: 0.9159291982650757\n",
      "1351/2105 [==================>...........] - ETA: 15:55 - accuracy: 0.9159 - loss: 0.2336\n",
      " training set -> batch:1352 loss:0.2389785200357437 and acc: 0.9145299196243286\n",
      "1352/2105 [==================>...........] - ETA: 15:53 - accuracy: 0.9145 - loss: 0.2390\n",
      " training set -> batch:1353 loss:0.23506152629852295 and acc: 0.9152892827987671\n",
      "1353/2105 [==================>...........] - ETA: 15:52 - accuracy: 0.9153 - loss: 0.2351\n",
      " training set -> batch:1354 loss:0.23699958622455597 and acc: 0.9139999747276306\n",
      "1354/2105 [==================>...........] - ETA: 15:50 - accuracy: 0.9140 - loss: 0.2370\n",
      " training set -> batch:1355 loss:0.2360374480485916 and acc: 0.913759708404541\n",
      "1355/2105 [==================>...........] - ETA: 15:48 - accuracy: 0.9138 - loss: 0.2360\n",
      " training set -> batch:1356 loss:0.23742742836475372 and acc: 0.9144737124443054\n",
      "1356/2105 [==================>...........] - ETA: 15:47 - accuracy: 0.9145 - loss: 0.2374\n",
      " training set -> batch:1357 loss:0.23512199521064758 and acc: 0.915145993232727\n",
      "1357/2105 [==================>...........] - ETA: 15:45 - accuracy: 0.9151 - loss: 0.2351\n",
      " training set -> batch:1358 loss:0.23741506040096283 and acc: 0.914893627166748\n",
      "1358/2105 [==================>...........] - ETA: 15:43 - accuracy: 0.9149 - loss: 0.2374\n",
      " training set -> batch:1359 loss:0.24201594293117523 and acc: 0.9137930870056152\n",
      "1359/2105 [==================>...........] - ETA: 15:41 - accuracy: 0.9138 - loss: 0.2420\n",
      " training set -> batch:1360 loss:0.23529332876205444 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1360 val loss:0.22283942997455597 and val acc: 0.9162843823432922\n",
      "1360/2105 [==================>...........] - ETA: 15:44 - accuracy: 0.9161 - loss: 0.2353\n",
      " training set -> batch:1361 loss:0.21286576986312866 and acc: 0.9181416034698486\n",
      "1361/2105 [==================>...........] - ETA: 15:42 - accuracy: 0.9181 - loss: 0.2129\n",
      " training set -> batch:1362 loss:0.20731833577156067 and acc: 0.9198718070983887\n",
      "1362/2105 [==================>...........] - ETA: 15:41 - accuracy: 0.9199 - loss: 0.2073\n",
      " training set -> batch:1363 loss:0.2052522748708725 and acc: 0.9194214940071106\n",
      "1363/2105 [==================>...........] - ETA: 15:39 - accuracy: 0.9194 - loss: 0.2053\n",
      " training set -> batch:1364 loss:0.21615910530090332 and acc: 0.9139999747276306\n",
      "1364/2105 [==================>...........] - ETA: 15:37 - accuracy: 0.9140 - loss: 0.2162\n",
      " training set -> batch:1365 loss:0.21404622495174408 and acc: 0.9147287011146545\n",
      "1365/2105 [==================>...........] - ETA: 15:36 - accuracy: 0.9147 - loss: 0.2140\n",
      " training set -> batch:1366 loss:0.20512883365154266 and acc: 0.9172932505607605\n",
      "1366/2105 [==================>...........] - ETA: 15:34 - accuracy: 0.9173 - loss: 0.2051\n",
      " training set -> batch:1367 loss:0.20246584713459015 and acc: 0.9178832173347473\n",
      "1367/2105 [==================>...........] - ETA: 15:32 - accuracy: 0.9179 - loss: 0.2025\n",
      " training set -> batch:1368 loss:0.20794111490249634 and acc: 0.917553186416626\n",
      "1368/2105 [==================>...........] - ETA: 15:31 - accuracy: 0.9176 - loss: 0.2079\n",
      " training set -> batch:1369 loss:0.20501622557640076 and acc: 0.9181034564971924\n",
      "1369/2105 [==================>...........] - ETA: 15:29 - accuracy: 0.9181 - loss: 0.2050\n",
      " training set -> batch:1370 loss:0.2032298892736435 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1370 val loss:0.21753095090389252 and val acc: 0.9128440618515015\n",
      "1370/2105 [==================>...........] - ETA: 15:31 - accuracy: 0.9178 - loss: 0.2032\n",
      " training set -> batch:1371 loss:0.21880373358726501 and acc: 0.9126105904579163\n",
      "1371/2105 [==================>...........] - ETA: 15:30 - accuracy: 0.9126 - loss: 0.2188\n",
      " training set -> batch:1372 loss:0.20743753015995026 and acc: 0.9155982732772827\n",
      "1372/2105 [==================>...........] - ETA: 15:28 - accuracy: 0.9156 - loss: 0.2074\n",
      " training set -> batch:1373 loss:0.1962960660457611 and acc: 0.9183884263038635\n",
      "1373/2105 [==================>...........] - ETA: 15:26 - accuracy: 0.9184 - loss: 0.1963\n",
      " training set -> batch:1374 loss:0.19318991899490356 and acc: 0.9190000295639038\n",
      "1374/2105 [==================>...........] - ETA: 15:25 - accuracy: 0.9190 - loss: 0.1932\n",
      " training set -> batch:1375 loss:0.20265412330627441 and acc: 0.9176356792449951\n",
      "1375/2105 [==================>...........] - ETA: 15:23 - accuracy: 0.9176 - loss: 0.2027\n",
      " training set -> batch:1376 loss:0.20377223193645477 and acc: 0.9172932505607605\n",
      "1376/2105 [==================>...........] - ETA: 15:21 - accuracy: 0.9173 - loss: 0.2038\n",
      " training set -> batch:1377 loss:0.2101142555475235 and acc: 0.9160584211349487\n",
      "1377/2105 [==================>...........] - ETA: 15:20 - accuracy: 0.9161 - loss: 0.2101\n",
      " training set -> batch:1378 loss:0.20536206662654877 and acc: 0.9166666865348816\n",
      "1378/2105 [==================>...........] - ETA: 15:18 - accuracy: 0.9167 - loss: 0.2054\n",
      " training set -> batch:1379 loss:0.1999277025461197 and acc: 0.9181034564971924\n",
      "1379/2105 [==================>...........] - ETA: 15:16 - accuracy: 0.9181 - loss: 0.1999\n",
      " training set -> batch:1380 loss:0.1974787712097168 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1380 val loss:0.2265876829624176 and val acc: 0.9082568883895874\n",
      "1380/2105 [==================>...........] - ETA: 15:19 - accuracy: 0.9186 - loss: 0.1975\n",
      " training set -> batch:1381 loss:0.21765755116939545 and acc: 0.9103982448577881\n",
      "1381/2105 [==================>...........] - ETA: 15:17 - accuracy: 0.9104 - loss: 0.2177\n",
      " training set -> batch:1382 loss:0.2194761335849762 and acc: 0.9102563858032227\n",
      "1382/2105 [==================>...........] - ETA: 15:15 - accuracy: 0.9103 - loss: 0.2195\n",
      " training set -> batch:1383 loss:0.21360766887664795 and acc: 0.9121900796890259\n",
      "1383/2105 [==================>...........] - ETA: 15:14 - accuracy: 0.9122 - loss: 0.2136\n",
      " training set -> batch:1384 loss:0.2173975706100464 and acc: 0.9120000004768372\n",
      "1384/2105 [==================>...........] - ETA: 15:12 - accuracy: 0.9120 - loss: 0.2174\n",
      " training set -> batch:1385 loss:0.21759957075119019 and acc: 0.911821722984314\n",
      "1385/2105 [==================>...........] - ETA: 15:10 - accuracy: 0.9118 - loss: 0.2176\n",
      " training set -> batch:1386 loss:0.20923419296741486 and acc: 0.9144737124443054\n",
      "1386/2105 [==================>...........] - ETA: 15:09 - accuracy: 0.9145 - loss: 0.2092\n",
      " training set -> batch:1387 loss:0.20571863651275635 and acc: 0.9142335653305054\n",
      "1387/2105 [==================>...........] - ETA: 15:07 - accuracy: 0.9142 - loss: 0.2057\n",
      " training set -> batch:1388 loss:0.20555561780929565 and acc: 0.9140070676803589\n",
      "1388/2105 [==================>...........] - ETA: 15:05 - accuracy: 0.9140 - loss: 0.2056\n",
      " training set -> batch:1389 loss:0.2056906521320343 and acc: 0.9129310250282288\n",
      "1389/2105 [==================>...........] - ETA: 15:04 - accuracy: 0.9129 - loss: 0.2057\n",
      " training set -> batch:1390 loss:0.20208536088466644 and acc: 0.9144295454025269\n",
      "\n",
      " validation set -> batch:1390 val loss:0.22538547217845917 and val acc: 0.9105504751205444\n",
      "1390/2105 [==================>...........] - ETA: 15:06 - accuracy: 0.9144 - loss: 0.2021\n",
      " training set -> batch:1391 loss:0.2157951146364212 and acc: 0.9126105904579163\n",
      "1391/2105 [==================>...........] - ETA: 15:05 - accuracy: 0.9126 - loss: 0.2158\n",
      " training set -> batch:1392 loss:0.20962949097156525 and acc: 0.9134615659713745\n",
      "1392/2105 [==================>...........] - ETA: 15:03 - accuracy: 0.9135 - loss: 0.2096\n",
      " training set -> batch:1393 loss:0.20314085483551025 and acc: 0.91425621509552\n",
      "1393/2105 [==================>...........] - ETA: 15:01 - accuracy: 0.9143 - loss: 0.2031\n",
      " training set -> batch:1394 loss:0.2101476490497589 and acc: 0.9120000004768372\n",
      "1394/2105 [==================>...........] - ETA: 15:00 - accuracy: 0.9120 - loss: 0.2101\n",
      " training set -> batch:1395 loss:0.21535517275333405 and acc: 0.9127907156944275\n",
      "1395/2105 [==================>...........] - ETA: 14:58 - accuracy: 0.9128 - loss: 0.2154\n",
      " training set -> batch:1396 loss:0.21215350925922394 and acc: 0.9144737124443054\n",
      "1396/2105 [==================>...........] - ETA: 14:56 - accuracy: 0.9145 - loss: 0.2122\n",
      " training set -> batch:1397 loss:0.21512340009212494 and acc: 0.915145993232727\n",
      "1397/2105 [==================>...........] - ETA: 14:55 - accuracy: 0.9151 - loss: 0.2151\n",
      " training set -> batch:1398 loss:0.21231088042259216 and acc: 0.914893627166748\n",
      "1398/2105 [==================>...........] - ETA: 14:53 - accuracy: 0.9149 - loss: 0.2123\n",
      " training set -> batch:1399 loss:0.20736141502857208 and acc: 0.915517270565033\n",
      "1399/2105 [==================>...........] - ETA: 14:51 - accuracy: 0.9155 - loss: 0.2074\n",
      " training set -> batch:1400 loss:0.2045729160308838 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1400 val loss:0.2360907644033432 and val acc: 0.9013761281967163\n",
      "1400/2105 [==================>...........] - ETA: 14:54 - accuracy: 0.9161 - loss: 0.2046\n",
      " training set -> batch:1401 loss:0.23896493017673492 and acc: 0.9026548862457275\n",
      "1401/2105 [==================>...........] - ETA: 14:52 - accuracy: 0.9027 - loss: 0.2390\n",
      " training set -> batch:1402 loss:0.24155953526496887 and acc: 0.9027777910232544\n",
      "1402/2105 [==================>...........] - ETA: 14:50 - accuracy: 0.9028 - loss: 0.2416\n",
      " training set -> batch:1403 loss:0.2386709600687027 and acc: 0.9028925895690918\n",
      "1403/2105 [==================>...........] - ETA: 14:49 - accuracy: 0.9029 - loss: 0.2387\n",
      " training set -> batch:1404 loss:0.2337108701467514 and acc: 0.9049999713897705\n",
      "1404/2105 [===================>..........] - ETA: 14:47 - accuracy: 0.9050 - loss: 0.2337\n",
      " training set -> batch:1405 loss:0.23085883259773254 and acc: 0.9040697813034058\n",
      "1405/2105 [===================>..........] - ETA: 14:45 - accuracy: 0.9041 - loss: 0.2309\n",
      " training set -> batch:1406 loss:0.23575027287006378 and acc: 0.9031955003738403\n",
      "1406/2105 [===================>..........] - ETA: 14:44 - accuracy: 0.9032 - loss: 0.2358\n",
      " training set -> batch:1407 loss:0.23181608319282532 and acc: 0.9032846689224243\n",
      "1407/2105 [===================>..........] - ETA: 14:42 - accuracy: 0.9033 - loss: 0.2318\n",
      " training set -> batch:1408 loss:0.23131690919399261 and acc: 0.9033687710762024\n",
      "1408/2105 [===================>..........] - ETA: 14:40 - accuracy: 0.9034 - loss: 0.2313\n",
      " training set -> batch:1409 loss:0.2304353564977646 and acc: 0.9025862216949463\n",
      "1409/2105 [===================>..........] - ETA: 14:39 - accuracy: 0.9026 - loss: 0.2304\n",
      " training set -> batch:1410 loss:0.2325928956270218 and acc: 0.9018456339836121\n",
      "\n",
      " validation set -> batch:1410 val loss:0.22385983169078827 and val acc: 0.911697268486023\n",
      "1410/2105 [===================>..........] - ETA: 14:41 - accuracy: 0.9018 - loss: 0.2326\n",
      " training set -> batch:1411 loss:0.226459801197052 and acc: 0.9137167930603027\n",
      "1411/2105 [===================>..........] - ETA: 14:39 - accuracy: 0.9137 - loss: 0.2265\n",
      " training set -> batch:1412 loss:0.22115649282932281 and acc: 0.9155982732772827\n",
      "1412/2105 [===================>..........] - ETA: 14:38 - accuracy: 0.9156 - loss: 0.2212\n",
      " training set -> batch:1413 loss:0.22282832860946655 and acc: 0.9152892827987671\n",
      "1413/2105 [===================>..........] - ETA: 14:36 - accuracy: 0.9153 - loss: 0.2228\n",
      " training set -> batch:1414 loss:0.2222871035337448 and acc: 0.9139999747276306\n",
      "1414/2105 [===================>..........] - ETA: 14:34 - accuracy: 0.9140 - loss: 0.2223\n",
      " training set -> batch:1415 loss:0.2193140685558319 and acc: 0.9156976938247681\n",
      "1415/2105 [===================>..........] - ETA: 14:33 - accuracy: 0.9157 - loss: 0.2193\n",
      " training set -> batch:1416 loss:0.214603453874588 and acc: 0.9172932505607605\n",
      "1416/2105 [===================>..........] - ETA: 14:31 - accuracy: 0.9173 - loss: 0.2146\n",
      " training set -> batch:1417 loss:0.22175095975399017 and acc: 0.9160584211349487\n",
      "1417/2105 [===================>..........] - ETA: 14:29 - accuracy: 0.9161 - loss: 0.2218\n",
      " training set -> batch:1418 loss:0.2186134308576584 and acc: 0.9166666865348816\n",
      "1418/2105 [===================>..........] - ETA: 14:28 - accuracy: 0.9167 - loss: 0.2186\n",
      " training set -> batch:1419 loss:0.21167966723442078 and acc: 0.9189655184745789\n",
      "1419/2105 [===================>..........] - ETA: 14:26 - accuracy: 0.9190 - loss: 0.2117\n",
      " training set -> batch:1420 loss:0.2116507738828659 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1420 val loss:0.20332077145576477 and val acc: 0.9197247624397278\n",
      "1420/2105 [===================>..........] - ETA: 14:28 - accuracy: 0.9195 - loss: 0.2117\n",
      " training set -> batch:1421 loss:0.20318208634853363 and acc: 0.9203540086746216\n",
      "1421/2105 [===================>..........] - ETA: 14:27 - accuracy: 0.9204 - loss: 0.2032\n",
      " training set -> batch:1422 loss:0.20095358788967133 and acc: 0.9209401607513428\n",
      "1422/2105 [===================>..........] - ETA: 14:25 - accuracy: 0.9209 - loss: 0.2010\n",
      " training set -> batch:1423 loss:0.19677837193012238 and acc: 0.9214876294136047\n",
      "1423/2105 [===================>..........] - ETA: 14:23 - accuracy: 0.9215 - loss: 0.1968\n",
      " training set -> batch:1424 loss:0.18982058763504028 and acc: 0.9229999780654907\n",
      "1424/2105 [===================>..........] - ETA: 14:22 - accuracy: 0.9230 - loss: 0.1898\n",
      " training set -> batch:1425 loss:0.1876985728740692 and acc: 0.9244186282157898\n",
      "1425/2105 [===================>..........] - ETA: 14:20 - accuracy: 0.9244 - loss: 0.1877\n",
      " training set -> batch:1426 loss:0.18210580945014954 and acc: 0.9257518649101257\n",
      "1426/2105 [===================>..........] - ETA: 14:18 - accuracy: 0.9258 - loss: 0.1821\n",
      " training set -> batch:1427 loss:0.18375003337860107 and acc: 0.9242700934410095\n",
      "1427/2105 [===================>..........] - ETA: 14:17 - accuracy: 0.9243 - loss: 0.1838\n",
      " training set -> batch:1428 loss:0.18488115072250366 and acc: 0.9255319237709045\n",
      "1428/2105 [===================>..........] - ETA: 14:15 - accuracy: 0.9255 - loss: 0.1849\n",
      " training set -> batch:1429 loss:0.1908724457025528 and acc: 0.9258620738983154\n",
      "1429/2105 [===================>..........] - ETA: 14:13 - accuracy: 0.9259 - loss: 0.1909\n",
      " training set -> batch:1430 loss:0.19426481425762177 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1430 val loss:0.2003978192806244 and val acc: 0.9220183491706848\n",
      "1430/2105 [===================>..........] - ETA: 14:16 - accuracy: 0.9253 - loss: 0.1943\n",
      " training set -> batch:1431 loss:0.20971737802028656 and acc: 0.9203540086746216\n",
      "1431/2105 [===================>..........] - ETA: 14:14 - accuracy: 0.9204 - loss: 0.2097\n",
      " training set -> batch:1432 loss:0.1989934742450714 and acc: 0.9230769276618958\n",
      "1432/2105 [===================>..........] - ETA: 14:12 - accuracy: 0.9231 - loss: 0.1990\n",
      " training set -> batch:1433 loss:0.19990909099578857 and acc: 0.922520637512207\n",
      "1433/2105 [===================>..........] - ETA: 14:11 - accuracy: 0.9225 - loss: 0.1999\n",
      " training set -> batch:1434 loss:0.20077480375766754 and acc: 0.9229999780654907\n",
      "1434/2105 [===================>..........] - ETA: 14:09 - accuracy: 0.9230 - loss: 0.2008\n",
      " training set -> batch:1435 loss:0.19945648312568665 and acc: 0.9234496355056763\n",
      "1435/2105 [===================>..........] - ETA: 14:07 - accuracy: 0.9234 - loss: 0.1995\n",
      " training set -> batch:1436 loss:0.19673383235931396 and acc: 0.923872172832489\n",
      "1436/2105 [===================>..........] - ETA: 14:06 - accuracy: 0.9239 - loss: 0.1967\n",
      " training set -> batch:1437 loss:0.19771789014339447 and acc: 0.9242700934410095\n",
      "1437/2105 [===================>..........] - ETA: 14:04 - accuracy: 0.9243 - loss: 0.1977\n",
      " training set -> batch:1438 loss:0.19288019835948944 and acc: 0.9255319237709045\n",
      "1438/2105 [===================>..........] - ETA: 14:02 - accuracy: 0.9255 - loss: 0.1929\n",
      " training set -> batch:1439 loss:0.18833723664283752 and acc: 0.9267241358757019\n",
      "1439/2105 [===================>..........] - ETA: 14:01 - accuracy: 0.9267 - loss: 0.1883\n",
      " training set -> batch:1440 loss:0.18945632874965668 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1440 val loss:0.20005373656749725 and val acc: 0.9197247624397278\n",
      "1440/2105 [===================>..........] - ETA: 14:03 - accuracy: 0.9270 - loss: 0.1895\n",
      " training set -> batch:1441 loss:0.19045208394527435 and acc: 0.9225663542747498\n",
      "1441/2105 [===================>..........] - ETA: 14:01 - accuracy: 0.9226 - loss: 0.1905\n",
      " training set -> batch:1442 loss:0.2037239372730255 and acc: 0.9220085740089417\n",
      "1442/2105 [===================>..........] - ETA: 14:00 - accuracy: 0.9220 - loss: 0.2037\n",
      " training set -> batch:1443 loss:0.19726687669754028 and acc: 0.9235537052154541\n",
      "1443/2105 [===================>..........] - ETA: 13:58 - accuracy: 0.9236 - loss: 0.1973\n",
      " training set -> batch:1444 loss:0.18892334401607513 and acc: 0.9259999990463257\n",
      "1444/2105 [===================>..........] - ETA: 13:56 - accuracy: 0.9260 - loss: 0.1889\n",
      " training set -> batch:1445 loss:0.1827598363161087 and acc: 0.9273256063461304\n",
      "1445/2105 [===================>..........] - ETA: 13:55 - accuracy: 0.9273 - loss: 0.1828\n",
      " training set -> batch:1446 loss:0.18763238191604614 and acc: 0.9229323267936707\n",
      "1446/2105 [===================>..........] - ETA: 13:53 - accuracy: 0.9229 - loss: 0.1876\n",
      " training set -> batch:1447 loss:0.1855562925338745 and acc: 0.9242700934410095\n",
      "1447/2105 [===================>..........] - ETA: 13:52 - accuracy: 0.9243 - loss: 0.1856\n",
      " training set -> batch:1448 loss:0.18114303052425385 and acc: 0.9264184236526489\n",
      "1448/2105 [===================>..........] - ETA: 13:50 - accuracy: 0.9264 - loss: 0.1811\n",
      " training set -> batch:1449 loss:0.18041589856147766 and acc: 0.9267241358757019\n",
      "1449/2105 [===================>..........] - ETA: 13:48 - accuracy: 0.9267 - loss: 0.1804\n",
      " training set -> batch:1450 loss:0.17605118453502655 and acc: 0.9278523325920105\n",
      "\n",
      " validation set -> batch:1450 val loss:0.19965779781341553 and val acc: 0.9231651425361633\n",
      "1450/2105 [===================>..........] - ETA: 13:50 - accuracy: 0.9279 - loss: 0.1761\n",
      " training set -> batch:1451 loss:0.19035524129867554 and acc: 0.9258849620819092\n",
      "1451/2105 [===================>..........] - ETA: 13:49 - accuracy: 0.9259 - loss: 0.1904\n",
      " training set -> batch:1452 loss:0.18024569749832153 and acc: 0.9284188151359558\n",
      "1452/2105 [===================>..........] - ETA: 13:47 - accuracy: 0.9284 - loss: 0.1802\n",
      " training set -> batch:1453 loss:0.17618025839328766 and acc: 0.9297520518302917\n",
      "1453/2105 [===================>..........] - ETA: 13:45 - accuracy: 0.9298 - loss: 0.1762\n",
      " training set -> batch:1454 loss:0.18418429791927338 and acc: 0.9279999732971191\n",
      "1454/2105 [===================>..........] - ETA: 13:44 - accuracy: 0.9280 - loss: 0.1842\n",
      " training set -> batch:1455 loss:0.18583433330059052 and acc: 0.9282945990562439\n",
      "1455/2105 [===================>..........] - ETA: 13:42 - accuracy: 0.9283 - loss: 0.1858\n",
      " training set -> batch:1456 loss:0.17736203968524933 and acc: 0.9304511547088623\n",
      "1456/2105 [===================>..........] - ETA: 13:41 - accuracy: 0.9305 - loss: 0.1774\n",
      " training set -> batch:1457 loss:0.1863621026277542 and acc: 0.9279196858406067\n",
      "1457/2105 [===================>..........] - ETA: 13:39 - accuracy: 0.9279 - loss: 0.1864\n",
      " training set -> batch:1458 loss:0.18074780702590942 and acc: 0.9290780425071716\n",
      "1458/2105 [===================>..........] - ETA: 13:37 - accuracy: 0.9291 - loss: 0.1807\n",
      " training set -> batch:1459 loss:0.17705853283405304 and acc: 0.9301724433898926\n",
      "1459/2105 [===================>..........] - ETA: 13:36 - accuracy: 0.9302 - loss: 0.1771\n",
      " training set -> batch:1460 loss:0.18000781536102295 and acc: 0.9295302033424377\n",
      "\n",
      " validation set -> batch:1460 val loss:0.2175147831439972 and val acc: 0.9151375889778137\n",
      "1460/2105 [===================>..........] - ETA: 13:38 - accuracy: 0.9295 - loss: 0.1800\n",
      " training set -> batch:1461 loss:0.22341744601726532 and acc: 0.9137167930603027\n",
      "1461/2105 [===================>..........] - ETA: 13:36 - accuracy: 0.9137 - loss: 0.2234\n",
      " training set -> batch:1462 loss:0.21142606437206268 and acc: 0.9166666865348816\n",
      "1462/2105 [===================>..........] - ETA: 13:34 - accuracy: 0.9167 - loss: 0.2114\n",
      " training set -> batch:1463 loss:0.20323634147644043 and acc: 0.9183884263038635\n",
      "1463/2105 [===================>..........] - ETA: 13:33 - accuracy: 0.9184 - loss: 0.2032\n",
      " training set -> batch:1464 loss:0.20876836776733398 and acc: 0.9169999957084656\n",
      "1464/2105 [===================>..........] - ETA: 13:31 - accuracy: 0.9170 - loss: 0.2088\n",
      " training set -> batch:1465 loss:0.21082687377929688 and acc: 0.9176356792449951\n",
      "1465/2105 [===================>..........] - ETA: 13:30 - accuracy: 0.9176 - loss: 0.2108\n",
      " training set -> batch:1466 loss:0.20881299674510956 and acc: 0.9182330965995789\n",
      "1466/2105 [===================>..........] - ETA: 13:28 - accuracy: 0.9182 - loss: 0.2088\n",
      " training set -> batch:1467 loss:0.20868274569511414 and acc: 0.9178832173347473\n",
      "1467/2105 [===================>..........] - ETA: 13:26 - accuracy: 0.9179 - loss: 0.2087\n",
      " training set -> batch:1468 loss:0.21091628074645996 and acc: 0.9166666865348816\n",
      "1468/2105 [===================>..........] - ETA: 13:25 - accuracy: 0.9167 - loss: 0.2109\n",
      " training set -> batch:1469 loss:0.22021861374378204 and acc: 0.915517270565033\n",
      "1469/2105 [===================>..........] - ETA: 13:23 - accuracy: 0.9155 - loss: 0.2202\n",
      " training set -> batch:1470 loss:0.21441809833049774 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1470 val loss:0.202394038438797 and val acc: 0.9197247624397278\n",
      "1470/2105 [===================>..........] - ETA: 13:25 - accuracy: 0.9169 - loss: 0.2144\n",
      " training set -> batch:1471 loss:0.20582665503025055 and acc: 0.9203540086746216\n",
      "1471/2105 [===================>..........] - ETA: 13:23 - accuracy: 0.9204 - loss: 0.2058\n",
      " training set -> batch:1472 loss:0.205180361866951 and acc: 0.9209401607513428\n",
      "1472/2105 [===================>..........] - ETA: 13:22 - accuracy: 0.9209 - loss: 0.2052\n",
      " training set -> batch:1473 loss:0.19805367290973663 and acc: 0.9235537052154541\n",
      "1473/2105 [===================>..........] - ETA: 13:20 - accuracy: 0.9236 - loss: 0.1981\n",
      " training set -> batch:1474 loss:0.19518277049064636 and acc: 0.9240000247955322\n",
      "1474/2105 [====================>.........] - ETA: 13:19 - accuracy: 0.9240 - loss: 0.1952\n",
      " training set -> batch:1475 loss:0.1985643059015274 and acc: 0.9234496355056763\n",
      "1475/2105 [====================>.........] - ETA: 13:17 - accuracy: 0.9234 - loss: 0.1986\n",
      " training set -> batch:1476 loss:0.19555780291557312 and acc: 0.923872172832489\n",
      "1476/2105 [====================>.........] - ETA: 13:15 - accuracy: 0.9239 - loss: 0.1956\n",
      " training set -> batch:1477 loss:0.19063952565193176 and acc: 0.9260948896408081\n",
      "1477/2105 [====================>.........] - ETA: 13:14 - accuracy: 0.9261 - loss: 0.1906\n",
      " training set -> batch:1478 loss:0.18817009031772614 and acc: 0.9264184236526489\n",
      "1478/2105 [====================>.........] - ETA: 13:12 - accuracy: 0.9264 - loss: 0.1882\n",
      " training set -> batch:1479 loss:0.18776783347129822 and acc: 0.9267241358757019\n",
      "1479/2105 [====================>.........] - ETA: 13:11 - accuracy: 0.9267 - loss: 0.1878\n",
      " training set -> batch:1480 loss:0.18563492596149445 and acc: 0.9278523325920105\n",
      "\n",
      " validation set -> batch:1480 val loss:0.20300805568695068 and val acc: 0.9185779690742493\n",
      "1480/2105 [====================>.........] - ETA: 13:12 - accuracy: 0.9279 - loss: 0.1856\n",
      " training set -> batch:1481 loss:0.19720254838466644 and acc: 0.9203540086746216\n",
      "1481/2105 [====================>.........] - ETA: 13:11 - accuracy: 0.9204 - loss: 0.1972\n",
      " training set -> batch:1482 loss:0.19472405314445496 and acc: 0.9198718070983887\n",
      "1482/2105 [====================>.........] - ETA: 13:09 - accuracy: 0.9199 - loss: 0.1947\n",
      " training set -> batch:1483 loss:0.19888897240161896 and acc: 0.9183884263038635\n",
      "1483/2105 [====================>.........] - ETA: 13:08 - accuracy: 0.9184 - loss: 0.1989\n",
      " training set -> batch:1484 loss:0.1937582939863205 and acc: 0.9200000166893005\n",
      "1484/2105 [====================>.........] - ETA: 13:06 - accuracy: 0.9200 - loss: 0.1938\n",
      " training set -> batch:1485 loss:0.1948588490486145 and acc: 0.9186046719551086\n",
      "1485/2105 [====================>.........] - ETA: 13:04 - accuracy: 0.9186 - loss: 0.1949\n",
      " training set -> batch:1486 loss:0.18841025233268738 and acc: 0.9210526347160339\n",
      "1486/2105 [====================>.........] - ETA: 13:03 - accuracy: 0.9211 - loss: 0.1884\n",
      " training set -> batch:1487 loss:0.18278667330741882 and acc: 0.9233576655387878\n",
      "1487/2105 [====================>.........] - ETA: 13:01 - accuracy: 0.9234 - loss: 0.1828\n",
      " training set -> batch:1488 loss:0.18801626563072205 and acc: 0.9228723645210266\n",
      "1488/2105 [====================>.........] - ETA: 13:00 - accuracy: 0.9229 - loss: 0.1880\n",
      " training set -> batch:1489 loss:0.18324926495552063 and acc: 0.9241379499435425\n",
      "1489/2105 [====================>.........] - ETA: 12:58 - accuracy: 0.9241 - loss: 0.1832\n",
      " training set -> batch:1490 loss:0.18303394317626953 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1490 val loss:0.2212374359369278 and val acc: 0.9082568883895874\n",
      "1490/2105 [====================>.........] - ETA: 13:00 - accuracy: 0.9237 - loss: 0.1830\n",
      " training set -> batch:1491 loss:0.21402516961097717 and acc: 0.9092920422554016\n",
      "1491/2105 [====================>.........] - ETA: 12:58 - accuracy: 0.9093 - loss: 0.2140\n",
      " training set -> batch:1492 loss:0.20782434940338135 and acc: 0.9113247990608215\n",
      "1492/2105 [====================>.........] - ETA: 12:56 - accuracy: 0.9113 - loss: 0.2078\n",
      " training set -> batch:1493 loss:0.20355401933193207 and acc: 0.9121900796890259\n",
      "1493/2105 [====================>.........] - ETA: 12:55 - accuracy: 0.9122 - loss: 0.2036\n",
      " training set -> batch:1494 loss:0.21770529448986053 and acc: 0.9100000262260437\n",
      "1494/2105 [====================>.........] - ETA: 12:53 - accuracy: 0.9100 - loss: 0.2177\n",
      " training set -> batch:1495 loss:0.22462664544582367 and acc: 0.9079457521438599\n",
      "1495/2105 [====================>.........] - ETA: 12:52 - accuracy: 0.9079 - loss: 0.2246\n",
      " training set -> batch:1496 loss:0.21785783767700195 and acc: 0.9097744226455688\n",
      "1496/2105 [====================>.........] - ETA: 12:50 - accuracy: 0.9098 - loss: 0.2179\n",
      " training set -> batch:1497 loss:0.21479003131389618 and acc: 0.9114963412284851\n",
      "1497/2105 [====================>.........] - ETA: 12:49 - accuracy: 0.9115 - loss: 0.2148\n",
      " training set -> batch:1498 loss:0.20834940671920776 and acc: 0.9140070676803589\n",
      "1498/2105 [====================>.........] - ETA: 12:47 - accuracy: 0.9140 - loss: 0.2083\n",
      " training set -> batch:1499 loss:0.201449453830719 and acc: 0.9163793325424194\n",
      "1499/2105 [====================>.........] - ETA: 12:45 - accuracy: 0.9164 - loss: 0.2014\n",
      " training set -> batch:1500 loss:0.20108990371227264 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1500 val loss:0.22016198933124542 and val acc: 0.9105504751205444\n",
      "1500/2105 [====================>.........] - ETA: 12:47 - accuracy: 0.9169 - loss: 0.2011\n",
      " training set -> batch:1501 loss:0.21352487802505493 and acc: 0.9126105904579163\n",
      "1501/2105 [====================>.........] - ETA: 12:45 - accuracy: 0.9126 - loss: 0.2135\n",
      " training set -> batch:1502 loss:0.20349310338497162 and acc: 0.9155982732772827\n",
      "1502/2105 [====================>.........] - ETA: 12:44 - accuracy: 0.9156 - loss: 0.2035\n",
      " training set -> batch:1503 loss:0.19378601014614105 and acc: 0.9183884263038635\n",
      "1503/2105 [====================>.........] - ETA: 12:42 - accuracy: 0.9184 - loss: 0.1938\n",
      " training set -> batch:1504 loss:0.20253343880176544 and acc: 0.9179999828338623\n",
      "1504/2105 [====================>.........] - ETA: 12:41 - accuracy: 0.9180 - loss: 0.2025\n",
      " training set -> batch:1505 loss:0.20523099601268768 and acc: 0.9166666865348816\n",
      "1505/2105 [====================>.........] - ETA: 12:39 - accuracy: 0.9167 - loss: 0.2052\n",
      " training set -> batch:1506 loss:0.19680282473564148 and acc: 0.9191729426383972\n",
      "1506/2105 [====================>.........] - ETA: 12:38 - accuracy: 0.9192 - loss: 0.1968\n",
      " training set -> batch:1507 loss:0.18942221999168396 and acc: 0.9215328693389893\n",
      "1507/2105 [====================>.........] - ETA: 12:36 - accuracy: 0.9215 - loss: 0.1894\n",
      " training set -> batch:1508 loss:0.18332932889461517 and acc: 0.9228723645210266\n",
      "1508/2105 [====================>.........] - ETA: 12:34 - accuracy: 0.9229 - loss: 0.1833\n",
      " training set -> batch:1509 loss:0.18300670385360718 and acc: 0.923275887966156\n",
      "1509/2105 [====================>.........] - ETA: 12:33 - accuracy: 0.9233 - loss: 0.1830\n",
      " training set -> batch:1510 loss:0.19651174545288086 and acc: 0.9194630980491638\n",
      "\n",
      " validation set -> batch:1510 val loss:0.2284109890460968 and val acc: 0.911697268486023\n",
      "1510/2105 [====================>.........] - ETA: 12:34 - accuracy: 0.9195 - loss: 0.1965\n",
      " training set -> batch:1511 loss:0.21791791915893555 and acc: 0.9137167930603027\n",
      "1511/2105 [====================>.........] - ETA: 12:33 - accuracy: 0.9137 - loss: 0.2179\n",
      " training set -> batch:1512 loss:0.22339484095573425 and acc: 0.9102563858032227\n",
      "1512/2105 [====================>.........] - ETA: 12:31 - accuracy: 0.9103 - loss: 0.2234\n",
      " training set -> batch:1513 loss:0.22964005172252655 and acc: 0.9090909361839294\n",
      "1513/2105 [====================>.........] - ETA: 12:30 - accuracy: 0.9091 - loss: 0.2296\n",
      " training set -> batch:1514 loss:0.2262999415397644 and acc: 0.9100000262260437\n",
      "1514/2105 [====================>.........] - ETA: 12:28 - accuracy: 0.9100 - loss: 0.2263\n",
      " training set -> batch:1515 loss:0.2206721305847168 and acc: 0.9108527302742004\n",
      "1515/2105 [====================>.........] - ETA: 12:26 - accuracy: 0.9109 - loss: 0.2207\n",
      " training set -> batch:1516 loss:0.2244107723236084 and acc: 0.9107142686843872\n",
      "1516/2105 [====================>.........] - ETA: 12:25 - accuracy: 0.9107 - loss: 0.2244\n",
      " training set -> batch:1517 loss:0.22237594425678253 and acc: 0.9114963412284851\n",
      "1517/2105 [====================>.........] - ETA: 12:23 - accuracy: 0.9115 - loss: 0.2224\n",
      " training set -> batch:1518 loss:0.21656347811222076 and acc: 0.9140070676803589\n",
      "1518/2105 [====================>.........] - ETA: 12:22 - accuracy: 0.9140 - loss: 0.2166\n",
      " training set -> batch:1519 loss:0.21287326514720917 and acc: 0.9163793325424194\n",
      "1519/2105 [====================>.........] - ETA: 12:20 - accuracy: 0.9164 - loss: 0.2129\n",
      " training set -> batch:1520 loss:0.21160531044006348 and acc: 0.9161073565483093\n",
      "\n",
      " validation set -> batch:1520 val loss:0.2029968798160553 and val acc: 0.9208715558052063\n",
      "1520/2105 [====================>.........] - ETA: 12:22 - accuracy: 0.9161 - loss: 0.2116\n",
      " training set -> batch:1521 loss:0.1983562558889389 and acc: 0.9214601516723633\n",
      "1521/2105 [====================>.........] - ETA: 12:20 - accuracy: 0.9215 - loss: 0.1984\n",
      " training set -> batch:1522 loss:0.1959577351808548 and acc: 0.9209401607513428\n",
      "1522/2105 [====================>.........] - ETA: 12:19 - accuracy: 0.9209 - loss: 0.1960\n",
      " training set -> batch:1523 loss:0.19230909645557404 and acc: 0.922520637512207\n",
      "1523/2105 [====================>.........] - ETA: 12:17 - accuracy: 0.9225 - loss: 0.1923\n",
      " training set -> batch:1524 loss:0.18905285000801086 and acc: 0.9240000247955322\n",
      "1524/2105 [====================>.........] - ETA: 12:15 - accuracy: 0.9240 - loss: 0.1891\n",
      " training set -> batch:1525 loss:0.19177290797233582 and acc: 0.9224806427955627\n",
      "1525/2105 [====================>.........] - ETA: 12:14 - accuracy: 0.9225 - loss: 0.1918\n",
      " training set -> batch:1526 loss:0.2025158703327179 and acc: 0.9201127886772156\n",
      "1526/2105 [====================>.........] - ETA: 12:12 - accuracy: 0.9201 - loss: 0.2025\n",
      " training set -> batch:1527 loss:0.2001848965883255 and acc: 0.9215328693389893\n",
      "1527/2105 [====================>.........] - ETA: 12:11 - accuracy: 0.9215 - loss: 0.2002\n",
      " training set -> batch:1528 loss:0.19417427480220795 and acc: 0.923758864402771\n",
      "1528/2105 [====================>.........] - ETA: 12:09 - accuracy: 0.9238 - loss: 0.1942\n",
      " training set -> batch:1529 loss:0.18961593508720398 and acc: 0.925000011920929\n",
      "1529/2105 [====================>.........] - ETA: 12:08 - accuracy: 0.9250 - loss: 0.1896\n",
      " training set -> batch:1530 loss:0.1952468752861023 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1530 val loss:0.2160487174987793 and val acc: 0.9162843823432922\n",
      "1530/2105 [====================>.........] - ETA: 12:09 - accuracy: 0.9220 - loss: 0.1952\n",
      " training set -> batch:1531 loss:0.216792032122612 and acc: 0.9137167930603027\n",
      "1531/2105 [====================>.........] - ETA: 12:08 - accuracy: 0.9137 - loss: 0.2168\n",
      " training set -> batch:1532 loss:0.22198230028152466 and acc: 0.9134615659713745\n",
      "1532/2105 [====================>.........] - ETA: 12:06 - accuracy: 0.9135 - loss: 0.2220\n",
      " training set -> batch:1533 loss:0.2238936871290207 and acc: 0.913223147392273\n",
      "1533/2105 [====================>.........] - ETA: 12:04 - accuracy: 0.9132 - loss: 0.2239\n",
      " training set -> batch:1534 loss:0.21570618450641632 and acc: 0.9160000085830688\n",
      "1534/2105 [====================>.........] - ETA: 12:03 - accuracy: 0.9160 - loss: 0.2157\n",
      " training set -> batch:1535 loss:0.2112344205379486 and acc: 0.9176356792449951\n",
      "1535/2105 [====================>.........] - ETA: 12:01 - accuracy: 0.9176 - loss: 0.2112\n",
      " training set -> batch:1536 loss:0.2069641798734665 and acc: 0.9191729426383972\n",
      "1536/2105 [====================>.........] - ETA: 12:00 - accuracy: 0.9192 - loss: 0.2070\n",
      " training set -> batch:1537 loss:0.21047231554985046 and acc: 0.918795645236969\n",
      "1537/2105 [====================>.........] - ETA: 11:58 - accuracy: 0.9188 - loss: 0.2105\n",
      " training set -> batch:1538 loss:0.20808625221252441 and acc: 0.9193262457847595\n",
      "1538/2105 [====================>.........] - ETA: 11:57 - accuracy: 0.9193 - loss: 0.2081\n",
      " training set -> batch:1539 loss:0.20550790429115295 and acc: 0.9198275804519653\n",
      "1539/2105 [====================>.........] - ETA: 11:55 - accuracy: 0.9198 - loss: 0.2055\n",
      " training set -> batch:1540 loss:0.20221491158008575 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1540 val loss:0.20372137427330017 and val acc: 0.9185779690742493\n",
      "1540/2105 [====================>.........] - ETA: 11:56 - accuracy: 0.9211 - loss: 0.2022\n",
      " training set -> batch:1541 loss:0.21618318557739258 and acc: 0.9159291982650757\n",
      "1541/2105 [====================>.........] - ETA: 11:55 - accuracy: 0.9159 - loss: 0.2162\n",
      " training set -> batch:1542 loss:0.22253845632076263 and acc: 0.9155982732772827\n",
      "1542/2105 [====================>.........] - ETA: 11:53 - accuracy: 0.9156 - loss: 0.2225\n",
      " training set -> batch:1543 loss:0.2237991988658905 and acc: 0.91425621509552\n",
      "1543/2105 [====================>.........] - ETA: 11:52 - accuracy: 0.9143 - loss: 0.2238\n",
      " training set -> batch:1544 loss:0.2168329358100891 and acc: 0.9160000085830688\n",
      "1544/2105 [=====================>........] - ETA: 11:50 - accuracy: 0.9160 - loss: 0.2168\n",
      " training set -> batch:1545 loss:0.21620607376098633 and acc: 0.9166666865348816\n",
      "1545/2105 [=====================>........] - ETA: 11:49 - accuracy: 0.9167 - loss: 0.2162\n",
      " training set -> batch:1546 loss:0.2261936217546463 and acc: 0.9144737124443054\n",
      "1546/2105 [=====================>........] - ETA: 11:47 - accuracy: 0.9145 - loss: 0.2262\n",
      " training set -> batch:1547 loss:0.2213318943977356 and acc: 0.9160584211349487\n",
      "1547/2105 [=====================>........] - ETA: 11:46 - accuracy: 0.9161 - loss: 0.2213\n",
      " training set -> batch:1548 loss:0.2164309173822403 and acc: 0.917553186416626\n",
      "1548/2105 [=====================>........] - ETA: 11:44 - accuracy: 0.9176 - loss: 0.2164\n",
      " training set -> batch:1549 loss:0.21404728293418884 and acc: 0.9181034564971924\n",
      "1549/2105 [=====================>........] - ETA: 11:42 - accuracy: 0.9181 - loss: 0.2140\n",
      " training set -> batch:1550 loss:0.2155665010213852 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1550 val loss:0.2065993994474411 and val acc: 0.9185779690742493\n",
      "1550/2105 [=====================>........] - ETA: 11:44 - accuracy: 0.9186 - loss: 0.2156\n",
      " training set -> batch:1551 loss:0.20146355032920837 and acc: 0.9203540086746216\n",
      "1551/2105 [=====================>........] - ETA: 11:42 - accuracy: 0.9204 - loss: 0.2015\n",
      " training set -> batch:1552 loss:0.20651395618915558 and acc: 0.9188033938407898\n",
      "1552/2105 [=====================>........] - ETA: 11:41 - accuracy: 0.9188 - loss: 0.2065\n",
      " training set -> batch:1553 loss:0.20132112503051758 and acc: 0.9204545617103577\n",
      "1553/2105 [=====================>........] - ETA: 11:39 - accuracy: 0.9205 - loss: 0.2013\n",
      " training set -> batch:1554 loss:0.2083025872707367 and acc: 0.9179999828338623\n",
      "1554/2105 [=====================>........] - ETA: 11:38 - accuracy: 0.9180 - loss: 0.2083\n",
      " training set -> batch:1555 loss:0.20520642399787903 and acc: 0.9176356792449951\n",
      "1555/2105 [=====================>........] - ETA: 11:36 - accuracy: 0.9176 - loss: 0.2052\n",
      " training set -> batch:1556 loss:0.20518572628498077 and acc: 0.9182330965995789\n",
      "1556/2105 [=====================>........] - ETA: 11:34 - accuracy: 0.9182 - loss: 0.2052\n",
      " training set -> batch:1557 loss:0.20773717761039734 and acc: 0.918795645236969\n",
      "1557/2105 [=====================>........] - ETA: 11:33 - accuracy: 0.9188 - loss: 0.2077\n",
      " training set -> batch:1558 loss:0.2086099237203598 and acc: 0.917553186416626\n",
      "1558/2105 [=====================>........] - ETA: 11:31 - accuracy: 0.9176 - loss: 0.2086\n",
      " training set -> batch:1559 loss:0.20480500161647797 and acc: 0.9189655184745789\n",
      "1559/2105 [=====================>........] - ETA: 11:30 - accuracy: 0.9190 - loss: 0.2048\n",
      " training set -> batch:1560 loss:0.20127816498279572 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1560 val loss:0.22799551486968994 and val acc: 0.9151375889778137\n",
      "1560/2105 [=====================>........] - ETA: 11:31 - accuracy: 0.9203 - loss: 0.2013\n",
      " training set -> batch:1561 loss:0.22216463088989258 and acc: 0.9148229956626892\n",
      "1561/2105 [=====================>........] - ETA: 11:30 - accuracy: 0.9148 - loss: 0.2222\n",
      " training set -> batch:1562 loss:0.22250793874263763 and acc: 0.9145299196243286\n",
      "1562/2105 [=====================>........] - ETA: 11:28 - accuracy: 0.9145 - loss: 0.2225\n",
      " training set -> batch:1563 loss:0.21980196237564087 and acc: 0.9152892827987671\n",
      "1563/2105 [=====================>........] - ETA: 11:26 - accuracy: 0.9153 - loss: 0.2198\n",
      " training set -> batch:1564 loss:0.21107634902000427 and acc: 0.9179999828338623\n",
      "1564/2105 [=====================>........] - ETA: 11:25 - accuracy: 0.9180 - loss: 0.2111\n",
      " training set -> batch:1565 loss:0.2105577439069748 and acc: 0.9186046719551086\n",
      "1565/2105 [=====================>........] - ETA: 11:23 - accuracy: 0.9186 - loss: 0.2106\n",
      " training set -> batch:1566 loss:0.20700573921203613 and acc: 0.9201127886772156\n",
      "1566/2105 [=====================>........] - ETA: 11:22 - accuracy: 0.9201 - loss: 0.2070\n",
      " training set -> batch:1567 loss:0.2029045671224594 and acc: 0.9215328693389893\n",
      "1567/2105 [=====================>........] - ETA: 11:20 - accuracy: 0.9215 - loss: 0.2029\n",
      " training set -> batch:1568 loss:0.19942642748355865 and acc: 0.9228723645210266\n",
      "1568/2105 [=====================>........] - ETA: 11:19 - accuracy: 0.9229 - loss: 0.1994\n",
      " training set -> batch:1569 loss:0.20596131682395935 and acc: 0.9198275804519653\n",
      "1569/2105 [=====================>........] - ETA: 11:17 - accuracy: 0.9198 - loss: 0.2060\n",
      " training set -> batch:1570 loss:0.20170485973358154 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1570 val loss:0.20827458798885345 and val acc: 0.9220183491706848\n",
      "1570/2105 [=====================>........] - ETA: 11:18 - accuracy: 0.9220 - loss: 0.2017\n",
      " training set -> batch:1571 loss:0.19649241864681244 and acc: 0.9247787594795227\n",
      "1571/2105 [=====================>........] - ETA: 11:17 - accuracy: 0.9248 - loss: 0.1965\n",
      " training set -> batch:1572 loss:0.19515466690063477 and acc: 0.9241452813148499\n",
      "1572/2105 [=====================>........] - ETA: 11:15 - accuracy: 0.9241 - loss: 0.1952\n",
      " training set -> batch:1573 loss:0.20188447833061218 and acc: 0.9245867729187012\n",
      "1573/2105 [=====================>........] - ETA: 11:14 - accuracy: 0.9246 - loss: 0.2019\n",
      " training set -> batch:1574 loss:0.20497089624404907 and acc: 0.9240000247955322\n",
      "1574/2105 [=====================>........] - ETA: 11:12 - accuracy: 0.9240 - loss: 0.2050\n",
      " training set -> batch:1575 loss:0.2033957988023758 and acc: 0.9244186282157898\n",
      "1575/2105 [=====================>........] - ETA: 11:11 - accuracy: 0.9244 - loss: 0.2034\n",
      " training set -> batch:1576 loss:0.2017231434583664 and acc: 0.9248120188713074\n",
      "1576/2105 [=====================>........] - ETA: 11:09 - accuracy: 0.9248 - loss: 0.2017\n",
      " training set -> batch:1577 loss:0.19962827861309052 and acc: 0.9251824617385864\n",
      "1577/2105 [=====================>........] - ETA: 11:08 - accuracy: 0.9252 - loss: 0.1996\n",
      " training set -> batch:1578 loss:0.19485177099704742 and acc: 0.9255319237709045\n",
      "1578/2105 [=====================>........] - ETA: 11:06 - accuracy: 0.9255 - loss: 0.1949\n",
      " training set -> batch:1579 loss:0.19901488721370697 and acc: 0.925000011920929\n",
      "1579/2105 [=====================>........] - ETA: 11:05 - accuracy: 0.9250 - loss: 0.1990\n",
      " training set -> batch:1580 loss:0.19505274295806885 and acc: 0.926174521446228\n",
      "\n",
      " validation set -> batch:1580 val loss:0.20192880928516388 and val acc: 0.9220183491706848\n",
      "1580/2105 [=====================>........] - ETA: 11:06 - accuracy: 0.9262 - loss: 0.1951\n",
      " training set -> batch:1581 loss:0.20884092152118683 and acc: 0.9214601516723633\n",
      "1581/2105 [=====================>........] - ETA: 11:04 - accuracy: 0.9215 - loss: 0.2088\n",
      " training set -> batch:1582 loss:0.2042209357023239 and acc: 0.9220085740089417\n",
      "1582/2105 [=====================>........] - ETA: 11:03 - accuracy: 0.9220 - loss: 0.2042\n",
      " training set -> batch:1583 loss:0.195597305893898 and acc: 0.9245867729187012\n",
      "1583/2105 [=====================>........] - ETA: 11:01 - accuracy: 0.9246 - loss: 0.1956\n",
      " training set -> batch:1584 loss:0.19038639962673187 and acc: 0.925000011920929\n",
      "1584/2105 [=====================>........] - ETA: 11:00 - accuracy: 0.9250 - loss: 0.1904\n",
      " training set -> batch:1585 loss:0.1853823959827423 and acc: 0.9263566136360168\n",
      "1585/2105 [=====================>........] - ETA: 10:58 - accuracy: 0.9264 - loss: 0.1854\n",
      " training set -> batch:1586 loss:0.1889517456293106 and acc: 0.9257518649101257\n",
      "1586/2105 [=====================>........] - ETA: 10:57 - accuracy: 0.9258 - loss: 0.1890\n",
      " training set -> batch:1587 loss:0.1850719004869461 and acc: 0.9270073175430298\n",
      "1587/2105 [=====================>........] - ETA: 10:55 - accuracy: 0.9270 - loss: 0.1851\n",
      " training set -> batch:1588 loss:0.18273860216140747 and acc: 0.9273049831390381\n",
      "1588/2105 [=====================>........] - ETA: 10:54 - accuracy: 0.9273 - loss: 0.1827\n",
      " training set -> batch:1589 loss:0.1879080832004547 and acc: 0.9258620738983154\n",
      "1589/2105 [=====================>........] - ETA: 10:52 - accuracy: 0.9259 - loss: 0.1879\n",
      " training set -> batch:1590 loss:0.18611650168895721 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1590 val loss:0.19259847700595856 and val acc: 0.9208715558052063\n",
      "1590/2105 [=====================>........] - ETA: 10:53 - accuracy: 0.9270 - loss: 0.1861\n",
      " training set -> batch:1591 loss:0.19993911683559418 and acc: 0.9203540086746216\n",
      "1591/2105 [=====================>........] - ETA: 10:52 - accuracy: 0.9204 - loss: 0.1999\n",
      " training set -> batch:1592 loss:0.19366693496704102 and acc: 0.9209401607513428\n",
      "1592/2105 [=====================>........] - ETA: 10:50 - accuracy: 0.9209 - loss: 0.1937\n",
      " training set -> batch:1593 loss:0.20691674947738647 and acc: 0.9183884263038635\n",
      "1593/2105 [=====================>........] - ETA: 10:49 - accuracy: 0.9184 - loss: 0.2069\n",
      " training set -> batch:1594 loss:0.19985444843769073 and acc: 0.9200000166893005\n",
      "1594/2105 [=====================>........] - ETA: 10:47 - accuracy: 0.9200 - loss: 0.1999\n",
      " training set -> batch:1595 loss:0.20340538024902344 and acc: 0.9195736646652222\n",
      "1595/2105 [=====================>........] - ETA: 10:46 - accuracy: 0.9196 - loss: 0.2034\n",
      " training set -> batch:1596 loss:0.2062486708164215 and acc: 0.9201127886772156\n",
      "1596/2105 [=====================>........] - ETA: 10:44 - accuracy: 0.9201 - loss: 0.2062\n",
      " training set -> batch:1597 loss:0.2120347023010254 and acc: 0.918795645236969\n",
      "1597/2105 [=====================>........] - ETA: 10:42 - accuracy: 0.9188 - loss: 0.2120\n",
      " training set -> batch:1598 loss:0.21143779158592224 and acc: 0.9193262457847595\n",
      "1598/2105 [=====================>........] - ETA: 10:41 - accuracy: 0.9193 - loss: 0.2114\n",
      " training set -> batch:1599 loss:0.20504486560821533 and acc: 0.9206896424293518\n",
      "1599/2105 [=====================>........] - ETA: 10:39 - accuracy: 0.9207 - loss: 0.2050\n",
      " training set -> batch:1600 loss:0.20427612960338593 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1600 val loss:0.18784518539905548 and val acc: 0.9185779690742493\n",
      "1600/2105 [=====================>........] - ETA: 10:40 - accuracy: 0.9211 - loss: 0.2043\n",
      " training set -> batch:1601 loss:0.18411679565906525 and acc: 0.9181416034698486\n",
      "1601/2105 [=====================>........] - ETA: 10:39 - accuracy: 0.9181 - loss: 0.1841\n",
      " training set -> batch:1602 loss:0.17870284616947174 and acc: 0.9198718070983887\n",
      "1602/2105 [=====================>........] - ETA: 10:37 - accuracy: 0.9199 - loss: 0.1787\n",
      " training set -> batch:1603 loss:0.1785881221294403 and acc: 0.9204545617103577\n",
      "1603/2105 [=====================>........] - ETA: 10:36 - accuracy: 0.9205 - loss: 0.1786\n",
      " training set -> batch:1604 loss:0.17628319561481476 and acc: 0.9200000166893005\n",
      "1604/2105 [=====================>........] - ETA: 10:34 - accuracy: 0.9200 - loss: 0.1763\n",
      " training set -> batch:1605 loss:0.18611204624176025 and acc: 0.9186046719551086\n",
      "1605/2105 [=====================>........] - ETA: 10:33 - accuracy: 0.9186 - loss: 0.1861\n",
      " training set -> batch:1606 loss:0.18576034903526306 and acc: 0.9201127886772156\n",
      "1606/2105 [=====================>........] - ETA: 10:31 - accuracy: 0.9201 - loss: 0.1858\n",
      " training set -> batch:1607 loss:0.18858329951763153 and acc: 0.9197080135345459\n",
      "1607/2105 [=====================>........] - ETA: 10:30 - accuracy: 0.9197 - loss: 0.1886\n",
      " training set -> batch:1608 loss:0.1841202676296234 and acc: 0.9210993051528931\n",
      "1608/2105 [=====================>........] - ETA: 10:28 - accuracy: 0.9211 - loss: 0.1841\n",
      " training set -> batch:1609 loss:0.18222960829734802 and acc: 0.9206896424293518\n",
      "1609/2105 [=====================>........] - ETA: 10:27 - accuracy: 0.9207 - loss: 0.1822\n",
      " training set -> batch:1610 loss:0.18574132025241852 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:1610 val loss:0.1918911188840866 and val acc: 0.9277523159980774\n",
      "1610/2105 [=====================>........] - ETA: 10:28 - accuracy: 0.9203 - loss: 0.1857\n",
      " training set -> batch:1611 loss:0.19305358827114105 and acc: 0.9280973672866821\n",
      "1611/2105 [=====================>........] - ETA: 10:26 - accuracy: 0.9281 - loss: 0.1931\n",
      " training set -> batch:1612 loss:0.20703601837158203 and acc: 0.9252136945724487\n",
      "1612/2105 [=====================>........] - ETA: 10:25 - accuracy: 0.9252 - loss: 0.2070\n",
      " training set -> batch:1613 loss:0.20118914544582367 and acc: 0.9256198406219482\n",
      "1613/2105 [=====================>........] - ETA: 10:23 - accuracy: 0.9256 - loss: 0.2012\n",
      " training set -> batch:1614 loss:0.19642682373523712 and acc: 0.9269999861717224\n",
      "1614/2105 [======================>.......] - ETA: 10:22 - accuracy: 0.9270 - loss: 0.1964\n",
      " training set -> batch:1615 loss:0.18926042318344116 and acc: 0.9292635917663574\n",
      "1615/2105 [======================>.......] - ETA: 10:20 - accuracy: 0.9293 - loss: 0.1893\n",
      " training set -> batch:1616 loss:0.18568357825279236 and acc: 0.9295112490653992\n",
      "1616/2105 [======================>.......] - ETA: 10:19 - accuracy: 0.9295 - loss: 0.1857\n",
      " training set -> batch:1617 loss:0.18803250789642334 and acc: 0.9288321137428284\n",
      "1617/2105 [======================>.......] - ETA: 10:17 - accuracy: 0.9288 - loss: 0.1880\n",
      " training set -> batch:1618 loss:0.1827399581670761 and acc: 0.9308510422706604\n",
      "1618/2105 [======================>.......] - ETA: 10:16 - accuracy: 0.9309 - loss: 0.1827\n",
      " training set -> batch:1619 loss:0.17682208120822906 and acc: 0.932758629322052\n",
      "1619/2105 [======================>.......] - ETA: 10:14 - accuracy: 0.9328 - loss: 0.1768\n",
      " training set -> batch:1620 loss:0.17633284628391266 and acc: 0.9337248206138611\n",
      "\n",
      " validation set -> batch:1620 val loss:0.19469213485717773 and val acc: 0.9231651425361633\n",
      "1620/2105 [======================>.......] - ETA: 10:15 - accuracy: 0.9337 - loss: 0.1763\n",
      " training set -> batch:1621 loss:0.1929393857717514 and acc: 0.9236725568771362\n",
      "1621/2105 [======================>.......] - ETA: 10:14 - accuracy: 0.9237 - loss: 0.1929\n",
      " training set -> batch:1622 loss:0.18785090744495392 and acc: 0.9252136945724487\n",
      "1622/2105 [======================>.......] - ETA: 10:12 - accuracy: 0.9252 - loss: 0.1879\n",
      " training set -> batch:1623 loss:0.18637169897556305 and acc: 0.9256198406219482\n",
      "1623/2105 [======================>.......] - ETA: 10:11 - accuracy: 0.9256 - loss: 0.1864\n",
      " training set -> batch:1624 loss:0.19736360013484955 and acc: 0.9240000247955322\n",
      "1624/2105 [======================>.......] - ETA: 10:09 - accuracy: 0.9240 - loss: 0.1974\n",
      " training set -> batch:1625 loss:0.20733052492141724 and acc: 0.9224806427955627\n",
      "1625/2105 [======================>.......] - ETA: 10:08 - accuracy: 0.9225 - loss: 0.2073\n",
      " training set -> batch:1626 loss:0.20558571815490723 and acc: 0.9219924807548523\n",
      "1626/2105 [======================>.......] - ETA: 10:06 - accuracy: 0.9220 - loss: 0.2056\n",
      " training set -> batch:1627 loss:0.20784196257591248 and acc: 0.9224452376365662\n",
      "1627/2105 [======================>.......] - ETA: 10:05 - accuracy: 0.9224 - loss: 0.2078\n",
      " training set -> batch:1628 loss:0.2067355513572693 and acc: 0.923758864402771\n",
      "1628/2105 [======================>.......] - ETA: 10:03 - accuracy: 0.9238 - loss: 0.2067\n",
      " training set -> batch:1629 loss:0.20796695351600647 and acc: 0.923275887966156\n",
      "1629/2105 [======================>.......] - ETA: 10:02 - accuracy: 0.9233 - loss: 0.2080\n",
      " training set -> batch:1630 loss:0.20627754926681519 and acc: 0.9244966506958008\n",
      "\n",
      " validation set -> batch:1630 val loss:0.19581471383571625 and val acc: 0.9197247624397278\n",
      "1630/2105 [======================>.......] - ETA: 10:03 - accuracy: 0.9245 - loss: 0.2063\n",
      " training set -> batch:1631 loss:0.18700259923934937 and acc: 0.9225663542747498\n",
      "1631/2105 [======================>.......] - ETA: 10:02 - accuracy: 0.9226 - loss: 0.1870\n",
      " training set -> batch:1632 loss:0.18800634145736694 and acc: 0.9220085740089417\n",
      "1632/2105 [======================>.......] - ETA: 10:00 - accuracy: 0.9220 - loss: 0.1880\n",
      " training set -> batch:1633 loss:0.18346628546714783 and acc: 0.9235537052154541\n",
      "1633/2105 [======================>.......] - ETA: 9:59 - accuracy: 0.9236 - loss: 0.1835 \n",
      " training set -> batch:1634 loss:0.19395166635513306 and acc: 0.9229999780654907\n",
      "1634/2105 [======================>.......] - ETA: 9:57 - accuracy: 0.9230 - loss: 0.1940\n",
      " training set -> batch:1635 loss:0.19510908424854279 and acc: 0.9234496355056763\n",
      "1635/2105 [======================>.......] - ETA: 9:56 - accuracy: 0.9234 - loss: 0.1951\n",
      " training set -> batch:1636 loss:0.1908397078514099 and acc: 0.9248120188713074\n",
      "1636/2105 [======================>.......] - ETA: 9:54 - accuracy: 0.9248 - loss: 0.1908\n",
      " training set -> batch:1637 loss:0.1859714537858963 and acc: 0.9270073175430298\n",
      "1637/2105 [======================>.......] - ETA: 9:53 - accuracy: 0.9270 - loss: 0.1860\n",
      " training set -> batch:1638 loss:0.19849957525730133 and acc: 0.9228723645210266\n",
      "1638/2105 [======================>.......] - ETA: 9:51 - accuracy: 0.9229 - loss: 0.1985\n",
      " training set -> batch:1639 loss:0.19847619533538818 and acc: 0.9215517044067383\n",
      "1639/2105 [======================>.......] - ETA: 9:50 - accuracy: 0.9216 - loss: 0.1985\n",
      " training set -> batch:1640 loss:0.20577317476272583 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1640 val loss:0.20195521414279938 and val acc: 0.9185779690742493\n",
      "1640/2105 [======================>.......] - ETA: 9:51 - accuracy: 0.9211 - loss: 0.2058\n",
      " training set -> batch:1641 loss:0.19345000386238098 and acc: 0.9214601516723633\n",
      "1641/2105 [======================>.......] - ETA: 9:49 - accuracy: 0.9215 - loss: 0.1935\n",
      " training set -> batch:1642 loss:0.19086265563964844 and acc: 0.9230769276618958\n",
      "1642/2105 [======================>.......] - ETA: 9:48 - accuracy: 0.9231 - loss: 0.1909\n",
      " training set -> batch:1643 loss:0.18916967511177063 and acc: 0.9235537052154541\n",
      "1643/2105 [======================>.......] - ETA: 9:46 - accuracy: 0.9236 - loss: 0.1892\n",
      " training set -> batch:1644 loss:0.18097497522830963 and acc: 0.9259999990463257\n",
      "1644/2105 [======================>.......] - ETA: 9:45 - accuracy: 0.9260 - loss: 0.1810\n",
      " training set -> batch:1645 loss:0.18556414544582367 and acc: 0.9244186282157898\n",
      "1645/2105 [======================>.......] - ETA: 9:43 - accuracy: 0.9244 - loss: 0.1856\n",
      " training set -> batch:1646 loss:0.18012063205242157 and acc: 0.9257518649101257\n",
      "1646/2105 [======================>.......] - ETA: 9:42 - accuracy: 0.9258 - loss: 0.1801\n",
      " training set -> batch:1647 loss:0.17547447979450226 and acc: 0.9270073175430298\n",
      "1647/2105 [======================>.......] - ETA: 9:40 - accuracy: 0.9270 - loss: 0.1755\n",
      " training set -> batch:1648 loss:0.17733925580978394 and acc: 0.9273049831390381\n",
      "1648/2105 [======================>.......] - ETA: 9:39 - accuracy: 0.9273 - loss: 0.1773\n",
      " training set -> batch:1649 loss:0.17469723522663116 and acc: 0.9284482598304749\n",
      "1649/2105 [======================>.......] - ETA: 9:37 - accuracy: 0.9284 - loss: 0.1747\n",
      " training set -> batch:1650 loss:0.17883038520812988 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1650 val loss:0.2085866779088974 and val acc: 0.9197247624397278\n",
      "1650/2105 [======================>.......] - ETA: 9:38 - accuracy: 0.9253 - loss: 0.1788\n",
      " training set -> batch:1651 loss:0.21094171702861786 and acc: 0.9181416034698486\n",
      "1651/2105 [======================>.......] - ETA: 9:37 - accuracy: 0.9181 - loss: 0.2109\n",
      " training set -> batch:1652 loss:0.2032061666250229 and acc: 0.9188033938407898\n",
      "1652/2105 [======================>.......] - ETA: 9:35 - accuracy: 0.9188 - loss: 0.2032\n",
      " training set -> batch:1653 loss:0.20298059284687042 and acc: 0.9194214940071106\n",
      "1653/2105 [======================>.......] - ETA: 9:34 - accuracy: 0.9194 - loss: 0.2030\n",
      " training set -> batch:1654 loss:0.20336121320724487 and acc: 0.9190000295639038\n",
      "1654/2105 [======================>.......] - ETA: 9:32 - accuracy: 0.9190 - loss: 0.2034\n",
      " training set -> batch:1655 loss:0.20063014328479767 and acc: 0.9205426573753357\n",
      "1655/2105 [======================>.......] - ETA: 9:31 - accuracy: 0.9205 - loss: 0.2006\n",
      " training set -> batch:1656 loss:0.2026124745607376 and acc: 0.9210526347160339\n",
      "1656/2105 [======================>.......] - ETA: 9:29 - accuracy: 0.9211 - loss: 0.2026\n",
      " training set -> batch:1657 loss:0.20499646663665771 and acc: 0.9206204414367676\n",
      "1657/2105 [======================>.......] - ETA: 9:28 - accuracy: 0.9206 - loss: 0.2050\n",
      " training set -> batch:1658 loss:0.20165738463401794 and acc: 0.9210993051528931\n",
      "1658/2105 [======================>.......] - ETA: 9:26 - accuracy: 0.9211 - loss: 0.2017\n",
      " training set -> batch:1659 loss:0.19472865760326385 and acc: 0.923275887966156\n",
      "1659/2105 [======================>.......] - ETA: 9:25 - accuracy: 0.9233 - loss: 0.1947\n",
      " training set -> batch:1660 loss:0.1948879510164261 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1660 val loss:0.24987980723381042 and val acc: 0.9128440618515015\n",
      "1660/2105 [======================>.......] - ETA: 9:25 - accuracy: 0.9237 - loss: 0.1949\n",
      " training set -> batch:1661 loss:0.23827366530895233 and acc: 0.9159291982650757\n",
      "1661/2105 [======================>.......] - ETA: 9:24 - accuracy: 0.9159 - loss: 0.2383\n",
      " training set -> batch:1662 loss:0.22519730031490326 and acc: 0.9188033938407898\n",
      "1662/2105 [======================>.......] - ETA: 9:22 - accuracy: 0.9188 - loss: 0.2252\n",
      " training set -> batch:1663 loss:0.221324622631073 and acc: 0.9194214940071106\n",
      "1663/2105 [======================>.......] - ETA: 9:21 - accuracy: 0.9194 - loss: 0.2213\n",
      " training set -> batch:1664 loss:0.22008036077022552 and acc: 0.9179999828338623\n",
      "1664/2105 [======================>.......] - ETA: 9:19 - accuracy: 0.9180 - loss: 0.2201\n",
      " training set -> batch:1665 loss:0.2148715853691101 and acc: 0.9195736646652222\n",
      "1665/2105 [======================>.......] - ETA: 9:18 - accuracy: 0.9196 - loss: 0.2149\n",
      " training set -> batch:1666 loss:0.21236920356750488 and acc: 0.9201127886772156\n",
      "1666/2105 [======================>.......] - ETA: 9:16 - accuracy: 0.9201 - loss: 0.2124\n",
      " training set -> batch:1667 loss:0.20845308899879456 and acc: 0.9206204414367676\n",
      "1667/2105 [======================>.......] - ETA: 9:15 - accuracy: 0.9206 - loss: 0.2085\n",
      " training set -> batch:1668 loss:0.20074890553951263 and acc: 0.9228723645210266\n",
      "1668/2105 [======================>.......] - ETA: 9:13 - accuracy: 0.9229 - loss: 0.2007\n",
      " training set -> batch:1669 loss:0.20508794486522675 and acc: 0.9206896424293518\n",
      "1669/2105 [======================>.......] - ETA: 9:12 - accuracy: 0.9207 - loss: 0.2051\n",
      " training set -> batch:1670 loss:0.19803881645202637 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1670 val loss:0.2269982546567917 and val acc: 0.91399085521698\n",
      "1670/2105 [======================>.......] - ETA: 9:13 - accuracy: 0.9228 - loss: 0.1980\n",
      " training set -> batch:1671 loss:0.21718496084213257 and acc: 0.9148229956626892\n",
      "1671/2105 [======================>.......] - ETA: 9:11 - accuracy: 0.9148 - loss: 0.2172\n",
      " training set -> batch:1672 loss:0.21366122364997864 and acc: 0.9134615659713745\n",
      "1672/2105 [======================>.......] - ETA: 9:10 - accuracy: 0.9135 - loss: 0.2137\n",
      " training set -> batch:1673 loss:0.21320028603076935 and acc: 0.913223147392273\n",
      "1673/2105 [======================>.......] - ETA: 9:08 - accuracy: 0.9132 - loss: 0.2132\n",
      " training set -> batch:1674 loss:0.21175801753997803 and acc: 0.9139999747276306\n",
      "1674/2105 [======================>.......] - ETA: 9:07 - accuracy: 0.9140 - loss: 0.2118\n",
      " training set -> batch:1675 loss:0.21127741038799286 and acc: 0.9147287011146545\n",
      "1675/2105 [======================>.......] - ETA: 9:05 - accuracy: 0.9147 - loss: 0.2113\n",
      " training set -> batch:1676 loss:0.20960545539855957 and acc: 0.9154135584831238\n",
      "1676/2105 [======================>.......] - ETA: 9:04 - accuracy: 0.9154 - loss: 0.2096\n",
      " training set -> batch:1677 loss:0.2180715799331665 and acc: 0.9142335653305054\n",
      "1677/2105 [======================>.......] - ETA: 9:02 - accuracy: 0.9142 - loss: 0.2181\n",
      " training set -> batch:1678 loss:0.21218076348304749 and acc: 0.914893627166748\n",
      "1678/2105 [======================>.......] - ETA: 9:01 - accuracy: 0.9149 - loss: 0.2122\n",
      " training set -> batch:1679 loss:0.20917776226997375 and acc: 0.9146551489830017\n",
      "1679/2105 [======================>.......] - ETA: 8:59 - accuracy: 0.9147 - loss: 0.2092\n",
      " training set -> batch:1680 loss:0.2177552729845047 and acc: 0.9127516746520996\n",
      "\n",
      " validation set -> batch:1680 val loss:0.27032551169395447 and val acc: 0.89449542760849\n",
      "1680/2105 [======================>.......] - ETA: 9:00 - accuracy: 0.9128 - loss: 0.2178\n",
      " training set -> batch:1681 loss:0.2739887833595276 and acc: 0.8938053250312805\n",
      "1681/2105 [======================>.......] - ETA: 8:58 - accuracy: 0.8938 - loss: 0.2740\n",
      " training set -> batch:1682 loss:0.2647337317466736 and acc: 0.8942307829856873\n",
      "1682/2105 [======================>.......] - ETA: 8:57 - accuracy: 0.8942 - loss: 0.2647\n",
      " training set -> batch:1683 loss:0.25939276814460754 and acc: 0.8956611752510071\n",
      "1683/2105 [======================>.......] - ETA: 8:56 - accuracy: 0.8957 - loss: 0.2594\n",
      " training set -> batch:1684 loss:0.2509514093399048 and acc: 0.8980000019073486\n",
      "1684/2105 [=======================>......] - ETA: 8:54 - accuracy: 0.8980 - loss: 0.2510\n",
      " training set -> batch:1685 loss:0.24173812568187714 and acc: 0.9001938104629517\n",
      "1685/2105 [=======================>......] - ETA: 8:53 - accuracy: 0.9002 - loss: 0.2417\n",
      " training set -> batch:1686 loss:0.23841948807239532 and acc: 0.902255654335022\n",
      "1686/2105 [=======================>......] - ETA: 8:51 - accuracy: 0.9023 - loss: 0.2384\n",
      " training set -> batch:1687 loss:0.23049268126487732 and acc: 0.9051094651222229\n",
      "1687/2105 [=======================>......] - ETA: 8:50 - accuracy: 0.9051 - loss: 0.2305\n",
      " training set -> batch:1688 loss:0.22291125357151031 and acc: 0.9069148898124695\n",
      "1688/2105 [=======================>......] - ETA: 8:48 - accuracy: 0.9069 - loss: 0.2229\n",
      " training set -> batch:1689 loss:0.2182738333940506 and acc: 0.9077585935592651\n",
      "1689/2105 [=======================>......] - ETA: 8:47 - accuracy: 0.9078 - loss: 0.2183\n",
      " training set -> batch:1690 loss:0.21289663016796112 and acc: 0.9093959927558899\n",
      "\n",
      " validation set -> batch:1690 val loss:0.2181091159582138 and val acc: 0.9105504751205444\n",
      "1690/2105 [=======================>......] - ETA: 8:47 - accuracy: 0.9094 - loss: 0.2129\n",
      " training set -> batch:1691 loss:0.21537092328071594 and acc: 0.9115044474601746\n",
      "1691/2105 [=======================>......] - ETA: 8:46 - accuracy: 0.9115 - loss: 0.2154\n",
      " training set -> batch:1692 loss:0.21718548238277435 and acc: 0.9123931527137756\n",
      "1692/2105 [=======================>......] - ETA: 8:44 - accuracy: 0.9124 - loss: 0.2172\n",
      " training set -> batch:1693 loss:0.21133854985237122 and acc: 0.9121900796890259\n",
      "1693/2105 [=======================>......] - ETA: 8:43 - accuracy: 0.9122 - loss: 0.2113\n",
      " training set -> batch:1694 loss:0.20742999017238617 and acc: 0.9139999747276306\n",
      "1694/2105 [=======================>......] - ETA: 8:41 - accuracy: 0.9140 - loss: 0.2074\n",
      " training set -> batch:1695 loss:0.20011265575885773 and acc: 0.9156976938247681\n",
      "1695/2105 [=======================>......] - ETA: 8:40 - accuracy: 0.9157 - loss: 0.2001\n",
      " training set -> batch:1696 loss:0.20146162807941437 and acc: 0.9135338068008423\n",
      "1696/2105 [=======================>......] - ETA: 8:38 - accuracy: 0.9135 - loss: 0.2015\n",
      " training set -> batch:1697 loss:0.19891387224197388 and acc: 0.9142335653305054\n",
      "1697/2105 [=======================>......] - ETA: 8:37 - accuracy: 0.9142 - loss: 0.1989\n",
      " training set -> batch:1698 loss:0.19799160957336426 and acc: 0.9140070676803589\n",
      "1698/2105 [=======================>......] - ETA: 8:36 - accuracy: 0.9140 - loss: 0.1980\n",
      " training set -> batch:1699 loss:0.1981605440378189 and acc: 0.9137930870056152\n",
      "1699/2105 [=======================>......] - ETA: 8:34 - accuracy: 0.9138 - loss: 0.1982\n",
      " training set -> batch:1700 loss:0.19574369490146637 and acc: 0.9144295454025269\n",
      "\n",
      " validation set -> batch:1700 val loss:0.28924301266670227 and val acc: 0.8956422209739685\n",
      "1700/2105 [=======================>......] - ETA: 8:35 - accuracy: 0.9144 - loss: 0.1957\n",
      " training set -> batch:1701 loss:0.28160709142684937 and acc: 0.8971238732337952\n",
      "1701/2105 [=======================>......] - ETA: 8:33 - accuracy: 0.8971 - loss: 0.2816\n",
      " training set -> batch:1702 loss:0.27007782459259033 and acc: 0.8985042572021484\n",
      "1702/2105 [=======================>......] - ETA: 8:32 - accuracy: 0.8985 - loss: 0.2701\n",
      " training set -> batch:1703 loss:0.26911410689353943 and acc: 0.8987603187561035\n",
      "1703/2105 [=======================>......] - ETA: 8:30 - accuracy: 0.8988 - loss: 0.2691\n",
      " training set -> batch:1704 loss:0.26381030678749084 and acc: 0.8999999761581421\n",
      "1704/2105 [=======================>......] - ETA: 8:29 - accuracy: 0.9000 - loss: 0.2638\n",
      " training set -> batch:1705 loss:0.2630465030670166 and acc: 0.9021317958831787\n",
      "1705/2105 [=======================>......] - ETA: 8:27 - accuracy: 0.9021 - loss: 0.2630\n",
      " training set -> batch:1706 loss:0.25315362215042114 and acc: 0.9041353464126587\n",
      "1706/2105 [=======================>......] - ETA: 8:26 - accuracy: 0.9041 - loss: 0.2532\n",
      " training set -> batch:1707 loss:0.24388834834098816 and acc: 0.9060218930244446\n",
      "1707/2105 [=======================>......] - ETA: 8:24 - accuracy: 0.9060 - loss: 0.2439\n",
      " training set -> batch:1708 loss:0.23791879415512085 and acc: 0.9078013896942139\n",
      "1708/2105 [=======================>......] - ETA: 8:23 - accuracy: 0.9078 - loss: 0.2379\n",
      " training set -> batch:1709 loss:0.2295231968164444 and acc: 0.9103448390960693\n",
      "1709/2105 [=======================>......] - ETA: 8:21 - accuracy: 0.9103 - loss: 0.2295\n",
      " training set -> batch:1710 loss:0.23027533292770386 and acc: 0.9102349281311035\n",
      "\n",
      " validation set -> batch:1710 val loss:0.21273890137672424 and val acc: 0.9185779690742493\n",
      "1710/2105 [=======================>......] - ETA: 8:22 - accuracy: 0.9102 - loss: 0.2303\n",
      " training set -> batch:1711 loss:0.22607658803462982 and acc: 0.9159291982650757\n",
      "1711/2105 [=======================>......] - ETA: 8:20 - accuracy: 0.9159 - loss: 0.2261\n",
      " training set -> batch:1712 loss:0.2267080545425415 and acc: 0.9166666865348816\n",
      "1712/2105 [=======================>......] - ETA: 8:19 - accuracy: 0.9167 - loss: 0.2267\n",
      " training set -> batch:1713 loss:0.2229825258255005 and acc: 0.9163222908973694\n",
      "1713/2105 [=======================>......] - ETA: 8:17 - accuracy: 0.9163 - loss: 0.2230\n",
      " training set -> batch:1714 loss:0.22461925446987152 and acc: 0.9160000085830688\n",
      "1714/2105 [=======================>......] - ETA: 8:16 - accuracy: 0.9160 - loss: 0.2246\n",
      " training set -> batch:1715 loss:0.22560232877731323 and acc: 0.9147287011146545\n",
      "1715/2105 [=======================>......] - ETA: 8:15 - accuracy: 0.9147 - loss: 0.2256\n",
      " training set -> batch:1716 loss:0.21926502883434296 and acc: 0.9163534045219421\n",
      "1716/2105 [=======================>......] - ETA: 8:13 - accuracy: 0.9164 - loss: 0.2193\n",
      " training set -> batch:1717 loss:0.21373865008354187 and acc: 0.9178832173347473\n",
      "1717/2105 [=======================>......] - ETA: 8:12 - accuracy: 0.9179 - loss: 0.2137\n",
      " training set -> batch:1718 loss:0.21592040359973907 and acc: 0.917553186416626\n",
      "1718/2105 [=======================>......] - ETA: 8:10 - accuracy: 0.9176 - loss: 0.2159\n",
      " training set -> batch:1719 loss:0.21622499823570251 and acc: 0.9172413945198059\n",
      "1719/2105 [=======================>......] - ETA: 8:09 - accuracy: 0.9172 - loss: 0.2162\n",
      " training set -> batch:1720 loss:0.21679995954036713 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1720 val loss:0.21022869646549225 and val acc: 0.9071100950241089\n",
      "1720/2105 [=======================>......] - ETA: 8:09 - accuracy: 0.9178 - loss: 0.2168\n",
      " training set -> batch:1721 loss:0.21873454749584198 and acc: 0.9070796370506287\n",
      "1721/2105 [=======================>......] - ETA: 8:08 - accuracy: 0.9071 - loss: 0.2187\n",
      " training set -> batch:1722 loss:0.2156151831150055 and acc: 0.9091880321502686\n",
      "1722/2105 [=======================>......] - ETA: 8:06 - accuracy: 0.9092 - loss: 0.2156\n",
      " training set -> batch:1723 loss:0.21512940526008606 and acc: 0.9101239442825317\n",
      "1723/2105 [=======================>......] - ETA: 8:05 - accuracy: 0.9101 - loss: 0.2151\n",
      " training set -> batch:1724 loss:0.2116420865058899 and acc: 0.9120000004768372\n",
      "1724/2105 [=======================>......] - ETA: 8:03 - accuracy: 0.9120 - loss: 0.2116\n",
      " training set -> batch:1725 loss:0.21179324388504028 and acc: 0.911821722984314\n",
      "1725/2105 [=======================>......] - ETA: 8:02 - accuracy: 0.9118 - loss: 0.2118\n",
      " training set -> batch:1726 loss:0.20883408188819885 and acc: 0.9125939607620239\n",
      "1726/2105 [=======================>......] - ETA: 8:00 - accuracy: 0.9126 - loss: 0.2088\n",
      " training set -> batch:1727 loss:0.21274907886981964 and acc: 0.9105839133262634\n",
      "1727/2105 [=======================>......] - ETA: 7:59 - accuracy: 0.9106 - loss: 0.2127\n",
      " training set -> batch:1728 loss:0.2121177762746811 and acc: 0.9122340679168701\n",
      "1728/2105 [=======================>......] - ETA: 7:58 - accuracy: 0.9122 - loss: 0.2121\n",
      " training set -> batch:1729 loss:0.21398334205150604 and acc: 0.9129310250282288\n",
      "1729/2105 [=======================>......] - ETA: 7:56 - accuracy: 0.9129 - loss: 0.2140\n",
      " training set -> batch:1730 loss:0.2151435762643814 and acc: 0.9135906100273132\n",
      "\n",
      " validation set -> batch:1730 val loss:0.2163981944322586 and val acc: 0.9105504751205444\n",
      "1730/2105 [=======================>......] - ETA: 7:56 - accuracy: 0.9136 - loss: 0.2151\n",
      " training set -> batch:1731 loss:0.2074575126171112 and acc: 0.9126105904579163\n",
      "1731/2105 [=======================>......] - ETA: 7:55 - accuracy: 0.9126 - loss: 0.2075\n",
      " training set -> batch:1732 loss:0.21054667234420776 and acc: 0.9113247990608215\n",
      "1732/2105 [=======================>......] - ETA: 7:54 - accuracy: 0.9113 - loss: 0.2105\n",
      " training set -> batch:1733 loss:0.2107713371515274 and acc: 0.9111570119857788\n",
      "1733/2105 [=======================>......] - ETA: 7:52 - accuracy: 0.9112 - loss: 0.2108\n",
      " training set -> batch:1734 loss:0.21154069900512695 and acc: 0.9110000133514404\n",
      "1734/2105 [=======================>......] - ETA: 7:51 - accuracy: 0.9110 - loss: 0.2115\n",
      " training set -> batch:1735 loss:0.20669817924499512 and acc: 0.911821722984314\n",
      "1735/2105 [=======================>......] - ETA: 7:49 - accuracy: 0.9118 - loss: 0.2067\n",
      " training set -> batch:1736 loss:0.20204930007457733 and acc: 0.9135338068008423\n",
      "1736/2105 [=======================>......] - ETA: 7:48 - accuracy: 0.9135 - loss: 0.2020\n",
      " training set -> batch:1737 loss:0.1984051913022995 and acc: 0.915145993232727\n",
      "1737/2105 [=======================>......] - ETA: 7:46 - accuracy: 0.9151 - loss: 0.1984\n",
      " training set -> batch:1738 loss:0.19173894822597504 and acc: 0.917553186416626\n",
      "1738/2105 [=======================>......] - ETA: 7:45 - accuracy: 0.9176 - loss: 0.1917\n",
      " training set -> batch:1739 loss:0.1897776573896408 and acc: 0.9189655184745789\n",
      "1739/2105 [=======================>......] - ETA: 7:43 - accuracy: 0.9190 - loss: 0.1898\n",
      " training set -> batch:1740 loss:0.19335083663463593 and acc: 0.9186241626739502\n",
      "\n",
      " validation set -> batch:1740 val loss:0.20981355011463165 and val acc: 0.9197247624397278\n",
      "1740/2105 [=======================>......] - ETA: 7:44 - accuracy: 0.9186 - loss: 0.1934\n",
      " training set -> batch:1741 loss:0.20322412252426147 and acc: 0.9203540086746216\n",
      "1741/2105 [=======================>......] - ETA: 7:42 - accuracy: 0.9204 - loss: 0.2032\n",
      " training set -> batch:1742 loss:0.21075420081615448 and acc: 0.9188033938407898\n",
      "1742/2105 [=======================>......] - ETA: 7:41 - accuracy: 0.9188 - loss: 0.2108\n",
      " training set -> batch:1743 loss:0.20382554829120636 and acc: 0.9204545617103577\n",
      "1743/2105 [=======================>......] - ETA: 7:39 - accuracy: 0.9205 - loss: 0.2038\n",
      " training set -> batch:1744 loss:0.19811487197875977 and acc: 0.921999990940094\n",
      "1744/2105 [=======================>......] - ETA: 7:38 - accuracy: 0.9220 - loss: 0.1981\n",
      " training set -> batch:1745 loss:0.20196433365345 and acc: 0.9224806427955627\n",
      "1745/2105 [=======================>......] - ETA: 7:37 - accuracy: 0.9225 - loss: 0.2020\n",
      " training set -> batch:1746 loss:0.20091362297534943 and acc: 0.9201127886772156\n",
      "1746/2105 [=======================>......] - ETA: 7:35 - accuracy: 0.9201 - loss: 0.2009\n",
      " training set -> batch:1747 loss:0.1996881365776062 and acc: 0.9206204414367676\n",
      "1747/2105 [=======================>......] - ETA: 7:34 - accuracy: 0.9206 - loss: 0.1997\n",
      " training set -> batch:1748 loss:0.20365315675735474 and acc: 0.9210993051528931\n",
      "1748/2105 [=======================>......] - ETA: 7:32 - accuracy: 0.9211 - loss: 0.2037\n",
      " training set -> batch:1749 loss:0.19859008491039276 and acc: 0.9224137663841248\n",
      "1749/2105 [=======================>......] - ETA: 7:31 - accuracy: 0.9224 - loss: 0.1986\n",
      " training set -> batch:1750 loss:0.19783556461334229 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1750 val loss:0.2143879234790802 and val acc: 0.9048165082931519\n",
      "1750/2105 [=======================>......] - ETA: 7:31 - accuracy: 0.9237 - loss: 0.1978\n",
      " training set -> batch:1751 loss:0.2173500508069992 and acc: 0.9059734344482422\n",
      "1751/2105 [=======================>......] - ETA: 7:30 - accuracy: 0.9060 - loss: 0.2174\n",
      " training set -> batch:1752 loss:0.2205273061990738 and acc: 0.9049145579338074\n",
      "1752/2105 [=======================>......] - ETA: 7:28 - accuracy: 0.9049 - loss: 0.2205\n",
      " training set -> batch:1753 loss:0.21085146069526672 and acc: 0.9070248007774353\n",
      "1753/2105 [=======================>......] - ETA: 7:27 - accuracy: 0.9070 - loss: 0.2109\n",
      " training set -> batch:1754 loss:0.2074490785598755 and acc: 0.9079999923706055\n",
      "1754/2105 [=======================>......] - ETA: 7:25 - accuracy: 0.9080 - loss: 0.2074\n",
      " training set -> batch:1755 loss:0.2047048807144165 and acc: 0.9089147448539734\n",
      "1755/2105 [========================>.....] - ETA: 7:24 - accuracy: 0.9089 - loss: 0.2047\n",
      " training set -> batch:1756 loss:0.2073177993297577 and acc: 0.9107142686843872\n",
      "1756/2105 [========================>.....] - ETA: 7:22 - accuracy: 0.9107 - loss: 0.2073\n",
      " training set -> batch:1757 loss:0.20466196537017822 and acc: 0.9114963412284851\n",
      "1757/2105 [========================>.....] - ETA: 7:21 - accuracy: 0.9115 - loss: 0.2047\n",
      " training set -> batch:1758 loss:0.20248927175998688 and acc: 0.9122340679168701\n",
      "1758/2105 [========================>.....] - ETA: 7:20 - accuracy: 0.9122 - loss: 0.2025\n",
      " training set -> batch:1759 loss:0.19896164536476135 and acc: 0.9129310250282288\n",
      "1759/2105 [========================>.....] - ETA: 7:18 - accuracy: 0.9129 - loss: 0.1990\n",
      " training set -> batch:1760 loss:0.20261095464229584 and acc: 0.9110738039016724\n",
      "\n",
      " validation set -> batch:1760 val loss:0.21463395655155182 and val acc: 0.9128440618515015\n",
      "1760/2105 [========================>.....] - ETA: 7:18 - accuracy: 0.9111 - loss: 0.2026\n",
      " training set -> batch:1761 loss:0.22158613801002502 and acc: 0.9126105904579163\n",
      "1761/2105 [========================>.....] - ETA: 7:17 - accuracy: 0.9126 - loss: 0.2216\n",
      " training set -> batch:1762 loss:0.2200203388929367 and acc: 0.9123931527137756\n",
      "1762/2105 [========================>.....] - ETA: 7:15 - accuracy: 0.9124 - loss: 0.2200\n",
      " training set -> batch:1763 loss:0.21945171058177948 and acc: 0.9121900796890259\n",
      "1763/2105 [========================>.....] - ETA: 7:14 - accuracy: 0.9122 - loss: 0.2195\n",
      " training set -> batch:1764 loss:0.21023885905742645 and acc: 0.9150000214576721\n",
      "1764/2105 [========================>.....] - ETA: 7:13 - accuracy: 0.9150 - loss: 0.2102\n",
      " training set -> batch:1765 loss:0.20341913402080536 and acc: 0.9166666865348816\n",
      "1765/2105 [========================>.....] - ETA: 7:11 - accuracy: 0.9167 - loss: 0.2034\n",
      " training set -> batch:1766 loss:0.21656982600688934 and acc: 0.9154135584831238\n",
      "1766/2105 [========================>.....] - ETA: 7:10 - accuracy: 0.9154 - loss: 0.2166\n",
      " training set -> batch:1767 loss:0.20876307785511017 and acc: 0.9178832173347473\n",
      "1767/2105 [========================>.....] - ETA: 7:08 - accuracy: 0.9179 - loss: 0.2088\n",
      " training set -> batch:1768 loss:0.21032996475696564 and acc: 0.917553186416626\n",
      "1768/2105 [========================>.....] - ETA: 7:07 - accuracy: 0.9176 - loss: 0.2103\n",
      " training set -> batch:1769 loss:0.21305328607559204 and acc: 0.9163793325424194\n",
      "1769/2105 [========================>.....] - ETA: 7:05 - accuracy: 0.9164 - loss: 0.2131\n",
      " training set -> batch:1770 loss:0.20939594507217407 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1770 val loss:0.2009439319372177 and val acc: 0.91399085521698\n",
      "1770/2105 [========================>.....] - ETA: 7:06 - accuracy: 0.9178 - loss: 0.2094\n",
      " training set -> batch:1771 loss:0.19693997502326965 and acc: 0.9148229956626892\n",
      "1771/2105 [========================>.....] - ETA: 7:04 - accuracy: 0.9148 - loss: 0.1969\n",
      " training set -> batch:1772 loss:0.19169563055038452 and acc: 0.9166666865348816\n",
      "1772/2105 [========================>.....] - ETA: 7:03 - accuracy: 0.9167 - loss: 0.1917\n",
      " training set -> batch:1773 loss:0.1909460425376892 and acc: 0.9173553586006165\n",
      "1773/2105 [========================>.....] - ETA: 7:01 - accuracy: 0.9174 - loss: 0.1909\n",
      " training set -> batch:1774 loss:0.19418643414974213 and acc: 0.9179999828338623\n",
      "1774/2105 [========================>.....] - ETA: 7:00 - accuracy: 0.9180 - loss: 0.1942\n",
      " training set -> batch:1775 loss:0.19023190438747406 and acc: 0.9195736646652222\n",
      "1775/2105 [========================>.....] - ETA: 6:59 - accuracy: 0.9196 - loss: 0.1902\n",
      " training set -> batch:1776 loss:0.18754805624485016 and acc: 0.9201127886772156\n",
      "1776/2105 [========================>.....] - ETA: 6:57 - accuracy: 0.9201 - loss: 0.1875\n",
      " training set -> batch:1777 loss:0.18464761972427368 and acc: 0.9206204414367676\n",
      "1777/2105 [========================>.....] - ETA: 6:56 - accuracy: 0.9206 - loss: 0.1846\n",
      " training set -> batch:1778 loss:0.18167711794376373 and acc: 0.9219858050346375\n",
      "1778/2105 [========================>.....] - ETA: 6:54 - accuracy: 0.9220 - loss: 0.1817\n",
      " training set -> batch:1779 loss:0.1797265112400055 and acc: 0.923275887966156\n",
      "1779/2105 [========================>.....] - ETA: 6:53 - accuracy: 0.9233 - loss: 0.1797\n",
      " training set -> batch:1780 loss:0.180860236287117 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1780 val loss:0.19725608825683594 and val acc: 0.9197247624397278\n",
      "1780/2105 [========================>.....] - ETA: 6:53 - accuracy: 0.9237 - loss: 0.1809\n",
      " training set -> batch:1781 loss:0.19427913427352905 and acc: 0.9203540086746216\n",
      "1781/2105 [========================>.....] - ETA: 6:52 - accuracy: 0.9204 - loss: 0.1943\n",
      " training set -> batch:1782 loss:0.19291792809963226 and acc: 0.9188033938407898\n",
      "1782/2105 [========================>.....] - ETA: 6:50 - accuracy: 0.9188 - loss: 0.1929\n",
      " training set -> batch:1783 loss:0.20227909088134766 and acc: 0.9173553586006165\n",
      "1783/2105 [========================>.....] - ETA: 6:49 - accuracy: 0.9174 - loss: 0.2023\n",
      " training set -> batch:1784 loss:0.19913679361343384 and acc: 0.9190000295639038\n",
      "1784/2105 [========================>.....] - ETA: 6:47 - accuracy: 0.9190 - loss: 0.1991\n",
      " training set -> batch:1785 loss:0.19598639011383057 and acc: 0.9195736646652222\n",
      "1785/2105 [========================>.....] - ETA: 6:46 - accuracy: 0.9196 - loss: 0.1960\n",
      " training set -> batch:1786 loss:0.1961609423160553 and acc: 0.9201127886772156\n",
      "1786/2105 [========================>.....] - ETA: 6:44 - accuracy: 0.9201 - loss: 0.1962\n",
      " training set -> batch:1787 loss:0.19272257387638092 and acc: 0.9206204414367676\n",
      "1787/2105 [========================>.....] - ETA: 6:43 - accuracy: 0.9206 - loss: 0.1927\n",
      " training set -> batch:1788 loss:0.18634609878063202 and acc: 0.9228723645210266\n",
      "1788/2105 [========================>.....] - ETA: 6:42 - accuracy: 0.9229 - loss: 0.1863\n",
      " training set -> batch:1789 loss:0.1895599216222763 and acc: 0.9215517044067383\n",
      "1789/2105 [========================>.....] - ETA: 6:40 - accuracy: 0.9216 - loss: 0.1896\n",
      " training set -> batch:1790 loss:0.19350415468215942 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:1790 val loss:0.19904382526874542 and val acc: 0.9208715558052063\n",
      "1790/2105 [========================>.....] - ETA: 6:40 - accuracy: 0.9211 - loss: 0.1935\n",
      " training set -> batch:1791 loss:0.1912262886762619 and acc: 0.9214601516723633\n",
      "1791/2105 [========================>.....] - ETA: 6:39 - accuracy: 0.9215 - loss: 0.1912\n",
      " training set -> batch:1792 loss:0.18725091218948364 and acc: 0.9220085740089417\n",
      "1792/2105 [========================>.....] - ETA: 6:37 - accuracy: 0.9220 - loss: 0.1873\n",
      " training set -> batch:1793 loss:0.18310225009918213 and acc: 0.9235537052154541\n",
      "1793/2105 [========================>.....] - ETA: 6:36 - accuracy: 0.9236 - loss: 0.1831\n",
      " training set -> batch:1794 loss:0.18717244267463684 and acc: 0.9229999780654907\n",
      "1794/2105 [========================>.....] - ETA: 6:35 - accuracy: 0.9230 - loss: 0.1872\n",
      " training set -> batch:1795 loss:0.18557901680469513 and acc: 0.9234496355056763\n",
      "1795/2105 [========================>.....] - ETA: 6:33 - accuracy: 0.9234 - loss: 0.1856\n",
      " training set -> batch:1796 loss:0.18306803703308105 and acc: 0.9248120188713074\n",
      "1796/2105 [========================>.....] - ETA: 6:32 - accuracy: 0.9248 - loss: 0.1831\n",
      " training set -> batch:1797 loss:0.18527266383171082 and acc: 0.9242700934410095\n",
      "1797/2105 [========================>.....] - ETA: 6:30 - accuracy: 0.9243 - loss: 0.1853\n",
      " training set -> batch:1798 loss:0.1951877474784851 and acc: 0.9228723645210266\n",
      "1798/2105 [========================>.....] - ETA: 6:29 - accuracy: 0.9229 - loss: 0.1952\n",
      " training set -> batch:1799 loss:0.19054216146469116 and acc: 0.9241379499435425\n",
      "1799/2105 [========================>.....] - ETA: 6:28 - accuracy: 0.9241 - loss: 0.1905\n",
      " training set -> batch:1800 loss:0.18884818255901337 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1800 val loss:0.20334254205226898 and val acc: 0.9208715558052063\n",
      "1800/2105 [========================>.....] - ETA: 6:28 - accuracy: 0.9253 - loss: 0.1888\n",
      " training set -> batch:1801 loss:0.20557759702205658 and acc: 0.9203540086746216\n",
      "1801/2105 [========================>.....] - ETA: 6:26 - accuracy: 0.9204 - loss: 0.2056\n",
      " training set -> batch:1802 loss:0.21680034697055817 and acc: 0.9188033938407898\n",
      "1802/2105 [========================>.....] - ETA: 6:25 - accuracy: 0.9188 - loss: 0.2168\n",
      " training set -> batch:1803 loss:0.210455521941185 and acc: 0.9204545617103577\n",
      "1803/2105 [========================>.....] - ETA: 6:23 - accuracy: 0.9205 - loss: 0.2105\n",
      " training set -> batch:1804 loss:0.208402618765831 and acc: 0.9200000166893005\n",
      "1804/2105 [========================>.....] - ETA: 6:22 - accuracy: 0.9200 - loss: 0.2084\n",
      " training set -> batch:1805 loss:0.20418496429920197 and acc: 0.9215116500854492\n",
      "1805/2105 [========================>.....] - ETA: 6:21 - accuracy: 0.9215 - loss: 0.2042\n",
      " training set -> batch:1806 loss:0.20167534053325653 and acc: 0.9229323267936707\n",
      "1806/2105 [========================>.....] - ETA: 6:19 - accuracy: 0.9229 - loss: 0.2017\n",
      " training set -> batch:1807 loss:0.19524435698986053 and acc: 0.9251824617385864\n",
      "1807/2105 [========================>.....] - ETA: 6:18 - accuracy: 0.9252 - loss: 0.1952\n",
      " training set -> batch:1808 loss:0.20247222483158112 and acc: 0.923758864402771\n",
      "1808/2105 [========================>.....] - ETA: 6:16 - accuracy: 0.9238 - loss: 0.2025\n",
      " training set -> batch:1809 loss:0.198293998837471 and acc: 0.925000011920929\n",
      "1809/2105 [========================>.....] - ETA: 6:15 - accuracy: 0.9250 - loss: 0.1983\n",
      " training set -> batch:1810 loss:0.19753001630306244 and acc: 0.9244966506958008\n",
      "\n",
      " validation set -> batch:1810 val loss:0.21838660538196564 and val acc: 0.9197247624397278\n",
      "1810/2105 [========================>.....] - ETA: 6:15 - accuracy: 0.9245 - loss: 0.1975\n",
      " training set -> batch:1811 loss:0.2082790732383728 and acc: 0.9225663542747498\n",
      "1811/2105 [========================>.....] - ETA: 6:13 - accuracy: 0.9226 - loss: 0.2083\n",
      " training set -> batch:1812 loss:0.20038379728794098 and acc: 0.9241452813148499\n",
      "1812/2105 [========================>.....] - ETA: 6:12 - accuracy: 0.9241 - loss: 0.2004\n",
      " training set -> batch:1813 loss:0.20500195026397705 and acc: 0.9245867729187012\n",
      "1813/2105 [========================>.....] - ETA: 6:11 - accuracy: 0.9246 - loss: 0.2050\n",
      " training set -> batch:1814 loss:0.20380868017673492 and acc: 0.925000011920929\n",
      "1814/2105 [========================>.....] - ETA: 6:09 - accuracy: 0.9250 - loss: 0.2038\n",
      " training set -> batch:1815 loss:0.19521470367908478 and acc: 0.9273256063461304\n",
      "1815/2105 [========================>.....] - ETA: 6:08 - accuracy: 0.9273 - loss: 0.1952\n",
      " training set -> batch:1816 loss:0.2007945477962494 and acc: 0.9257518649101257\n",
      "1816/2105 [========================>.....] - ETA: 6:06 - accuracy: 0.9258 - loss: 0.2008\n",
      " training set -> batch:1817 loss:0.20406723022460938 and acc: 0.9242700934410095\n",
      "1817/2105 [========================>.....] - ETA: 6:05 - accuracy: 0.9243 - loss: 0.2041\n",
      " training set -> batch:1818 loss:0.199015274643898 and acc: 0.9255319237709045\n",
      "1818/2105 [========================>.....] - ETA: 6:04 - accuracy: 0.9255 - loss: 0.1990\n",
      " training set -> batch:1819 loss:0.19314570724964142 and acc: 0.9267241358757019\n",
      "1819/2105 [========================>.....] - ETA: 6:02 - accuracy: 0.9267 - loss: 0.1931\n",
      " training set -> batch:1820 loss:0.18809528648853302 and acc: 0.9278523325920105\n",
      "\n",
      " validation set -> batch:1820 val loss:0.21693645417690277 and val acc: 0.9197247624397278\n",
      "1820/2105 [========================>.....] - ETA: 6:02 - accuracy: 0.9279 - loss: 0.1881\n",
      " training set -> batch:1821 loss:0.21660961210727692 and acc: 0.9192478060722351\n",
      "1821/2105 [========================>.....] - ETA: 6:01 - accuracy: 0.9192 - loss: 0.2166\n",
      " training set -> batch:1822 loss:0.2122066766023636 and acc: 0.9209401607513428\n",
      "1822/2105 [========================>.....] - ETA: 5:59 - accuracy: 0.9209 - loss: 0.2122\n",
      " training set -> batch:1823 loss:0.20776475965976715 and acc: 0.922520637512207\n",
      "1823/2105 [========================>.....] - ETA: 5:58 - accuracy: 0.9225 - loss: 0.2078\n",
      " training set -> batch:1824 loss:0.2077236920595169 and acc: 0.9229999780654907\n",
      "1824/2105 [========================>.....] - ETA: 5:57 - accuracy: 0.9230 - loss: 0.2077\n",
      " training set -> batch:1825 loss:0.20076704025268555 and acc: 0.9244186282157898\n",
      "1825/2105 [=========================>....] - ETA: 5:55 - accuracy: 0.9244 - loss: 0.2008\n",
      " training set -> batch:1826 loss:0.19539466500282288 and acc: 0.9257518649101257\n",
      "1826/2105 [=========================>....] - ETA: 5:54 - accuracy: 0.9258 - loss: 0.1954\n",
      " training set -> batch:1827 loss:0.1975567489862442 and acc: 0.9260948896408081\n",
      "1827/2105 [=========================>....] - ETA: 5:52 - accuracy: 0.9261 - loss: 0.1976\n",
      " training set -> batch:1828 loss:0.20399293303489685 and acc: 0.9255319237709045\n",
      "1828/2105 [=========================>....] - ETA: 5:51 - accuracy: 0.9255 - loss: 0.2040\n",
      " training set -> batch:1829 loss:0.20022742450237274 and acc: 0.925000011920929\n",
      "1829/2105 [=========================>....] - ETA: 5:50 - accuracy: 0.9250 - loss: 0.2002\n",
      " training set -> batch:1830 loss:0.1971057802438736 and acc: 0.9253355860710144\n",
      "\n",
      " validation set -> batch:1830 val loss:0.22432862222194672 and val acc: 0.9162843823432922\n",
      "1830/2105 [=========================>....] - ETA: 5:49 - accuracy: 0.9253 - loss: 0.1971\n",
      " training set -> batch:1831 loss:0.22404144704341888 and acc: 0.9159291982650757\n",
      "1831/2105 [=========================>....] - ETA: 5:48 - accuracy: 0.9159 - loss: 0.2240\n",
      " training set -> batch:1832 loss:0.21320250630378723 and acc: 0.9188033938407898\n",
      "1832/2105 [=========================>....] - ETA: 5:47 - accuracy: 0.9188 - loss: 0.2132\n",
      " training set -> batch:1833 loss:0.21515250205993652 and acc: 0.9194214940071106\n",
      "1833/2105 [=========================>....] - ETA: 5:45 - accuracy: 0.9194 - loss: 0.2152\n",
      " training set -> batch:1834 loss:0.20903629064559937 and acc: 0.9210000038146973\n",
      "1834/2105 [=========================>....] - ETA: 5:44 - accuracy: 0.9210 - loss: 0.2090\n",
      " training set -> batch:1835 loss:0.2068873643875122 and acc: 0.9215116500854492\n",
      "1835/2105 [=========================>....] - ETA: 5:42 - accuracy: 0.9215 - loss: 0.2069\n",
      " training set -> batch:1836 loss:0.2009628266096115 and acc: 0.9229323267936707\n",
      "1836/2105 [=========================>....] - ETA: 5:41 - accuracy: 0.9229 - loss: 0.2010\n",
      " training set -> batch:1837 loss:0.1968809813261032 and acc: 0.9242700934410095\n",
      "1837/2105 [=========================>....] - ETA: 5:40 - accuracy: 0.9243 - loss: 0.1969\n",
      " training set -> batch:1838 loss:0.19115068018436432 and acc: 0.9255319237709045\n",
      "1838/2105 [=========================>....] - ETA: 5:38 - accuracy: 0.9255 - loss: 0.1912\n",
      " training set -> batch:1839 loss:0.19362317025661469 and acc: 0.9258620738983154\n",
      "1839/2105 [=========================>....] - ETA: 5:37 - accuracy: 0.9259 - loss: 0.1936\n",
      " training set -> batch:1840 loss:0.1918662041425705 and acc: 0.926174521446228\n",
      "\n",
      " validation set -> batch:1840 val loss:0.21609202027320862 and val acc: 0.9185779690742493\n",
      "1840/2105 [=========================>....] - ETA: 5:37 - accuracy: 0.9262 - loss: 0.1919\n",
      " training set -> batch:1841 loss:0.22153553366661072 and acc: 0.9170354008674622\n",
      "1841/2105 [=========================>....] - ETA: 5:35 - accuracy: 0.9170 - loss: 0.2215\n",
      " training set -> batch:1842 loss:0.21613146364688873 and acc: 0.9177350401878357\n",
      "1842/2105 [=========================>....] - ETA: 5:34 - accuracy: 0.9177 - loss: 0.2161\n",
      " training set -> batch:1843 loss:0.2134520262479782 and acc: 0.9194214940071106\n",
      "1843/2105 [=========================>....] - ETA: 5:33 - accuracy: 0.9194 - loss: 0.2135\n",
      " training set -> batch:1844 loss:0.20905660092830658 and acc: 0.9200000166893005\n",
      "1844/2105 [=========================>....] - ETA: 5:31 - accuracy: 0.9200 - loss: 0.2091\n",
      " training set -> batch:1845 loss:0.20752118527889252 and acc: 0.9195736646652222\n",
      "1845/2105 [=========================>....] - ETA: 5:30 - accuracy: 0.9196 - loss: 0.2075\n",
      " training set -> batch:1846 loss:0.21470534801483154 and acc: 0.9163534045219421\n",
      "1846/2105 [=========================>....] - ETA: 5:28 - accuracy: 0.9164 - loss: 0.2147\n",
      " training set -> batch:1847 loss:0.207759290933609 and acc: 0.918795645236969\n",
      "1847/2105 [=========================>....] - ETA: 5:27 - accuracy: 0.9188 - loss: 0.2078\n",
      " training set -> batch:1848 loss:0.20106354355812073 and acc: 0.9202127456665039\n",
      "1848/2105 [=========================>....] - ETA: 5:26 - accuracy: 0.9202 - loss: 0.2011\n",
      " training set -> batch:1849 loss:0.20384268462657928 and acc: 0.9206896424293518\n",
      "1849/2105 [=========================>....] - ETA: 5:24 - accuracy: 0.9207 - loss: 0.2038\n",
      " training set -> batch:1850 loss:0.2013254016637802 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1850 val loss:0.2191719263792038 and val acc: 0.9128440618515015\n",
      "1850/2105 [=========================>....] - ETA: 5:24 - accuracy: 0.9220 - loss: 0.2013\n",
      " training set -> batch:1851 loss:0.22200950980186462 and acc: 0.9115044474601746\n",
      "1851/2105 [=========================>....] - ETA: 5:23 - accuracy: 0.9115 - loss: 0.2220\n",
      " training set -> batch:1852 loss:0.2154732644557953 and acc: 0.9123931527137756\n",
      "1852/2105 [=========================>....] - ETA: 5:21 - accuracy: 0.9124 - loss: 0.2155\n",
      " training set -> batch:1853 loss:0.20950382947921753 and acc: 0.91425621509552\n",
      "1853/2105 [=========================>....] - ETA: 5:20 - accuracy: 0.9143 - loss: 0.2095\n",
      " training set -> batch:1854 loss:0.20687416195869446 and acc: 0.9139999747276306\n",
      "1854/2105 [=========================>....] - ETA: 5:18 - accuracy: 0.9140 - loss: 0.2069\n",
      " training set -> batch:1855 loss:0.2058897465467453 and acc: 0.911821722984314\n",
      "1855/2105 [=========================>....] - ETA: 5:17 - accuracy: 0.9118 - loss: 0.2059\n",
      " training set -> batch:1856 loss:0.2043379843235016 and acc: 0.9125939607620239\n",
      "1856/2105 [=========================>....] - ETA: 5:16 - accuracy: 0.9126 - loss: 0.2043\n",
      " training set -> batch:1857 loss:0.20583994686603546 and acc: 0.9142335653305054\n",
      "1857/2105 [=========================>....] - ETA: 5:14 - accuracy: 0.9142 - loss: 0.2058\n",
      " training set -> batch:1858 loss:0.19991937279701233 and acc: 0.9157801270484924\n",
      "1858/2105 [=========================>....] - ETA: 5:13 - accuracy: 0.9158 - loss: 0.1999\n",
      " training set -> batch:1859 loss:0.20353221893310547 and acc: 0.915517270565033\n",
      "1859/2105 [=========================>....] - ETA: 5:12 - accuracy: 0.9155 - loss: 0.2035\n",
      " training set -> batch:1860 loss:0.19653744995594025 and acc: 0.9177852272987366\n",
      "\n",
      " validation set -> batch:1860 val loss:0.21041493117809296 and val acc: 0.9162843823432922\n",
      "1860/2105 [=========================>....] - ETA: 5:11 - accuracy: 0.9178 - loss: 0.1965\n",
      " training set -> batch:1861 loss:0.21023616194725037 and acc: 0.9170354008674622\n",
      "1861/2105 [=========================>....] - ETA: 5:10 - accuracy: 0.9170 - loss: 0.2102\n",
      " training set -> batch:1862 loss:0.20540577173233032 and acc: 0.9166666865348816\n",
      "1862/2105 [=========================>....] - ETA: 5:09 - accuracy: 0.9167 - loss: 0.2054\n",
      " training set -> batch:1863 loss:0.20266975462436676 and acc: 0.9163222908973694\n",
      "1863/2105 [=========================>....] - ETA: 5:07 - accuracy: 0.9163 - loss: 0.2027\n",
      " training set -> batch:1864 loss:0.1936764121055603 and acc: 0.9190000295639038\n",
      "1864/2105 [=========================>....] - ETA: 5:06 - accuracy: 0.9190 - loss: 0.1937\n",
      " training set -> batch:1865 loss:0.18967320024967194 and acc: 0.9195736646652222\n",
      "1865/2105 [=========================>....] - ETA: 5:04 - accuracy: 0.9196 - loss: 0.1897\n",
      " training set -> batch:1866 loss:0.18673297762870789 and acc: 0.9210526347160339\n",
      "1866/2105 [=========================>....] - ETA: 5:03 - accuracy: 0.9211 - loss: 0.1867\n",
      " training set -> batch:1867 loss:0.1817939579486847 and acc: 0.9233576655387878\n",
      "1867/2105 [=========================>....] - ETA: 5:02 - accuracy: 0.9234 - loss: 0.1818\n",
      " training set -> batch:1868 loss:0.19094371795654297 and acc: 0.9210993051528931\n",
      "1868/2105 [=========================>....] - ETA: 5:00 - accuracy: 0.9211 - loss: 0.1909\n",
      " training set -> batch:1869 loss:0.19814129173755646 and acc: 0.9206896424293518\n",
      "1869/2105 [=========================>....] - ETA: 4:59 - accuracy: 0.9207 - loss: 0.1981\n",
      " training set -> batch:1870 loss:0.1918988823890686 and acc: 0.9228187799453735\n",
      "\n",
      " validation set -> batch:1870 val loss:0.223899245262146 and val acc: 0.9185779690742493\n",
      "1870/2105 [=========================>....] - ETA: 4:59 - accuracy: 0.9228 - loss: 0.1919\n",
      " training set -> batch:1871 loss:0.2151295244693756 and acc: 0.9203540086746216\n",
      "1871/2105 [=========================>....] - ETA: 4:57 - accuracy: 0.9204 - loss: 0.2151\n",
      " training set -> batch:1872 loss:0.21279695630073547 and acc: 0.9220085740089417\n",
      "1872/2105 [=========================>....] - ETA: 4:56 - accuracy: 0.9220 - loss: 0.2128\n",
      " training set -> batch:1873 loss:0.2170218676328659 and acc: 0.922520637512207\n",
      "1873/2105 [=========================>....] - ETA: 4:54 - accuracy: 0.9225 - loss: 0.2170\n",
      " training set -> batch:1874 loss:0.21507103741168976 and acc: 0.9240000247955322\n",
      "1874/2105 [=========================>....] - ETA: 4:53 - accuracy: 0.9240 - loss: 0.2151\n",
      " training set -> batch:1875 loss:0.2059035450220108 and acc: 0.9263566136360168\n",
      "1875/2105 [=========================>....] - ETA: 4:52 - accuracy: 0.9264 - loss: 0.2059\n",
      " training set -> batch:1876 loss:0.20084285736083984 and acc: 0.9276315569877625\n",
      "1876/2105 [=========================>....] - ETA: 4:50 - accuracy: 0.9276 - loss: 0.2008\n",
      " training set -> batch:1877 loss:0.19990383088588715 and acc: 0.9288321137428284\n",
      "1877/2105 [=========================>....] - ETA: 4:49 - accuracy: 0.9288 - loss: 0.1999\n",
      " training set -> batch:1878 loss:0.19482187926769257 and acc: 0.929964542388916\n",
      "1878/2105 [=========================>....] - ETA: 4:48 - accuracy: 0.9300 - loss: 0.1948\n",
      " training set -> batch:1879 loss:0.19015879929065704 and acc: 0.931034505367279\n",
      "1879/2105 [=========================>....] - ETA: 4:46 - accuracy: 0.9310 - loss: 0.1902\n",
      " training set -> batch:1880 loss:0.19686155021190643 and acc: 0.9286912679672241\n",
      "\n",
      " validation set -> batch:1880 val loss:0.2182682752609253 and val acc: 0.9151375889778137\n",
      "1880/2105 [=========================>....] - ETA: 4:46 - accuracy: 0.9287 - loss: 0.1969\n",
      " training set -> batch:1881 loss:0.2243325561285019 and acc: 0.9159291982650757\n",
      "1881/2105 [=========================>....] - ETA: 4:44 - accuracy: 0.9159 - loss: 0.2243\n",
      " training set -> batch:1882 loss:0.22498632967472076 and acc: 0.9166666865348816\n",
      "1882/2105 [=========================>....] - ETA: 4:43 - accuracy: 0.9167 - loss: 0.2250\n",
      " training set -> batch:1883 loss:0.21860994398593903 and acc: 0.9173553586006165\n",
      "1883/2105 [=========================>....] - ETA: 4:42 - accuracy: 0.9174 - loss: 0.2186\n",
      " training set -> batch:1884 loss:0.21297959983348846 and acc: 0.9190000295639038\n",
      "1884/2105 [=========================>....] - ETA: 4:40 - accuracy: 0.9190 - loss: 0.2130\n",
      " training set -> batch:1885 loss:0.21997331082820892 and acc: 0.9186046719551086\n",
      "1885/2105 [=========================>....] - ETA: 4:39 - accuracy: 0.9186 - loss: 0.2200\n",
      " training set -> batch:1886 loss:0.224212646484375 and acc: 0.9182330965995789\n",
      "1886/2105 [=========================>....] - ETA: 4:38 - accuracy: 0.9182 - loss: 0.2242\n",
      " training set -> batch:1887 loss:0.21559113264083862 and acc: 0.9206204414367676\n",
      "1887/2105 [=========================>....] - ETA: 4:36 - accuracy: 0.9206 - loss: 0.2156\n",
      " training set -> batch:1888 loss:0.21706491708755493 and acc: 0.9202127456665039\n",
      "1888/2105 [=========================>....] - ETA: 4:35 - accuracy: 0.9202 - loss: 0.2171\n",
      " training set -> batch:1889 loss:0.21102488040924072 and acc: 0.9224137663841248\n",
      "1889/2105 [=========================>....] - ETA: 4:34 - accuracy: 0.9224 - loss: 0.2110\n",
      " training set -> batch:1890 loss:0.21588601171970367 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1890 val loss:0.21665780246257782 and val acc: 0.9128440618515015\n",
      "1890/2105 [=========================>....] - ETA: 4:33 - accuracy: 0.9220 - loss: 0.2159\n",
      " training set -> batch:1891 loss:0.2049509882926941 and acc: 0.9159291982650757\n",
      "1891/2105 [=========================>....] - ETA: 4:32 - accuracy: 0.9159 - loss: 0.2050\n",
      " training set -> batch:1892 loss:0.19652123749256134 and acc: 0.9188033938407898\n",
      "1892/2105 [=========================>....] - ETA: 4:30 - accuracy: 0.9188 - loss: 0.1965\n",
      " training set -> batch:1893 loss:0.19079405069351196 and acc: 0.9204545617103577\n",
      "1893/2105 [=========================>....] - ETA: 4:29 - accuracy: 0.9205 - loss: 0.1908\n",
      " training set -> batch:1894 loss:0.18808893859386444 and acc: 0.9210000038146973\n",
      "1894/2105 [=========================>....] - ETA: 4:28 - accuracy: 0.9210 - loss: 0.1881\n",
      " training set -> batch:1895 loss:0.1897672414779663 and acc: 0.9205426573753357\n",
      "1895/2105 [==========================>...] - ETA: 4:26 - accuracy: 0.9205 - loss: 0.1898\n",
      " training set -> batch:1896 loss:0.18935784697532654 and acc: 0.9210526347160339\n",
      "1896/2105 [==========================>...] - ETA: 4:25 - accuracy: 0.9211 - loss: 0.1894\n",
      " training set -> batch:1897 loss:0.18255434930324554 and acc: 0.9233576655387878\n",
      "1897/2105 [==========================>...] - ETA: 4:24 - accuracy: 0.9234 - loss: 0.1826\n",
      " training set -> batch:1898 loss:0.1783563196659088 and acc: 0.9255319237709045\n",
      "1898/2105 [==========================>...] - ETA: 4:22 - accuracy: 0.9255 - loss: 0.1784\n",
      " training set -> batch:1899 loss:0.18053875863552094 and acc: 0.9241379499435425\n",
      "1899/2105 [==========================>...] - ETA: 4:21 - accuracy: 0.9241 - loss: 0.1805\n",
      " training set -> batch:1900 loss:0.18366260826587677 and acc: 0.9236577153205872\n",
      "\n",
      " validation set -> batch:1900 val loss:0.20548859238624573 and val acc: 0.9174311757087708\n",
      "1900/2105 [==========================>...] - ETA: 4:20 - accuracy: 0.9237 - loss: 0.1837\n",
      " training set -> batch:1901 loss:0.1963016241788864 and acc: 0.9203540086746216\n",
      "1901/2105 [==========================>...] - ETA: 4:19 - accuracy: 0.9204 - loss: 0.1963\n",
      " training set -> batch:1902 loss:0.19014614820480347 and acc: 0.9220085740089417\n",
      "1902/2105 [==========================>...] - ETA: 4:18 - accuracy: 0.9220 - loss: 0.1901\n",
      " training set -> batch:1903 loss:0.1929902285337448 and acc: 0.9204545617103577\n",
      "1903/2105 [==========================>...] - ETA: 4:16 - accuracy: 0.9205 - loss: 0.1930\n",
      " training set -> batch:1904 loss:0.18557721376419067 and acc: 0.9229999780654907\n",
      "1904/2105 [==========================>...] - ETA: 4:15 - accuracy: 0.9230 - loss: 0.1856\n",
      " training set -> batch:1905 loss:0.18470171093940735 and acc: 0.9234496355056763\n",
      "1905/2105 [==========================>...] - ETA: 4:14 - accuracy: 0.9234 - loss: 0.1847\n",
      " training set -> batch:1906 loss:0.1832638680934906 and acc: 0.9229323267936707\n",
      "1906/2105 [==========================>...] - ETA: 4:12 - accuracy: 0.9229 - loss: 0.1833\n",
      " training set -> batch:1907 loss:0.1797439604997635 and acc: 0.9242700934410095\n",
      "1907/2105 [==========================>...] - ETA: 4:11 - accuracy: 0.9243 - loss: 0.1797\n",
      " training set -> batch:1908 loss:0.18096116185188293 and acc: 0.9246453642845154\n",
      "1908/2105 [==========================>...] - ETA: 4:10 - accuracy: 0.9246 - loss: 0.1810\n",
      " training set -> batch:1909 loss:0.179156094789505 and acc: 0.925000011920929\n",
      "1909/2105 [==========================>...] - ETA: 4:08 - accuracy: 0.9250 - loss: 0.1792\n",
      " training set -> batch:1910 loss:0.1739369034767151 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1910 val loss:0.2046501636505127 and val acc: 0.9197247624397278\n",
      "1910/2105 [==========================>...] - ETA: 4:08 - accuracy: 0.9270 - loss: 0.1739\n",
      " training set -> batch:1911 loss:0.19613513350486755 and acc: 0.9214601516723633\n",
      "1911/2105 [==========================>...] - ETA: 4:06 - accuracy: 0.9215 - loss: 0.1961\n",
      " training set -> batch:1912 loss:0.20295484364032745 and acc: 0.9220085740089417\n",
      "1912/2105 [==========================>...] - ETA: 4:05 - accuracy: 0.9220 - loss: 0.2030\n",
      " training set -> batch:1913 loss:0.21382199227809906 and acc: 0.9204545617103577\n",
      "1913/2105 [==========================>...] - ETA: 4:04 - accuracy: 0.9205 - loss: 0.2138\n",
      " training set -> batch:1914 loss:0.2108411341905594 and acc: 0.9210000038146973\n",
      "1914/2105 [==========================>...] - ETA: 4:02 - accuracy: 0.9210 - loss: 0.2108\n",
      " training set -> batch:1915 loss:0.20209626853466034 and acc: 0.9234496355056763\n",
      "1915/2105 [==========================>...] - ETA: 4:01 - accuracy: 0.9234 - loss: 0.2021\n",
      " training set -> batch:1916 loss:0.20187802612781525 and acc: 0.923872172832489\n",
      "1916/2105 [==========================>...] - ETA: 4:00 - accuracy: 0.9239 - loss: 0.2019\n",
      " training set -> batch:1917 loss:0.1978183537721634 and acc: 0.9251824617385864\n",
      "1917/2105 [==========================>...] - ETA: 3:58 - accuracy: 0.9252 - loss: 0.1978\n",
      " training set -> batch:1918 loss:0.19132781028747559 and acc: 0.9264184236526489\n",
      "1918/2105 [==========================>...] - ETA: 3:57 - accuracy: 0.9264 - loss: 0.1913\n",
      " training set -> batch:1919 loss:0.1882190853357315 and acc: 0.9275861978530884\n",
      "1919/2105 [==========================>...] - ETA: 3:56 - accuracy: 0.9276 - loss: 0.1882\n",
      " training set -> batch:1920 loss:0.19136756658554077 and acc: 0.9278523325920105\n",
      "\n",
      " validation set -> batch:1920 val loss:0.2103584259748459 and val acc: 0.9231651425361633\n",
      "1920/2105 [==========================>...] - ETA: 3:55 - accuracy: 0.9279 - loss: 0.1914\n",
      " training set -> batch:1921 loss:0.19985491037368774 and acc: 0.9247787594795227\n",
      "1921/2105 [==========================>...] - ETA: 3:54 - accuracy: 0.9248 - loss: 0.1999\n",
      " training set -> batch:1922 loss:0.2122359275817871 and acc: 0.9230769276618958\n",
      "1922/2105 [==========================>...] - ETA: 3:52 - accuracy: 0.9231 - loss: 0.2122\n",
      " training set -> batch:1923 loss:0.20731763541698456 and acc: 0.9235537052154541\n",
      "1923/2105 [==========================>...] - ETA: 3:51 - accuracy: 0.9236 - loss: 0.2073\n",
      " training set -> batch:1924 loss:0.21157236397266388 and acc: 0.9229999780654907\n",
      "1924/2105 [==========================>...] - ETA: 3:50 - accuracy: 0.9230 - loss: 0.2116\n",
      " training set -> batch:1925 loss:0.20955097675323486 and acc: 0.9234496355056763\n",
      "1925/2105 [==========================>...] - ETA: 3:48 - accuracy: 0.9234 - loss: 0.2096\n",
      " training set -> batch:1926 loss:0.20827510952949524 and acc: 0.9229323267936707\n",
      "1926/2105 [==========================>...] - ETA: 3:47 - accuracy: 0.9229 - loss: 0.2083\n",
      " training set -> batch:1927 loss:0.20408782362937927 and acc: 0.9242700934410095\n",
      "1927/2105 [==========================>...] - ETA: 3:46 - accuracy: 0.9243 - loss: 0.2041\n",
      " training set -> batch:1928 loss:0.2060558944940567 and acc: 0.9246453642845154\n",
      "1928/2105 [==========================>...] - ETA: 3:44 - accuracy: 0.9246 - loss: 0.2061\n",
      " training set -> batch:1929 loss:0.20447488129138947 and acc: 0.9258620738983154\n",
      "1929/2105 [==========================>...] - ETA: 3:43 - accuracy: 0.9259 - loss: 0.2045\n",
      " training set -> batch:1930 loss:0.20016683638095856 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1930 val loss:0.1985284388065338 and val acc: 0.9197247624397278\n",
      "1930/2105 [==========================>...] - ETA: 3:42 - accuracy: 0.9270 - loss: 0.2002\n",
      " training set -> batch:1931 loss:0.20093728601932526 and acc: 0.9203540086746216\n",
      "1931/2105 [==========================>...] - ETA: 3:41 - accuracy: 0.9204 - loss: 0.2009\n",
      " training set -> batch:1932 loss:0.20231570303440094 and acc: 0.9198718070983887\n",
      "1932/2105 [==========================>...] - ETA: 3:40 - accuracy: 0.9199 - loss: 0.2023\n",
      " training set -> batch:1933 loss:0.20209917426109314 and acc: 0.9194214940071106\n",
      "1933/2105 [==========================>...] - ETA: 3:38 - accuracy: 0.9194 - loss: 0.2021\n",
      " training set -> batch:1934 loss:0.21419647336006165 and acc: 0.9179999828338623\n",
      "1934/2105 [==========================>...] - ETA: 3:37 - accuracy: 0.9180 - loss: 0.2142\n",
      " training set -> batch:1935 loss:0.2112504541873932 and acc: 0.9195736646652222\n",
      "1935/2105 [==========================>...] - ETA: 3:36 - accuracy: 0.9196 - loss: 0.2113\n",
      " training set -> batch:1936 loss:0.2138630896806717 and acc: 0.9191729426383972\n",
      "1936/2105 [==========================>...] - ETA: 3:34 - accuracy: 0.9192 - loss: 0.2139\n",
      " training set -> batch:1937 loss:0.21313147246837616 and acc: 0.918795645236969\n",
      "1937/2105 [==========================>...] - ETA: 3:33 - accuracy: 0.9188 - loss: 0.2131\n",
      " training set -> batch:1938 loss:0.21092678606510162 and acc: 0.9193262457847595\n",
      "1938/2105 [==========================>...] - ETA: 3:32 - accuracy: 0.9193 - loss: 0.2109\n",
      " training set -> batch:1939 loss:0.20592139661312103 and acc: 0.9215517044067383\n",
      "1939/2105 [==========================>...] - ETA: 3:30 - accuracy: 0.9216 - loss: 0.2059\n",
      " training set -> batch:1940 loss:0.2034764289855957 and acc: 0.9219798445701599\n",
      "\n",
      " validation set -> batch:1940 val loss:0.195511594414711 and val acc: 0.9288991093635559\n",
      "1940/2105 [==========================>...] - ETA: 3:30 - accuracy: 0.9220 - loss: 0.2035\n",
      " training set -> batch:1941 loss:0.1884286254644394 and acc: 0.9303097128868103\n",
      "1941/2105 [==========================>...] - ETA: 3:28 - accuracy: 0.9303 - loss: 0.1884\n",
      " training set -> batch:1942 loss:0.18869945406913757 and acc: 0.9284188151359558\n",
      "1942/2105 [==========================>...] - ETA: 3:27 - accuracy: 0.9284 - loss: 0.1887\n",
      " training set -> batch:1943 loss:0.1846376657485962 and acc: 0.9297520518302917\n",
      "1943/2105 [==========================>...] - ETA: 3:26 - accuracy: 0.9298 - loss: 0.1846\n",
      " training set -> batch:1944 loss:0.17873242497444153 and acc: 0.9309999942779541\n",
      "1944/2105 [==========================>...] - ETA: 3:24 - accuracy: 0.9310 - loss: 0.1787\n",
      " training set -> batch:1945 loss:0.1758476048707962 and acc: 0.932170569896698\n",
      "1945/2105 [==========================>...] - ETA: 3:23 - accuracy: 0.9322 - loss: 0.1758\n",
      " training set -> batch:1946 loss:0.1751607358455658 and acc: 0.9332706928253174\n",
      "1946/2105 [==========================>...] - ETA: 3:21 - accuracy: 0.9333 - loss: 0.1752\n",
      " training set -> batch:1947 loss:0.1750098466873169 and acc: 0.9333941340446472\n",
      "1947/2105 [==========================>...] - ETA: 3:20 - accuracy: 0.9334 - loss: 0.1750\n",
      " training set -> batch:1948 loss:0.17195767164230347 and acc: 0.9343971610069275\n",
      "1948/2105 [==========================>...] - ETA: 3:19 - accuracy: 0.9344 - loss: 0.1720\n",
      " training set -> batch:1949 loss:0.18069159984588623 and acc: 0.932758629322052\n",
      "1949/2105 [==========================>...] - ETA: 3:17 - accuracy: 0.9328 - loss: 0.1807\n",
      " training set -> batch:1950 loss:0.1840549111366272 and acc: 0.9328858852386475\n",
      "\n",
      " validation set -> batch:1950 val loss:0.2045130431652069 and val acc: 0.9243119359016418\n",
      "1950/2105 [==========================>...] - ETA: 3:17 - accuracy: 0.9329 - loss: 0.1841\n",
      " training set -> batch:1951 loss:0.20215025544166565 and acc: 0.9236725568771362\n",
      "1951/2105 [==========================>...] - ETA: 3:15 - accuracy: 0.9237 - loss: 0.2022\n",
      " training set -> batch:1952 loss:0.20075976848602295 and acc: 0.9241452813148499\n",
      "1952/2105 [==========================>...] - ETA: 3:14 - accuracy: 0.9241 - loss: 0.2008\n",
      " training set -> batch:1953 loss:0.20172560214996338 and acc: 0.9245867729187012\n",
      "1953/2105 [==========================>...] - ETA: 3:13 - accuracy: 0.9246 - loss: 0.2017\n",
      " training set -> batch:1954 loss:0.19343334436416626 and acc: 0.9269999861717224\n",
      "1954/2105 [==========================>...] - ETA: 3:11 - accuracy: 0.9270 - loss: 0.1934\n",
      " training set -> batch:1955 loss:0.18543481826782227 and acc: 0.9292635917663574\n",
      "1955/2105 [==========================>...] - ETA: 3:10 - accuracy: 0.9293 - loss: 0.1854\n",
      " training set -> batch:1956 loss:0.1870504766702652 and acc: 0.9276315569877625\n",
      "1956/2105 [==========================>...] - ETA: 3:09 - accuracy: 0.9276 - loss: 0.1871\n",
      " training set -> batch:1957 loss:0.18038833141326904 and acc: 0.92974454164505\n",
      "1957/2105 [==========================>...] - ETA: 3:07 - accuracy: 0.9297 - loss: 0.1804\n",
      " training set -> batch:1958 loss:0.18057312071323395 and acc: 0.929964542388916\n",
      "1958/2105 [==========================>...] - ETA: 3:06 - accuracy: 0.9300 - loss: 0.1806\n",
      " training set -> batch:1959 loss:0.1778089553117752 and acc: 0.931034505367279\n",
      "1959/2105 [==========================>...] - ETA: 3:05 - accuracy: 0.9310 - loss: 0.1778\n",
      " training set -> batch:1960 loss:0.17451111972332 and acc: 0.9320470094680786\n",
      "\n",
      " validation set -> batch:1960 val loss:0.2087407112121582 and val acc: 0.9220183491706848\n",
      "1960/2105 [==========================>...] - ETA: 3:04 - accuracy: 0.9320 - loss: 0.1745\n",
      " training set -> batch:1961 loss:0.21153892576694489 and acc: 0.9214601516723633\n",
      "1961/2105 [==========================>...] - ETA: 3:03 - accuracy: 0.9215 - loss: 0.2115\n",
      " training set -> batch:1962 loss:0.20285092294216156 and acc: 0.9230769276618958\n",
      "1962/2105 [==========================>...] - ETA: 3:01 - accuracy: 0.9231 - loss: 0.2029\n",
      " training set -> batch:1963 loss:0.20939823985099792 and acc: 0.9194214940071106\n",
      "1963/2105 [==========================>...] - ETA: 3:00 - accuracy: 0.9194 - loss: 0.2094\n",
      " training set -> batch:1964 loss:0.20205432176589966 and acc: 0.9210000038146973\n",
      "1964/2105 [==========================>...] - ETA: 2:59 - accuracy: 0.9210 - loss: 0.2021\n",
      " training set -> batch:1965 loss:0.20401433110237122 and acc: 0.9215116500854492\n",
      "1965/2105 [===========================>..] - ETA: 2:57 - accuracy: 0.9215 - loss: 0.2040\n",
      " training set -> batch:1966 loss:0.2061994969844818 and acc: 0.9210526347160339\n",
      "1966/2105 [===========================>..] - ETA: 2:56 - accuracy: 0.9211 - loss: 0.2062\n",
      " training set -> batch:1967 loss:0.19941581785678864 and acc: 0.9224452376365662\n",
      "1967/2105 [===========================>..] - ETA: 2:55 - accuracy: 0.9224 - loss: 0.1994\n",
      " training set -> batch:1968 loss:0.1917954385280609 and acc: 0.9246453642845154\n",
      "1968/2105 [===========================>..] - ETA: 2:53 - accuracy: 0.9246 - loss: 0.1918\n",
      " training set -> batch:1969 loss:0.19067466259002686 and acc: 0.9258620738983154\n",
      "1969/2105 [===========================>..] - ETA: 2:52 - accuracy: 0.9259 - loss: 0.1907\n",
      " training set -> batch:1970 loss:0.18708737194538116 and acc: 0.9270133972167969\n",
      "\n",
      " validation set -> batch:1970 val loss:0.28715071082115173 and val acc: 0.8979358077049255\n",
      "1970/2105 [===========================>..] - ETA: 2:51 - accuracy: 0.9270 - loss: 0.1871\n",
      " training set -> batch:1971 loss:0.28621700406074524 and acc: 0.8982300758361816\n",
      "1971/2105 [===========================>..] - ETA: 2:50 - accuracy: 0.8982 - loss: 0.2862\n",
      " training set -> batch:1972 loss:0.2805621027946472 and acc: 0.8995726704597473\n",
      "1972/2105 [===========================>..] - ETA: 2:49 - accuracy: 0.8996 - loss: 0.2806\n",
      " training set -> batch:1973 loss:0.28523996472358704 and acc: 0.9008264541625977\n",
      "1973/2105 [===========================>..] - ETA: 2:47 - accuracy: 0.9008 - loss: 0.2852\n",
      " training set -> batch:1974 loss:0.2722283601760864 and acc: 0.9039999842643738\n",
      "1974/2105 [===========================>..] - ETA: 2:46 - accuracy: 0.9040 - loss: 0.2722\n",
      " training set -> batch:1975 loss:0.2694464325904846 and acc: 0.9050387740135193\n",
      "1975/2105 [===========================>..] - ETA: 2:45 - accuracy: 0.9050 - loss: 0.2694\n",
      " training set -> batch:1976 loss:0.25944846868515015 and acc: 0.9069548845291138\n",
      "1976/2105 [===========================>..] - ETA: 2:43 - accuracy: 0.9070 - loss: 0.2594\n",
      " training set -> batch:1977 loss:0.253663569688797 and acc: 0.9087591171264648\n",
      "1977/2105 [===========================>..] - ETA: 2:42 - accuracy: 0.9088 - loss: 0.2537\n",
      " training set -> batch:1978 loss:0.2470717430114746 and acc: 0.9104610085487366\n",
      "1978/2105 [===========================>..] - ETA: 2:41 - accuracy: 0.9105 - loss: 0.2471\n",
      " training set -> batch:1979 loss:0.2516620457172394 and acc: 0.9086207151412964\n",
      "1979/2105 [===========================>..] - ETA: 2:39 - accuracy: 0.9086 - loss: 0.2517\n",
      " training set -> batch:1980 loss:0.2523428797721863 and acc: 0.9077181220054626\n",
      "\n",
      " validation set -> batch:1980 val loss:0.2230854481458664 and val acc: 0.9174311757087708\n",
      "1980/2105 [===========================>..] - ETA: 2:39 - accuracy: 0.9077 - loss: 0.2523\n",
      " training set -> batch:1981 loss:0.22175411880016327 and acc: 0.9170354008674622\n",
      "1981/2105 [===========================>..] - ETA: 2:37 - accuracy: 0.9170 - loss: 0.2218\n",
      " training set -> batch:1982 loss:0.24119502305984497 and acc: 0.9123931527137756\n",
      "1982/2105 [===========================>..] - ETA: 2:36 - accuracy: 0.9124 - loss: 0.2412\n",
      " training set -> batch:1983 loss:0.22977641224861145 and acc: 0.9152892827987671\n",
      "1983/2105 [===========================>..] - ETA: 2:35 - accuracy: 0.9153 - loss: 0.2298\n",
      " training set -> batch:1984 loss:0.22424523532390594 and acc: 0.9160000085830688\n",
      "1984/2105 [===========================>..] - ETA: 2:33 - accuracy: 0.9160 - loss: 0.2242\n",
      " training set -> batch:1985 loss:0.2230219542980194 and acc: 0.9147287011146545\n",
      "1985/2105 [===========================>..] - ETA: 2:32 - accuracy: 0.9147 - loss: 0.2230\n",
      " training set -> batch:1986 loss:0.22097790241241455 and acc: 0.9144737124443054\n",
      "1986/2105 [===========================>..] - ETA: 2:31 - accuracy: 0.9145 - loss: 0.2210\n",
      " training set -> batch:1987 loss:0.21834014356136322 and acc: 0.9142335653305054\n",
      "1987/2105 [===========================>..] - ETA: 2:29 - accuracy: 0.9142 - loss: 0.2183\n",
      " training set -> batch:1988 loss:0.21498554944992065 and acc: 0.9157801270484924\n",
      "1988/2105 [===========================>..] - ETA: 2:28 - accuracy: 0.9158 - loss: 0.2150\n",
      " training set -> batch:1989 loss:0.21417665481567383 and acc: 0.9146551489830017\n",
      "1989/2105 [===========================>..] - ETA: 2:27 - accuracy: 0.9147 - loss: 0.2142\n",
      " training set -> batch:1990 loss:0.20711274445056915 and acc: 0.916946291923523\n",
      "\n",
      " validation set -> batch:1990 val loss:0.20339028537273407 and val acc: 0.9197247624397278\n",
      "1990/2105 [===========================>..] - ETA: 2:26 - accuracy: 0.9169 - loss: 0.2071\n",
      " training set -> batch:1991 loss:0.20179671049118042 and acc: 0.9203540086746216\n",
      "1991/2105 [===========================>..] - ETA: 2:25 - accuracy: 0.9204 - loss: 0.2018\n",
      " training set -> batch:1992 loss:0.20088019967079163 and acc: 0.9209401607513428\n",
      "1992/2105 [===========================>..] - ETA: 2:23 - accuracy: 0.9209 - loss: 0.2009\n",
      " training set -> batch:1993 loss:0.1936095803976059 and acc: 0.922520637512207\n",
      "1993/2105 [===========================>..] - ETA: 2:22 - accuracy: 0.9225 - loss: 0.1936\n",
      " training set -> batch:1994 loss:0.18853801488876343 and acc: 0.9240000247955322\n",
      "1994/2105 [===========================>..] - ETA: 2:21 - accuracy: 0.9240 - loss: 0.1885\n",
      " training set -> batch:1995 loss:0.18145594000816345 and acc: 0.9263566136360168\n",
      "1995/2105 [===========================>..] - ETA: 2:19 - accuracy: 0.9264 - loss: 0.1815\n",
      " training set -> batch:1996 loss:0.1817537248134613 and acc: 0.9248120188713074\n",
      "1996/2105 [===========================>..] - ETA: 2:18 - accuracy: 0.9248 - loss: 0.1818\n",
      " training set -> batch:1997 loss:0.1860099881887436 and acc: 0.9242700934410095\n",
      "1997/2105 [===========================>..] - ETA: 2:17 - accuracy: 0.9243 - loss: 0.1860\n",
      " training set -> batch:1998 loss:0.18203683197498322 and acc: 0.9255319237709045\n",
      "1998/2105 [===========================>..] - ETA: 2:15 - accuracy: 0.9255 - loss: 0.1820\n",
      " training set -> batch:1999 loss:0.17878861725330353 and acc: 0.9267241358757019\n",
      "1999/2105 [===========================>..] - ETA: 2:14 - accuracy: 0.9267 - loss: 0.1788\n",
      " training set -> batch:2000 loss:0.17381788790225983 and acc: 0.9278523325920105\n",
      "\n",
      " validation set -> batch:2000 val loss:0.21297487616539001 and val acc: 0.9185779690742493\n",
      "2000/2105 [===========================>..] - ETA: 2:13 - accuracy: 0.9279 - loss: 0.1738\n",
      " training set -> batch:2001 loss:0.2066841870546341 and acc: 0.9192478060722351\n",
      "2001/2105 [===========================>..] - ETA: 2:12 - accuracy: 0.9192 - loss: 0.2067\n",
      " training set -> batch:2002 loss:0.19524385035037994 and acc: 0.9220085740089417\n",
      "2002/2105 [===========================>..] - ETA: 2:11 - accuracy: 0.9220 - loss: 0.1952\n",
      " training set -> batch:2003 loss:0.1880597472190857 and acc: 0.9245867729187012\n",
      "2003/2105 [===========================>..] - ETA: 2:09 - accuracy: 0.9246 - loss: 0.1881\n",
      " training set -> batch:2004 loss:0.18666070699691772 and acc: 0.925000011920929\n",
      "2004/2105 [===========================>..] - ETA: 2:08 - accuracy: 0.9250 - loss: 0.1867\n",
      " training set -> batch:2005 loss:0.1782195121049881 and acc: 0.9273256063461304\n",
      "2005/2105 [===========================>..] - ETA: 2:07 - accuracy: 0.9273 - loss: 0.1782\n",
      " training set -> batch:2006 loss:0.174427792429924 and acc: 0.9285714030265808\n",
      "2006/2105 [===========================>..] - ETA: 2:05 - accuracy: 0.9286 - loss: 0.1744\n",
      " training set -> batch:2007 loss:0.17406316101551056 and acc: 0.92974454164505\n",
      "2007/2105 [===========================>..] - ETA: 2:04 - accuracy: 0.9297 - loss: 0.1741\n",
      " training set -> batch:2008 loss:0.17517004907131195 and acc: 0.929964542388916\n",
      "2008/2105 [===========================>..] - ETA: 2:03 - accuracy: 0.9300 - loss: 0.1752\n",
      " training set -> batch:2009 loss:0.17602623999118805 and acc: 0.9301724433898926\n",
      "2009/2105 [===========================>..] - ETA: 2:01 - accuracy: 0.9302 - loss: 0.1760\n",
      " training set -> batch:2010 loss:0.17211519181728363 and acc: 0.931208074092865\n",
      "\n",
      " validation set -> batch:2010 val loss:0.24305197596549988 and val acc: 0.9071100950241089\n",
      "2010/2105 [===========================>..] - ETA: 2:00 - accuracy: 0.9312 - loss: 0.1721\n",
      " training set -> batch:2011 loss:0.2316114902496338 and acc: 0.9092920422554016\n",
      "2011/2105 [===========================>..] - ETA: 1:59 - accuracy: 0.9093 - loss: 0.2316\n",
      " training set -> batch:2012 loss:0.22042348980903625 and acc: 0.9113247990608215\n",
      "2012/2105 [===========================>..] - ETA: 1:58 - accuracy: 0.9113 - loss: 0.2204\n",
      " training set -> batch:2013 loss:0.22051149606704712 and acc: 0.9111570119857788\n",
      "2013/2105 [===========================>..] - ETA: 1:57 - accuracy: 0.9112 - loss: 0.2205\n",
      " training set -> batch:2014 loss:0.2103559374809265 and acc: 0.9139999747276306\n",
      "2014/2105 [===========================>..] - ETA: 1:55 - accuracy: 0.9140 - loss: 0.2104\n",
      " training set -> batch:2015 loss:0.20508858561515808 and acc: 0.9147287011146545\n",
      "2015/2105 [===========================>..] - ETA: 1:54 - accuracy: 0.9147 - loss: 0.2051\n",
      " training set -> batch:2016 loss:0.20430317521095276 and acc: 0.9154135584831238\n",
      "2016/2105 [===========================>..] - ETA: 1:53 - accuracy: 0.9154 - loss: 0.2043\n",
      " training set -> batch:2017 loss:0.19888761639595032 and acc: 0.9160584211349487\n",
      "2017/2105 [===========================>..] - ETA: 1:51 - accuracy: 0.9161 - loss: 0.1989\n",
      " training set -> batch:2018 loss:0.20328766107559204 and acc: 0.914893627166748\n",
      "2018/2105 [===========================>..] - ETA: 1:50 - accuracy: 0.9149 - loss: 0.2033\n",
      " training set -> batch:2019 loss:0.19792583584785461 and acc: 0.9163793325424194\n",
      "2019/2105 [===========================>..] - ETA: 1:49 - accuracy: 0.9164 - loss: 0.1979\n",
      " training set -> batch:2020 loss:0.20035399496555328 and acc: 0.9144295454025269\n",
      "\n",
      " validation set -> batch:2020 val loss:0.24368636310100555 and val acc: 0.9025229215621948\n",
      "2020/2105 [===========================>..] - ETA: 1:48 - accuracy: 0.9144 - loss: 0.2004\n",
      " training set -> batch:2021 loss:0.23860935866832733 and acc: 0.9015486836433411\n",
      "2021/2105 [===========================>..] - ETA: 1:46 - accuracy: 0.9015 - loss: 0.2386\n",
      " training set -> batch:2022 loss:0.23854660987854004 and acc: 0.9017093777656555\n",
      "2022/2105 [===========================>..] - ETA: 1:45 - accuracy: 0.9017 - loss: 0.2385\n",
      " training set -> batch:2023 loss:0.23943033814430237 and acc: 0.9028925895690918\n",
      "2023/2105 [===========================>..] - ETA: 1:44 - accuracy: 0.9029 - loss: 0.2394\n",
      " training set -> batch:2024 loss:0.24461381137371063 and acc: 0.9010000228881836\n",
      "2024/2105 [===========================>..] - ETA: 1:42 - accuracy: 0.9010 - loss: 0.2446\n",
      " training set -> batch:2025 loss:0.2389686554670334 and acc: 0.9021317958831787\n",
      "2025/2105 [===========================>..] - ETA: 1:41 - accuracy: 0.9021 - loss: 0.2390\n",
      " training set -> batch:2026 loss:0.23613934218883514 and acc: 0.9031955003738403\n",
      "2026/2105 [===========================>..] - ETA: 1:40 - accuracy: 0.9032 - loss: 0.2361\n",
      " training set -> batch:2027 loss:0.23821228742599487 and acc: 0.9023722410202026\n",
      "2027/2105 [===========================>..] - ETA: 1:39 - accuracy: 0.9024 - loss: 0.2382\n",
      " training set -> batch:2028 loss:0.23416733741760254 and acc: 0.902482271194458\n",
      "2028/2105 [===========================>..] - ETA: 1:37 - accuracy: 0.9025 - loss: 0.2342\n",
      " training set -> batch:2029 loss:0.22875599563121796 and acc: 0.9043103456497192\n",
      "2029/2105 [===========================>..] - ETA: 1:36 - accuracy: 0.9043 - loss: 0.2288\n",
      " training set -> batch:2030 loss:0.22850680351257324 and acc: 0.9052013158798218\n",
      "\n",
      " validation set -> batch:2030 val loss:0.2482450008392334 and val acc: 0.9013761281967163\n",
      "2030/2105 [===========================>..] - ETA: 1:35 - accuracy: 0.9052 - loss: 0.2285\n",
      " training set -> batch:2031 loss:0.24262790381908417 and acc: 0.9026548862457275\n",
      "2031/2105 [===========================>..] - ETA: 1:34 - accuracy: 0.9027 - loss: 0.2426\n",
      " training set -> batch:2032 loss:0.23897579312324524 and acc: 0.9027777910232544\n",
      "2032/2105 [===========================>..] - ETA: 1:32 - accuracy: 0.9028 - loss: 0.2390\n",
      " training set -> batch:2033 loss:0.24076694250106812 and acc: 0.9008264541625977\n",
      "2033/2105 [===========================>..] - ETA: 1:31 - accuracy: 0.9008 - loss: 0.2408\n",
      " training set -> batch:2034 loss:0.24094563722610474 and acc: 0.8989999890327454\n",
      "2034/2105 [===========================>..] - ETA: 1:30 - accuracy: 0.8990 - loss: 0.2409\n",
      " training set -> batch:2035 loss:0.24287880957126617 and acc: 0.8982558250427246\n",
      "2035/2105 [============================>.] - ETA: 1:28 - accuracy: 0.8983 - loss: 0.2429\n",
      " training set -> batch:2036 loss:0.2375795543193817 and acc: 0.8994361162185669\n",
      "2036/2105 [============================>.] - ETA: 1:27 - accuracy: 0.8994 - loss: 0.2376\n",
      " training set -> batch:2037 loss:0.23172356188297272 and acc: 0.9014598727226257\n",
      "2037/2105 [============================>.] - ETA: 1:26 - accuracy: 0.9015 - loss: 0.2317\n",
      " training set -> batch:2038 loss:0.22652769088745117 and acc: 0.9033687710762024\n",
      "2038/2105 [============================>.] - ETA: 1:25 - accuracy: 0.9034 - loss: 0.2265\n",
      " training set -> batch:2039 loss:0.2274933010339737 and acc: 0.9034482836723328\n",
      "2039/2105 [============================>.] - ETA: 1:23 - accuracy: 0.9034 - loss: 0.2275\n",
      " training set -> batch:2040 loss:0.22291940450668335 and acc: 0.9052013158798218\n",
      "\n",
      " validation set -> batch:2040 val loss:0.2320992797613144 and val acc: 0.9151375889778137\n",
      "2040/2105 [============================>.] - ETA: 1:22 - accuracy: 0.9052 - loss: 0.2229\n",
      " training set -> batch:2041 loss:0.23700077831745148 and acc: 0.9159291982650757\n",
      "2041/2105 [============================>.] - ETA: 1:21 - accuracy: 0.9159 - loss: 0.2370\n",
      " training set -> batch:2042 loss:0.2320905327796936 and acc: 0.9177350401878357\n",
      "2042/2105 [============================>.] - ETA: 1:20 - accuracy: 0.9177 - loss: 0.2321\n",
      " training set -> batch:2043 loss:0.22912299633026123 and acc: 0.9183884263038635\n",
      "2043/2105 [============================>.] - ETA: 1:18 - accuracy: 0.9184 - loss: 0.2291\n",
      " training set -> batch:2044 loss:0.22362841665744781 and acc: 0.9179999828338623\n",
      "2044/2105 [============================>.] - ETA: 1:17 - accuracy: 0.9180 - loss: 0.2236\n",
      " training set -> batch:2045 loss:0.22274042665958405 and acc: 0.9176356792449951\n",
      "2045/2105 [============================>.] - ETA: 1:16 - accuracy: 0.9176 - loss: 0.2227\n",
      " training set -> batch:2046 loss:0.2265826016664505 and acc: 0.9182330965995789\n",
      "2046/2105 [============================>.] - ETA: 1:14 - accuracy: 0.9182 - loss: 0.2266\n",
      " training set -> batch:2047 loss:0.21975891292095184 and acc: 0.9206204414367676\n",
      "2047/2105 [============================>.] - ETA: 1:13 - accuracy: 0.9206 - loss: 0.2198\n",
      " training set -> batch:2048 loss:0.2164960354566574 and acc: 0.9210993051528931\n",
      "2048/2105 [============================>.] - ETA: 1:12 - accuracy: 0.9211 - loss: 0.2165\n",
      " training set -> batch:2049 loss:0.2136639952659607 and acc: 0.9206896424293518\n",
      "2049/2105 [============================>.] - ETA: 1:11 - accuracy: 0.9207 - loss: 0.2137\n",
      " training set -> batch:2050 loss:0.20922689139842987 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:2050 val loss:0.20783735811710358 and val acc: 0.9128440618515015\n",
      "2050/2105 [============================>.] - ETA: 1:10 - accuracy: 0.9211 - loss: 0.2092\n",
      " training set -> batch:2051 loss:0.20151042938232422 and acc: 0.9137167930603027\n",
      "2051/2105 [============================>.] - ETA: 1:08 - accuracy: 0.9137 - loss: 0.2015\n",
      " training set -> batch:2052 loss:0.19458498060703278 and acc: 0.9155982732772827\n",
      "2052/2105 [============================>.] - ETA: 1:07 - accuracy: 0.9156 - loss: 0.1946\n",
      " training set -> batch:2053 loss:0.19379006326198578 and acc: 0.9152892827987671\n",
      "2053/2105 [============================>.] - ETA: 1:06 - accuracy: 0.9153 - loss: 0.1938\n",
      " training set -> batch:2054 loss:0.18845905363559723 and acc: 0.9169999957084656\n",
      "2054/2105 [============================>.] - ETA: 1:04 - accuracy: 0.9170 - loss: 0.1885\n",
      " training set -> batch:2055 loss:0.18354009091854095 and acc: 0.9195736646652222\n",
      "2055/2105 [============================>.] - ETA: 1:03 - accuracy: 0.9196 - loss: 0.1835\n",
      " training set -> batch:2056 loss:0.18160271644592285 and acc: 0.9201127886772156\n",
      "2056/2105 [============================>.] - ETA: 1:02 - accuracy: 0.9201 - loss: 0.1816\n",
      " training set -> batch:2057 loss:0.1811571568250656 and acc: 0.9197080135345459\n",
      "2057/2105 [============================>.] - ETA: 1:00 - accuracy: 0.9197 - loss: 0.1812\n",
      " training set -> batch:2058 loss:0.18253131210803986 and acc: 0.9202127456665039\n",
      "2058/2105 [============================>.] - ETA: 59s - accuracy: 0.9202 - loss: 0.1825 \n",
      " training set -> batch:2059 loss:0.17708411812782288 and acc: 0.9215517044067383\n",
      "2059/2105 [============================>.] - ETA: 58s - accuracy: 0.9216 - loss: 0.1771\n",
      " training set -> batch:2060 loss:0.17752224206924438 and acc: 0.9211409687995911\n",
      "\n",
      " validation set -> batch:2060 val loss:0.21418572962284088 and val acc: 0.9208715558052063\n",
      "2060/2105 [============================>.] - ETA: 57s - accuracy: 0.9211 - loss: 0.1775\n",
      " training set -> batch:2061 loss:0.20814399421215057 and acc: 0.9214601516723633\n",
      "2061/2105 [============================>.] - ETA: 56s - accuracy: 0.9215 - loss: 0.2081\n",
      " training set -> batch:2062 loss:0.20235706865787506 and acc: 0.9220085740089417\n",
      "2062/2105 [============================>.] - ETA: 54s - accuracy: 0.9220 - loss: 0.2024\n",
      " training set -> batch:2063 loss:0.19773134589195251 and acc: 0.9235537052154541\n",
      "2063/2105 [============================>.] - ETA: 53s - accuracy: 0.9236 - loss: 0.1977\n",
      " training set -> batch:2064 loss:0.19745951890945435 and acc: 0.9229999780654907\n",
      "2064/2105 [============================>.] - ETA: 52s - accuracy: 0.9230 - loss: 0.1975\n",
      " training set -> batch:2065 loss:0.19215182960033417 and acc: 0.9244186282157898\n",
      "2065/2105 [============================>.] - ETA: 50s - accuracy: 0.9244 - loss: 0.1922\n",
      " training set -> batch:2066 loss:0.18911759555339813 and acc: 0.9248120188713074\n",
      "2066/2105 [============================>.] - ETA: 49s - accuracy: 0.9248 - loss: 0.1891\n",
      " training set -> batch:2067 loss:0.1830958127975464 and acc: 0.9260948896408081\n",
      "2067/2105 [============================>.] - ETA: 48s - accuracy: 0.9261 - loss: 0.1831\n",
      " training set -> batch:2068 loss:0.17931817471981049 and acc: 0.9264184236526489\n",
      "2068/2105 [============================>.] - ETA: 46s - accuracy: 0.9264 - loss: 0.1793\n",
      " training set -> batch:2069 loss:0.17586299777030945 and acc: 0.9275861978530884\n",
      "2069/2105 [============================>.] - ETA: 45s - accuracy: 0.9276 - loss: 0.1759\n",
      " training set -> batch:2070 loss:0.17205679416656494 and acc: 0.9286912679672241\n",
      "\n",
      " validation set -> batch:2070 val loss:0.2133459597826004 and val acc: 0.9208715558052063\n",
      "2070/2105 [============================>.] - ETA: 44s - accuracy: 0.9287 - loss: 0.1721\n",
      " training set -> batch:2071 loss:0.2164025902748108 and acc: 0.9214601516723633\n",
      "2071/2105 [============================>.] - ETA: 43s - accuracy: 0.9215 - loss: 0.2164\n",
      " training set -> batch:2072 loss:0.20532290637493134 and acc: 0.9241452813148499\n",
      "2072/2105 [============================>.] - ETA: 42s - accuracy: 0.9241 - loss: 0.2053\n",
      " training set -> batch:2073 loss:0.20285838842391968 and acc: 0.9235537052154541\n",
      "2073/2105 [============================>.] - ETA: 40s - accuracy: 0.9236 - loss: 0.2029\n",
      " training set -> batch:2074 loss:0.19548046588897705 and acc: 0.9259999990463257\n",
      "2074/2105 [============================>.] - ETA: 39s - accuracy: 0.9260 - loss: 0.1955\n",
      " training set -> batch:2075 loss:0.1859205812215805 and acc: 0.9282945990562439\n",
      "2075/2105 [============================>.] - ETA: 38s - accuracy: 0.9283 - loss: 0.1859\n",
      " training set -> batch:2076 loss:0.18078307807445526 and acc: 0.9295112490653992\n",
      "2076/2105 [============================>.] - ETA: 36s - accuracy: 0.9295 - loss: 0.1808\n",
      " training set -> batch:2077 loss:0.18139243125915527 and acc: 0.92974454164505\n",
      "2077/2105 [============================>.] - ETA: 35s - accuracy: 0.9297 - loss: 0.1814\n",
      " training set -> batch:2078 loss:0.18954019248485565 and acc: 0.9290780425071716\n",
      "2078/2105 [============================>.] - ETA: 34s - accuracy: 0.9291 - loss: 0.1895\n",
      " training set -> batch:2079 loss:0.18589767813682556 and acc: 0.9293103218078613\n",
      "2079/2105 [============================>.] - ETA: 33s - accuracy: 0.9293 - loss: 0.1859\n",
      " training set -> batch:2080 loss:0.1876891404390335 and acc: 0.9295302033424377\n",
      "\n",
      " validation set -> batch:2080 val loss:0.23686322569847107 and val acc: 0.9094036817550659\n",
      "2080/2105 [============================>.] - ETA: 31s - accuracy: 0.9295 - loss: 0.1877\n",
      " training set -> batch:2081 loss:0.22508081793785095 and acc: 0.9115044474601746\n",
      "2081/2105 [============================>.] - ETA: 30s - accuracy: 0.9115 - loss: 0.2251\n",
      " training set -> batch:2082 loss:0.21557602286338806 and acc: 0.9145299196243286\n",
      "2082/2105 [============================>.] - ETA: 29s - accuracy: 0.9145 - loss: 0.2156\n",
      " training set -> batch:2083 loss:0.20767328143119812 and acc: 0.9163222908973694\n",
      "2083/2105 [============================>.] - ETA: 28s - accuracy: 0.9163 - loss: 0.2077\n",
      " training set -> batch:2084 loss:0.20087112486362457 and acc: 0.9179999828338623\n",
      "2084/2105 [============================>.] - ETA: 26s - accuracy: 0.9180 - loss: 0.2009\n",
      " training set -> batch:2085 loss:0.20057831704616547 and acc: 0.9176356792449951\n",
      "2085/2105 [============================>.] - ETA: 25s - accuracy: 0.9176 - loss: 0.2006\n",
      " training set -> batch:2086 loss:0.21085937321186066 and acc: 0.9144737124443054\n",
      "2086/2105 [============================>.] - ETA: 24s - accuracy: 0.9145 - loss: 0.2109\n",
      " training set -> batch:2087 loss:0.21430356800556183 and acc: 0.9133211970329285\n",
      "2087/2105 [============================>.] - ETA: 22s - accuracy: 0.9133 - loss: 0.2143\n",
      " training set -> batch:2088 loss:0.22005176544189453 and acc: 0.9140070676803589\n",
      "2088/2105 [============================>.] - ETA: 21s - accuracy: 0.9140 - loss: 0.2201\n",
      " training set -> batch:2089 loss:0.21605519950389862 and acc: 0.9146551489830017\n",
      "2089/2105 [============================>.] - ETA: 20s - accuracy: 0.9147 - loss: 0.2161\n",
      " training set -> batch:2090 loss:0.21561920642852783 and acc: 0.9152684807777405\n",
      "\n",
      " validation set -> batch:2090 val loss:0.22683708369731903 and val acc: 0.911697268486023\n",
      "2090/2105 [============================>.] - ETA: 19s - accuracy: 0.9153 - loss: 0.2156\n",
      " training set -> batch:2091 loss:0.2458972930908203 and acc: 0.9081858396530151\n",
      "2091/2105 [============================>.] - ETA: 17s - accuracy: 0.9082 - loss: 0.2459\n",
      " training set -> batch:2092 loss:0.23586580157279968 and acc: 0.9102563858032227\n",
      "2092/2105 [============================>.] - ETA: 16s - accuracy: 0.9103 - loss: 0.2359\n",
      " training set -> batch:2093 loss:0.23686730861663818 and acc: 0.9101239442825317\n",
      "2093/2105 [============================>.] - ETA: 15s - accuracy: 0.9101 - loss: 0.2369\n",
      " training set -> batch:2094 loss:0.22622738778591156 and acc: 0.9129999876022339\n",
      "2094/2105 [============================>.] - ETA: 14s - accuracy: 0.9130 - loss: 0.2262\n",
      " training set -> batch:2095 loss:0.22712311148643494 and acc: 0.913759708404541\n",
      "2095/2105 [============================>.] - ETA: 12s - accuracy: 0.9138 - loss: 0.2271\n",
      " training set -> batch:2096 loss:0.22201967239379883 and acc: 0.9154135584831238\n",
      "2096/2105 [============================>.] - ETA: 11s - accuracy: 0.9154 - loss: 0.2220\n",
      " training set -> batch:2097 loss:0.21687625348567963 and acc: 0.9169707894325256\n",
      "2097/2105 [============================>.] - ETA: 10s - accuracy: 0.9170 - loss: 0.2169\n",
      " training set -> batch:2098 loss:0.21021012961864471 and acc: 0.9193262457847595\n",
      "2098/2105 [============================>.] - ETA: 8s - accuracy: 0.9193 - loss: 0.2102 \n",
      " training set -> batch:2099 loss:0.20789094269275665 and acc: 0.9198275804519653\n",
      "2099/2105 [============================>.] - ETA: 7s - accuracy: 0.9198 - loss: 0.2079\n",
      " training set -> batch:2100 loss:0.20709729194641113 and acc: 0.9203020334243774\n",
      "\n",
      " validation set -> batch:2100 val loss:0.20800745487213135 and val acc: 0.9185779690742493\n",
      "2100/2105 [============================>.] - ETA: 6s - accuracy: 0.9203 - loss: 0.2071\n",
      " training set -> batch:2101 loss:0.20329615473747253 and acc: 0.9203540086746216\n",
      "2101/2105 [============================>.] - ETA: 5s - accuracy: 0.9204 - loss: 0.2033\n",
      " training set -> batch:2102 loss:0.2056524157524109 and acc: 0.9188033938407898\n",
      "2102/2105 [============================>.] - ETA: 3s - accuracy: 0.9188 - loss: 0.2057\n",
      " training set -> batch:2103 loss:0.2010239213705063 and acc: 0.9204545617103577\n",
      "2103/2105 [============================>.] - ETA: 2s - accuracy: 0.9205 - loss: 0.2010\n",
      " training set -> batch:2104 loss:0.1999823898077011 and acc: 0.9200000166893005\n",
      "2104/2105 [============================>.] - ETA: 1s - accuracy: 0.9200 - loss: 0.2000\n",
      " training set -> batch:2105 loss:0.1923435628414154 and acc: 0.92164546251297\n",
      "2105/2105 [==============================] - 2698s 1s/step - accuracy: 0.9216 - loss: 0.1923 - val_accuracy: 0.9323 - val_loss: 0.1978\n",
      "\n",
      "execution time: 0:45:17\n"
     ]
    }
   ],
   "source": [
    "# time the function\n",
    "start_time = time.time()\n",
    "\n",
    "# making the transformation here since insude model.fit it create a lot of warnings\n",
    "data_train = data_feature_extraction(train_dataset, model.name)\n",
    "data_val = data_feature_extraction(valid_dataset, model.name)\n",
    "histories_per_step = History_per_step(data_val, 10)\n",
    "\n",
    "# train the model\n",
    "history = model.fit(data_train, \n",
    "                    epochs=1, \n",
    "                    steps_per_epoch=STEP_EPOCH_TRAIN,\n",
    "                    validation_data=data_val,\n",
    "                    validation_steps=3,\n",
    "                    callbacks=[tensorboard_callback,\n",
    "                               *checkpoint_callback,\n",
    "                               histories_per_step])\n",
    "\n",
    "# print execution time\n",
    "elapsed_time_secs = time.time() - start_time\n",
    "print('\\nexecution time: {}'.format(timedelta(seconds=round(elapsed_time_secs))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:\n",
      "  - loss [training dataset]: 0.192\n",
      "  - loss [validation dataset: 0.208\n",
      "\n",
      "Accuracy:\n",
      "  - accuracy [training dataset]: 92.16%\n",
      "  - accuracy [validation dataset: 91.86%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 900x300 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.plot_acc_loss(steps_loss_train=histories_per_step.steps, loss_train=histories_per_step.losses,\n",
    "                 steps_acc_train=histories_per_step.steps, accuracy_train=histories_per_step.accuracies,\n",
    "                 steps_loss_eval=histories_per_step.val_steps, loss_eval=histories_per_step.val_losses,\n",
    "                 steps_acc_eval=histories_per_step.val_steps, accuracy_eval=histories_per_step.val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Get more information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tensorflow.python.keras.metrics.Mean object at 0x7fbac96167d0>, <tensorflow.python.keras.metrics.SparseCategoricalAccuracy object at 0x7fbac967c2d0>]\n",
      "['loss', 'accuracy']\n"
     ]
    }
   ],
   "source": [
    "print(model.metrics)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'verbose': 1, 'epochs': 1, 'steps': 2105}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['accuracy', 'loss', 'val_accuracy', 'val_loss'])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# dir(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Exploration of the model's structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model,\n",
    "                          'model.png',\n",
    "                          show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<transformers.modeling_tf_bert.TFBertMainLayer at 0x7fbbaba2a2d0>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x7fbae16fded0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x7fbac9616150>]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert [] []\n",
      "dropout_37 [] []\n",
      "classifier [] []\n"
     ]
    }
   ],
   "source": [
    "# _inbound_nodes and inbound_nodes give the same !\n",
    "# to see method available: dir(model.layers[2])\n",
    "for layer in model.layers:\n",
    "    print(layer.name, layer._inbound_nodes, layer._outbound_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Validation of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard model: tf_bert_classification\n"
     ]
    }
   ],
   "source": [
    "# get probablility for each classes\n",
    "if model.name=='custom_tf_bert_classification':\n",
    "        print('custom model: {}'.format(model.name))\n",
    "        y_pred = tf.nn.softmax(model.predict(valid_dataset))\n",
    "elif model.name=='tf_bert_classification':\n",
    "        print('standard model: {}'.format(model.name))\n",
    "        y_pred = tf.squeeze(tf.nn.softmax(model.predict(valid_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([872, 2])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# get predicted classes\n",
    "y_pred_argmax = tf.math.argmax(y_pred, axis=1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([872])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred_argmax).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Extracting true classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# extracting and flatten true classes\n",
    "y_true_tf=valid_dataset.map(pp.label_extraction).flat_map(lambda x: valid_dataset.from_tensor_slices(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "y_true=list(y_true_tf.as_numpy_iterator())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(872, 872)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_true), len(y_pred_argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Model performanes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.94      0.89      0.91       428\n",
      "    positive       0.90      0.95      0.92       444\n",
      "\n",
      "    accuracy                           0.92       872\n",
      "   macro avg       0.92      0.92      0.92       872\n",
      "weighted avg       0.92      0.92      0.92       872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred_argmax, target_names=info.features[\"label\"].names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance on the  dataset:\n",
      "   Metric             \n",
      "accuracy...........   0.9186\n",
      "recall.............   0.9459\n",
      "auc................   0.9181\n",
      "precision (p=0.5)..   0.8994\n",
      "precision (avg)....   0.8783\n",
      "precision (micro)..   0.9186\n",
      "precision (macro)..   0.9200\n",
      "f1.................    0.9221\n",
      "r2.................    0.6742\n"
     ]
    }
   ],
   "source": [
    "mm.print_metrics(y_true, y_pred_argmax, mode='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.plot_confusion_matrix(confusion_matrix(y_true, y_pred_argmax), info.features[\"label\"].names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mm.roc_curves(to_categorical(y_true), y_pred.numpy(), info.features[\"label\"].names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:env_test]",
   "language": "python",
   "name": "conda-env-env_test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
