# Multilingual text classification using embedding, bert and deep learning

## Introduction
The idea is to explore recent new development in the area of NLP (Embedding, Bert, Deep Learning)
to classify documents with different languages (english, german, french and italian).

## Code structure
Here is the code structure that was developed to do machine learning on GCP (Dataflow, AI Platform Training and Prediction/Serving).
Clearly some adaption will be needed for each use case or is other tools are used.
![alt text](./doc/img/tree.png)

## Jupyter Lab for exploration
You can use a local Jupyter Lab notebook or one created using on GCP using AI Platform Notebook.
You can also create one on GCP using some kind of automatic process. You will find more information in the [link](doc/INSTRUCTION.md).
If you use a local jupyter notebook you need GCP SDK to be installed to be able to access GCP services for training and serving.
You can also use you prefer IDE like PyCharm or Visual Studio Code.

## Python packages
For this project, we use Anaconda to create conda environment to get all the pythons packages we need
How to create conda environment abd which packages is describe in this 
[link](doc/conda_env.md).

## Code, training and serving
How to run the code to train and serve our models is decribe in this 
[link](doc/DOC.md).

## Results
Main results will be describe here

## Conclusion
Conclusion of this project will appear here.
